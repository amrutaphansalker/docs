<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../favicon.ico">
  
  <title>Reference - DataTorrent Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../menu.css" rel="stylesheet">
  <link href="../css/version-select.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reference";
    var mkdocs_page_input_path = "operator_development_ref.md";
    var mkdocs_page_url = "/operator_development_ref/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-44586211-2', 'docs.datatorrent.com');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DataTorrent Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">DataTorrent RTS</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Demos</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../demos/">Running Apps</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../sandbox/">Sandbox</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Cloud Integration</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../aws_emr_manual/">AWS</a>
                </li>
                <li class="">
                    
    <a class="" href="../azure_deployment/">Azure</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Development</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../create/">Creating Applications</a>
                </li>
                <li class="">
                    
    <a class="" href="../beginner/">Beginner's Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../demo_videos/">Videos</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Tutorials</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../tutorials/topnwords/">Top N Words</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../tutorials/salesdemo/">Sales Dimensions</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../apex_development_setup/">Apex Development Setup</a>
                </li>
                <li class="">
                    
    <a class="" href="../configure_IDE/">Generate New Project in IDE</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_development/">Applications</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_packages/">Application Packages</a>
                </li>
                <li class="">
                    
    <a class="" href="../configuration_packages/">Configuration Packages</a>
                </li>
                <li class=" current">
                    
    <span class="caption-text">Operator Development</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../operator_development/">Guide</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">Reference</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#operator-development-reference">Operator Development Reference</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#1-introduction">1: Introduction</a></li>
        
            <li><a class="toctree-l5" href="#2-operators">2: Operators</a></li>
        
            <li><a class="toctree-l5" href="#3-computation-model">3: Computation Model</a></li>
        
            <li><a class="toctree-l5" href="#4-commonly-used-operators">4: Commonly Used Operators</a></li>
        
            <li><a class="toctree-l5" href="#5-fault-tolerance">5: Fault Tolerance</a></li>
        
            <li><a class="toctree-l5" href="#6-partitioning">6: Partitioning</a></li>
        
            <li><a class="toctree-l5" href="#7-library">7: Library</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Operators</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../library_operators/">Operators List</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/drools_operator/">Drools Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/python_operator/">Python Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/PMML_operator/">PMML Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/tcpinputoperator/">TCP Input Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/eventhubinput/">Event Hub Input Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/eventhuboutput/">Event Hub Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/abstracthttpserver/">Abstract HTTP Server Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/azure_blob/">Azure Blob Storage Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/aoooperator/">Analytics Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/elasticsearch/">Elasticsearch Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/block_reader/">Block Reader</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/deduper/">Deduper</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/dimensions_computation/">Dimension Computation</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/file_output/">File Output</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/file_splitter/">File Splitter</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/hdht/">HDHT</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/kafkaInputOperator/">Kafka Input</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/snapshot_server/">Snapshot Server</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../app_data_framework/">App Data Framework</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_api/">REST API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">App Templates</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/common/import-launch/">Import and Launch App-template</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/common/customize/">Customizing an app-template</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/database-to-database-sync/">Database-to-database-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-line-copy/">HDFS-line-copy</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-part-file-copy/">HDFS-part-file-copy</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-to-hdfs-filter-transform/">HDFS-to-HDFS-filter-transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-cassandra-filter-transform/">Kafka-to-Cassandra-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-database-sync/">Kafka-to-Database-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-hdfs-filter-transform/">Kafka-to-HDFS-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-kafka-filter-transform/">Kafka-to-Kafka-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kinesis-to-redshift/">Kinesis-to-Redshift</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kinesis-to-s3/">Kinesis-to-S3</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/s3-to-hdfs-sync/">S3-to-HDFS-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/s3-to-redshift/">S3-to-HDFS-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/database-to-hdfs/">Database dump to HDFS App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/database-to-database-sync/">Database to Database Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-sync/">HDFS Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-line-copy/">HDFS to HDFS Line Copy App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-to-kafka-sync/">HDFS to Kafka Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-to-s3-sync/">HDFS to S3 Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-database-sync/">Kafka to Database Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-hdfs-filter/">Kafka to HDFS Filter App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-hdfs-sync/">Kafka to HDFS Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kinesis-to-s3/">Kinesis to S3 App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/s3-to-hdfs-sync/">S3 to HDFS Sync App</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Applications</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../omni_channel_fraud_app/">Omni Channel Fraud Prevention Application</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Services</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cep_workbench/">CEP Workbench</a>
                </li>
                <li class="">
                    
    <a class="" href="../oas_dashboards/">OAS Dashboards</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Platform</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../rts/">RTS</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_configurations/">Application Configurations</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtmanage/">dtManage</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtdashboard/">dtDashboard</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway/">dtGateway</a>
                </li>
                <li class="">
                    
    <a class="" href="../services/">Services</a>
                </li>
                <li class="">
                    
    <a class="" href="../jar_artifacts/">JAR Artifacts</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_systemalerts/">Alerts</a>
                </li>
                <li class="">
                    
    <a class="" href="../apex/">Apache Apex</a>
                </li>
                <li class="">
                    
    <a class="" href="../apex_malhar/">Apache Apex-Malhar</a>
                </li>
                <li class="">
                    
    <a class="" href="../appbackplane/">Application Backplane</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Deployment and Operations</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Licensing/">Licensing</a>
                </li>
                <li class="">
                    
    <a class="" href="../installation/">Installation</a>
                </li>
                <li class="">
                    
    <a class="" href="../configuration/">Configuration</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_security/">Security</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_systemalerts/">System Alerts</a>
                </li>
                <li class="">
                    
    <a class="" href="../apexcli/">Apex CLI</a>
                </li>
                <li class="">
                    
    <a class="" href="../troubleshooting/">Troubleshooting</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../release_notes/">Release Notes</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../glossary/">Glossary</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../additional_docs/">Resources</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DataTorrent Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Development &raquo;</li>
        
      
        
          <li>Operator Development &raquo;</li>
        
      
    
    <li>Reference</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="operator-development-reference">Operator Development Reference</h1>
<h2 id="1-introduction">1: Introduction</h2>
<p>A streaming application is a DAG that consists of computations (called
operators) and data flow (called streams). In this document we will
discuss details on how an operator works and its internals. This
document aims to enable the reader to write efficient operators and make
informed design choices.</p>
<h2 id="2-operators">2: Operators</h2>
<p>Operators are basic computation units of the application. They are
interconnected via streams to form an application. Operators are classes
that implement the Operator interface. They read from incoming streams
of tuples and write to other streams. Reading and writing to streams is
done through connection points called ports. An operator may have no
ports (if the operator is an Input Adapter and the only operator in the
DAG) and there is no limit to the number of ports an operator can have.
Operators also
have <strong>properties</strong> and <strong>attributes</strong>. Properties
customize the functional definition of the operator, while attributes
customize the operational behavior of the operator.</p>
<p>Operators are designed to be simple and easy to use and develop. Their
job is to process tuples one at a time and emit a tuple as per business
logic. Ports that read tuples from a stream are input ports as they
implement the InputPort interface. Ports that write tuples to a stream
are output ports which implement the OutputPort interface. Given
operators A and B where an input port of B is connected to an output
port of A, we say that A is an upstream operator of B and B is a
downstream operator of A. The platform ensures that tuples emitted by A
reach B. Thus the operator solely focuses on the business logic
processing. The engine, i.e. Streaming Application Master (STRAM) is
cognizant of the connection order of the operators. This connectivity
and the dataflow is an intrinsic part of engine’s decision-making. All
guarantees of data delivery upon outage is done via attributes that can
vary per application. The same operator can be reused in different
applications. This means that the same operator code can be used in
various recovery mechanisms by different applications. Two instances of
the same operator within an application can also have different recovery
mechanisms (attributes).</p>
<p>Operators can have properties that are used to customize functional
behavior of the operator. Within an application attributes can be
assigned to an operator that impact the operability of the operator.
Later sections of this chapter cover the internal details of operators.</p>
<h3 id="api">API</h3>
<p>To write your own operator you need to implement the Operator interface.
This interface provides the basic API for an operator developer. It
extends the Component interface; important parts of both interfaces are
discussed below.</p>
<h4 id="interface-component">Interface - Component</h4>
<ul>
<li><code>setup (OperatorContext context)</code> This is part of  Component. This is
    invoked as part of initialization of the operator. This is part of
    initial pre-runtime setup of the operator that is called during
    initialization time. Care should be taken to not use this method to
    set parameters. It is strongly advocated that users use setter
    functions to set parameters. The setter functions work for both the
    Java API as well as the properties file based API of an application.
    Setter functions can also be invoked during runtime via CLI. Setup
    is useful for validation checks that can only be done during
    initialization time. Setup is also very useful for initializing
    transient objects like connections with outside systems (e.g.
    sockets or database sessions). These objects should not be part of
    the state of the operator as they are in session, i.e. upon recovery
    from a node outage, the connection has to be reestablished in setup.
    All objects that are not serializable must be transient and
    initialized during setup or at the declaration site.</li>
<li><code>teardown()</code> This too is part of Component and is called as part of
    terminating the operator. Any session related objects should be shut
    down or deallocated in this callback. Graceful termination of
    outside connections (for example sockets, database sessions, etc.)
    should be done in this method</li>
</ul>
<h4 id="interface-operator">Interface Operator</h4>
<ul>
<li><code>beginWindow(long windowId)</code> Invoked at the start of a window. The
    windowId parameter identifies the window. All tuples received and
    emitted in that window belong to this windowId. A window gets
    started upon receipt of the first beginWindow tuple on any input
    port. The window computation starts at the first line of the
    beginWindow().</li>
<li><code>endWindow()</code> Invoked at the end of window. This call is only made
    after all the input ports receive an end_window tuple. All tuples
    emitted in endWindow belong to the same windowId (passed during
    beginWindow). The window computation ends with the last line in
    endWindow().</li>
</ul>
<h4 id="class-defaultinputport">Class DefaultInputPort</h4>
<ul>
<li><code>process(T tuple)</code> Invoked within an input port for every tuple
    received from the port of an upstream operator. This method is part
    of the input port interface. The schema of the tuple is same as the
    schema of the input port. This callback is an abstract call that
    must be implemented by the operator developer.</li>
</ul>
<h4 id="class-defaultoutputport">Class DefaultOutputPort</h4>
<ul>
<li><code>emit(T tuple)</code> To be called by the operator developer when a tuple
    has to be emitted on the output port. Tuples can be emitted in
    beginWindow, endWindow, or process callbacks. The schema of the
    tuple is the same as the schema of the output port.</li>
</ul>
<h4 id="interface-inputport">Interface InputPort</h4>
<ul>
<li><code>StreamCodec &lt; T &gt; getStreamCodec()</code> A stream codec serializes or
    deserializes the data that can be received on the port. If null,
    STRAM uses the generic codec.</li>
</ul>
<h4 id="interface-outputport">Interface OutputPort</h4>
<ul>
<li><code>Unifier &lt; T &gt; getUnifier()</code> When operators are partitioned via round
    robin partitioning they may need to merge the outputs of the
    partitions. For example MaxMap operator may have N partitions, each
    would emit its maximum value. The maximum value is computed by
    finding the maximum of these N values. Thus another operator is
    needed to unify these tuples. getUnifier() returns such an operator.
    Developers need to implement the process() api. The schema of the
    input tuples sent to merge() and the output port are identical to
    the schema of the output port of the operator. If the operator has
    only one input and one output port, and if both have identical
    schema, then the operator itself can act as its unifier (see MaxMap
    as an example). The default unifier is a passthrough operator and it
    just passes the tuples from each partition to the downstream
    operators.</li>
</ul>
<h4 id="interface-activationlistener">Interface ActivationListener</h4>
<p>An operator may be subjected to activate/deactivate cycle multiple times
during its lifetime which is bounded by setup/teardown method pair. So
it's advised that all the operations which need to be done right before
the first window is delivered to the operator be done during activate
and opposite be done in the deactivate.</p>
<p>An example of where one would consider implementing ActivationListener
is an input operator which wants to consume a high throughput stream.
Since there is typically at least a few hundreds of milliseconds between
the time the setup method is called and the first window, the operator
developer would want to place the code to activate the stream inside
activate instead of setup.</p>
<ul>
<li><code>activate(CONTEXT context)</code> Gets called just before the first
    beginWindow() call of the operator activation. Any processing that
    needs to be done, just before the operator starts processing tuples,
    must be done here.</li>
<li><code>deactivate()</code> Gets called just before the operator is deactivated.
    Opposite of the operations done in activate() must be done in
    deactivate().</li>
</ul>
<h4 id="interface-checkpointlistener">Interface CheckpointListener</h4>
<p>Operators which need to be notified as soon as they are checkpointed or
committed, must implement this interface.</p>
<ul>
<li><code>checkpointed(long windowId)</code> - Called when the operator is
    checkpointed. The windowId parameter contains the window id after
    which the operator was checkpointed.</li>
<li><code>committed(long windowId)</code> - Called when the operator is committed. A
    commit operation is performed when all the operators in the DAG have
    successfully checkpointed a particular window id. This window id is
    passed as a parameter to the call back.</li>
</ul>
<h4 id="interface-idletimehandeler">Interface IdleTimeHandeler</h4>
<p>An operator must implement this interface if it is interested in being
notified when it's idling. An operator can be said to be idling when</p>
<ol>
<li>An operator which is an Input Adaptor, is not emitting any tuple or</li>
<li>
<p>A generic operator or an Output Adapter is not processing any inputs</p>
</li>
<li>
<p><code>handleIdleTime()</code> - When the operator is idling, it is explicitly
    notified of such a state. The operators which implement this
    interface should make use of this idle time to do any auxiliary
    processing they may want to do when operator is idling. If the
    operator has no need to do such auxiliary processing, they should
    not implement this interface. In such a case, the engine will put
    the operator in scaled back processing mode to better utilize CPU.
    It resumes its normal processing as soon as it detects tuples being
    received or generated. If this interface is implemented, care should
    be taken to ensure that it will not result in busy loop because the
    engine keeps calling handleIdleTime until it does not have tuples
    which it can give to the operator.</p>
</li>
</ol>
<p>All the above callbacks happen in a single dedicated thread, i.e. at any
one point of time only one of these callbacks is being executed. No
thread locking issues exist when writing code that process tuples.
Tuples are queued in a buffer and await their turn for process() calls
on their respective ports. Tuples are always in-order within a stream,
i.e. an input port receives them in the same order that they were
emitted. If an operator has two input ports then the order between
tuples on two different streams is not guaranteed.</p>
<p>Figure 1 shows an operator with two ports. It has a guarantee that
process() method on input port i1 will be invoked for tuples in the
following order: t11, t12, t13, t14; and process() method on input port
i2 will be invoked for tuples in the following order: t21, t22, t23,
t24, t25. But whether i2::process( t21) would happen before
i1:process(t11) is entirely dependent on when they arrive. Users are
strongly advised to not depend on the order of tuples in two different
streams as in a distributed application this order cannot be guaranteed.
Such operators would not be idempotent.</p>
<p>Tuples always belong to a specific window (identified by its window id).
All tuples emitted by the upstream operator, whether in beginWindow(),
endWindow(), or process(), are part of the same window and hence each of
these tuples would invoke process() on the input port of a downstream
operator in the same window. Output ports need not be connected for
tuples to be emitted. This leniency allows STRAM to monitor throughput
on an unconnected port, and information that is useful for future
changes, including dynamic modification of the DAG. For more details and
specific interfaces please refer to their API classes.</p>
<p><img alt="" src="../images/operator_development/image00.png" /></p>
<h3 id="ports">Ports</h3>
<p>Ports are connection points of an operator and are transient objects
declared in the Operator class. Tuples flow in and out through these
ports. Input ports read from the stream while output port write to one.
Input ports are implementation of the InputPort interface, while output
ports are implementations of the OutputPort interface. An output port
can be tagged as an error port by placing an annotation on it (error =
true). Later in this section we would discuss the implications of
tagging an output port as an error port. Ports have schemas as part of
their declaration. The schema of a port is useful for compile time error
checking as well as runtime checks required for dynamically inserting
operators at runtime.</p>
<p>We classify the operators in 3 buckets by observing the number of input
and output ports.</p>
<ul>
<li><strong>Input Adapters (Having no input ports)</strong>: The operator with no input
    ports is called an Input Adapter. (An Adapter is a special term used
    to denote an operator which interacts with the systems external to
    the DAG. We’ll use the terms Input Adapter and Input Operator
    interchangeably.) An Input Adapter is useful for reading data from a
    source external to the DAG. Some examples of external data sources
    are HDFS, HBase, Sockets, Message Busses, Cassandra, MySql, Oracle,
    Redis, Memcache, HTTP (GET), RSS feed, etc. It ingests the data from
    one or more external sources, creates a stream, introduces control
    tuples in the stream and emits them via its output ports. The
    streams at the output ports of the Input Operator are ready to be
    consumed by downstream operators. The Apache Apex Malhar library
    implements quite a few Input Operators named after the data sources
    they interface with. Input adapters have a window generator that
    generates begin_window and end_window tuples periodically. Since
    the STRAM initializes all window generators, they all start with
    same window id and the application is synchronized on these
    windows.</li>
</ul>
<p>Input Operators have two modes, namely Synchronous and Asynchronous</p>
<ul>
<li><strong>Synchronous (Pull mode)</strong>: Examples of synchronous data intake is an
    HDFS file reader. The operator can thus decide the intake rate, i.e.
    "how much data to read per window?". This operator can be
    implemented with a single thread. Scaling such an operator is
    determinable at static/compile time.</li>
<li>
<p><strong>Asynchronous (Push mode)</strong>: Examples of asynchronous data is typical
    message busses, where data is pushed to the reader (the Input
    Operator). In such a scenario, the operator does not know the
    inbound rate and hence is asynchronous with respect to the platform
    windows in nature. Asynchronous input operators create another
    thread to receive the incoming data. The basic operator task of
    taking the incoming data and converting to/emitting a tuple is done
    by the main thread of the operator. A connection buffer is provided
    to ensure that the two threads communicate effectively. Users are
    advised to pay particular attention to the inbound rate and design
    an asynchronous operator carefully.</p>
</li>
<li>
<p><strong>Output Adapter (Having no output port)</strong>: The operator having no
    output ports is called an Output Adapter. An Output Operator is
    useful for handing over the data to the systems outside of the DAG.
    Typically output operators remove the control tuples from the
    stream, transform it, and write the resulting data to external
    sources like HDFS, HBASE, Sockets, Message Busses, Cassandra, MySql,
    Oracle, HTTP Server (POST), and others. The Apache Apex Malhar
    library has a few Output Operators also available ready for use in a
    DAG.</p>
</li>
<li>
<p><strong>Generic Operator (Having both input and output ports)</strong>:  These are
    the most generic operators. They use one or more input ports to read
    the input tuples and use one or more output ports to emit the
    computed tuples. Unifiers are examples of generic operators where
    process() is used for all input ports instead of process() per input
    port.</p>
</li>
<li><strong>Singleton Operator (Having no input or output ports)</strong>: These
    operators are self-sufficient in implementing the business logic and
    have no input or output ports. An application with such an operator
    will not have any other operator in the DAG. Such an operator
    effectively acts as both an Input Adapter as well as an Output
    Adapter.</li>
</ul>
<p>Both output adapters and generic operators process tuples on the input
port and look alike, the difference is that a general operator emits
tuples while output adapter has to ensure tight integration with outside
sources. As far as the interface is concerned, both have to implement
the
Operator<a href="https://www.google.com/url?q=http://docs.google.com/apidocs/com/datatorrent/api/Operator.html&amp;sa=D&amp;usg=AFQjCNGqoCKvez6sQaf8ALriKfbxtwMbRw"> </a>interface.
Both input adapters and generic operators emit tuples, but input
adapters generate the streams by talking to external systems and at
times need two threads. Input operators do not have a process API, and
implement the InputOperator interface. Only input adapters have window
generators which insert begin_window and end_window events (control
tuples) in the stream. The details of window generators are discussed in
a later Chapter.</p>
<p>Ports have to be transient objects within the Operator as they should
not be part of a checkpointed state. The importance of the transient
keyword is explained in the section on checkpointing. Ports are objects
in the operator class that the platform recognizes. All objects that
implement the interface InputPort and OutputPort make this list. A
special kind of output port is also supported, namely a port for
emitting error tuples. Though this port is in essence an output port,
i.e. it writes tuples to a stream, it is identified as error port via an
annotation (error = true). The STRAM can then leverage this information
for better reporting, statistics, tools (for example UI), monitoring
etc. The port need not be connected for the STRAM to gather information
on it, as emitted tuples can be counted even if no stream is connected
to it.</p>
<h3 id="examples">Examples</h3>
<h4 id="input-port">Input Port</h4>
<p>An example of an input port implementing the abstract class
DefaultInputPort is</p>
<pre><code class="java">
@InputPortFieldAnnotation(name = &quot;data&quot;)
public final transient DefaultInputPort &lt;KeyValPair &lt; K,V &gt; &gt; data = new DefaultInputPort &lt;KeyValPair &lt; K,V &gt; &gt;(this); {
  @Override&lt;/p&gt;
  public void process(KeyValPair &lt; K,V &gt; tuple) {
    // code for processing the tuple&lt;/p&gt;
  }
};

</code></pre>

<p>process() is an abstract method and must be provided by the developer of
the operator. This method is responsible for implementing the processing
logic for the incoming tuple.</p>
<h4 id="output-port">Output Port</h4>
<p>Here is an example of an output port implementing the abstract class
DefaultOutputPort:</p>
<pre><code class="java">
@OutputPortFieldAnnotation(name = &quot;sum&quot;, optional = true)
public final transient DefaultOutputPort &lt; HashMap &lt; K, V &gt; &gt; sum = new DefaultOutputPort &lt; HashMap &lt; K, V &gt; &gt;(this) {
  @Override
  public Unifier &lt; HashMap &lt; K, V &gt; &gt; getUnifier()
  { // The unifier is also an operator, that is initialized in the container of the downstream operator&lt;/p&gt;
    return new UnifierHashMapSumKeys &lt; K, V &gt;();
  }
;
</code></pre>

<h4 id="error-port">Error Port</h4>
<p>An example of an error port is:</p>
<pre><code class="java">
@OutputPortFieldAnnotation(name = &quot;sum&quot;, optional = true, error=true)
public final transient DefaultOutputPort &lt;KeyValPair &lt;String,Integer &gt; &gt; monitor = new DefaultOutputPort &lt; KeyValPair &lt; String,Integer &gt; &gt;(this);

</code></pre>

<h3 id="unifiers">Unifiers</h3>
<p>An operator that uses round robin partitioning may need a unifier to
merge data back. An example of this is a sum operator that takes a
HashMap as its input tuple. In the above example of an output port, the
key-value pairs in the HashMap consist of keys and their values that are
to be added. If this operator is partitioned in N ways, each partition
would emit its own sum for a key. So the unifier (merge operator) needs
to unify them and compute the final result per key as sum of all the
values from every partition for that key. An operator that uses a sticky
key partition does not need to provide its own merge as the default
merge put in by the platform is a passthrough merge (i.e. just emits the
incoming tuple on the output port).</p>
<h3 id="port-declarations">Port Declarations</h3>
<p>Annotations should be added to the ports, as these are very useful in
validating them. Annotations are declared with
<code>@InputPortFieldAnnotation()</code> for input ports and
<code>@OutputPortFieldAnnotation()</code> for output ports. Currently the following
annotations are supported on the ports</p>
<ul>
<li><code>name = “portname”</code></li>
<li><code>optional = true (or false)</code></li>
<li><code>error = true (or false)</code></li>
</ul>
<p>Over time mode annotations would be added as per needs of our customers.</p>
<p>The schema of the tuple emitted on a port is part of the port
declaration and thus is defined as compile time. These types may be
defined using Java generics. The schema declaration on the ports allow
compile time validation when an output port of an operator is connected
to input port of another operator. On IDEs like Eclipse or NetBeans,
this check is done as you type. The application creator thus experiences
productivity gains by leveraging type safety alerts generated by the
IDEs. The port schema also enables the STRAM to do schema validation
before run time addition of an operator. The added sub-query can do a
check (instanceof) before accepting a tuple.</p>
<h3 id="operator-properties">Operator Properties</h3>
<p>Operator can have properties which are used to customize the
functionality of the operator. Operator properties should be accessed
using standard Java beans convention (read via getter method, and write
via setter method) so that values can be injected and validated as
per<a href="https://www.google.com/url?q=http://docs.oracle.com/javaee/6/tutorial/doc/gircz.html&amp;sa=D&amp;usg=AFQjCNE_i2aABJVp1qFtSWyXRxJ-91ETXw"> </a><a href="https://www.google.com/url?q=http://docs.oracle.com/javaee/6/tutorial/doc/gircz.html&amp;sa=D&amp;usg=AFQjCNE_i2aABJVp1qFtSWyXRxJ-91ETXw">javax
annotation</a> .
Through properties is a broad definition of anything instantiated inside
an operator, as a generic definition we count variables of the object
that can be read and written to by the user via getter and setter
methods.</p>
<p>The DAG (including all the operators) is initialized on the client side
and then passed to the STRAM. This means that the setter method is not
called during initialization in the container. So during node recovery
the checkpointed state needs to have these values. In order for the
properties to be saved in the checkpoint, they cannot be transient. This
is irrespective of whether they are set only during initialization and
would have same values in every checkpoint. The platform enables runtime
changes to property values via setter functions. If a setter function is
called during runtime, STRAM would call the setter function after
checkpoint is loaded, so runtime changes are remembered irrespective of
whether the checkpoint happened between property set and operator
outage.</p>
<h3 id="operator-attributes">Operator Attributes</h3>
<p>Operators can have attributes. Attributes are provided by the platform
and do not belong to user code. Attributes do not change the basic
functionality of an operator, and most often the operator code would be
oblivious to these attributes. The platform recognizes these and takes
decisions (compile time or run time) that mainly impacts the performance
of an operator. A very common example of an attribute is
APPLICATION_WINDOW_COUNT. This attribute is by default equal to the
streaming window (count = 1), and an user can set this on any operator.
Attributes also exist at the application level and on a stream. If an
operator can only work with certain values of an attribute, they can be
accessed during setup call on an operator. During development of the
operator the values of these attributes could be used to provide better
operability for the operator. Details of attributes are covered in
<a href="../configuration/">Configuration</a>.</p>
<h3 id="templates">Templates</h3>
<p>Since operators are java classes they can be templatized (generic types)
for reuse. The streaming platform leverages all features in Java for
type safety and parameterization. Templates have no performance impact
as they exist only at compile time. Templates greatly reduce the number
of operators that need to be maintained, and enable operators for lot
more reuse. Apache Apex Malhar Library Templates are examples of
operator templates. In general, it is advisable to templatize operators
wherever possible. This allows the core computation to be used for
derived types. For example Sum operator can be used for any tuple type
that
extends Number.
In general an operator should set the schema of the tuple to the minimum
required for its functionality. Apache Apex Malhar Library package
sample code has examples of a lot of such operators.</p>
<h3 id="validations">Validations</h3>
<p>The platform provides various ways of validating application
specifications and data input. Validation of an application is done in
three phases, namely:</p>
<ol>
<li><strong>Compile Time</strong>: Caught during application development, and is most
    cost effective. These checks are mainly done on declarative objects
    and leverage the Java compiler. For example "schema of two ports on
    the same stream should match".</li>
<li><strong>Initialization Time</strong>: When the application is being initialized,
    before submitting to Hadoop. These checks are related to
    configuration/context of an application, and are done by the logical
    DAG builder implementation. For example, check that "a particular
    port must be connected"</li>
<li><strong>Runtime</strong>: Validations done when the application is running. This is
    the costliest of all checks. These are checks that can only be done
    at runtime as they involve data integrity. For example divide by 0
    check, or negative age value.</li>
</ol>
<p>Operator properties and ports are validated at compile time (IDE) as
well as launch time (STRAM). Java annotations are used extensively to
check validations such as ranges for the properties, string formatting,
etc. For example, a port, by default has to be connected. This check is
done during compile time in your IDE, as well as during launch time
(CLI->STRAM). If you have an port whose connectivity is optional, it
needs to be mentioned via annotation given below:</p>
<pre><code class="java">
@InputPortFieldAnnotation(name = &quot;data&quot;, optional=true)
public transient DefaultInputPort data = new DefaultInputPort(this) { }

</code></pre>

<h3 id="compile-time-validations">Compile Time Validations</h3>
<p>Compile time validations apply when an application is specified in Java
code and include all checks that can be done by Java compiler in the
development environment (including IDEs like NetBeans or Eclipse).</p>
<p>Examples include the following</p>
<ol>
<li><strong>Schema Validation</strong>: The tuples on ports are POJO (plain old java
    objects) and java compiler checks to ensure that all the ports on a
    stream have the same schema. This check works in any IDE as well as
    with a command line java compiler.</li>
<li><strong>Stream Check</strong>: Single Output port and at least one Input port per
    stream. A stream can only have one output port writer. This is part
    of the addStream API. This check ensures that developers only
    connect one output port to a stream. The same signature also ensures
    that there is at least one input port for a stream</li>
<li><strong>Naming</strong>: Compile time checks ensures that applications components
    like operators and streams are named</li>
</ol>
<p>We will continue to add as many checks as possible in compile time. But
the Java compiler cannot do 100% of the checks, and validations have to
be done during initialization time. Further, for the errors that can
happen only during run time, run time checks are supported.</p>
<h3 id="initializationinstantiation-time">Initialization/Instantiation Time</h3>
<p>Initialization time validates include various checks that are done post
compile, and before the application starts running in a cluster (or
local mode). These are mainly configuration/contextual in nature. This
checks are as critical to proper functionality of the application as the
compile time validations.</p>
<p>Examples include the following</p>
<ul>
<li><a href="https://www.google.com/url?q=http://docs.oracle.com/javaee/6/tutorial/doc/gircz.html&amp;sa=D&amp;usg=AFQjCNE_i2aABJVp1qFtSWyXRxJ-91ETXw">JavaBeans
    Validation</a></li>
</ul>
<p>The most common checks for properties are listed below</p>
<p><a href="#"></a><a href="#"></a></p>
<table>
<col width="33%" />
<col width="33%" />
<col width="33%" />
<tbody>
<tr class="odd">
<td align="left"><p>Constraint</p></td>
<td align="left"><p>Description</p></td>
<td align="left"><p>Example</p></td>
</tr>
<tr class="even">
<td align="left"><p>@Max</p></td>
<td align="left"><p>The value of the field or property must be an integer value lower than or equal to the number in the value element</p></td>
<td align="left"><p>@Max(100)</p>
<p>int success_ratio;</p></td>
</tr>
<tr class="odd">
<td align="left"><p>@Min</p></td>
<td align="left"><p>The value of the field or property must be an integer value greater than or equal to the number in the value element</p></td>
<td align="left"><p>@Min(0)</p>
<p>int mass;</p></td>
</tr>
<tr class="even">
<td align="left"><p>@NotNull</p></td>
<td align="left"><p>The value of the field or property must not be null</p></td>
<td align="left"><p>@NotNull</p>
<p>String username;</p></td>
</tr>
<tr class="odd">
<td align="left"><p>@Pattern</p></td>
<td align="left"><p>The value of the field or property must match the regular expression defined in the regexp element</p></td>
<td align="left"><p>@Pattern(regexp = &quot;lte|lt|eq|ne|gt|gte&quot;, message = &quot;Value has to be one of lte, lt, eq, ne, gt, gte&quot;)</p>
<p>String cmp;</p></td>
</tr>
</tbody>
</table>

<ul>
<li><strong>Input port connectivity</strong>: An input port must be connected, unless
    optional = true. For example:</li>
</ul>
<p><code>@InputPortFieldAnnotation(name = "data", optional = true)</code></p>
<ul>
<li><strong>Output port connectivity</strong>: At least one output port must be
    connected, unless optional = true.</li>
</ul>
<p><code>@OutputPortFieldAnnotation(name = "result", optional = true)</code></p>
<ul>
<li>
<p><strong>Unique names for operators and streams</strong>: Check is done to ensure that
    operators and streams are named uniquely within the application.</p>
</li>
<li>
<p><strong>Cycles in the dag</strong>: Check is done to ensure that the application is a
    DAG and no cycle exists.</p>
</li>
<li><strong>Port annotations with the same name</strong>: Checked to ensure that the port
    annotations are one per port</li>
<li><strong>One stream per port</strong>: A port can connect to only one stream. This
    check is very clear for an input port as by definition only one
    stream can input to a port. This check is also done for an output
    port, even though an output port can technically write to two
    streams. This is done as the message queue infrastructure does a
    much better job and allows STRAM to better do run time
    optimizations. If you must have two streams originating from a
    single output port, use a StreamDuplicator  operator.</li>
</ul>
<h3 id="runtime-validations">Runtime Validations</h3>
<p>Run time checks are those that are done when the application is running.
The<a href="https://www.google.com/url?q=http://docs.google.com/RealTimeStreamingPlatform.html&amp;sa=D&amp;usg=AFQjCNEYYLIb0bnIWPvAaNxtIoN3K8BM1A"> </a>Apache
Apex platform provides rich run time error handling mechanisms. These
checks also include those done by application business logic. In these
cases the platform allows applications to count and audit these.</p>
<p>Examples include the following:</p>
<ul>
<li><strong>Error Tuples</strong>: Applications can log error tuples during computation.
    These are counted on a per window basis and are part of application
    statistics. This allows for an automated way to integrate error
    catching into a monitoring system. These tuples can be written into
    a file. The default storage engine is HDFS. This allows application
    developers to debug and analyse data.</li>
<li><strong>Test Framework</strong>: The ability to test the application at run time
    includes the need to verify the functionality against a previous
    run. Users can do that via a test framework that allows an
    application to process parallel sets of streams, get the
    computations on a selected list of output ports and compare the
    results. The test would be user driven, would verify that the atomic
    micro-batches generated are identical and certify an application.
    This test allows users to validate any change to the application
    operator logic in run time.</li>
<li><strong>Error Ports</strong>: Ports that emit error tuples are supported by the
    <code>@OutputPortFieldAnnotation</code>. This means that users have to create
    these ports just like any other ports with an annotation
    <code>@OutputPortFieldAnnotation (error = true)</code>. The platform can then
    support a “number of error tuples” computation just by counting the
    errors emitted on this port and aid in persistence.</li>
<li><strong>Bucket Testing</strong>: When any new application logic needs to be tried
    out, it is useful to first try it on a very small sample of a
    stream. The concept of bucket testing will be available in a future
    release. In this mode, the incoming streams (input adapters) would
    have a Sampler operator inserted on appropriate streams in the
    application. Users would thus be able to test a modification to the
    application on a small subset of data. This feature will be
    available in a later release.</li>
<li><strong>Unifier Output Port</strong>: A unifier has to have only one output port as
    the platform needs to know which port to connect the input port of
    the downstream operator to. This is achieved by checking that
    unifier operator has only one output port.</li>
</ul>
<p>Regarding the stream schema and the ability to validate dynamic DAG
modifications, the engine has only partial knowledge of the port types
since type information may be erased by the Java compiler and the exact
type used to instantiate a parameterized operator may not be known
(HashMap, ArrayList, Object, etc.). Many validations are performed
before an application is fully launched (compile time, startup). There
are cases that need runtime checks, including connectivity, error
tuples, operator uptime, container uptime, and others. Run time schema
checks can still be performed using the instanceof() function. Object
oriented design and usage of base class APIs is very critical to enable
run time schema checks. If the port of the operator to be inserted on
the stream only accesses the base class API, then run time insertion can
be done via usage of the instanceof() function. In later versions,
stronger support would be provided by the platform for run time check
for dynamic insertion of objects.</p>
<h3 id="transient-fields">Transient Fields</h3>
<p>During the application lifecycle, operators are serialized as part of
checkpointing in the distributed environment. The platform uses Kryo for
serialization. If a field is marked "transient" it will not be
serialized. A number of guidelines need to be followed to avoid error or
inefficiencies due to serialization:</p>
<ul>
<li>Port objects need to be declared as public static final. The port
    objects are non-static inner classes that cannot be deserialized due
    to absence of a default constructor. There is no need to serialize
    port objects as port objects are stateless. State can be maintained
    by the enclosing operator instance.</li>
<li>Use non-transient fields for properties that configure/customize the
    operator (and are set at initialization time). These properties are
    required when operator is setup in the executing container and need
    to be serialized. You can allow runtime changes to properties via
    setter functions. This means that such objects/properties must be
    non-transient as you now rely on the set value being serialized.</li>
<li>Use non-transient for objects that are required across window
    boundaries. This will ensure that the state is checkpointed and
    recovered when the operator is transferred between containers as
    result of rescheduling or an outage. An operator whose functionality
    depends on tuples from previous windows must have those objects as
    non-transient.</li>
<li>Use transient objects for state of the operator in the following two
    cases:</li>
<li>If the computation and resultant output from an incoming tuple does
    not depend on any previous tuples. This means that the operator is
    completely stateless. For such an operator, all serializable objects
    should be declared as transient to reduce the size of the checkpoint
    state.</li>
<li>If the computation and resultant output from an incoming tuple only
    depends on other tuples in the same window. Such an operator can
    technically be made stateless. The down side is that you would need
    to ensure that application window is always equal to the streaming
    window. If the application window is not equal (i.e. more than) the
    streaming window, then the checkpoint will still happen on streaming
    window boundaries, and therefore the state would include the state
    of the operator within a window. This is because once the
    application developer sets an application window, the endWindow call
    is skipped until the application window boundary. As an operator
    developer you can force this operator to be stateless by checking
    for the application window value during the setup call.</li>
</ul>
<p>To force the checkpoint of such operators to align with the application
window boundary set the attribute “CHECKPOINT_WINDOW_COUNT” to
“APPLICATION_WINDOW_COUNT”. This will ensure more efficient execution
as it avoids unnecessary serialization. Currently, by default
checkpointing happens at the checkpoint period, and that is most likely
more frequent than an application window. For long application windows
it is more efficient to checkpoint more often and thereby avoid a very
long replay queue. For such an operator the parameters including those
that only exist within a window are part of the state of the operator as
the checkpoint would happen on an intermediate streaming window
boundary. As an operator developer, if you are using transient objects
for state within a window it is very critical that you ensure that the
Application window is equal to streaming window. This can be done either
during the setup call, or you can set the checkpointing window count to
application window count.</p>
<h3 id="stateless-vs-stateful">Stateless vs Stateful</h3>
<p>The platform intends to discern stateless vs. stateful without direct
specification from the developer. This depends on declaring objects as
transient. Therefore, care should be taken to ensure that objects that
may form state of the operator are not declared as transient. This
aspect is a critical part of the operator design.</p>
<p>The state of an operator is defined as all the non-transient fields of
the operator. To exclude a variable from being included in the state,
the developer must declare that variable as transient. Since the
computing model of the platform is to treat windows as atomic
micro-batches, the operator state is checkpointed after an endWindow and
before the next beginWindow event. In a future version, we will be
adding an attribute that would allow an operator developer to force the
checkpoint to align with the application window boundary.</p>
<p>Checkpointing is a process of serializing the operator object to disk
(HDFS). It is a costly procedure and it blocks computations. To avoid
this cost checkpointing is done every Nth window, or every T time
period, where T is significantly greater than the streaming window
period. A stateless operator (all variables are transient) can recover
much quicker than a stateful one and pay a far lower checkpointing
penalty. In the future, the platform may interpret an operator as
stateless and remove the checkpointing penalty. The needed windows are
kept by the upstream buffer server and are used to recompute the lost
windows, and also rebuild the buffer server in the current container. A
passthrough operator is an example of a stateless operator. For example,
an operator that takes a line, splits it into key/val pairs, and sends
one HashMap\&lt;key,val> per line is stateless as the outbound tuple is
derived solely from the in-bound tuple.</p>
<p>The Stateless vs. Stateful distinction of an operator does not impact
the core platform engine, i.e. the platform does not distinguish between
these. However it is a very good distinction to learn for those who want
to write operators. The data that has to be transferred to the next
window must be stored in non-transient objects, as a node recovery
between these two windows must restore its state. If the outbound tuple
depends on tuples before the current tuple, then the operator has a
state. These objects must be non-transient. The state of an operator is
independent of the number of ports (ports are transient objects).</p>
<p>A stateless operator is defined as one where no data is needed to be
kept at the end of any API call (e.g. beginWindow, process, or
endWindow). In other words, all variables are transient and one where
the outbound tuple solely depends on the current in-bound tuple. Such an
operator is completely stateless. Another stateless operator is one
where all the computations of a window can be derived from all the
tuples the operator receives within that window. This guarantees that
the output of any window can be reconstructed by simply replaying the
tuples that arrived in that window. But for such an operator the
operator developer needs a mechanism to ensure that no checkpointing
must be done within an application window. The downside of such an
operator is that if it is used for a very large application window, e.g.
a few hours, then the upstream buffer server must maintain that many
tuples. Stateless operators have much more efficient recovery, however
operator developers must take into account the cost of maintaining the
buffers of an application window in the upstream bufferserver.</p>
<p>A stateful operator is defined as one where data needs to be stored at
the end of a window for computations, i.e some variables are
non-transient. Stateful operators are also those where outbound tuples
depends on more than one incoming tuple (for example aggregates), and
the operator developer has allowed checkpointing within an application
window. Stateful operators have costlier recovery as compared to
stateless operators.</p>
<p>If a container object has to be cleared after every window, it is better
to clear it in endWindow as compared to beginWindow. Since checkpointing
is done after endWindow, in cases where the checkpointing is done only
after application window or streaming window, this object is empty. If
the operator developer is not sure about application developer not
asking for checkpointing within an application window, clearing
container objects in endWindow is more efficient as the object in most
cases does not become part of the checkpoint state.</p>
<h3 id="single-vs-multiple-inputs">Single vs Multiple Inputs</h3>
<p>A single-input operator by definition has a single upstream operator,
since there can only be one writing port for a stream.  If an operator
has a single upstream operator, then the beginWindow on the upstream
also blocks the beginWindow of the single-input operator. For a window
to start processing on any operator at least one upstream operator has
to start processing that window. The platform supports “at least one
upstream operator should start processing” model to allow processing to
start as soon as possible. For a single input operator this is very
efficient as setting up internal objects for processing can be done in
parallel and before the first tuple arrives.</p>
<p>A multi-input operator can have more than one upstream operator. Each
input port has an upstream operator. In some cases all the input ports
may be connected to output ports of the same upstream operator. In
either case the multi-input operator will not close a window until all
the upstream operators close this window. Thus the closing of a window
is a blocking event. A multi-input operator is also the point in the DAG
where windows of all upstream operators are synchronized. The windows
(atomic micro-batches) from a faster (or just ahead in processing)
upstream operators are queued up until the slower upstream operator
catches up. The STRAM monitors and guarantees these conditions. These
may occur dynamically due to changes in throughputs on various streams,
caused by internal or external events.</p>
<h3 id="hierarchical-operators">Hierarchical Operators</h3>
<p>Hierarchical operators are those whose functional logic it itself a DAG.
The difference between an application and a hierarchical operator is
that the later has ports and thus can be inserted in other applications.
Hierarchical operators are very useful for reuse, and enforcing common
design practices. The development for hierarchical operator is underway
and will be available in a future version.</p>
<h3 id="macros">Macros</h3>
<p>Macros are sets of instructions that run via the CLI to insert a
sub-query (sub-DAG) into the application. They have a similar result as
the hierarchical operators, except they are executed at run time. In the
future when the CLI supports application creation, macros can be used
during application creation time. Macros would still differ from a
hierarchical operators as the operator would have a scope that the macro
may not.</p>
<h2 id="3-computation-model">3: Computation Model</h2>
<p>In this section we discuss details of the computation model of an
operator. It is very important for an operator developer to understand
the nuances of the operator computational model to be able to leverage
all the rich features provided by the platform.</p>
<h3 id="single-dedicated-thread-execution">Single Dedicated Thread Execution</h3>
<p>All code of an operator always executes in a single dedicated thread
within a Hadoop container. This is a design principle of the platform,
and that ensures that all the calls on the operator are invoked in the
same thread. This frees the user from synchronization considerations in
the operator logic. This makes coding very easy for the operator, as
only one tuple is processed at any given time and no locking has to be
done. Arrival of two tuples on the same stream is not an issue as they
always arrive in order. However, for operators that process multiple
input streams, the platform serializes (de-parallelizes) the multiple
parallel streams into one so that the processing of individual tuples
does not overlap with that of another tuple on the same stream or any
other stream processed by the operator. Since the streams are coming
from different sources in a distributed architecture, there is no
guarantee when tuples in two different streams would arrive with respect
to each other. Thus the order of tuples from two different streams is
random and cannot be guaranteed. The only guarantee is that all tuples
of the streams that the operator listens to that belong to a window id
would arrive in that window. Operator logic should thus be written in
such a way that it does not depend on the order of tuples from two
different streams. An operator that has such a dependency is not
idempotent. The serialization of streams also ensures that all the
streams are synchronized on window boundaries. The management of
in-flowing tuples is handled by the platform to allow the operator code
to run in a single thread execution, and frees the developer to focus
solely on the business logic.</p>
<h3 id="mutability-of-tuples">Mutability of tuples</h3>
<p>Tuples emitted and received from ports are POJO (plain old java
objects). Operator developers should be careful about the ownership
semantics of tuples that are mutable objects and passed through
THREAD_LOCAL or CONTAINER_LOCAL streams (shared object references).
Immutable objects can be passed without regard to the type of stream
being used. In general immutable objects are better as they free the
downstream tuple from having to worry about the life span of the tuple.
For performance reasons it may be okay to use mutable objects during
processing (internal data of the objects), but emitted tuples should
ideally be immutable. Emitting mutable tuples may be ok if they are not
accessed by the operator post emit.</p>
<p>Within the operator code, care should be taken to not change a tuple
emitted by that operator. The example below may result in bad data
(empty HashMap) for a downstream operator:</p>
<pre><code class="java">// Bad example, as object 't' is being changed post emit
HashMap &lt; String, Integer &gt; t = new HashMap &lt;String, Integer &gt; ();
t.put(&quot;i&quot;, 2);
outputport.emit(t);
t.clear();

</code></pre>

<h3 id="passthrough-vs-end-of-window-tuples">Passthrough vs End of Window tuples</h3>
<p>Tuples can be emitted during beginWindow(), process(), or endWindow().
All these tuples would have the windowId associated with the window
marked by beginWindow() and endWindow() calls. The same operator can
emit a tuple in each of the above calls.</p>
<p>A passthrough tuple is one that is emitted during beginWindow() or
process(). The reason for emitting a tuple during beginWindow() is rare
as the tuple would be emitted without receiving any data. The most usual
passthrough tuple is the one that is emitted during an input port's
process() call. In case of multiple input ports, care should be given
that a passthrough tuple is not dependent of the order of tuple arrival
on two different input ports, as this order is only guaranteed within a
stream, i.e. on one input port. Passthrough tuples do not always imply
that the operator is stateless. For an operator to be stateless, an
outbound tuple should depend only on one tuple of the process call.</p>
<p>An end of window tuple is the one that is emitted during endWindow()
call. These tuples are usually aggregates, are and the ones that wait
for all tuples to be received in the window before they are emitted.
They are thus by definition not impacted by the order in which tuples
may arrive on different input windows. However existence of an end of
window tuple almost always means that the operator is stateful.</p>
<p>A passthrough tuple has lower latency than an end of window tuple. The
platform does not differentiate between a “passthrough” tuple or an “end
of window” tuple, and in fact does not even recognize them as different.
For downstream operators there is no semantic difference between a tuple
that is emitted as passthrough or as end of window. All the tuples are
always received as part of the process() call on the input ports of
downstream operators. This means that the difference in latency of a
tuple emitted during end of window as compared to that during process()
is "window period/2". The rest of the downstream operators make no
contribution to latency other than their processing time, which would be
identical for both passthrough and end of window emission. This is true
for the entire DAG irrespective of which operator decides between
emitting a tuple during process call or during endWindow() call. An end
of window tuple also has one safety feature that it is easy to ensure
that the outbound tuple does not depend on the order in which tuples
arrived on different input ports. Operators that only emit during
endWindow() can be clearly marked as "independent of tuple order in two
input streams". This is very useful as it allows a host of
optimizations, re-partitioning, and other operations to be done. Even if
the STRAM is not able to dynamically figure this difference, there is a
possibility of adding annotations on the operator to signal the STRAM
about their behavior in the future. In the future, the platform would
leverage this data.</p>
<h3 id="streaming-window-vs-application-window">Streaming Window vs Application Window</h3>
<p>The platform supports two windowing concepts. The first one is the
streaming window, i.e. the smallest atomic micro-batch. All bookkeeping
operations are done between an end of a window and the start of the next
one. This includes checkpointing, inserting the
debugger, inserting charting, recovery from the start of such a window,
etc. The second is the application window. This is decided by the
functionality of the application. For example if an application is
computing revenue per hour, then the application window is one hour. For
better operability, minimizing the streaming window is recommended as
that denotes the smallest loss of computations in the event of an
outage. The platform supports both these natively. By default, the
checkpointing is aligned with the endWindow() call, and hence it aligns
with end of the application window. For large application windows this
may be a problem as the upstream bufferserver has to retain a very long
queue. The way around is to set "checkpoint=true" for within the
application window, and to write the operator
in such a fashion that the state of the operator consists of both the
data that is passed between windows, as well as the dependency of an
outbound tuple on all previous tuples. Such a state definition is safe.
The downside is that the state may be large and make checkpointing
costly. A way around this is to partition the operator based on the size
of the state to ensure checkpoints are being saved equally by
partitions.</p>
<p>There are two types of application windows: aggregate application
windows and sliding application windows. Aggregate application windows
are for applications that do computations per fixed time period. It is
specified by an attribute since exactly the same code works for a
streaming window as application window. Once the aggregate application
window flag is specified (by default it is equal to streaming window)
the the begin_window starts an application window and then the
intermediate begin and end windows are skipped until the application
window boundary is reached. Then the end_window is called. Sliding
application windows are for applications that do computations for past
fixed time period. In such a case the operator code needs access to all
the state of the computations in a sliding fashion (e.g. the last 5
minutes). The platform has a Java interface written for sliding windows
that allows operator developers to easily code a sliding window
operator. For details refer to Real-Time Streaming Platform
Guide.</p>
<h2 id="4-commonly-used-operators">4: Commonly Used Operators</h2>
<p>There are operators in the platform that support commonly needed
functions. In this chapter we review them.</p>
<h3 id="window-generator">Window Generator</h3>
<p>A Window Generator is inserted for every input adapter. This operator is
tasked with creating windows. The windows from two different window
generators get synced on the first operator that listens to streams
whose windows originate from the window generators. Since the downstream
operator syncs the streams on endWindow, all the tuples from a window
generator that started early would wait until the lagging window
generator starts sending window events.</p>
<p>As much as possible, window generators should not be directly used by
users in their designs. The API of the window generator class is not
public and therefore should not be relied upon. An application that uses
window generators directly also risks complications and compatibility
issues with future releases of the platform.</p>
<h3 id="default-unifier">Default Unifier</h3>
<p>A default unifier is provided for merging partitions. The default
unifier is a passthrough, i.e. it forwards all the tuples from
partitions to downstream operators, thus enabling the downstream
operators to connect to one upstream source (i.e. the unifier). The
unifiers works the same in Nx1 partitions as well as NxM partitions; the
only difference is that a NxM partition would have M unifiers - one for
each downstream operator partition.</p>
<h3 id="sinks">Sinks</h3>
<p>Sinks are objects that implement the Sink interface. The sink is the
basic interface that has the process(tuple) API. The platform
transparently connects output ports to sinks for needed functionality.
These sink objects are used inside the engine for various purposes.
Examples include stats collection, debugging, chart data collection,
etc. We have also included a host of sinks in testbench library. These
are very valuable as they help developers quickly run their tests. Using
sinks in tests is recommended as they follow the same API concepts and
are fully supported by the platform. All the sinks are included in unit
and performance tests and their functionality and performance is
guaranteed.</p>
<ul>
<li>CollectorTestSink</li>
<li>PartitionAwareSink</li>
<li>WindowIdActivatedSink</li>
<li>ArrayListTestSink</li>
<li>CountAndLastTupleTestSink</li>
<li>HashTestSink</li>
<li>CountTestSink</li>
</ul>
<h3 id="inputoperator">InputOperator</h3>
<p>The InputOperator interface should be used to develop input adapters.
This is the interface that all input adapters implement. Operators that
implement this interface need to implement the following method:</p>
<p>public void emitTuples();</p>
<p>This method is called continuously between the beginWindow() and
endWindow() calls and is supposed to fetch data from an external system
and write it out to the output port(s).</p>
<h3 id="apache-apex-malhar-library-templates">Apache Apex Malhar Library Templates</h3>
<p>The Util and Common libraries have a collection of operators that can be
extended to create custom operators. These include operators for key/val
pairs, matching, filters, unifiers, and others.</p>
<h3 id="databasewindow-synchronization-operator">Database/Window Synchronization Operator</h3>
<p>Database adapters (input or output) need to be able to instrument the
at-most-once mechanism. This is needed as that is the final output
state, or once-only processing of incoming data. The standard library
templates for databases have such a mechanism built in. A base operator
is provided for precisely such a behavior. These operators rely on last
completely processed window being written to these outside system for
ouputAdapters, and retaining the last read event for inputAdapters.</p>
<h2 id="5-fault-tolerance">5: Fault Tolerance</h2>
<p>Fault tolerance in the platform is defined as the ability to recognize
an outage in any part of the application, provision replacement
resources, initialize the lost operators to a last-known state, and
recompute the lost data. The default method is to bring the failed part
of the DAG back to a known checkpointed state and recompute atomic micro
batches from there on (also called the “at-least-once” recovery
mechanism). Operators can be set for "at-most-once" recovery, in which
case the new operator starts from the next available window. Operators
can be set for an “exactly-once” recovery, in which case the operator
only recomputes the window it was processing when the outage happened.
At-most-once recovery as an attribute will be available in a later
version. For now, “exactly-once” recovery is
achieved by setting the checkpoint interval to 1. This mechanism is very
costly as checkpointing is done after every
window. In the future when the platform
provides the ability to recognize a stateless operator, an exactly-once
mechanism will be significantly less costly as checkpointing is not
needed for stateless operators. At-most-once recovery in most
applications is an outbound need, i.e. the data being written out to a
system outside the application needs to be written only once.  This is
instrumented for output adapters (where it really matters) by saving the
processed window ID into the outbound system (database, files, etc.) to
be used as a GUID or primary key. The default output adapters provided
in the Apache Apex Malhar Library Templates  include such
mechanism.</p>
<p>A choice of a recovery mechanism is decided both by the operator design
as well as the application needs. If computations are data-loss
tolerant, an at-most-once mechanism works. For computations that are
data-loss intolerant, an at-least-once mechanism works. For computations
that write to an outside state and cannot handle re-computations, an
exactly-once model is needed.</p>
<h3 id="checkpointing">Checkpointing</h3>
<p>The STRAM provides checkpointing parameters to StreamingContainer during
intialization. A checkpoint period is given to StreamingContainer of the
containers that have window generators. A control tuple is sent when the
checkpoint interval is completed. This tuple traverses through the data
path via streams and triggers each StreamingContainer in the path to
instrument a checkpoint of the operator that receives this tuple. This
ensures that all the operators checkpoint at exactly the same window
boundary. The only delay is the latency of the control tuple to reach
all the operators. The checkpoint is thus done after the endWindow()
call of the current window and before the beginWindow() call of the next
window. Since all the operators are computing in parallel (separate
threads) they each process the “checkpoint” control tuple independently.
The asynchronous design of the platform means that there is no guarantee
that two operators would checkpoint at exactly the same time, but there
is guarantee that they would checkpoint at the same window boundary.
This feature also ensures that purging old data can be done very
efficiently, since when the checkpoint window tuple is done traversing
the DAG, the checkpoint state of the entire DAG increments to this
window id.</p>
<p>In case of an operator that has an application window that is different
from the streaming window, the checkpointing happens after the
application window is complete. Though this allows the operators to
treat the application window as an atomic unit, it does need the
upstream bufferserver to keep tuples for the entire application window.</p>
<p>By default, checkpoints are not done inside of an application window.
Application developers can choose to override this and specify that
checkpoint windows be used. This is possible only if the operator is
completely stateless, i.e. an outbound tuple is only emitted in process
call and only depends on the tuple of that call. If the operator is
stateful within a window, the operator developer should disallow
checkpointing within the window as the atomic computation could be for
an application window. If the application developer allows for
checkpointing within an application window, then the checkpoint window
is followed by the STRAM. If the application window is not an exact
multiple of the checkpoint window, then the checkpoints get done a
little early. For example, in an application with streaming window = 1
sec, if the checkpoint window is 30 for the application, and application
window is 100, then the operator will checkpoint at 30, 60, 90, 100,
130, 160, 190, 200 secs. For such a case, STRAM purge process will take
these into account by keeping required tuples in the bufferservers.</p>
<p>Checkpointing involves pausing an operator and serializing the object to
HDFS. After the checkpoint state is saved, the operator may start
processing events again. Thus, checkpointing has a latency cost
implications in the throughput. It is important to ensure that
checkpointing is done with minimal required objects. This means that all
data that is not part of an operator state must be declared as
transient. An operator developer can also create a stateless operator as
long as the life span is only within a streaming window (i.e not part of
the state of the operator). By default this would work and such data can
be declared as transient. The serialized data is stored as a file, and
is the state that the operator can be brought back to. The ID of the
last completed window (per operator) is sent back to the STRAM in the
next heartbeat. The default implementation for serialization
uses<a href="https://www.google.com/url?q=https://github.com/EsotericSoftware/kryo&amp;sa=D&amp;usg=AFQjCNHIL2kXmyh5q_b3VC72KigxO6i4_A"> </a><a href="https://www.google.com/url?q=https://github.com/EsotericSoftware/kryo&amp;sa=D&amp;usg=AFQjCNHIL2kXmyh5q_b3VC72KigxO6i4_A">KRYO</a> .
Multiple past checkpoints are kept per operator. Depending on the
downstream checkpoint, one of these are chosen to start from. These are
purged only after they are are no longer needed. STRAM takes the purge
decision and informs all bufferservers about these.</p>
<p>A complete recovery of an operator needs that the operator be brought
back to a checkpoint state and then all the lost atomic windows being
replayed by upstream buffer server. The above design keeps the
bookkeeping cost very low, and still allowing rapid catch up of
processing. In the next section we would see how this simple abstraction
allows applications to recover under different requirements.</p>
<h3 id="recovery-mechanisms">Recovery Mechanisms</h3>
<p>Recovery mechanisms are ways to recover from a container (or an
operator) outage. In this section we explain a single container outage.
Multiple container outages are handled as independent events. Recovery
requires the upstream buffer server to replay windows and it would
simply go one more level upstream if the immediate upstream container is
also down. If multiple operators are present in a container, then the
container recovery treats each operator as independent objects when
figuring out the recovery steps. Application developers can set any of
the below recovery mechanisms for node outage. In general, the cost of
recovery depends on the state of the operator and the recovery mechanism
selected, while the data loss tolerance is specified by the application.
For example a data loss tolerant application would prefer “at-most-once”
recovery.</p>
<p>All recovery mechanisms treat a streaming window as an atomic
computation unit. In all three recovery mechanisms the new operator
connects to the upstream bufferserver and asks for data from a
particular window onwards. Thus all recoveries translate to deciding
which atomic units to re-compute, and which state the new operator
should start from. A partially computed micro-batch is always dropped.
They are re-computed in at-least-once or exactly-once mode. In
at-most-once mode, they get skipped. Atomic micro-batches are a critical
guiding principle as this allows for very low bookkeeping cost, high
throughput, low recovery time, and high scalability.</p>
<p>Within an application, each operator can have its own recovery
mechanism. The operators can be developed oblivious to the recovery mode
in which they will function. Yet, in the cases where they do need to
know, the processing mode can be obtained as:</p>
<pre><code class="java">ProcessingMode mode =
context.attrValue(OperatorContext.PROCESSING\_MODE,      
ProcessingMode.AT\_LEAST\_ONCE);
</code></pre>

<h4 id="at-least-once">At-Least-Once</h4>
<p>At-least-once recovery is the default recovery mechanism, i.e it is used
if no mechanism is specified. In this method, the lost operator is
brought back to its latest checkpointed state and the upstream buffer
server is asked to replay all windows since the checkpointed window.
There is no data loss in this recovery mode. The viable checkpoint state
is defined as the one whose window ID is in the past as compared to all
the checkpoints of all the downstream operators. All downstream
operators are restarted at their checkpointed state in the same
container. They ignore the data until the stream catches up to their
state by subscribing after their checkpointed window. All the lost
atomic micro-batches are thus recomputed and the application catches up
with live incoming data. This is the at-least-once mechanism, as lost
windows are recomputed. For example, if the streaming window is 0.5
seconds and checkpointing is being done every 30 seconds, then upon node
outage all windows since the last checkpoint (up to 60 windows) need to
be re-processed. If the application can handle loss of data, then this
is not the most optimal recovery mechanism.</p>
<p>In general in this recovery the average time lag on a node outage in at
least recovery is:</p>
<pre><code class="java">Recovery time = (CP/2 \* SW) \* T + HC

Where:

CP        Checkpointing period (default value is 30 seconds)

SW        Streaming window period (default value is 0.5 seconds)

T         Time taken to re-compute one lost window

HC       Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s) to its checkpointed state
</code></pre>

<p>A lower CP is a trade off between the cost of checkpointing and the need
to have lower latency during node recovery. Input adapters cannot do
at-least-once without the support from sources outside Hadoop. For an
output adapter, care needs to be taken if external systems cannot handle
re-writing the same data.</p>
<h4 id="at-most-once">At-Most-Once</h4>
<p>Applications that can tolerate data loss get the quickest recovery in
return. The engine brings the operators to the most recent checkpointed
state and connects its ports to the upstream buffer server, subscribing
to data from the start of the next window. It then starts processing
that window. The downstream operators realize some windows were lost,
but continue to process. Thus, an at-most-once mechanism forces all
downstream operators to follow. In the cases where an at-most-once
operator has more than one input port, it’s possible that they play
different windows. In this case, some of the windows get the data from
just from a few of the input ports and some of the windows get lost - by
definition of at-most-once. This is acceptable because we care about
catching up to a steady state as fast as possible, and once achieved not
losing any data.</p>
<p>For example, if the operator has ports in1 and in2 and a checkpointed
window of 95, and their buffer server responds with window id 100 and
102 respectively (window 100 was received before 102),  then the
operator will work on the tuples from only that buffer server which is
at window 100. At the completion of that window, if tuples from 101 were
received before 102, then it will work with the one with the data
received for window 101, and then it will go on to process window 102.
But if tuples from window 102 were received before window 101, then
window 102 will be processed and window 101 will be skipped completely.
But from window 102 onwards the operator will resume regular processing
unless one of the inputs starts skipping the windows.</p>
<p>In general in this recovery the average time lag on a node outage in
at-most-once recovery is:</p>
<pre><code class="java">Recovery time = SW/2 + HC

Where:

SW        Streaming window period (default value is 0.5 seconds)

HC       Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s)
</code></pre>

<h4 id="exactly-once">Exactly-Once</h4>
<p>This recovery mechanism is for applications that need no data-loss as
well as no recomputation. Since a window is an atomic compute unit,
exactly-once applies to the window as a whole. In this recovery the
operator is brought back to the start of the window in which the outage
happened, and the window is recomputed. The window is considered closed
when all the data computations are done and end window tuple is emitted.
 Exactly-once requires every window to be checkpointed. Hence, it’s the
most expensive of the three recovery modes. The operator asks the
upstream buffer server to send data after the most recent checkpoint.
The upstream node behaves the same as in at-most-once. Checkpointing
after every streaming window is very costly, but users will most often
do exactly-once per application window in which case the cost of running
an operator in this recovery mode may not be as costly. In the current
set, up exactly-once can be achieved by setting checkpointing window to
1 on an operator.</p>
<h2 id="6-partitioning">6: Partitioning</h2>
<p>Partitioning is the fundamental building block for scaling a streaming
application. Any of the logical units in an application may face a
resource crunch. This could be an operator that does not have enough CPU
or RAM to complete its computations, or it could be a stream that just
cannot fit within a NIC bandwidth limit. The platform provides rich
partitioning schemes to help scale an application. The platform is
designed to scale at a Hadoop level and has several functionalities
built in that support scalability.</p>
<h3 id="static-vs-dynamic">Static vs Dynamic</h3>
<p>In general, partitioning can be done statically (during launch time) or
dynamically (during run time). There is no other difference between
these. The same logic and schemes hold true to both launch time as well
as run time partitioning. The advantage of dynamic partitioning is that
the application can respond to runtime demands and leverage resources
from Hadoop to meet SLAs or other constraints. This helps to optimize
resources, since they may be provisioned only when exactly needed.</p>
<p>The implementation of dynamic partitioning impacts runtime as the new
partitions are rolled out and stream processing is restarted based on
the recovery mechanism of the operator. In the case of at-least-once
processing, the operator partitions go back to a checkpointed state. In
the case of at-most-once processing, the new partitions simply start
from the latest window (i.e. there is data loss). Finally, in the case
of exactly-once processing, the operator restarts at the current window.
This becomes very tricky when the operator is stateful. In certain
cases, the STRAM needs to ensure that all the partitions are at the same
state. Scaling up may be accomplished by splitting all the partitions
into N different partitions, or by splitting the largest partition into
two, but scaling down needs to ensure that the partitions being merged
are checkpointed at the same window id. Thus, dynamic partitioning is
costlier and impacts application performance. Static partitioning,
however, is done at launch time and if correctly done may not need
further partitioning. In general this means that extra resources, if
any, stay idle and are wasted. A choice between (possibly) wasting
resources, or impacting application performance is a decision for the
user to make. With commodity hardware this decision is a close one.</p>
<h3 id="partitioner">Partitioner</h3>
<p>How does the platform know what data an operator partition needs to
receive in order to implement its logic? It depends on the functionality
of the operator. In a simplified example, an operator may want to
process a range of data, let us say all names that start with A through
C should be belong to partition 1 and the rest to partition 2 (a custom
stream codec would hash A through C to  1 and the rest to 2). The
operator needs the ability to declare that there should be 2 partitions
(one receives tuples with hash code 1, the other tuples with hash code
2). We provide the interface Partitioner, which is designed to give
control over the distribution scheme to the operator developer. In this
case, the operator would provide a mapping that declares:</p>
<ul>
<li>Operator Instance one receives on port inputPortName, all tuples
    with hash code 1</li>
<li>Operator Instance two receives on port inputPortName, all tuples
    with hash code 2</li>
</ul>
<p>While this may look complicated at first glance, it is necessary for
more complex scenarios.</p>
<h3 id="multiple-ports">Multiple Ports</h3>
<p>Operators may have multiple input ports. The partitioning of incoming
streams on each of the ports depends entirely on the logic or the
operator. One port may be connected to a high throughput stream that we
would like to partition, while the other port may deliver low throughput
stream, but each tuple is required for the functionality of each
operator instance, regardless which portion of the other stream it
handles. An example for this could be the processing of location
updates. In this case, the operator may receive (high speed) location
updates on one input port, and (slow) location queries on the other. We
would like to partition the first stream by location, and receive
location query on the the other port. Only the operator instance that
handles the corresponding location will respond to the query.  A
different mapping would be needed for an operator with 2 ports that
joins tuples. In this case, each of the streams would need to be
partitioned in exactly the same way. Since the StreamCodec could be
custom, there is no way for the platform to check if two ports are
listening to streams that are partitioned the same way (or in a
particular manner).</p>
<h3 id="streamcodec">StreamCodec</h3>
<p>The StreamCodec is responsible for serialization of data tuples into
bytes (object to byte array) and deserialization (byte array to object)
for transmission through buffer server streams. The codec is defined on
input ports and is used by the platform to serialize the data when
emitted through the upstream operator’s output port before writing to
the stream and to deserialize after reading from the stream, before
handing over the tuple object to the input port.</p>
<p>When partitioning is enabled for a stream, the codec is responsible to
assign the partition key for a tuple before it is transmitted over the
stream. The buffer server then uses the key to determine the partition
that will process the tuple. The default codec is using Object’s
hashCode function to calculate this value. If the hashCode function
cannot compute the partition key (for example, when using data
structures such as HashMap or ArrayList) it is necessary to supply a
codec that understands the data and can compute the partition key.</p>
<p>If no codec is set on an input port explicitly, the default stream codec
is used. The default implementation
uses<a href="https://www.google.com/url?q=https://code.google.com/p/kryo/&amp;sa=D&amp;usg=AFQjCNFs-pX3X8-WfDMu1LDSIxlncJERdg"> </a><a href="https://www.google.com/url?q=https://code.google.com/p/kryo/&amp;sa=D&amp;usg=AFQjCNFs-pX3X8-WfDMu1LDSIxlncJERdg">Kryo</a> for
serialization. It generically supports most standard types and is used
for all port types of library operators. To customize how objects are
serialized or the partition key is computed, the operator developer can
supply a custom stream codec by overriding InputPort.getStreamCodec().</p>
<h3 id="unifier">Unifier</h3>
<p>When an operator is partitioned into N physical operator instances, the
downstream operator needs to get streams from all the partitions. All
operator instances however, have fixed number of ports, and hence have
fixed number of streams they can read from or write to. This anomaly is
solved by having the STRAM insert a dummy merge operator before the
downstream operator. This merge operator is CONTAINER_LOCAL (intra jvm
process) with the partitioned operator. Since the STRAM knows exactly
how many partitions exists at execution roll out time, it inserts a
merge operator with precisely that number of inputs. The default merge
operator is a passthrough, i.e. it simply forwards all the tuples from
its input ports onto the output port. It has a single output port whose
schema is same as that of the output port of the partitioned operator.
The downstream operator thus does not notice any difference as the
outputs of all partitions are combined.</p>
<p>An example of an operator that needs specific functionality for the
merge to work is SumMap operator. This operator takes a HashMap\&lt;K,V>
as input. Its output is the sum of all the values of a particular key.
Since HashMap is a single tuple that could be a collection of various
keys, the input port has to be partitioned on a round-robin basis. This
means that any particular key may appear in any of the partitions. So
the only way to get correct output is for the merge operator to do a
key-wise sum again. This can be implemented in the unifier logic.</p>
<p>Another example is this would be top N calculation, where top N results
from each partition need to be consolidated into aggregate top N at the
end of the processing window. For this, the output port of any operator
can define a “nifier” object, which is an operator that has no input
ports but instead a single method that accepts tuples emitted from
partitions and implements the logic required to merge them. The outputs
of each instance of a partitioned operator feeds to this unifier
operator and thus intercepts the output stream in front of the
downstream operator.</p>
<p>The unifier is an operator that implements Unifier interface. This
interface needs one method to be implemented - process(\&lt;type> tuple).
The schema (\&lt;type>) of the process() method is same as the schema of
the output port in which the getUnifer method is implemented. Thus, for
parent operators that have only one output port, proper coding habits
allow usage of the parent operator itself to be used as the Unifier.
Since a Unifier is an operator, it has access to the operator interface,
namely beginWindow(), and endWindow() in addition to process. The
platform automatically inserts a process call that allows developers to
do a proper merge as seen above. Any object that implements this API can
be used as a unifier. Code specific to the unifying functionality for
output streams of a particular operator should be coded in this class.
Some examples of usage of a parent operator as an unifier are MinMap or
MaxMap. This is the interface for a unifier operator:</p>
<pre><code class="java">// &lt; T &gt; is the tuple schema for the output port for which
// the unifier object would be used
public interface Unifier &lt; T &gt; extends Operator
{
  public void process(T tuple);
}
</code></pre>

<p>When using a unifier that collects all the output tuples in one
operator, users can get away from a sticky key
partitioning scheme, as long as the the unifier
can function within one Hadoop container (usually \~ 1GB
RAM), i.e. a single unifier object that has
enough resources (CPU, RAM, and Network) to process the outputs of all
the partitions. Combining the output of all partitions allows the basic
partitioning to be done by round-robin without a sticky key, and thus
avoids any skew. For operators which are partitioned by sticky key, the
default merge (passthrough) works fine.</p>
<p>Care should be taken in cases where partitioning is done mainly to
manage outbound throughput, especially if it is more than the NIC
capacity of a Hadoop node. In this case, a Unifier does not help as all
the tuples flow back into the same operator. In almost all cases, the
outbound throughput problem should be resolved by partitioning both the
current operator and the downstream operator. One way to look at this is
that the stream in question has a network resource requirement that no
single Hadoop node can provide, hence all operators on that stream must
be partitioned, and no partition should read/write the full stream.</p>
<p><img alt="" src="../images/operator_development/image01.png" /></p>
<h2 id="7-library">7: Library</h2>
<p>The platform includes a set of operator templates, which are available
under Apache 2.0 license. These are open source operator library under
Apache Apex Malhar project.They are provided to enable quick application
development. As an operator developer, you can leverage this by either
extending them, or creating your own library. In addition to reducing
the development time, they also help reduce maintenance cost. The
library operators can be benchmarked, tested, and have data that the
STRAM can leverage for running applications optimally.</p>
<h3 id="common-operator-functions">Common Operator Functions</h3>
<p>There are several common operator functions that can be utilized. We
describe a few categories below. You can find many more operators in the
Apache Apex Malhar project. These are provided for a developer to
quickly create an application. They are not meant to replace or
substitute custom operators. Developers should judiciously decide on
which operators to use.</p>
<h4 id="filtering-selecting-map">Filtering-Selecting-Map</h4>
<p>Filtering is a very common operation. Filters can be employed in the
following ways:</p>
<ul>
<li>Conversions of tuple schemas. Examples include selecting/extracting
    a particular set of keys from the tuple (a collection) and drop the
    rest. Changing the contents (lower/upper case, round-up/down, etc.).
    Contents of input tuples and output tuples are different in this
    computation.</li>
<li>Passing through certain tuples. Examples include letting tuples that
    meet certain conditions go through. Tuple content remains the same
    in this computation</li>
<li>Comparison. This is similar to conversion, except that the tuple
    (flag) is just an alert.</li>
<li>Map can be done on file contents, or lines (word count). Combiner
    (map-side reduce) can be done over the streaming window.</li>
</ul>
<p>Filtering operations usually do not increase the throughput. In most
cases the throughput will decrease. These operators are also most likely
stateless. The resource requirements are usually not directly impacted
by the micro-batch size.</p>
<h4 id="aggregations">Aggregations</h4>
<p>Aggregate computations are those that need the entire window (atomic
micro-batch) to be computed for the results to be known. A lot of
applications need these to be computed over the application window.
Common aggregation examples include</p>
<ul>
<li>Counters like sum, average, unique count etc.</li>
<li>Range of the incoming data. Compute maximum, minimum, median etc.</li>
<li>Match. The first or the last match in the micro-batch.</li>
<li>Frequency. Most or least frequent</li>
<li>Reduce</li>
</ul>
<p>Aggregate functions are very effective in ensuring that the micro batch
is treated in an atomic way. Computations are dependent on the size of
the micro-batch and this size is usually decided between what the
application needs (application window) and how much micro-batch
processing makes sense from an operability point of view</p>
<h4 id="joins">Joins</h4>
<p>Joins are very common operations done on streams. These computations are
mostly done over the application window. The concept of the atomic micro
batch is very critical for joins. The tuples in the two streams can
arrive at unpredictable times within the window and thus at the end of
the window, the computation can guarantee that join operation is done
over all the tuples in that window.</p>
<h4 id="input-adapters">Input Adapters</h4>
<p>Input adapters enable the application to get data from outside sources.
Data read is done by the Input Adapter either by pulling from the source
or by data being pushed to the adapter. Input adapters have no input
ports. The primary function of an Input Adapter is to emitdata from
outside the DAG, as tuples for rest of the application. Once the data is
emitted as tuples all the mechanisms and abstractions in the platform
apply. To enable a data flow to have a recovery mechanism that spans
outside of the application, the outside source must have support for
such mechanisms. For scalability (partitioning) the outside source must
have support or enabling features. As an example, if the application
needs at-least-once recovery then an input adapter must be able to ask
for data from a specific timestamp or frame number upon re-connect after
an outage. The design of an Input Adapter needs very careful
consideration and needs application logic to be built correctly as
streaming platform infrastructure is sometimes not applicable to outside
sources. Examples of outside sources include HDFS, HBase, HTTP, Messages
busses like ZeroMQ, RabbitMQ, ActiveMQ, Kafka, Rendezvous, Databases
like MongoDB, MySql, Oracle.</p>
<h4 id="output-adapters">Output Adapters</h4>
<p>Output adapters write out results of the application to outside sources.
Data is written to a message bus, a database, to files (HDFS), or sent
over the network to a data collection service. The primary function of
an Output Adapter is to write data to destinations outside the DAG, and
manage data integrity during node outages. Output adapters have no
output ports. The data received by the Output Adapter follows all the
mechanisms supported by the streaming platform. For recovery mechanisms,
the Output Adapter has to store the state of the current written data.
Since the platform is atomic on a streaming window, output adapters
should use end of window as a commit. This way, during a recomputation
due to operator outage, data integrity can be ensured.</p>
<h4 id="event-generators">Event Generators</h4>
<p>Event generators are operators that generate events without an input
port. They are needed for testing other operators, for functionality or
load. These are different from input adapters as event generators do not
connect to any source. Various event adapters are available in the
testbench  library.</p>
<h4 id="stream-manipulation">Stream Manipulation</h4>
<p>Stream manipulation operators are those that do not change the content
of the tuples, but may either change schema, or just merge or split the
stream. Schema change operators are needed to allow data to flow between
two sets of ports with different schemas. You can use merge and split
streams between streams with different execution attributes. For
example, you would use a split operator if you want one stream to be
CONTAINER_LOCAL, while another to be across containers. Merge is also
used to merge in streams from upstream partition nodes.</p>
<h4 id="user-defined">User Defined</h4>
<p>Operator developers can easily develop their own operators, allowing for
quick turnaround in application development. Functionality-wise, any
user defined operator is the same as one provided as a bundled solution.
Standard operators do get tested with each build, are benchmarked, and
are supported. An operator developer should take on these
responsibilities for user defined operators.</p>
<h4 id="sample-code">Sample Code</h4>
<p>Code samples are included in the samples project. We will continually
add examples of various operator templates to this package. This package
is part of the open source Malhar project, and users are encouraged to
add their examples to enable community reuse and verification. Here is the github link to the project: https://github.com/apache/incubator-apex-malhar/tree/master/samples.</p>
<h4 id="latency-and-throughput">Latency and Throughput</h4>
<p>The latency of an atomic window compute operation of an operator is
defined as the time between the first begin window received on any input
port and the last of the end window tuple sent out by the operator on
any output port. Since the streaming window period is not contributed to
by the operator, the real latency contribution of the operator is the
above latency minus the streaming window period. Operator developers can
leverage distributed computing by doing as much computations upfront as
possible. This is very useful for an operator that only emits tuples in
end of window call.</p>
<p>Throughput of an operator is defined on a per port basis. The incoming
throughput is the number of tuples it processes per unit time on each
port. The outgoing throughput is the number of tuples it emits per unit
time on each output port. Per-port data is found in the “stream view” of
the application, while the incoming and outgoing totals are found on the
operator view of the application.</p>
<p>© 2012-2018 DataTorrent Inc.  Patent pending        </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../library_operators/" class="btn btn-neutral float-right" title="Operators List">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../operator_development/" class="btn btn-neutral" title="Guide"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../operator_development/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../library_operators/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>
      <script src="../menu.js"></script>
      <script src="../js/version-select.js"></script>

</body>
</html>
