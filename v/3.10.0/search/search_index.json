{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials, operator library and code samples to rapidly create DataTorrent applications using Java.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demos/", 
            "text": "Running Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications and they are available for import from the \nAppFactory\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nAppFactory\n section of the DataTorrent console.\n\n\nSelect the list view in the top right corner of the page, choose one of the available packages such as \nPi Demo\n and click the \nImport\n button.\n\n\nImported application packages and included applications will be listed under \nDevelop \n Application Packages\n page.\n\n\n\n\nLaunching Demo Applications\n\n\nOnce imported, applications can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nLaunch\n, and select one of the imported demo packages.  In this example, we will be using the \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available applications, locate PiDemo and click the launch button.\n\n\n\n\n\n\n\n\nClick \nLaunch\n on the confirmation modal.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the notification, or by navigating to the \nMonitor\n page and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nConfiguring Launch Parameters\n\n\nSome applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for Twitter Demo, or changing performance settings for larger applications.\n\n\n\n\n\n\nNavigate to \nAppFactory\n in DataTorrent console.  In this example, we will use the \nTwitter Demo\n application package. Click the \nImport\n button. \n\n\n\n\n\n\nRetrieve Twitter API access information by registering for a \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to the \nKeys and Access Tokens\n tab.  The Twitter Demo application requires the following to be specified by the user:\n\n\n\n\ndt.operator.TweetSampler.accessToken\n\n\ndt.operator.TweetSampler.accessTokenSecret\n\n\ndt.operator.TweetSampler.consumerKey\n\n\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\n\n\nNavigate to \nLaunch\n (or \nDevelop \n Application Packages\n). From the list of Applications, select \nTwitterDemo\n and press the corresponding \nlaunch\n button. When the launch modal appears, press the \nConfigure\n button.\n\n\n\n\n\n\n\n\nInput the Twitter keys and access tokens in the Required Properties section, then press the \nsave\n button. The keys and access tokens are now saved within a new application configuration. \n\n\n\n\nNote\n: Application configurations can be found in the \nLaunch\n page in the \nConfigurations\n tab, or in \nDevelop \n Application Configurations\n.\n\n\n\n\n\n\nPress the \nlaunch\n button to launch the Application Configuration.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboard\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the Monitor section of \ndtManage\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Running Apps"
        }, 
        {
            "location": "/demos/#running-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications and they are available for import from the  AppFactory  section of the DataTorrent management console.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  AppFactory  section of the DataTorrent console.  Select the list view in the top right corner of the page, choose one of the available packages such as  Pi Demo  and click the  Import  button.  Imported application packages and included applications will be listed under  Develop   Application Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#launching-demo-applications", 
            "text": "Once imported, applications can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.    Navigate to  Launch , and select one of the imported demo packages.  In this example, we will be using the  PiDemo  application package.    From the list of available applications, locate PiDemo and click the launch button.     Click  Launch  on the confirmation modal.     Once launched, view the running application by following the link provided in the notification, or by navigating to the  Monitor  page and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Launching Demo Applications"
        }, 
        {
            "location": "/demos/#configuring-launch-parameters", 
            "text": "Some applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for Twitter Demo, or changing performance settings for larger applications.    Navigate to  AppFactory  in DataTorrent console.  In this example, we will use the  Twitter Demo  application package. Click the  Import  button.     Retrieve Twitter API access information by registering for a  Twitter Developer  account, creating a new  Twitter Application , and navigating to the  Keys and Access Tokens  tab.  The Twitter Demo application requires the following to be specified by the user:   dt.operator.TweetSampler.accessToken  dt.operator.TweetSampler.accessTokenSecret  dt.operator.TweetSampler.consumerKey  dt.operator.TweetSampler.consumerSecret     Navigate to  Launch  (or  Develop   Application Packages ). From the list of Applications, select  TwitterDemo  and press the corresponding  launch  button. When the launch modal appears, press the  Configure  button.     Input the Twitter keys and access tokens in the Required Properties section, then press the  save  button. The keys and access tokens are now saved within a new application configuration.    Note : Application configurations can be found in the  Launch  page in the  Configurations  tab, or in  Develop   Application Configurations .    Press the  launch  button to launch the Application Configuration.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboard .", 
            "title": "Configuring Launch Parameters"
        }, 
        {
            "location": "/demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the Monitor section of  dtManage  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nThe DataTorrent Sandbox provides a quick and simple way to experience\nDataTorrent RTS without setting up and managing a complete Hadoop cluster.\nThe Sandbox has the latest version of DataTorrent RTS Enterprise Edition\npre-installed along with all the Hadoop services required to launch and run\nthe included demo applications.\n\n\nInstallation\n\n\nYou'll need to install Virtual Box in order to run the sandbox.\nOracle VirtualBox is a virtual machine manager (version 4.3 or later)\nand can be downloaded from \nVirtualBox\n.\n\n\nDownload the sandbox from\n\ndatatorrent.com/download\n and import it\ninto Virtual Box. Then select the newly imported image in the left pane of the\nVirtual Box console and click the \nStart\n button to\nstart the sandbox virtual machine. When the machine comes up, you should see a\nbrowser window showing a login dialog which is discussed in the next section.\n\n\nAccessing Console\n\n\nWhen accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\n\n\n\n\nIf the various HDFS services are not yet ready, you may see a red warning saying\nHDFS is not ready like this:\n\n\n\n\nThis warning should disappear after a few minutes and the console should appear\nlooking something like this:\n\n\n\n\nIf the warning persists after several minutes, check that all required services\nare running as described in the \nService Management\n section below.\nPlease go to \nTroubleshooting\n for detailed instructions.\n\n\nInside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting \nhttp://localhost:9090/\n\n\nConfiguring the Sandbox\n\n\nThe sandbox is configured with 6GB RAM; if your development machine has 16GB or\nmore, you can increase the sandbox RAM to 8GB or more using the VirtualBox\nconsole. This will yield better performance and support larger applications.\n\n\nAdditionally, you can change the network adapter from \nNAT\n to\n\nBridged Adapter\n; this will allow you to login to the sandbox from your\nhost as well as transfer files back and forth using \nssh\n and \nscp\n on Linux\nor \nssh\n tools like \nPuTTY\n and \npscp\n on Windows.\n\n\nA simpler option for sharing files is to mount a shared folder from the host\non the guest; this may be the only option in some environments where the\nsandbox is unable to acquire a separate IP address from DHCP. See, for example,\n\nhttp://www.htpcbeginner.com/mount-virtualbox-shared-folder-on-ubuntu-linux/\n or\n\nhttp://www.htpcbeginner.com/setup-virtualbox-shared-folders-linux-windows/\n.\n\n\nOf course all such configuration must be done when when the sandbox is not running.\n\n\nRunning Demo Applications\n\n\nOnce authenticated, you can continue to \nDemo Applications\n section to learn how to import, launch, and run demo applications.\n\n\nService Management \n\n\nThe sandbox automatically launches the following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nDepending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see the warning message described above.\n\n\nIf the warning persists after several minutes, check the status of each of these\nfollowing services: you can do that with a shell script like this:\n\n\n#!/bin/bash\nservices='hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-yarn-resourcemanager hadoop-yarn-nodemanager dtgateway'\nfor s in $services; do\n    sudo service \"$s\" status\ndone\n\n\n\nIf any of these services are not running, you can start them by running a similar\nscript but with \nstatus\n replaced by \nstart\n.\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "The DataTorrent Sandbox provides a quick and simple way to experience\nDataTorrent RTS without setting up and managing a complete Hadoop cluster.\nThe Sandbox has the latest version of DataTorrent RTS Enterprise Edition\npre-installed along with all the Hadoop services required to launch and run\nthe included demo applications.", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#installation", 
            "text": "You'll need to install Virtual Box in order to run the sandbox.\nOracle VirtualBox is a virtual machine manager (version 4.3 or later)\nand can be downloaded from  VirtualBox .  Download the sandbox from datatorrent.com/download  and import it\ninto Virtual Box. Then select the newly imported image in the left pane of the\nVirtual Box console and click the  Start  button to\nstart the sandbox virtual machine. When the machine comes up, you should see a\nbrowser window showing a login dialog which is discussed in the next section.", 
            "title": "Installation"
        }, 
        {
            "location": "/sandbox/#accessing-console", 
            "text": "When accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.     If the various HDFS services are not yet ready, you may see a red warning saying\nHDFS is not ready like this:   This warning should disappear after a few minutes and the console should appear\nlooking something like this:   If the warning persists after several minutes, check that all required services\nare running as described in the  Service Management  section below.\nPlease go to  Troubleshooting  for detailed instructions.  Inside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting  http://localhost:9090/", 
            "title": "Accessing Console"
        }, 
        {
            "location": "/sandbox/#configuring-the-sandbox", 
            "text": "The sandbox is configured with 6GB RAM; if your development machine has 16GB or\nmore, you can increase the sandbox RAM to 8GB or more using the VirtualBox\nconsole. This will yield better performance and support larger applications.  Additionally, you can change the network adapter from  NAT  to Bridged Adapter ; this will allow you to login to the sandbox from your\nhost as well as transfer files back and forth using  ssh  and  scp  on Linux\nor  ssh  tools like  PuTTY  and  pscp  on Windows.  A simpler option for sharing files is to mount a shared folder from the host\non the guest; this may be the only option in some environments where the\nsandbox is unable to acquire a separate IP address from DHCP. See, for example, http://www.htpcbeginner.com/mount-virtualbox-shared-folder-on-ubuntu-linux/  or http://www.htpcbeginner.com/setup-virtualbox-shared-folders-linux-windows/ .  Of course all such configuration must be done when when the sandbox is not running.", 
            "title": "Configuring the Sandbox"
        }, 
        {
            "location": "/sandbox/#running-demo-applications", 
            "text": "Once authenticated, you can continue to  Demo Applications  section to learn how to import, launch, and run demo applications.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/sandbox/#service-management", 
            "text": "The sandbox automatically launches the following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see the warning message described above.  If the warning persists after several minutes, check the status of each of these\nfollowing services: you can do that with a shell script like this:  #!/bin/bash\nservices='hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-yarn-resourcemanager hadoop-yarn-nodemanager dtgateway'\nfor s in $services; do\n    sudo service \"$s\" status\ndone  If any of these services are not running, you can start them by running a similar\nscript but with  status  replaced by  start .", 
            "title": "Service Management "
        }, 
        {
            "location": "/sandbox/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/aws_emr_manual/", 
            "text": "Overview\n\n\nThis document describes steps to run DT apps on AWS cluster. Users can easily try out apps from the \nAppFactory\n by downloading the app installers from the DataTorrent website. A zip package containing \nbash\n scripts will be downloaded on user\u2019s machine and user needs to follow the instructions below to deploy apps.\n\n\nSetup\n\n\n\n\nInstall the AWS Command Line Interface\n\n\nRefer to the \nlink\n for instructions\n\n\n\n\n\n\nConfigure the AWS Command Line Interface\n\n\n\n\n  $ aws configure\n  AWS Access Key ID [None]: MENTION ACCESS_KEY_ID\n  AWS Secret Access Key [None]: MENTION SECRET_ACCESS_KEY\n  Default region name [None]: MENTION REGION\n  Default output format [None]: _Press ENTER_\n\n\n\n\n\nFor detailed instructions, refer to the \nlink\n\n\n\n\n\n\nVerify that AWS Command Line Interface is configured by ensuring the following returns correct values\n\n\n\n\n$ aws configure get aws_access_key_id\n$ aws configure get aws_secret_access_key\n$ aws configure get region\n\n\n\n\n\n\n\n\n\n\n\nPackage contents\n\n\nThe zip file (see \nend-to-end steps\n below) will contain following files:\n\n\n\n\n\n\nDTapp-EMR-Deploy.sh\n - Script that user needs to execute to set up AWS EMR Cluster. The cluster will be created and DT-RTS installed along with the app.\n\n\n\n\n\n\nconfig.properties\n - This file contains properties used by the script. File has sample values for the properties as mentioned below.\n\n(\nNote:\n \nSample values for the properties should be sufficient unless it needs to be changed.\n)\n\n\n\n\n\n\n\n    \n\n        \n\n        \n\n        \n\n    \n\n    \n\n        \n\n            \nProperty\n\n            \nSample Value\n\n            \nDescription\n\n        \n\n        \n\n            \nSECURITY_GROUP_NAME\n\n            \nDTapp-security-group\n\n            \nSecurity Group with this name will be created and inbound rule with user machine\u2019s external IP address will be added to it.\n\n        \n\n        \n\n            \nBUCKET_NAME\n\n            \ndtapp-emr\n\n            \nS3 bucket name. A bucket with this name will be created and script that installs DT-RTS will be placed into it. This script is used as a bootstrap script during cluster launch.\n\n        \n\n        \n\n            \nS3_REGION\n\n            \nus-east-1\n\n            \nRegion where S3 bucket is created\n\n        \n\n        \n\n            \nLOG_URI\n\n            \ns3n://dtapp-logs\n\n            \nS3 URI where log files would be stored\n\n        \n\n        \n\n            \nCLUSTER_NAME\n\n            \nDTapp\n\n            \nName of the EMR cluster\n\n        \n\n        \n\n            \nCORE_INSTANCE_TYPE\n\n            \nm1.large\n\n            \nType of machine for core instance\n\n        \n\n        \n\n            \nCORE_INSTANCE_COUNT\n\n            \n1\n\n            \nNumber of core instances to be created\n\n        \n\n        \n\n            \nMASTER_INSTANCE_TYPE\n\n            \nm1.medium\n\n            \nType of machine for master instance\n\n        \n\n        \n\n            \nMASTER_INSTANCE_COUNT\n\n            \n1\n\n            \nNumber of Master instances to be created\n\n        \n\n    \n\n\n\n\n\n\n\n\n\nconfiguration.json\n - This file contains hdfs block size,replication factor and virtual to physical memory ratio.\n\n\n\n\n\n\nDTapp-EMR-Terminate.sh\n - Script to terminate the cluster\n\n\n\n\n\n\nREADME\n - Readme file with instructions\n\n\n\n\n\n\nEnd to End workflow\n\n\nThe steps below describe the end to end flow to run any app provided in the AppFactory. Let's consider that we want to run Kinesis to S3 app on the AWS cluster.\n\n\n\n\n\n\nDownload the zip file for the app.\n\nhttps://www.datatorrent.com/downloads/aws-apps/app-kinesis-to-s3.zip\n\n\n\n\n\n\nExtract the zip\n\n\nunzip app-kinesis-to-s3.zip\n\n\n\n\n\n\nVerify the files. The directory structure should be as follows:\n\n\nuser@localhost:~/Downloads/app-kinesis-to-s3$ ls\nconfig.properties  DTapp-EMR-Deploy.sh  DTapp-EMR-Terminate.sh README configuration.json\n\n\n\n\n\n\n\nEdit \nconfig.properties\n file. Modify the properties as described in the \nPackage Contents\n section.\n\n\n\n\n\n\nCheck the \nconfiguration.json\n file which contains properties like blocksize, replication factor etc. User can change this  configuration, if required.\n\n\n\n\n\n\nAssign Execute permissions.\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ chmod u+x /path/to/app-kinesis-to-s3/DTapp-EMR-*\n\n\n\n\n\n\n\nExecute the shell script.\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Deploy.sh\n\n\n\n\n\n\n\nWait till the EMR cluster is up. Once the cluster is up, a link to configure the DT-RTS will be provided. Click on the link. E.g  \nhttp://ec2-54-91-43-25.compute-1.amazonaws.com:9090\n\n\n\n\n\n\nDT-RTS Welcome Screen will appear. Click \ncontinue\n\n  \n\n\n\n\n\n\nDefault paths for Hadoop and the DFS root will be automatically populated, e.g. \n/usr/bin/hadoop\n and \n/user/dtadmin/datatorrent\n. You can leave these values alone unless you have some specific reason to change them. Click \ncontinue\n\n  \n\n\n\n\n\n\nA valid license would be needed to continue further. Click on \nRequest License\n.\n    \n\n\n\n\n\n\nDownload the appropriate license.\n    \n\n\n\n\n\n\nUpload the license. Once the license is successfully uploaded, Click Continue\n    \n\n    \n\n\n\n\n\n\nOnce the configuration is complete, Click Continue\n  \n\n\n\n\n\n\nThe app is imported and is ready to launch\n  \n\n  To know more about how to configure a particular app, visit the \nApp Templates\n section of our \ndocumentation\n\n\n\n\n\n\nTo terminate the AWS cluster, execute the termination script:\n\n\n user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Terminate.sh", 
            "title": "AWS"
        }, 
        {
            "location": "/aws_emr_manual/#overview", 
            "text": "This document describes steps to run DT apps on AWS cluster. Users can easily try out apps from the  AppFactory  by downloading the app installers from the DataTorrent website. A zip package containing  bash  scripts will be downloaded on user\u2019s machine and user needs to follow the instructions below to deploy apps.", 
            "title": "Overview"
        }, 
        {
            "location": "/aws_emr_manual/#setup", 
            "text": "Install the AWS Command Line Interface  Refer to the  link  for instructions    Configure the AWS Command Line Interface     $ aws configure\n  AWS Access Key ID [None]: MENTION ACCESS_KEY_ID\n  AWS Secret Access Key [None]: MENTION SECRET_ACCESS_KEY\n  Default region name [None]: MENTION REGION\n  Default output format [None]: _Press ENTER_   For detailed instructions, refer to the  link    Verify that AWS Command Line Interface is configured by ensuring the following returns correct values   $ aws configure get aws_access_key_id\n$ aws configure get aws_secret_access_key\n$ aws configure get region", 
            "title": "Setup"
        }, 
        {
            "location": "/aws_emr_manual/#package-contents", 
            "text": "The zip file (see  end-to-end steps  below) will contain following files:    DTapp-EMR-Deploy.sh  - Script that user needs to execute to set up AWS EMR Cluster. The cluster will be created and DT-RTS installed along with the app.    config.properties  - This file contains properties used by the script. File has sample values for the properties as mentioned below. ( Note:   Sample values for the properties should be sufficient unless it needs to be changed. )    \n     \n         \n         \n         \n     \n     \n         \n             Property \n             Sample Value \n             Description \n         \n         \n             SECURITY_GROUP_NAME \n             DTapp-security-group \n             Security Group with this name will be created and inbound rule with user machine\u2019s external IP address will be added to it. \n         \n         \n             BUCKET_NAME \n             dtapp-emr \n             S3 bucket name. A bucket with this name will be created and script that installs DT-RTS will be placed into it. This script is used as a bootstrap script during cluster launch. \n         \n         \n             S3_REGION \n             us-east-1 \n             Region where S3 bucket is created \n         \n         \n             LOG_URI \n             s3n://dtapp-logs \n             S3 URI where log files would be stored \n         \n         \n             CLUSTER_NAME \n             DTapp \n             Name of the EMR cluster \n         \n         \n             CORE_INSTANCE_TYPE \n             m1.large \n             Type of machine for core instance \n         \n         \n             CORE_INSTANCE_COUNT \n             1 \n             Number of core instances to be created \n         \n         \n             MASTER_INSTANCE_TYPE \n             m1.medium \n             Type of machine for master instance \n         \n         \n             MASTER_INSTANCE_COUNT \n             1 \n             Number of Master instances to be created \n         \n         configuration.json  - This file contains hdfs block size,replication factor and virtual to physical memory ratio.    DTapp-EMR-Terminate.sh  - Script to terminate the cluster    README  - Readme file with instructions", 
            "title": "Package contents"
        }, 
        {
            "location": "/aws_emr_manual/#end-to-end-workflow", 
            "text": "The steps below describe the end to end flow to run any app provided in the AppFactory. Let's consider that we want to run Kinesis to S3 app on the AWS cluster.    Download the zip file for the app. https://www.datatorrent.com/downloads/aws-apps/app-kinesis-to-s3.zip    Extract the zip  unzip app-kinesis-to-s3.zip    Verify the files. The directory structure should be as follows:  user@localhost:~/Downloads/app-kinesis-to-s3$ ls\nconfig.properties  DTapp-EMR-Deploy.sh  DTapp-EMR-Terminate.sh README configuration.json    Edit  config.properties  file. Modify the properties as described in the  Package Contents  section.    Check the  configuration.json  file which contains properties like blocksize, replication factor etc. User can change this  configuration, if required.    Assign Execute permissions.   user@localhost:~/Downloads/app-kinesis-to-s3$ chmod u+x /path/to/app-kinesis-to-s3/DTapp-EMR-*    Execute the shell script.   user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Deploy.sh    Wait till the EMR cluster is up. Once the cluster is up, a link to configure the DT-RTS will be provided. Click on the link. E.g   http://ec2-54-91-43-25.compute-1.amazonaws.com:9090    DT-RTS Welcome Screen will appear. Click  continue \n      Default paths for Hadoop and the DFS root will be automatically populated, e.g.  /usr/bin/hadoop  and  /user/dtadmin/datatorrent . You can leave these values alone unless you have some specific reason to change them. Click  continue \n      A valid license would be needed to continue further. Click on  Request License .\n        Download the appropriate license.\n        Upload the license. Once the license is successfully uploaded, Click Continue\n     \n        Once the configuration is complete, Click Continue\n      The app is imported and is ready to launch\n   \n  To know more about how to configure a particular app, visit the  App Templates  section of our  documentation    To terminate the AWS cluster, execute the termination script:   user@localhost:~/Downloads/app-kinesis-to-s3$ ./DTapp-EMR-Terminate.sh", 
            "title": "End to End workflow"
        }, 
        {
            "location": "/azure_deployment/", 
            "text": "About Azure Deployment\n\n\nDataTorrent RTS supports seamless deployment in the cloud environment such as Microsoft Azure. It also supports built-in connectors for reading and writing data to popular data sources and stores that are supported by Azure such as Azure event hub and Azure blob storage.\n\n\nDT RTS also provides ready-made applications to demonstrate capabilities of the platform which can be customized to suit your requirement. This allows quick and easy development of streaming analytics application leveraging capabilities of Azure cloud platform.\n\n\nAzure Operators\n\n\nDatatorrent RTS provides rich library of operators for building streaming applications. It also includes the following operators which are specifically developed for Microsoft azure eco-system. These can be used for reading and writing data to azure data sources or data stores.\n\n\nAzure Blob Storage Output Operator\n\n\nAzure Blob storage is a service for storing large amounts of unstructured object data, such as text or binary data, that can be accessed from anywhere in the world through HTTP or HTTPS. Blob storage can be used to expose data publicly to the world or to store application data privately.\n\n\nAzure Event Hub Input Operator\n\n\nEvent Hub input operator receives events from Azure Event Hub in the form of raw byte arrays. It processes them and converts into byte array tuples. These tuples are then processed by downstream operators.\n\n\nAzure Event Hub Output Operator\n\n\nEvent Hub Output operator receives events from upstream operator(s) in the form of byte array or string tuples. It converts them into raw byte array format. These tuples are then emitted to Event Hub in Azure cloud.\n\n\nFor more details about including these operators in an application, refer to \nAzure Blob Output Storage Operator Guide\n, \nEvent Hub Input Operator Guide\n, and \nEvent Hub Output Operator Guide\n.\n\n\nDeploying DT RTS on Azure\n\n\nDataTorrent RTS can be deployed on Azure either from the web interface or from the CLI.\n\n\nPrerequisites\n\n\nFor deploying DataTorrent RTS on Azure, you must have the following:\n\n\n\n\nActive Azure account\n\n\nActive Azure subscription\n\n\nAzure CLI (For CLI Deployment)\n\n\n\n\nDeploying DT RTS from Website\n\n\nFrom the \nDT website\n \n \nDownload\n page, you can connect to the Azure Resource Manager page wherein you can use a template to create a Linux based HDinsight cluster and install DataTorrent RTS on the edge node.\n\n\nTo deploy DT RTS from Azure Website, do the following:\n\n\n\n\nFrom the DT Website for \ndownloads\n, Access the Azure login page.\n\nNote:\n You can proceed only if you have the DT RTS license. In case you do not have the license, you are directed to the \nDetails\n page for obtaining the license for DT RTS.\n\n\nOn the Azure login page, enter the Azure user credentials. The \nTemplate\n page is displayed.\n\n\n\nUnder \nBasics\n \n \nResource\n* group \n*, either select\n Create new \nto create a new resource group or select\n Use existing\n and choose an existing resource group.\n\n\nSelect the location.\n\n\nUnder \nSettings\n , enter the following details based on your requirements:\n\n\n\n\n\n\n\n\n\n\nItems\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for a cluster.\n\n\n\n\n\n\nCluster Login User Name\n\n\nEnter a user name that can used to submit jobs to the cluster and to log into cluster dashboards.\n\n\n\n\n\n\nCluster Login Password\n\n\nEnter a password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.\n\n\n\n\n\n\nSsH User Name\n\n\nEnter the SsH user name which can be used to remotely access the cluster and the edge node virtual machine.\n\n\n\n\n\n\nSsH Password\n\n\nEnter the SsH password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.\n\n\n\n\n\n\nHeadnode Size\n\n\nSpecify the VM size of the head nodes.\n\n\n\n\n\n\nWorkernode Size\n\n\nSpecify the VM size of the workernodes.\n\n\n\n\n\n\nWorkernode Count\n\n\nSpecify number of nodes in the cluster.\n\n\n\n\n\n\nEdgenode Size\n\n\nSpecify the size of the edgenode.\n\n\n\n\n\n\nDatatorrent Gateway Port\n\n\nSpecify the port where you want to run DataTorrent RTS.\n\n\n\n\n\n\nDatatorrent sub-domain suffix\n\n\nSpecify the suffix of the DataTorrent sub-domain. This becomes the part of the cluster URL after DT RTS is deployed on Azure.\n\n\n\n\n\n\nBlock size for HDFS\n\n\nSpecify the block size for HDFS. Maintain the default value,  if you are not sure about fine-tuning this value.\n\n\n\n\n\n\nReplication Factor for HDFS\n\n\nFor example, if the replication factor was set to 3 (default value in HDFS) there would be one original block and two replicas.\n\n\n\n\n\n\nVirtual to Physical Memory Ratio for YARN\n\n\nSpecify the virtual to physical memory ration for YARN.  Maintain the default value, if you are not sure about fine-tuning this value.\n\n\n\n\n\n\n\n\n\n\nAccept the terms and conditions and click \nPurchase\n. This spawns the Hadoop cluster with HDInsight.  The deployment process begins which may take about 20-30 minutes to complete. The notification of \nSuccessful Deployment\n is shown on the Azure portal. After the successful deployment, the HDInsight cluster is also displayed under \nResources\n.\n\n\nClick the HDInsight cluster name link and then click \nApplications\n.\nThe DataTorrent RTS is listed under the installed applications.\n\n\n\n\nVerifying and completing the deployment\n\n\n\n\nIn the browser, enter the URL in the following format:\n\nhttps://\nclustername\n-dat.apps.azurehdinsight.net\n.\n\nclustername\n is the name of the cluster that you had specified while creating the HDInsight cluster.\n\n\nEnter the credentials that was specified for the cluster.\nThe welcome screen for DT RTS is displayed.\n\n\nInstall DataTorrent RTS using the Installation wizard.\n\n\n\n\nDeploying DT RTS from CLI\n\n\nDT RTS can be deployed on Azure from the CLI. For this you must download the ZIP file from the DT Website. This ZIP file contains the JSON file template which can be edited to configure the parameters. The path of this JSON file is provided in the Azure CLI commands which deploys DT RTS.\n\n\nThe Azure CLI must be downloaded before the deployment.\n\n\nTo deploy DT RTS from CLI, do the following:\n\n\n\n\nDownload Azure CLI. Refer \nDownload\n.\n\n\nDownload and extract the ZIP file from DT website.\nThis ZIP file contains the JSON file template which can be edited to configure the parameters.\n\n\nEdit the JSON file template to configure the following parameters:\n\n\n\n\n\n\n\n\n\n\nItems\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nclusterName\n\n\nEnter a name for a cluster.\n\n\n\n\n\n\nclusterLoginUserName\n\n\nEnter a user name that can used to submit jobs to the cluster and to log into cluster dashboards.\n\n\n\n\n\n\nclusterLoginPassword\n\n\nEnter a password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.\n\n\n\n\n\n\nsshUserName\n\n\nEnter the SsH user name which can be used to remotely access the cluster and the edge node virtual machine.\n\n\n\n\n\n\nsshPassword\n\n\nEnter the SsH password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.\n\n\n\n\n\n\nheadnodeSize\n\n\nSpecify the VM size of the head nodes.\n\n\n\n\n\n\nworkernodeSize\n\n\nSpecify the VM size of the workernodes.\n\n\n\n\n\n\nworkernodeCount\n\n\nSpecify number of nodes in the cluster.\n\n\n\n\n\n\nedgenodeSize\n\n\nSpecify the size of the edgenode.\n\n\n\n\n\n\ndatatorrentGatewayPort\n\n\nSpecify the port where you want to run DataTorrent RTS.\n\n\n\n\n\n\ndatatorrentSub-domainSuffix\n\n\nSpecify the suffix of the DataTorrent sub-domain. This becomes the part of the cluster URL after DT RTS is deployed on Azure.\n\n\n\n\n\n\nblockSizeForHDFS\n\n\nSpecify the block size for HDFS. Maintain the default value, if you are not sure about fine-tuning this value.\n\n\n\n\n\n\nreplicationFactorForHDFS\n\n\nFor example, if the replication factor was set to 3 (default value in HDFS) there would be one original block and two replicas.\n\n\n\n\n\n\nvirtualToPhysicalMemoryRatioForYARN\n\n\nSpecify the virtual to physical memory ration for YARN.  Maintain the default value, if you are not sure about fine-tuning this value.\n\n\n\n\n\n\n\n\n\n\nFrom the command prompt, login to Microsoft Azure using the following command:\n\n\n\n\naz login\n\n\n\n\nThe URL and the authentication code is provided.\n\n\n\n\n\n\nIn a web browser, enter this URL and the authentication code and then click \nContinue\n.\n\n\n\n\n\n\nOn the Microsoft Azure Login page, enter your Microsoft Azure account credentials and login.\n\n\n\n\n\n\nGo to Azure CLI and enter the following command:\n\n\n\n\n\n\n```\naz login\n\n\naz group deployment create \\\n    --name \n \\\n    --resource-group \n \\\n    --template-uri https://raw.githubusercontent.com/Datatorrent/moodI/master/utils/azure/hdinsight-with-Datatorrent-RTS/azuredeploy.json \\\n    --parameters @/\n/azuredeploy.parameters.json\n ```\n\n\nNote\n : Change the highlighted value within \n to actual values.\n\n\nA notification is displayed when the deployment is completed. The DataTorrent RTS is deployed on Azure. You can verify and complete the installation using the steps at Verifying and completing the deployment.", 
            "title": "Azure"
        }, 
        {
            "location": "/azure_deployment/#about-azure-deployment", 
            "text": "DataTorrent RTS supports seamless deployment in the cloud environment such as Microsoft Azure. It also supports built-in connectors for reading and writing data to popular data sources and stores that are supported by Azure such as Azure event hub and Azure blob storage.  DT RTS also provides ready-made applications to demonstrate capabilities of the platform which can be customized to suit your requirement. This allows quick and easy development of streaming analytics application leveraging capabilities of Azure cloud platform.", 
            "title": "About Azure Deployment"
        }, 
        {
            "location": "/azure_deployment/#azure-operators", 
            "text": "Datatorrent RTS provides rich library of operators for building streaming applications. It also includes the following operators which are specifically developed for Microsoft azure eco-system. These can be used for reading and writing data to azure data sources or data stores.", 
            "title": "Azure Operators"
        }, 
        {
            "location": "/azure_deployment/#azure-blob-storage-output-operator", 
            "text": "Azure Blob storage is a service for storing large amounts of unstructured object data, such as text or binary data, that can be accessed from anywhere in the world through HTTP or HTTPS. Blob storage can be used to expose data publicly to the world or to store application data privately.", 
            "title": "Azure Blob Storage Output Operator"
        }, 
        {
            "location": "/azure_deployment/#azure-event-hub-input-operator", 
            "text": "Event Hub input operator receives events from Azure Event Hub in the form of raw byte arrays. It processes them and converts into byte array tuples. These tuples are then processed by downstream operators.", 
            "title": "Azure Event Hub Input Operator"
        }, 
        {
            "location": "/azure_deployment/#azure-event-hub-output-operator", 
            "text": "Event Hub Output operator receives events from upstream operator(s) in the form of byte array or string tuples. It converts them into raw byte array format. These tuples are then emitted to Event Hub in Azure cloud.  For more details about including these operators in an application, refer to  Azure Blob Output Storage Operator Guide ,  Event Hub Input Operator Guide , and  Event Hub Output Operator Guide .", 
            "title": "Azure Event Hub Output Operator"
        }, 
        {
            "location": "/azure_deployment/#deploying-dt-rts-on-azure", 
            "text": "DataTorrent RTS can be deployed on Azure either from the web interface or from the CLI.", 
            "title": "Deploying DT RTS on Azure"
        }, 
        {
            "location": "/azure_deployment/#prerequisites", 
            "text": "For deploying DataTorrent RTS on Azure, you must have the following:   Active Azure account  Active Azure subscription  Azure CLI (For CLI Deployment)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure_deployment/#deploying-dt-rts-from-website", 
            "text": "From the  DT website     Download  page, you can connect to the Azure Resource Manager page wherein you can use a template to create a Linux based HDinsight cluster and install DataTorrent RTS on the edge node.  To deploy DT RTS from Azure Website, do the following:   From the DT Website for  downloads , Access the Azure login page. Note:  You can proceed only if you have the DT RTS license. In case you do not have the license, you are directed to the  Details  page for obtaining the license for DT RTS.  On the Azure login page, enter the Azure user credentials. The  Template  page is displayed.  Under  Basics     Resource * group  *, either select  Create new  to create a new resource group or select  Use existing  and choose an existing resource group.  Select the location.  Under  Settings  , enter the following details based on your requirements:      Items  Description      Cluster Name  Enter a name for a cluster.    Cluster Login User Name  Enter a user name that can used to submit jobs to the cluster and to log into cluster dashboards.    Cluster Login Password  Enter a password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.    SsH User Name  Enter the SsH user name which can be used to remotely access the cluster and the edge node virtual machine.    SsH Password  Enter the SsH password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.    Headnode Size  Specify the VM size of the head nodes.    Workernode Size  Specify the VM size of the workernodes.    Workernode Count  Specify number of nodes in the cluster.    Edgenode Size  Specify the size of the edgenode.    Datatorrent Gateway Port  Specify the port where you want to run DataTorrent RTS.    Datatorrent sub-domain suffix  Specify the suffix of the DataTorrent sub-domain. This becomes the part of the cluster URL after DT RTS is deployed on Azure.    Block size for HDFS  Specify the block size for HDFS. Maintain the default value,  if you are not sure about fine-tuning this value.    Replication Factor for HDFS  For example, if the replication factor was set to 3 (default value in HDFS) there would be one original block and two replicas.    Virtual to Physical Memory Ratio for YARN  Specify the virtual to physical memory ration for YARN.  Maintain the default value, if you are not sure about fine-tuning this value.      Accept the terms and conditions and click  Purchase . This spawns the Hadoop cluster with HDInsight.  The deployment process begins which may take about 20-30 minutes to complete. The notification of  Successful Deployment  is shown on the Azure portal. After the successful deployment, the HDInsight cluster is also displayed under  Resources .  Click the HDInsight cluster name link and then click  Applications .\nThe DataTorrent RTS is listed under the installed applications.", 
            "title": "Deploying DT RTS from Website"
        }, 
        {
            "location": "/azure_deployment/#verifying-and-completing-the-deployment", 
            "text": "In the browser, enter the URL in the following format: https:// clustername -dat.apps.azurehdinsight.net . clustername  is the name of the cluster that you had specified while creating the HDInsight cluster.  Enter the credentials that was specified for the cluster.\nThe welcome screen for DT RTS is displayed.  Install DataTorrent RTS using the Installation wizard.", 
            "title": "Verifying and completing the deployment"
        }, 
        {
            "location": "/azure_deployment/#deploying-dt-rts-from-cli", 
            "text": "DT RTS can be deployed on Azure from the CLI. For this you must download the ZIP file from the DT Website. This ZIP file contains the JSON file template which can be edited to configure the parameters. The path of this JSON file is provided in the Azure CLI commands which deploys DT RTS.  The Azure CLI must be downloaded before the deployment.  To deploy DT RTS from CLI, do the following:   Download Azure CLI. Refer  Download .  Download and extract the ZIP file from DT website.\nThis ZIP file contains the JSON file template which can be edited to configure the parameters.  Edit the JSON file template to configure the following parameters:      Items  Description      clusterName  Enter a name for a cluster.    clusterLoginUserName  Enter a user name that can used to submit jobs to the cluster and to log into cluster dashboards.    clusterLoginPassword  Enter a password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.    sshUserName  Enter the SsH user name which can be used to remotely access the cluster and the edge node virtual machine.    sshPassword  Enter the SsH password. This password must be at least 10 characters in length and must contain at least one digit, one non-alphanumeric character, and one upper or lower-case letter.    headnodeSize  Specify the VM size of the head nodes.    workernodeSize  Specify the VM size of the workernodes.    workernodeCount  Specify number of nodes in the cluster.    edgenodeSize  Specify the size of the edgenode.    datatorrentGatewayPort  Specify the port where you want to run DataTorrent RTS.    datatorrentSub-domainSuffix  Specify the suffix of the DataTorrent sub-domain. This becomes the part of the cluster URL after DT RTS is deployed on Azure.    blockSizeForHDFS  Specify the block size for HDFS. Maintain the default value, if you are not sure about fine-tuning this value.    replicationFactorForHDFS  For example, if the replication factor was set to 3 (default value in HDFS) there would be one original block and two replicas.    virtualToPhysicalMemoryRatioForYARN  Specify the virtual to physical memory ration for YARN.  Maintain the default value, if you are not sure about fine-tuning this value.      From the command prompt, login to Microsoft Azure using the following command:   az login  The URL and the authentication code is provided.    In a web browser, enter this URL and the authentication code and then click  Continue .    On the Microsoft Azure Login page, enter your Microsoft Azure account credentials and login.    Go to Azure CLI and enter the following command:    ```\naz login  az group deployment create \\\n    --name   \\\n    --resource-group   \\\n    --template-uri https://raw.githubusercontent.com/Datatorrent/moodI/master/utils/azure/hdinsight-with-Datatorrent-RTS/azuredeploy.json \\\n    --parameters @/ /azuredeploy.parameters.json\n ```  Note  : Change the highlighted value within   to actual values.  A notification is displayed when the deployment is completed. The DataTorrent RTS is deployed on Azure. You can verify and complete the installation using the steps at Verifying and completing the deployment.", 
            "title": "Deploying DT RTS from CLI"
        }, 
        {
            "location": "/create/", 
            "text": "Creating Applications\n\n\nHere are a few links to get you started:\n\n\n\n\n\n\nBeginner's Guide\n An introductory guide that shows you how to generate a new\n  ready-to-run Apex application using the maven archetype and a good deal of the application\n  landscape at an introductory level.\n\n\n\n\n\n\nOperator Development\n\n\n\n\n\n\nOperator Library\n\n\n\n\n\n\nDemo Videos\n\n\n\n\n\n\nApplication Development Tutorials\n\n\nExplore Apache Apex application development with one of the tutorial applications below.\n\n\n\n\n\n\nTop N Words\n is a complete guide to writing your first Apache Apex application using \nJava\n.\n\n\n\n\n\n\n\n\nSales Dimensions\n is an introduction to assembling and visualizing sales analytics application.\n\n\n\n\n\n\n\n\nAdvanced Topics\n\n\n\n\nApplication Development\n - comprehensive guide to developing Apache Apex applications\n\n\nApplication Packaging\n - creating application packages, changing settings, and launching application packages\n\n\nOperator Development\n - creating new operators for Apache Apex applications\n\n\ndtGateway REST API\n - complete listing of all services offered by dtGateway", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#creating-applications", 
            "text": "Here are a few links to get you started:    Beginner's Guide  An introductory guide that shows you how to generate a new\n  ready-to-run Apex application using the maven archetype and a good deal of the application\n  landscape at an introductory level.    Operator Development    Operator Library    Demo Videos", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#application-development-tutorials", 
            "text": "Explore Apache Apex application development with one of the tutorial applications below.    Top N Words  is a complete guide to writing your first Apache Apex application using  Java .     Sales Dimensions  is an introduction to assembling and visualizing sales analytics application.", 
            "title": "Application Development Tutorials"
        }, 
        {
            "location": "/create/#advanced-topics", 
            "text": "Application Development  - comprehensive guide to developing Apache Apex applications  Application Packaging  - creating application packages, changing settings, and launching application packages  Operator Development  - creating new operators for Apache Apex applications  dtGateway REST API  - complete listing of all services offered by dtGateway", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/beginner/", 
            "text": "Beginner's Guide to Apache Apex\n\n\nIntroduction\n\n\nApache Apex is a fault-tolerant, high-performance platform and framework for building\ndistributed applications; it is built on Hadoop. This guide is targeted at Java\ndevelopers who are getting started with building Apex applications.\n\n\nQuickstart\n\n\nThose eager to get started right away can download, build, and run a few sample\napplications; just follow these steps:\n\n\n\n\nMake sure you have \nJava JDK\n, \nmaven\n and \ngit\n installed.\n\n\nClone the examples git repo: \nhttps://github.com/DataTorrent/examples\n\n\nSwitch to \ntutorials/fileOutput\n and build it either in your favorite IDE or on the command line with:\n\n\n\n\ncd examples/tutorials/fileOutput\nmvn clean package -DskipTests\n\n\n\n\n\n\nRun the test in your IDE or on the command line with:\n\n\n\n\nmvn test\n\n\n\n\nSome of the applications are discussed in the \nSample Applications\n section below.\nThe rest of this document provides a more leisurely introduction to Apex.\n\n\nPreliminaries\n\n\nBefore beginning development, you'll need to make sure the following prerequisites\nare present; details of setting up your development environment are\n\nhere\n:\n\n\n\n\nRecent versions of the Java JDK, maven, git.\n\n\nA Hadoop cluster where the application can be deployed.\n\n\nA working internet connection.\n\n\n\n\nRunning the maven archetype\n\n\nApex applications use the \nmaven\n build tool. The maven archetype is useful for avoiding\nthe tedious process of creating a suitable \npom.xml\n maven build file by hand; it also\ngenerates a simple default application with two operators that you can build and run\nwithout having to write any code or make any changes. To run it, place the following\nlines in a text file (called, say \nnewapp.sh\n) and run it with the shell\n(e.g. \nbash newapp.sh\n):\n\n\nv=\n3.3.0-incubating\n\nmvn -B archetype:generate \\\n  -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=\n$v\n \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.myapexapp \\\n  -DartifactId=myapexapp \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\nAs new versions are released, you might need to update the version number (the\nthe variable \nv\n above). You can also run the archetype from your Java IDE as described\n\nhere\n.\n\n\nIt should create a new project directory named \nmyapexapp\n with these 3 Java source files:\n\n\nsrc/test/java/com/example/myapexapp/ApplicationTest.java\nsrc/main/java/com/example/myapexapp/Application.java\nsrc/main/java/com/example/myapexapp/RandomNumberGenerator.java\n\n\n\n\nThe project should also contain these properties files:\n\n\nsrc/site/conf/my-app-conf1.xml\nsrc/main/resources/META-INF/properties.xml\n\n\n\n\nYou should now be able to step into the new directory and build the project:\n\n\ncd myapexapp; mvn clean package -DskipTests\n\n\n\n\nThis will create a directory named \ntarget\n and an application package file\nwithin it named \nmyapexapp-1.0-SNAPSHOT.apa\n.\n\n\nThese files are discussed further in the sections below.\n\n\nThe default application\n\n\nWe now discuss the default application generated by the archetype in some detail.\nAdditional, more realistic applications are presented in the section titled\n\nSample Applications\n below.\n\n\nThe default application creates an application in \nApplication.java\n with 2 operators:\n\n\n@ApplicationAnnotation(name=\nMyFirstApplication\n)\npublic class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf) {\n    RandomNumberGenerator randomGenerator = dag.addOperator(\nrandomGenerator\n, RandomNumberGenerator.class);\n    randomGenerator.setNumTuples(500);\n    ConsoleOutputOperator cons = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\n    dag.addStream(\nrandomData\n, randomGenerator.out, cons.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}\n\n\n\n\nThe application is named \nMyFirstApplication\n via the annotation\n\n@ApplicationAnnotation\n; this name will be displayed in the UI console and must be\nunique among the applications running on a cluster. It can also be changed at launch time.\n\n\nThe \npopulateDAG\n method is the only one that you'll need to implement. Its contents\nfall into three categories: operator creation, operator configuration, and stream\ncreation.\n\n\nTwo operators are created: \nrandomGenerator\n and \nconsole\n. The first is\ndefined in \nRandomNumberGenerator.java\n; it generates random\nfloating point values and emits them on its output port. The second is an instance\nof \nConsoleOutputOperator\n class defined in \nMalhar\n \n the library of pre-built\noperators. The first argument to \naddOperator()\n is the name of this operator instance\n(there can be multiple instances of the same class, so we need a unique name to\ndistinguish them); the second can be either a class object that needs to be instantiated\nas shown for \nrandomGenerator\n or an actual instance of that class as shown for \nconsole\n.\n\n\nThe operators can be configured by calling setter methods on them; the call to\n\nsetNumTuples\n is an example. However, operators are typically configured via XML\nproperties files as discussed in later sections below.\n\n\nFinally, the \naddStream\n call creates a stream named \nrandomData\n connecting the\noutput port of first operator to the input port of the second.\n\n\nRunning the application and the unit test\n\n\nThe file \nApplicationTest.java\n contains a unit test that can be run from an IDE\nby highlighting the \ntestApplication\n method and selecting the appropriate\noption from the dropdown; it can also be run the maven command line:\n\n\nmvn -Dtest=ApplicationTest#testApplication test\n\n\n\n\nIt runs for about 10 seconds printing each random number with a prefix of \nhello world:\n.\nThe first argument explicitly selects the test to run (\ntestApplication\n)\nfrom the named class (\nApplicationTest\n); you can omit it and just run \nmvn test\n to\nrun all of the unit tests.\n\n\nIt is important to note that this particular test is actually a test of the entire\napplication rather than a single class or a method within a class. It uses a class\ncalled \nLocalMode\n to essentially simulate a cluster. It is an extremely useful\ntechnique for testing your application without the need for a cluster. It can be used\nin more elaborate ways to test complex applications as discussed in the section\nentitled \nLocal Mode Testing\n below.\n\n\nTo run the application, you need access to a cluster with Hadoop installed; there are\nmultiple options here:\n\n\n\n\nDownload and setup the sandbox as described\n  \nhere\n. This is\n  the simplest option for experimenting with Apex since it has all the necessary pieces\n  installed.\n\n\nDownload and install the free or licensed version of DataTorrent RTS from\n  \nhere\n.\n\n\nUse an existing DataTorrent RTS licensed installation.\n\n\nClone the Apex source code on a cluster with Hadoop already installed, build it and\n  use the \napexcli\n command line tool (previously named \ndtcli\n) from there to run your\n  application as described in \nthis video\n.\n\n\n\n\nWith the first 3 methods, you have a browser-based GUI console and you can simply\nnavigate to \nDevelop\n \n \nUpload Package\n and upload the \n.apa\n file built earlier;\nthen run it using the \nlaunch\n button. If using the command line tool, run that tool\nthen use the command \nlaunch myapexapp-1.0-SNAPSHOT.apa\n to launch the application. You\ncan also specify a particular XML configuration file that is packaged with the\napplication to use during launch, for example:\n\nlaunch -apconf my-app-conf1.xml myapexapp-1.0-SNAPSHOT.apa\n\nFor an exhaustive list of commands available with this tool, please see\n\nhere\n.\n\n\nMore details about configuration files are provided in the next section.\n\n\nConfiguring your application\n\n\nApplication configuration is typically done via XML properties files though it can also\nbe done with code as shown above. The \nproperties.xml\n file mentioned earlier has some\nexamples:\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.randomGenerator.prop.numTuples\n/name\n\n  \nvalue\n1000\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.console.prop.stringFormat\n/name\n\n  \nvalue\nhello world: %s\n/value\n\n\n/property\n\n\n\n\n\nThe number of tuples output per window (windows are discussed in greater detail below)\nis set both in the code and in this file; in such cases, values in this properties\nfile override those set in the code.\n\n\nConfiguration values can also be placed in XML\nfiles under \nsite/conf\n and the file \nmy-app-conf1.xml\n mentioned above is an example.\nThese files are not processed automatically; they need to be explicitly selected at\nlaunch time. To do this in the GUI, select the \nUse saved configuration\n checkbox in\nthe launch dialog and choose the desired file from the dropdown. When a property is\nspecified in multiple files, precedence rules determine the final value; those rules\nare discussed\n\nhere\n.\n\n\nAttributes and Properties\n\n\nProperties are simply public accessor methods in the operator classes and govern the\nfunctionality of the operator. Attributes on the other hand are pre-defined and affect\nhow the operator or application behaves with respect to its environment. We have already\nseen a couple of examples of properties, namely, the number of tuples emitted per window\nby the random number generator (\nnumTuples\n) and the prefix string appended to each\nvalue before it is output by the console operator (\nstringFormat\n).\n\n\nOperator properties that are more complex objects than the primitive types can also\nbe initialized from XML files. For example, if we have properties declared as\n\nint[] counts;\n and \nString[] paths;\n in an operator named \nfoo\n, we can initialize\nthem with:\n\n\nproperty\n\n  \nname\ndt.operator.foo.prop.counts\n/name\n\n  \nvalue\n10, 20, 30\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.operator.foo.prop.paths\n/name\n\n  \nvalue\n/tmp/path1\n, \n/tmp/path3\n, \n/tmp/path3\n/value\n\n\n/property\n\n\n\n\n\nAn example of an attribute is the amount of memory allocated to the Buffer Server (see\nsection below entitled \nBuffer Server\n); it is named \nBUFFER_MEMORY_MB\n can be set\nlike this:\n\n\nproperty\n\n  \nname\ndt.application.{appName}.operator.{opName}.port.{portName}.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n128\n/value\n\n\n/property\n\n\n\n\n\nHere \n{appName}\n, \n{opName}\n, \n{portName}\n are appropriate application, operator\nand port names respectively; they can also be replaced with asterisks (wildcards). The\ndefault value is 512MB.\n\n\nSome additional attributes include:\n\n\n\n\n\n\n\n\nAttribute name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPARTITIONER\n\n\ncustom partitioner class associated with an operator\n\n\n\n\n\n\nPARTITION_PARALLEL\n\n\nif true, triggers parallel partitioning of a downstream operator when an upstream operator is partitioned.\n\n\n\n\n\n\nSTREAM_CODEC\n\n\ncontrols serialization/deserialization as well as destination partitions\n\n\n\n\n\n\nRECOVERY_ATTEMPTS\n\n\nmaximum restart attempts of a failed operator\n\n\n\n\n\n\n\n\nCurrently available attribute names are in \nContext\n class of the \napi\n module in\nApex source code.\n\n\nFor additional examples of initialization of properties, including list and map\nobjects, please look\n\nhere\n.\n\n\nLocal Mode Testing\n\n\nAs noted above, the \nLocalMode\n class is used for testing the application locally in\nyour development environment. A common, though suboptimal, use looks like this:\n\n\ntry {\n  LocalMode lma = LocalMode.newInstance();\n  Configuration conf = new Configuration(false);\n  conf.addResource(this.getClass().getResourceAsStream(\n/META-INF/properties.xml\n));\n  lma.prepareDAG(new Application(), conf);\n  LocalMode.Controller lc = lma.getController();\n  lc.run(10000); // runs for 10 seconds and quits\n} catch (ConstraintViolationException e) {\n  Assert.fail(\nconstraint violations: \n + e.getConstraintViolations());\n}\n\n\n\n\nHere, a \nConfiguration\n object containing all the appropriate settings of properties and\nattributes for the application is created by parsing the default \nproperties.xml\n file,\na new \nApplication\n object is created and configured and finally a controller used for\ntimed execution of the application. This approach, though occasionally useful to uncover\nshallow bugs, has one glaring deficiency \n it does not check the results in any way\nas most unit tests do. We strongly recommend avoiding this usage pattern.\n\n\nA far better (and recommended) approach is to write data to some storage system\nthat can then be queried for verification. An example of this approach is\n\nhere\n and looks like this:\n\n\ntry {\n  LocalMode lma = LocalMode.newInstance();\n  lma.prepareDAG(new Application(), getConfig());\n  LocalMode.Controller lc = lma.getController();\n  lc.runAsync();\n\n  // wait for output files to show up\n  while ( ! check() ) {\n    System.out.println(\nSleeping ....\n);\n    Thread.sleep(1000);\n  }\n} catch (ConstraintViolationException e) {\n  Assert.fail(\nconstraint violations: \n + e.getConstraintViolations());\n}\n\n\n\n\n\nHere, we invoke \nrunAsync\n on the controller to fork off a separate thread to run the\napplication while the main thread enters a loop where it looks for the presence of the\nexpected output files. A pair of functions can be defined to setup prerequisites such\nas starting external servers, creating directories, etc. and perform the corresponding\nteardown upon test termination; these functions are annotated with \n@Before\n and \n@After\n.\n\n\nChecking logs\n\n\nLogs for the Application Master and the various containers can be retrieved and viewed\non the UI console by navigating to the \nPhysical\n tab, clicking on the specific container\nin question, clicking on the blue \nlogs\n button and then selecting the appropriate file\nfrom the dropdown. If you don't have access to the UI, you'll need to log in to the\nappropriate node on the cluster and check the logs there.\n\n\nA good starting point is the\nYARN log file which is usually present at \n/var/log/hadoop-yarn\n or \n/var/log/hadoop\n or\nsimilar locations and named \nyarn-{user}-resourcemanager-{host}.log\n or\n\nhadoop-cmf-yarn-RESOURCEMANAGER-{host}.log.out\n (where \n{user}\n and \n{host}\n have\nappropriate values) or something similar. This file will\nindicate what containers were allocated for each application, whether the allocation\nsucceeded or failed and where each container is running. Typically, application and\ncontainer ids will have the respective forms \napplication_1448033276100_0001\n and\n\ncontainer_1462948052533_0001_01_022468\n.\n\n\nIf the application failed \nduring\n launch,\nthe YARN logs will usually have enough information to diagnose the root cause -- for\nexample, a container requiring more memory than is available. If the application launch\nwas successful, you'll see the containers transition through various states: \nNEW\n,\n\nALLOCATED\n, \nACQUIRED\n, \nRUNNING\n.\n\n\nIf it failed \nafter\n launch, the logs of the particular container that failed will shed\nmore light on the issue. Those logs are located on the appropriate node under a\ndirectory with the same name as the container id which itself is under a directory\nnamed with the application id, for example:\n\n\ndtadmin@dtbox:/sfw/hadoop/shared/logs$ cd nodemanager/application_1465857463845_0001/\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls\ncontainer_1465857463845_0001_01_000001  container_1465857463845_0001_01_000003\ncontainer_1465857463845_0001_01_000002  container_1465857463845_0001_01_000004\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *1\nAppMaster.stderr  AppMaster.stdout  dt.log\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *2\ndt.log  stderr  stdout\n\n\n\n\nThe Application Master container id always has the suffix \n_000001\n.\n\n\nOperators, Ports and Streams\n\n\nAn \noperator\n is, simply put, a class that\nimplements the \nOperator\n interface. Though the class could be written to directly\nimplement that interface, a more common and easier method is to extend \nBaseOperator\n\nsince it provides default empty implementations of all the required methods.\nWe've already seen an example above, namely \nRandomNumberGenerator\n.\n\n\nA \nport\n is a class that can either emit (output port) or ingest (input port) data.\nInput and output ports implement the \nInputPort\n and \nOutputPort\n interfaces\nrespectively. More commonly, output ports are simply defined as instances of\n\nDefaultOutputPort\n, for example:\n\n\npublic final transient DefaultOutputPort\nDouble\n out = new DefaultOutputPort\nDouble\n();\n\n\n\n\nand input ports are defined as anonymous inner classes that extend \nDefaultInputPort\n:\n\n\npublic final transient DefaultInputPort\nDouble\n input = new DefaultInputPort\nDouble\n()\n{\n  @Override\n  public void process(Double v) {\n    out.emit(v);\n  }\n}\n\n\n\n\nA \nstream\n is the set of links connecting a single port of an upstream operator to one or\nmore input ports of downstream operators. We've already seen an example of a stream above,\nnamely \nrandomData\n. If the upstream operator is not partitioned, tuples are delivered to\nthe input ports of a stream in the same order in which they were written to the output\nport; this guarantee may change in the future. For a more detailed explanation of these\nconcepts, please\nlook \nhere\n.\n\n\nAnnotations\n\n\nAnnotations are an important tool for expressing desired guarantees which are then\nverified in a validation phase before running the application. Some examples:\n\n\n@Min(1)\nint index;\n\n@NotNull\nString name\n\n@NotNull\n@Size(min=1, max=128)\nprivate String[] path;\n\n@NotNull\n@Size(min = 1)\nprivate final Set\nString\n files;\n\n@Valid\nFooBar object;\n\n\n\n\nThe \n@Min\n and \n@Max\n annotations check lower and upper bounds; \n@Size\n checks the\nsize of a collection or array against minimum and maximum limits; \n@Valid\n requests\nrecursive validation checks on the object.\n\n\nThere are also a few Apex-specific annotations; we've seen one example above, namely\n\n@ApplicationAnnotation\n used to set the name of the application. A couple of others\nare useful to declare that a port within an operator need not be connected:\n\n\n@InputPortFieldAnnotation(optional = true)\npublic final transient InputPort\nObject\n inportWithCodec = new DefaultInputPort\n();\n\n@OutputPortFieldAnnotation(optional = true)\npublic final transient DefaultOutputPort\nMap\nString, Object\n outBindings\n   = new DefaultOutputPort\n();\n\n\n\n\nFor additional information about annotations, please see:\n\nhere\n and\n\nhere\n\n\nPartitioners, Unifiers and StreamCodecs\n\n\nPartitioning is a mechanism for load balancing; it involves replicating one or more\noperators so that the load can be shared by all the replicas. Partitioning is\naccomplished by the \ndefinePartitions\n method of the \nPartitioner\n interface.\nA couple of implementations are available: \nStatelessPartitioner\n (static partitioning)\nand \nStatelessThroughputBasedPartitioner\n (dynamic partitioning, see below).\nThese can be used by setting the \nPARTITIONER\n\nattribute on the operator by including a stanza like the following in your\nconfiguration file (where \n{appName}\n and \n{opName}\n are the\nappropriate application and operator names):\n\n\nproperty\n\n  \nname\ndt.application.{appName}.operator.{opName}.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\n\nThe number after the colon specifies the number of desired partitions. This can be\ndone for any operator that is not a connector (i.e. input or output operator) and is not\nannotated with \n@OperatorAnnotation(partitionable = false)\n. No code changes are necessary.\nIncoming tuples entering input ports of the operator are automatically distributed among the\npartitions based on their hash code by default. You can get greater control of how tuples\nare distributed to partitions by using a \nStreamCodec\n; further discussion of stream\ncodecs is deferred to the Advanced Guide [\ncoming soon\n].\nA small sample program illustrating use of stream codecs is\n\nhere\n.\n\n\nConnectors need special care since they interact with external systems. Many connectors\n(e.g. Kafka input, file input and output operators) implement the \nPartitioner\n\ninterface and support partitioning using custom implementations of \ndefinePartitions\n\n\nwithin\n the operator.\nDocumentation and/or source code of the individual connectors should be consulted for\ndetails.\n\n\nSometimes there is need to replicate an entire linear segment of the DAG; this is\nknown as \nParallel Partitioning\n and is achieved by setting the \nPARTITION_PARALLEL\n\nattribute on the input port of each downstream operator that is part of the linear\nsegment. Both of these mechanisms are described in greater detail in the\n\nAdvanced Features\n section of the\n\nTop N Words tutorial\n\n\nAs mentioned above, the \nStatelessPartitioner\n is used for \nstatic\n\npartitioning since it occurs once before the application starts. Dynamic partitioning\nwhile the application is running is also possible using the\n\nStatelessThroughputBasedPartitioner\n or a custom partitioner. Implementing such\na partitioner needs special care, especially if the operator to be partitioned\nhas some accumulated state since this state typically needs to be redistributed among\nthe newly created partitions. An example of a custom partitioner that does dynamic\npartitioning is \nhere\n.\n\n\nUnifiers are the flip side of the partitioning coin: When data that was intended to be\nprocessed by a single instance is now processed by multiple partitions, each instance\ncomputes partial results since it processes only part of the stream; these partial\nresults need to be combined to form the final result; this is the function of a unifier.\n\n\nFor example, suppose an operator is processing numbers and computes the sum of all the\nvalues seen in a window. If it is partitioned into N replicas, each replica is computing\na partial sum and we would need a unifier that computes the overall sum from\nthese N partial sums. A sample application that shows how to define and use a unifier is\navailable \nhere\n.\n\n\nA unifier for an operator is provided by a suitable override of the \ngetUnifier()\n method\nof the output port, for example:\n\n\n  public final transient DefaultOutputPort\nHighLow\nInteger\n out\n    = new DefaultOutputPort\nHighLow\nInteger\n() {\n    @Override\n    public Unifier\nHighLow\nInteger\n getUnifier() {\n      return new UnifierRange\nInteger\n()\n    }\n  };\n\n\n\n\nIf no unifier is supplied for a partitioned operator, the platform will supply a default\npass-through unifier.\n\n\nWhen the number of partitions is large and the unifier involves non-trivial computations\nthere is a risk that it can become a bottleneck; in such cases, the \nUNIFIER_LIMIT\n\nattribute can be set on the appropriate output port. The platform will then automatically\ngenerate the required number of parallel unifiers, cascading into multiple levels if\nnecessary, to ensure that the number of input streams at each unifier does not exceed\nthis limit.\n\n\nBuffer Server\n\n\nThe \nBuffer Server\n is a separate service within a container which implements\na publish-subscribe model. It is present whenever the container hosts an operator\nwith an output port connected to another operator outside the container.\n\n\nThe output port is the publisher and the connected input ports of downstream operators\nare the subscribers. It buffers tuples so that they can be replayed when a\ndownstream operator fails and is restarted. As described earlier, the memory allocated\nto a buffer server is user configurable via an attribute named \nBUFFER_MEMORY_MB\n and\ndefaults to 512MB.\n\n\nThe total memory required by a container that hosts many such operators may climb rapidly;\nreducing the value of this attribute is advisable in such cases in a memory constrained\nenvironment.\n\n\nAllocating Operator Memory\n\n\nA container is a JVM process; the maximum memory each such container can consume is\nuser configurable via the \nMEMORY_MB\n attribute whose default value is 1GB.\nIf it is too large, container allocation may fail before the application even\nbegins; if it is too small, the application may fail at run time as it tries to\nallocate memory and runs up against this limit.\nAn example of setting this limit:\n\n\nproperty\n\n  \nname\ndt.application.MyFirstApplication.operator.randomGenerator.attr.MEMORY_MB\n/name\n\n  \nvalue\n300\n/value\n\n\n/property\n\n\n\n\n\nAs before, wildcards can be used to set the same value for all operators by replacing\nthe operator name \nrandomGenerator\n with an asterisk. The application name can also be\nomitted, so a shorter version is:\n\n\nproperty\n\n  \nname\ndt.operator.*.attr.MEMORY_MB\n/name\n\n  \nvalue\n300\n/value\n\n\n/property\n\n\n\n\n\nThe Application Master (aka \nStrAM\n) is a supervisory Java process associated with each\napplication; memory allocation for it is specified slightly differently:\n\n\nproperty\n\n  \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n  \nvalue\n700\n/value\n\n\n/property\n\n\n\n\n\nFor small applications, a value as low as 700 may be adequate; for larger applications,\na value 2000 or more may be needed. If this value is too small, the application typically\nfails at startup with no user-visible diagnostics; YARN logs need to be examined in such\ncases.\n\n\nSample Applications\n\n\nThis section briefly discusses some sample applications using commonly used connectors.\nThe applications themselves are available at\n\nExamples\n.\nWe briefly describe a few of them below.\n\n\nEach example has a brief \nREADME.md\n file (in markdown format) describing what the\napplication does. In most cases, the unit tests function as full application tests that\ncan be run locally in your development environment without the need for a cluster as\ndescribed above.\n\n\nApplication \n file copy\n\n\nThe \nfileIO-simple\n application copies all data verbatim from files added to an input\ndirectory to rolling files in an output directory. The input and output directories,\nthe output file base name and maximum size are configured in \nMETA_INF/properties.xml\n:\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.input.prop.directory\n/name\n\n  \nvalue\n/tmp/SimpleFileIO/input-dir\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.filePath\n/name\n\n  \nvalue\n/tmp/SimpleFileIO/output-dir\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.fileName\n/name\n\n  \nvalue\nmyfile\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.application.SimpleFileIO.operator.output.prop.maxLength\n/name\n\n  \nvalue\n1000000\n/value\n\n\n/property\n\n\n\n\n\nThe application DAG is created in \nApplication.java\n:\n\n\n@Override\npublic void populateDAG(DAG dag, Configuration conf)\n{\n  // create operators\n  FileLineInputOperator in = dag.addOperator(\ninput\n, new FileLineInputOperator());\n  FileOutputOperator out = dag.addOperator(\noutput\n, new FileOutputOperator());\n\n  // create streams\n  dag.addStream(\ndata\n, in.output, out.input);\n}\n\n\n\n\nThe \nFileLineInputOperator\n is part of Malhar and is a concrete class that extends\n\nAbstractFileInputOperator\n. The \nFileOutputOperator\n is defined locally\nand extends the \nAbstractFileOutputOperator\n and overrides 3 methods:\n+ \ngetFileName\n which simply returns the current file name\n+ \ngetBytesForTuple\n which appends a newline to the argument string, converts it to an array of bytes and returns it.\n+ \nsetup\n which creates the actual file name by appending the operator id to the configured base name (this last step is necessary when partitioning is involved to ensure that multiple partitions do not write to the same file).\n\n\nOutput files are created with temporary names like \nmyfile_p2.0.1465929407447.tmp\n and\nrenamed to \nmyfile_p2.0\n when they reach the maximum configured size.\n\n\nApplication \n database to file\n\n\nThe \njdbcIngest\n application reads rows from a table in \nMySQL\n, creates Java objects\n(_POJO_s) and writes them to a file in the user specified directory in HDFS.\n\n\nApplication configuration values are specified in 2 files: \n\nMETA_INF/properties.xml\n and \nsrc/site/conf/example.xml\n. The former uses the in-memory\ndatabase \nHSQLDB\n and is used by the unit test in JdbcInputAppTest; this test can be\nrun, as described earlier, either in your IDE or using maven on the command line.\nThe latter uses \nMySql\n and is intended for use on a cluster. To run on a cluster you'll\nneed a couple of preparatory steps:\n\n\n\n\nMake sure \nMySql\n is installed on the cluster.\n\n\nChange \nexample.xml\n to reflect proper values for \ndatabaseUrl\n, \nuserName\n, \npassword\n and \nfilePath\n.\n\n\nCreate the required table and rows by running the SQL queries in the file \nsrc/test/resources/example.sql\n.\n\n\nCreate the HDFS output directory if necessary.\n\n\nBuild the project to create the \n.apa\n package\n\n\nLaunch the application, selecting \nexample.xml\n as the configuration file during launch.\n\n\nVerify that the expected output file is present.\n\n\n\n\nFurther details on these steps are in the project \nREADME.md\n file.\n\n\nThe application uses two operators: The first is \nFileLineOutputOperator\n which extends\n\nAbstractFileOutputOperator\n and provides implementations for two methods:\n\ngetFileName\n and \ngetBytesForTuple\n. The former creates a file name using the operator\nid \n this is important if this operator has multiple partitions and prevents the\npartitions from writing to the same file (which could cause garbled data). The latter\nsimply converts the incoming object to an array of bytes and returns it.\n\n\nThe second is \nJdbcPOJOInputOperator\n which comes from Malhar; it reads records from\na table and outputs them on the output port; they type of object that is emitted is\nspecified by the value of the  \nTUPLE_CLASS\n attribute in the configuration file\nnamely \nPojoEvent\n in this case. This operator also needs a couple of additional\nproperties: (a) a list of \nFieldInfo\n objects that describe the mapping from table\ncolumns to fields of the Pojo; and (b) a \nstore\n object that deals with the details\nof establishing a connection to the database server.\n\n\nThe application itself is then created in the usual way in \nJdbcHDFSApp.populateDAG\n:\n\n\nJdbcPOJOInputOperator jdbcInputOperator = dag.addOperator(\nJdbcInput\n, new JdbcPOJOInputOperator());\njdbcInputOperator.setFieldInfos(addFieldInfos());\n\njdbcInputOperator.setStore(new JdbcStore());\n\nFileLineOutputOperator fileOutput = dag.addOperator(\nFileOutputOperator\n, new FileLineOutputOperator());\n\ndag.addStream(\nPOJO's\n, jdbcInputOperator.outputPort, fileOutput.input).setLocality(Locality.CONTAINER_LOCAL);\n\n\n\n\nAppFactory\n\n\nAppFactory\n is a source of application templates. There are many more applications for various use cases to help jump start development effort. \n\n\nAdditional Resources\n\n\n\n\n\n\nDevelopment environment setup\n\n\n\n\n\n\nTroubleshooting guide", 
            "title": "Beginner's Guide"
        }, 
        {
            "location": "/beginner/#beginners-guide-to-apache-apex", 
            "text": "", 
            "title": "Beginner's Guide to Apache Apex"
        }, 
        {
            "location": "/beginner/#introduction", 
            "text": "Apache Apex is a fault-tolerant, high-performance platform and framework for building\ndistributed applications; it is built on Hadoop. This guide is targeted at Java\ndevelopers who are getting started with building Apex applications.", 
            "title": "Introduction"
        }, 
        {
            "location": "/beginner/#quickstart", 
            "text": "Those eager to get started right away can download, build, and run a few sample\napplications; just follow these steps:   Make sure you have  Java JDK ,  maven  and  git  installed.  Clone the examples git repo:  https://github.com/DataTorrent/examples  Switch to  tutorials/fileOutput  and build it either in your favorite IDE or on the command line with:   cd examples/tutorials/fileOutput\nmvn clean package -DskipTests   Run the test in your IDE or on the command line with:   mvn test  Some of the applications are discussed in the  Sample Applications  section below.\nThe rest of this document provides a more leisurely introduction to Apex.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/beginner/#preliminaries", 
            "text": "Before beginning development, you'll need to make sure the following prerequisites\nare present; details of setting up your development environment are here :   Recent versions of the Java JDK, maven, git.  A Hadoop cluster where the application can be deployed.  A working internet connection.", 
            "title": "Preliminaries"
        }, 
        {
            "location": "/beginner/#running-the-maven-archetype", 
            "text": "Apex applications use the  maven  build tool. The maven archetype is useful for avoiding\nthe tedious process of creating a suitable  pom.xml  maven build file by hand; it also\ngenerates a simple default application with two operators that you can build and run\nwithout having to write any code or make any changes. To run it, place the following\nlines in a text file (called, say  newapp.sh ) and run it with the shell\n(e.g.  bash newapp.sh ):  v= 3.3.0-incubating \nmvn -B archetype:generate \\\n  -DarchetypeGroupId=org.apache.apex \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion= $v  \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.myapexapp \\\n  -DartifactId=myapexapp \\\n  -Dversion=1.0-SNAPSHOT  As new versions are released, you might need to update the version number (the\nthe variable  v  above). You can also run the archetype from your Java IDE as described here .  It should create a new project directory named  myapexapp  with these 3 Java source files:  src/test/java/com/example/myapexapp/ApplicationTest.java\nsrc/main/java/com/example/myapexapp/Application.java\nsrc/main/java/com/example/myapexapp/RandomNumberGenerator.java  The project should also contain these properties files:  src/site/conf/my-app-conf1.xml\nsrc/main/resources/META-INF/properties.xml  You should now be able to step into the new directory and build the project:  cd myapexapp; mvn clean package -DskipTests  This will create a directory named  target  and an application package file\nwithin it named  myapexapp-1.0-SNAPSHOT.apa .  These files are discussed further in the sections below.", 
            "title": "Running the maven archetype"
        }, 
        {
            "location": "/beginner/#the-default-application", 
            "text": "We now discuss the default application generated by the archetype in some detail.\nAdditional, more realistic applications are presented in the section titled Sample Applications  below.  The default application creates an application in  Application.java  with 2 operators:  @ApplicationAnnotation(name= MyFirstApplication )\npublic class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf) {\n    RandomNumberGenerator randomGenerator = dag.addOperator( randomGenerator , RandomNumberGenerator.class);\n    randomGenerator.setNumTuples(500);\n    ConsoleOutputOperator cons = dag.addOperator( console , new ConsoleOutputOperator());\n    dag.addStream( randomData , randomGenerator.out, cons.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}  The application is named  MyFirstApplication  via the annotation @ApplicationAnnotation ; this name will be displayed in the UI console and must be\nunique among the applications running on a cluster. It can also be changed at launch time.  The  populateDAG  method is the only one that you'll need to implement. Its contents\nfall into three categories: operator creation, operator configuration, and stream\ncreation.  Two operators are created:  randomGenerator  and  console . The first is\ndefined in  RandomNumberGenerator.java ; it generates random\nfloating point values and emits them on its output port. The second is an instance\nof  ConsoleOutputOperator  class defined in  Malhar    the library of pre-built\noperators. The first argument to  addOperator()  is the name of this operator instance\n(there can be multiple instances of the same class, so we need a unique name to\ndistinguish them); the second can be either a class object that needs to be instantiated\nas shown for  randomGenerator  or an actual instance of that class as shown for  console .  The operators can be configured by calling setter methods on them; the call to setNumTuples  is an example. However, operators are typically configured via XML\nproperties files as discussed in later sections below.  Finally, the  addStream  call creates a stream named  randomData  connecting the\noutput port of first operator to the input port of the second.", 
            "title": "The default application"
        }, 
        {
            "location": "/beginner/#running-the-application-and-the-unit-test", 
            "text": "The file  ApplicationTest.java  contains a unit test that can be run from an IDE\nby highlighting the  testApplication  method and selecting the appropriate\noption from the dropdown; it can also be run the maven command line:  mvn -Dtest=ApplicationTest#testApplication test  It runs for about 10 seconds printing each random number with a prefix of  hello world: .\nThe first argument explicitly selects the test to run ( testApplication )\nfrom the named class ( ApplicationTest ); you can omit it and just run  mvn test  to\nrun all of the unit tests.  It is important to note that this particular test is actually a test of the entire\napplication rather than a single class or a method within a class. It uses a class\ncalled  LocalMode  to essentially simulate a cluster. It is an extremely useful\ntechnique for testing your application without the need for a cluster. It can be used\nin more elaborate ways to test complex applications as discussed in the section\nentitled  Local Mode Testing  below.  To run the application, you need access to a cluster with Hadoop installed; there are\nmultiple options here:   Download and setup the sandbox as described\n   here . This is\n  the simplest option for experimenting with Apex since it has all the necessary pieces\n  installed.  Download and install the free or licensed version of DataTorrent RTS from\n   here .  Use an existing DataTorrent RTS licensed installation.  Clone the Apex source code on a cluster with Hadoop already installed, build it and\n  use the  apexcli  command line tool (previously named  dtcli ) from there to run your\n  application as described in  this video .   With the first 3 methods, you have a browser-based GUI console and you can simply\nnavigate to  Develop     Upload Package  and upload the  .apa  file built earlier;\nthen run it using the  launch  button. If using the command line tool, run that tool\nthen use the command  launch myapexapp-1.0-SNAPSHOT.apa  to launch the application. You\ncan also specify a particular XML configuration file that is packaged with the\napplication to use during launch, for example: launch -apconf my-app-conf1.xml myapexapp-1.0-SNAPSHOT.apa \nFor an exhaustive list of commands available with this tool, please see here .  More details about configuration files are provided in the next section.", 
            "title": "Running the application and the unit test"
        }, 
        {
            "location": "/beginner/#configuring-your-application", 
            "text": "Application configuration is typically done via XML properties files though it can also\nbe done with code as shown above. The  properties.xml  file mentioned earlier has some\nexamples:  property \n   name dt.application.MyFirstApplication.operator.randomGenerator.prop.numTuples /name \n   value 1000 /value  /property  property \n   name dt.application.MyFirstApplication.operator.console.prop.stringFormat /name \n   value hello world: %s /value  /property   The number of tuples output per window (windows are discussed in greater detail below)\nis set both in the code and in this file; in such cases, values in this properties\nfile override those set in the code.  Configuration values can also be placed in XML\nfiles under  site/conf  and the file  my-app-conf1.xml  mentioned above is an example.\nThese files are not processed automatically; they need to be explicitly selected at\nlaunch time. To do this in the GUI, select the  Use saved configuration  checkbox in\nthe launch dialog and choose the desired file from the dropdown. When a property is\nspecified in multiple files, precedence rules determine the final value; those rules\nare discussed here .", 
            "title": "Configuring your application"
        }, 
        {
            "location": "/beginner/#attributes-and-properties", 
            "text": "Properties are simply public accessor methods in the operator classes and govern the\nfunctionality of the operator. Attributes on the other hand are pre-defined and affect\nhow the operator or application behaves with respect to its environment. We have already\nseen a couple of examples of properties, namely, the number of tuples emitted per window\nby the random number generator ( numTuples ) and the prefix string appended to each\nvalue before it is output by the console operator ( stringFormat ).  Operator properties that are more complex objects than the primitive types can also\nbe initialized from XML files. For example, if we have properties declared as int[] counts;  and  String[] paths;  in an operator named  foo , we can initialize\nthem with:  property \n   name dt.operator.foo.prop.counts /name \n   value 10, 20, 30 /value  /property  property \n   name dt.operator.foo.prop.paths /name \n   value /tmp/path1 ,  /tmp/path3 ,  /tmp/path3 /value  /property   An example of an attribute is the amount of memory allocated to the Buffer Server (see\nsection below entitled  Buffer Server ); it is named  BUFFER_MEMORY_MB  can be set\nlike this:  property \n   name dt.application.{appName}.operator.{opName}.port.{portName}.attr.BUFFER_MEMORY_MB /name \n   value 128 /value  /property   Here  {appName} ,  {opName} ,  {portName}  are appropriate application, operator\nand port names respectively; they can also be replaced with asterisks (wildcards). The\ndefault value is 512MB.  Some additional attributes include:     Attribute name  Description      PARTITIONER  custom partitioner class associated with an operator    PARTITION_PARALLEL  if true, triggers parallel partitioning of a downstream operator when an upstream operator is partitioned.    STREAM_CODEC  controls serialization/deserialization as well as destination partitions    RECOVERY_ATTEMPTS  maximum restart attempts of a failed operator     Currently available attribute names are in  Context  class of the  api  module in\nApex source code.  For additional examples of initialization of properties, including list and map\nobjects, please look here .", 
            "title": "Attributes and Properties"
        }, 
        {
            "location": "/beginner/#local-mode-testing", 
            "text": "As noted above, the  LocalMode  class is used for testing the application locally in\nyour development environment. A common, though suboptimal, use looks like this:  try {\n  LocalMode lma = LocalMode.newInstance();\n  Configuration conf = new Configuration(false);\n  conf.addResource(this.getClass().getResourceAsStream( /META-INF/properties.xml ));\n  lma.prepareDAG(new Application(), conf);\n  LocalMode.Controller lc = lma.getController();\n  lc.run(10000); // runs for 10 seconds and quits\n} catch (ConstraintViolationException e) {\n  Assert.fail( constraint violations:   + e.getConstraintViolations());\n}  Here, a  Configuration  object containing all the appropriate settings of properties and\nattributes for the application is created by parsing the default  properties.xml  file,\na new  Application  object is created and configured and finally a controller used for\ntimed execution of the application. This approach, though occasionally useful to uncover\nshallow bugs, has one glaring deficiency   it does not check the results in any way\nas most unit tests do. We strongly recommend avoiding this usage pattern.  A far better (and recommended) approach is to write data to some storage system\nthat can then be queried for verification. An example of this approach is here  and looks like this:  try {\n  LocalMode lma = LocalMode.newInstance();\n  lma.prepareDAG(new Application(), getConfig());\n  LocalMode.Controller lc = lma.getController();\n  lc.runAsync();\n\n  // wait for output files to show up\n  while ( ! check() ) {\n    System.out.println( Sleeping .... );\n    Thread.sleep(1000);\n  }\n} catch (ConstraintViolationException e) {\n  Assert.fail( constraint violations:   + e.getConstraintViolations());\n}  Here, we invoke  runAsync  on the controller to fork off a separate thread to run the\napplication while the main thread enters a loop where it looks for the presence of the\nexpected output files. A pair of functions can be defined to setup prerequisites such\nas starting external servers, creating directories, etc. and perform the corresponding\nteardown upon test termination; these functions are annotated with  @Before  and  @After .", 
            "title": "Local Mode Testing"
        }, 
        {
            "location": "/beginner/#checking-logs", 
            "text": "Logs for the Application Master and the various containers can be retrieved and viewed\non the UI console by navigating to the  Physical  tab, clicking on the specific container\nin question, clicking on the blue  logs  button and then selecting the appropriate file\nfrom the dropdown. If you don't have access to the UI, you'll need to log in to the\nappropriate node on the cluster and check the logs there.  A good starting point is the\nYARN log file which is usually present at  /var/log/hadoop-yarn  or  /var/log/hadoop  or\nsimilar locations and named  yarn-{user}-resourcemanager-{host}.log  or hadoop-cmf-yarn-RESOURCEMANAGER-{host}.log.out  (where  {user}  and  {host}  have\nappropriate values) or something similar. This file will\nindicate what containers were allocated for each application, whether the allocation\nsucceeded or failed and where each container is running. Typically, application and\ncontainer ids will have the respective forms  application_1448033276100_0001  and container_1462948052533_0001_01_022468 .  If the application failed  during  launch,\nthe YARN logs will usually have enough information to diagnose the root cause -- for\nexample, a container requiring more memory than is available. If the application launch\nwas successful, you'll see the containers transition through various states:  NEW , ALLOCATED ,  ACQUIRED ,  RUNNING .  If it failed  after  launch, the logs of the particular container that failed will shed\nmore light on the issue. Those logs are located on the appropriate node under a\ndirectory with the same name as the container id which itself is under a directory\nnamed with the application id, for example:  dtadmin@dtbox:/sfw/hadoop/shared/logs$ cd nodemanager/application_1465857463845_0001/\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls\ncontainer_1465857463845_0001_01_000001  container_1465857463845_0001_01_000003\ncontainer_1465857463845_0001_01_000002  container_1465857463845_0001_01_000004\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *1\nAppMaster.stderr  AppMaster.stdout  dt.log\ndtadmin@dtbox:/sfw/hadoop/shared/logs/nodemanager/application_1465857463845_0001$ ls *2\ndt.log  stderr  stdout  The Application Master container id always has the suffix  _000001 .", 
            "title": "Checking logs"
        }, 
        {
            "location": "/beginner/#operators-ports-and-streams", 
            "text": "An  operator  is, simply put, a class that\nimplements the  Operator  interface. Though the class could be written to directly\nimplement that interface, a more common and easier method is to extend  BaseOperator \nsince it provides default empty implementations of all the required methods.\nWe've already seen an example above, namely  RandomNumberGenerator .  A  port  is a class that can either emit (output port) or ingest (input port) data.\nInput and output ports implement the  InputPort  and  OutputPort  interfaces\nrespectively. More commonly, output ports are simply defined as instances of DefaultOutputPort , for example:  public final transient DefaultOutputPort Double  out = new DefaultOutputPort Double ();  and input ports are defined as anonymous inner classes that extend  DefaultInputPort :  public final transient DefaultInputPort Double  input = new DefaultInputPort Double ()\n{\n  @Override\n  public void process(Double v) {\n    out.emit(v);\n  }\n}  A  stream  is the set of links connecting a single port of an upstream operator to one or\nmore input ports of downstream operators. We've already seen an example of a stream above,\nnamely  randomData . If the upstream operator is not partitioned, tuples are delivered to\nthe input ports of a stream in the same order in which they were written to the output\nport; this guarantee may change in the future. For a more detailed explanation of these\nconcepts, please\nlook  here .", 
            "title": "Operators, Ports and Streams"
        }, 
        {
            "location": "/beginner/#annotations", 
            "text": "Annotations are an important tool for expressing desired guarantees which are then\nverified in a validation phase before running the application. Some examples:  @Min(1)\nint index;\n\n@NotNull\nString name\n\n@NotNull\n@Size(min=1, max=128)\nprivate String[] path;\n\n@NotNull\n@Size(min = 1)\nprivate final Set String  files;\n\n@Valid\nFooBar object;  The  @Min  and  @Max  annotations check lower and upper bounds;  @Size  checks the\nsize of a collection or array against minimum and maximum limits;  @Valid  requests\nrecursive validation checks on the object.  There are also a few Apex-specific annotations; we've seen one example above, namely @ApplicationAnnotation  used to set the name of the application. A couple of others\nare useful to declare that a port within an operator need not be connected:  @InputPortFieldAnnotation(optional = true)\npublic final transient InputPort Object  inportWithCodec = new DefaultInputPort ();\n\n@OutputPortFieldAnnotation(optional = true)\npublic final transient DefaultOutputPort Map String, Object  outBindings\n   = new DefaultOutputPort ();  For additional information about annotations, please see: here  and here", 
            "title": "Annotations"
        }, 
        {
            "location": "/beginner/#partitioners-unifiers-and-streamcodecs", 
            "text": "Partitioning is a mechanism for load balancing; it involves replicating one or more\noperators so that the load can be shared by all the replicas. Partitioning is\naccomplished by the  definePartitions  method of the  Partitioner  interface.\nA couple of implementations are available:  StatelessPartitioner  (static partitioning)\nand  StatelessThroughputBasedPartitioner  (dynamic partitioning, see below).\nThese can be used by setting the  PARTITIONER \nattribute on the operator by including a stanza like the following in your\nconfiguration file (where  {appName}  and  {opName}  are the\nappropriate application and operator names):  property \n   name dt.application.{appName}.operator.{opName}.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   The number after the colon specifies the number of desired partitions. This can be\ndone for any operator that is not a connector (i.e. input or output operator) and is not\nannotated with  @OperatorAnnotation(partitionable = false) . No code changes are necessary.\nIncoming tuples entering input ports of the operator are automatically distributed among the\npartitions based on their hash code by default. You can get greater control of how tuples\nare distributed to partitions by using a  StreamCodec ; further discussion of stream\ncodecs is deferred to the Advanced Guide [ coming soon ].\nA small sample program illustrating use of stream codecs is here .  Connectors need special care since they interact with external systems. Many connectors\n(e.g. Kafka input, file input and output operators) implement the  Partitioner \ninterface and support partitioning using custom implementations of  definePartitions  within  the operator.\nDocumentation and/or source code of the individual connectors should be consulted for\ndetails.  Sometimes there is need to replicate an entire linear segment of the DAG; this is\nknown as  Parallel Partitioning  and is achieved by setting the  PARTITION_PARALLEL \nattribute on the input port of each downstream operator that is part of the linear\nsegment. Both of these mechanisms are described in greater detail in the Advanced Features  section of the Top N Words tutorial  As mentioned above, the  StatelessPartitioner  is used for  static \npartitioning since it occurs once before the application starts. Dynamic partitioning\nwhile the application is running is also possible using the StatelessThroughputBasedPartitioner  or a custom partitioner. Implementing such\na partitioner needs special care, especially if the operator to be partitioned\nhas some accumulated state since this state typically needs to be redistributed among\nthe newly created partitions. An example of a custom partitioner that does dynamic\npartitioning is  here .  Unifiers are the flip side of the partitioning coin: When data that was intended to be\nprocessed by a single instance is now processed by multiple partitions, each instance\ncomputes partial results since it processes only part of the stream; these partial\nresults need to be combined to form the final result; this is the function of a unifier.  For example, suppose an operator is processing numbers and computes the sum of all the\nvalues seen in a window. If it is partitioned into N replicas, each replica is computing\na partial sum and we would need a unifier that computes the overall sum from\nthese N partial sums. A sample application that shows how to define and use a unifier is\navailable  here .  A unifier for an operator is provided by a suitable override of the  getUnifier()  method\nof the output port, for example:    public final transient DefaultOutputPort HighLow Integer  out\n    = new DefaultOutputPort HighLow Integer () {\n    @Override\n    public Unifier HighLow Integer  getUnifier() {\n      return new UnifierRange Integer ()\n    }\n  };  If no unifier is supplied for a partitioned operator, the platform will supply a default\npass-through unifier.  When the number of partitions is large and the unifier involves non-trivial computations\nthere is a risk that it can become a bottleneck; in such cases, the  UNIFIER_LIMIT \nattribute can be set on the appropriate output port. The platform will then automatically\ngenerate the required number of parallel unifiers, cascading into multiple levels if\nnecessary, to ensure that the number of input streams at each unifier does not exceed\nthis limit.", 
            "title": "Partitioners, Unifiers and StreamCodecs"
        }, 
        {
            "location": "/beginner/#buffer-server", 
            "text": "The  Buffer Server  is a separate service within a container which implements\na publish-subscribe model. It is present whenever the container hosts an operator\nwith an output port connected to another operator outside the container.  The output port is the publisher and the connected input ports of downstream operators\nare the subscribers. It buffers tuples so that they can be replayed when a\ndownstream operator fails and is restarted. As described earlier, the memory allocated\nto a buffer server is user configurable via an attribute named  BUFFER_MEMORY_MB  and\ndefaults to 512MB.  The total memory required by a container that hosts many such operators may climb rapidly;\nreducing the value of this attribute is advisable in such cases in a memory constrained\nenvironment.", 
            "title": "Buffer Server"
        }, 
        {
            "location": "/beginner/#allocating-operator-memory", 
            "text": "A container is a JVM process; the maximum memory each such container can consume is\nuser configurable via the  MEMORY_MB  attribute whose default value is 1GB.\nIf it is too large, container allocation may fail before the application even\nbegins; if it is too small, the application may fail at run time as it tries to\nallocate memory and runs up against this limit.\nAn example of setting this limit:  property \n   name dt.application.MyFirstApplication.operator.randomGenerator.attr.MEMORY_MB /name \n   value 300 /value  /property   As before, wildcards can be used to set the same value for all operators by replacing\nthe operator name  randomGenerator  with an asterisk. The application name can also be\nomitted, so a shorter version is:  property \n   name dt.operator.*.attr.MEMORY_MB /name \n   value 300 /value  /property   The Application Master (aka  StrAM ) is a supervisory Java process associated with each\napplication; memory allocation for it is specified slightly differently:  property \n   name dt.attr.MASTER_MEMORY_MB /name \n   value 700 /value  /property   For small applications, a value as low as 700 may be adequate; for larger applications,\na value 2000 or more may be needed. If this value is too small, the application typically\nfails at startup with no user-visible diagnostics; YARN logs need to be examined in such\ncases.", 
            "title": "Allocating Operator Memory"
        }, 
        {
            "location": "/beginner/#sample-applications", 
            "text": "This section briefly discusses some sample applications using commonly used connectors.\nThe applications themselves are available at Examples .\nWe briefly describe a few of them below.  Each example has a brief  README.md  file (in markdown format) describing what the\napplication does. In most cases, the unit tests function as full application tests that\ncan be run locally in your development environment without the need for a cluster as\ndescribed above.", 
            "title": "Sample Applications"
        }, 
        {
            "location": "/beginner/#application-file-copy", 
            "text": "The  fileIO-simple  application copies all data verbatim from files added to an input\ndirectory to rolling files in an output directory. The input and output directories,\nthe output file base name and maximum size are configured in  META_INF/properties.xml :  property \n   name dt.application.SimpleFileIO.operator.input.prop.directory /name \n   value /tmp/SimpleFileIO/input-dir /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.filePath /name \n   value /tmp/SimpleFileIO/output-dir /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.fileName /name \n   value myfile /value  /property  property \n   name dt.application.SimpleFileIO.operator.output.prop.maxLength /name \n   value 1000000 /value  /property   The application DAG is created in  Application.java :  @Override\npublic void populateDAG(DAG dag, Configuration conf)\n{\n  // create operators\n  FileLineInputOperator in = dag.addOperator( input , new FileLineInputOperator());\n  FileOutputOperator out = dag.addOperator( output , new FileOutputOperator());\n\n  // create streams\n  dag.addStream( data , in.output, out.input);\n}  The  FileLineInputOperator  is part of Malhar and is a concrete class that extends AbstractFileInputOperator . The  FileOutputOperator  is defined locally\nand extends the  AbstractFileOutputOperator  and overrides 3 methods:\n+  getFileName  which simply returns the current file name\n+  getBytesForTuple  which appends a newline to the argument string, converts it to an array of bytes and returns it.\n+  setup  which creates the actual file name by appending the operator id to the configured base name (this last step is necessary when partitioning is involved to ensure that multiple partitions do not write to the same file).  Output files are created with temporary names like  myfile_p2.0.1465929407447.tmp  and\nrenamed to  myfile_p2.0  when they reach the maximum configured size.", 
            "title": "Application &mdash; file copy"
        }, 
        {
            "location": "/beginner/#application-database-to-file", 
            "text": "The  jdbcIngest  application reads rows from a table in  MySQL , creates Java objects\n(_POJO_s) and writes them to a file in the user specified directory in HDFS.  Application configuration values are specified in 2 files:  META_INF/properties.xml  and  src/site/conf/example.xml . The former uses the in-memory\ndatabase  HSQLDB  and is used by the unit test in JdbcInputAppTest; this test can be\nrun, as described earlier, either in your IDE or using maven on the command line.\nThe latter uses  MySql  and is intended for use on a cluster. To run on a cluster you'll\nneed a couple of preparatory steps:   Make sure  MySql  is installed on the cluster.  Change  example.xml  to reflect proper values for  databaseUrl ,  userName ,  password  and  filePath .  Create the required table and rows by running the SQL queries in the file  src/test/resources/example.sql .  Create the HDFS output directory if necessary.  Build the project to create the  .apa  package  Launch the application, selecting  example.xml  as the configuration file during launch.  Verify that the expected output file is present.   Further details on these steps are in the project  README.md  file.  The application uses two operators: The first is  FileLineOutputOperator  which extends AbstractFileOutputOperator  and provides implementations for two methods: getFileName  and  getBytesForTuple . The former creates a file name using the operator\nid   this is important if this operator has multiple partitions and prevents the\npartitions from writing to the same file (which could cause garbled data). The latter\nsimply converts the incoming object to an array of bytes and returns it.  The second is  JdbcPOJOInputOperator  which comes from Malhar; it reads records from\na table and outputs them on the output port; they type of object that is emitted is\nspecified by the value of the   TUPLE_CLASS  attribute in the configuration file\nnamely  PojoEvent  in this case. This operator also needs a couple of additional\nproperties: (a) a list of  FieldInfo  objects that describe the mapping from table\ncolumns to fields of the Pojo; and (b) a  store  object that deals with the details\nof establishing a connection to the database server.  The application itself is then created in the usual way in  JdbcHDFSApp.populateDAG :  JdbcPOJOInputOperator jdbcInputOperator = dag.addOperator( JdbcInput , new JdbcPOJOInputOperator());\njdbcInputOperator.setFieldInfos(addFieldInfos());\n\njdbcInputOperator.setStore(new JdbcStore());\n\nFileLineOutputOperator fileOutput = dag.addOperator( FileOutputOperator , new FileLineOutputOperator());\n\ndag.addStream( POJO's , jdbcInputOperator.outputPort, fileOutput.input).setLocality(Locality.CONTAINER_LOCAL);", 
            "title": "Application &mdash; database to file"
        }, 
        {
            "location": "/beginner/#appfactory", 
            "text": "AppFactory  is a source of application templates. There are many more applications for various use cases to help jump start development effort.", 
            "title": "AppFactory"
        }, 
        {
            "location": "/beginner/#additional-resources", 
            "text": "Development environment setup    Troubleshooting guide", 
            "title": "Additional Resources"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nThe following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance. \n\n\nApplication Builder\n\n\n\n\nDimensional Computing\n\n\n\n\nElastic Scalability\n\n\n\n\nFault Tolerance", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "The following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance.", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demo_videos/#application-builder", 
            "text": "", 
            "title": "Application Builder"
        }, 
        {
            "location": "/demo_videos/#dimensional-computing", 
            "text": "", 
            "title": "Dimensional Computing"
        }, 
        {
            "location": "/demo_videos/#elastic-scalability", 
            "text": "", 
            "title": "Elastic Scalability"
        }, 
        {
            "location": "/demo_videos/#fault-tolerance", 
            "text": "", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/tutorials/topnwords/", 
            "text": "Top N Words Application\n\n\nThe Top N words application is a tutorial on building a word counting application using:\n\n\n\n\nApache Apex platform\n\n\nApache Apex Malhar, an associated library of operators\n\n\nOther related tools\n\n\n\n\nNote: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.\n\n\nThe Top N Words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words and computes word frequencies for both that specific file\nas well as across all files processed so far. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on \ndtDashboard\n, the browser-based dashboard of DataTorrent RTS.\n\n\nA simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:\n\n\n\n\nThe Apex platform\n\n\nThe operator library\n\n\nThe tools required for developing and deploying\n    applications on a cluster\n\n\napex\n \n the command-line tool for managing\n    application packages and the constituent applications\n\n\ndtManage\n \n for monitoring the applications\n\n\ndtDashboard\n \n for visualizing the output\n\n\n\n\nIn the context of such an application, a number of questions arise:\n\n\n\n\nWhat operators do we need ?\n\n\nHow many are present in the Malhar library ?\n\n\nHow many need to be written from scratch ?\n\n\nHow are operators wired together ?\n\n\nHow do we monitor the running application ?\n\n\nHow do we display the output data in an aesthetically pleasing way ?\n\n\n\n\nThe answers to these and other questions are explored in the sections below.\n\n\nFor this tutorial, use the DataTorrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and the latest version of DataTorrent RTS configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the licensed version of DataTorrent RTS, you\ncan use that setup instead.\n\n\nSetting up your development environment\n\n\nTo begin with, please follow the steps outlined in:\n\nApache Apex Development Environment Setup\n\nto setup your development environment; you can skip the sandbox download\nand installation if you already have a Hadoop cluster with Datatorrent\nRTS installed where you can deploy applications.\n\n\nSample input files\n\n\nFor this tutorial, you need some sample text files to use as input to the application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  \nWqgi\n).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  \ndiv\n, \ntd\n and \np\n dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:\n\n\nOpen a terminal and run the following commands to create a directory named\n\ndata\n under your home directory and download 3 files there:\n\n\ncd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt\n\n\n\nBuilding Top N Aords Application\n\n\nThis chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on a cluster (or sandbox). We will use the \ndtManage\n GUI tool to launch the\napplication.\n\n\nStep 1: Clone the Apex Malhar repository\n\n\nClone the Malhar repository (we will use some of these source files in a later\nsection):\n\n\n\n\n\n\nOpen a terminal window and create a new directory where you want the code\n    to reside, for example: \ncd ~/src; mkdir dt; cd dt\n\n\n\n\n\n\nDownload the code for Malhar:\n\n\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\nYou should now see a directory named \nincubator-apex-malhar\n.\n\n\n\n\n\n\nStep 2: Create a new application project\n\n\nCreate a new application project as described in (you can use either an IDE\nor the command line):\n\nApache Apex Development Environment Setup\n\n\nStep 3: Copy application files to the new project\n\n\nWe will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.\n\n\n\n\nDelete files \nApplication.java\n and \nRandomNumberGenerator.java\n\n   under \nsrc/main/java/com/example/topnwordcount\n.\n\n\nDelete file \nApplicationTest.java\n file under\n   \nsrc/test/java/com/example/topnwordcount\n.\n\n\n\n\nCopy the following files from:\n\n\nincubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/\n\n\nto\n\n\nsrc/main/java/com/example/topnwordcount\n\n\n\n\nApplicationWithQuerySupport.java\n\n\nFileWordCount.java\n\n\nLineReader.java\n\n\nWCPair.java\n\n\nWindowWordCount.java\n\n\nWordCountWriter.java\n\n\nWordReader.java\n\n\n\n\n\n\n\n\nCopy the file \nWordDataSchema.json\n from\n\n\nincubator-apex-malhar/demos/wordcount/src/main/resources/\n\n\nto\n\n\nsrc/main/resources/\n\n\nin the new project.\n\n\nNote\n: This file defines the format of data sent to the visualization widgets within \ndtDashboard\n.\n\n\n\n\n\n\nStep 4: Configure the application and operators\n\n\nNext, we need to configure application properties. These\nproperties accomplish the following aims:\n\n\n\n\nLimit the amount of memory used by most operators so that more memory can\n  be allocated for \nfileWordCount\n which maintains the frequency counts.\n\n\nSet the locality of a couple of streams to \nCONTAINER_LOCAL\n to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).\n\n\nDefine the regular expression for matching the non-word string that\n  delimits words.\n\n\nDefine number of top (word, frequency) pairs we want output.\n\n\nDefine the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.\n\n\nDefine the topics for sending queries and retrieving data for visualization.\n\n\n\n\nTo do this:\n\n\nOpen the file \nsrc/main/resources/META-INF/properties.xml\n, and replace its\ncontent with the following:\n\n\nconfiguration\n\n \nproperty\n\n   \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n   \nvalue\n500\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.*.operator.*.attr.MEMORY_MB\n/name\n\n   \nvalue\n200\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB\n/name\n\n   \nvalue\n512\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.lineReader.directory\n/name\n\n   \nvalue\n/tmp/test/input-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n/name\n\n   \nvalue\n[\\p{Punct}\\s]+\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wcWriter.filePath\n/name\n\n   \nvalue\n/tmp/test/output-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n/name\n\n   \nvalue\n10\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryFileStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFile\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFileResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobal\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobalResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TwitterDemo.operator.wsResult.numRetries\n/name\n\n   \nvalue\n2147483647\n/value\n\n \n/property\n\n\n/configuration\n\n\n\n\n\nNote\n:\nThe package name within the Java files we just copied currently reflects the\npackage from which they were copied. This may be flagged as an error by your IDE\nbut the application should build with no errors when built with maven on the\ncommand line. You can fix the errors in the IDE by changing the relevant line\nwithin each file from:\n\n\npackage com.datatorrent.demos.wordcount;\n\n\n\nto reflect the current location of the file, for example:\n\n\npackage com.example.topnwordcount;\n\n\n\nStep 5: Build the top N words count application\n\n\nFrom your IDE build the application in the usual way\n\n\nFrom the command line build it with:\n\n\ncd topNwordcount; mvn clean package -DskipTests\n\n\n\nIn either case, if the build is successful, it should have created the\napplication package file\n\ntopNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa\n.\n\n\nStep 6: Upload the top N words application package\n\n\nTo upload the top N words application package\n\n\n\n\nLog on to the DataTorrent Console using the default username and password\n   (both are \ndtadmin\n).\n\n\nOn the top navigation bar, click \nDevelop\n.\n\n\nClick \nApplication Packages\n.\n\n\nUnder \nApplications\n, click the \nupload package\n button.\n  \n\n\nNavigate to the location of the \ntopNwordcount-1.0-SNAPSHOT.apa\n\n   application package file is stored.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep 7: Launch the top N words application\n\n\nNote\n: If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nIn the top navigation bar, click \nLaunch\n.\n\n\n\n\nUnder the \nApplications\n tab, locate the top N word count application, and click\n   the \nlaunch\n button.\n\n\nNote\n: To configure the application using a configuration file, use the dropdown\nnext to the \nlaunch\n button and select a file under the \nlaunch with xml\n section.\n\n\n\n\n\n\nA message indicating success of the launch operation should appear along with\nthe application ID.\n\n\nNote\n: After a successful launch, monitor the top N words application following\ninstructions in the chapter \nMonitoring with dtManage\n.\n\n\nMonitoring the Application\n\n\ndtManage\n is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.\n\n\nMonitor the Application\n\n\nTo monitor the top N words application\n\n\n\n\nLog on to the Datatorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nUnder \nDatatorrent Applications\n, check if the application started.\n\n\nWait till the state entry changes to \nRUNNING\n.\n\n\nClick \nTopNWordsWithQueries\n to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and \nmetric-view\n.\n\n\nUnder \nStramEvents\n, ensure that all the operators have started.\n    \n\n\n\n\nDAGs and widgets\n\n\nWhen monitoring an application, the logical view is selected by default, with\nthe following panels, also called widgets:  \nApplication\nOverview\n, \nStram Events\n, \nLogical DAG\n, \nLogical Operators\n, \nStreams\n, \nAssociated Services\n, and \nMetrics Chart\n.\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).\n\n\nLogical view and associated widgets (panels)\n\n\nThis section describes the widgets that you see when you select the logical\ntab.\n\n\nApplication Overview\n\n\nThis panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The \nvisualize\n button allows you to\ncreate one or more custom dashboards to visualize the application output.\n\n\n\nStram Events\n\n\nAs shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled \ndetails\n\nappears next to the event; click on it for additional details about the\nfailure.\n\n\nLogical DAG\n\n\nThe logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.\n\n\nTo customize these properties\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator, click the Top list and\n   select a metric.\n\n\nTo display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.\n\n\n\n\n\n\nLogical Operators\n\n\nThis panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.\n\n\n\n\nStreams\n\n\nThis panel displays details of each stream such as the name, locality, source,\nand sinks.\n\n\n\n\nAssociated Services\n\n\nThis panel displays the services associated with the applications and their running statuses. \n\n\nMetrics Chart\n\n\nThis panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:\n\n\n\n\nPhysical view and associated widgets\n\n\nThe physical tab displays the \nApplication Overview\n and \nMetrics Chart\n\ndiscussed above along with additional panels: \nPhysical Operators\n and\n\nContainers\n. The \nPhysical Operators\n table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.\n\n\nPhysical Operators\n\n\n\n\nContainers\n\n\nFor each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the \nContainers\n table.\n\n\n\n\nIf the state of all the physical operators and containers is \nACTIVE\n\nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.\n\n\nThe physical-dag-view\n\n\nThe \nphysical-dag-view\n tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.\n\n\nThe metric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nView application logs\n\n\nWhen debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines. \ndtManage\n simplifies\nthis task by making all relevant logs accessible from the console.\n\n\nFor example, to examine logs for the \nFileWordCount\n operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.\n\n\nThe numeric values in the \ncontainer\n column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the \nContainer Overview\n panel, you should see a blue \nlogs\n dropdown\nbutton; click on it to see a menu containing three entries: \ndt.log\n, \nstderr\n,\nand \nstdout\n.\n\n\n\n\nAll messages output using \nlog4j\n classes will appear in \ndt.log\n\nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.\n\n\nVisualizing the Application Output\n\n\nThis chapter covers how to add input files to the monitored input directory and\nvisualize the output.\n\n\nWhen adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled \nFurther Explorations\n.\n\n\n\n\nNote: If you are have trouble with any of the following steps, or have not\ncomplete the preceding sections in this tutorial, you can import \nWord Count Demo\n\nfrom AppFactory. The \nWord Count Demo\n Application Package contains the\nTopNWordsWithQueries Application.\n\n\n\n\nStep 1: Add files to the monitored directory\n\n\nTo add the files to the monitored input directory\n\n\n\n\nLog on to the Datatorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nClick TopNWordsWithQueries to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and  \nmetric-view\n.\n\n\nClick the \nlogical\n tab and make sure that the DAG is visible.\n\n\nCreate the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands:\nhdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir\n\n\n\n\n\n\n\nYou should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.\n\n\nYou can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:\n\n\nhdfs dfs -cat /tmp/test/output-dir/rfc4844.txt\n\n\n\nFor operating on these input and output directories, you may find the following\nshell aliases and functions useful:\n\n\nin=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}\n\n\n\nPut them in a file called, say, \naliases\n and read them into your shell with:\n\nsource aliases\n.\n\n\nThereafter, you can list contents of the input and output directories with\n\nls-input\n and \nls-output\n, remove all files from them with \nclean-input\n and\n\nclean-output\n, drop an input file \nfoo.txt\n into the input directory with\n\nput-file foo.txt\n and finally, retrieve the corresponding output file with\n\nget-file foo.txt\n.\n\n\nNote\n: When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.\n\n\nStep 2: Visualize the results by generating dashboards\n\n\nTo generate dashboards\n\n\n\n\nPerform step I above.\n\n\nMake sure that the logical tab is selected and the \nApplication Overview\n\n  panel is visible.\n\n\n\n\nClick \nvisualize\n to see a dropdown containing previously created dashboards\n (if any), as well as the \ngenerate new dashboard\n entry.\n\n\n\n\n\n\nSelect the \ngenerate new dashboard\n entry.\n\n\nYou should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.\n\n\n\n\n\n\n\nAdd more files, one at a time, to the input directory as described in\n  step I above.\n\n\n\n\nObserve the charts changing to reflect the new data.\n\n\n\n\nYou can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.\n\n\nStep 3: Add widgets\n\n\nTo derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets: \nbar chart\n,\n\npie chart\n, \nhorizontal bar chart\n, \ntable\n, and \nnote\n.\n\n\nTo add a widget\n\n\n\n\nGenerate a dashboard by following instructions of Step II above.\n\n\nClick the \nadd widget\n button below the name of the dashboard.\n\n\nIn the \nData Source\n list, select a data source for your widget.\n\n\n\n\nSelect a widget type under \nAvailable Widgets\n.\n\n\n\n\n\n\n\n\nClick \nadd widget\n.\n\n\n\n\n\n\nThe widget is added to your dashboard.\n\n\nStep 4: Configure a widget\n\n\nAfter you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo configure a widget\n\n\n\n\nTo change the size of the widget, click the border of the widget, and\n  resize it.\n\n\nTo move the widget around, click the widget, and drag it to the desired\n  position.\n\n\nTo change the title and other properties, click the \nedit\n button in the\n  top-right corner of the widget.\n    \n\n  You can now enter a new title in the \nTitle\n box or configure the rest of the\n  options in any suitable way.\n\n\nClick \nOK\n.\n\n\nTo remove a widget, click the delete (x) button in the top-right corner of\n  the widget.\n\n\n\n\nPerform additional tasks on dashboards\n\n\nAt any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.\n\n\nTo perform additional tasks\n\n\n\n\nEnsure that you generated a dashboard as described in Step II above and\n   select it.\n\n\nClick \nsettings\n button (next to buttons named \nadd widget\n,\n   \nauto generate\n, and \nsave settings\n), below the name of the dashboard to see the \nDashboard Settings\n dialog:\n    \n\n\nType a new name for the dashboard in the \nName of dashboard\n box.\n\n\nType a suitable description in the box below.\n\n\nMake sure that \nTopNWordsWithQueries\n is selected under \nChoose apps to\n    visualize\n.\n\n\nClick \nSave\n.\n\n\n\n\nDelete a dashboard\n\n\nYou can delete a dashboard at any time.\n\n\n\n\nLog on to the DataTorrent Console (default username and password are both\n  \ndtadmin\n)\n\n\nOn the top navigation bar, click \nVisualize\n.\n\n\n\n\nSelect a dashboard.\n\n\n\n\n\n\n\n\nClick delete.\n\n\n\n\n\n\n\n\nNote: The delete button becomes visible only if one or more rows are selected.\n\n\nAdvanced RTS Features\n\n\nThis section touches on some advanced features of the RTS platform in the context of the\n\nTop N Words\n application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.\n\n\nThe first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.\n\n\nManaging Memory Allocation for Containers\n\n\nIn this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.\n\n\nRecall the following facts from the earlier sections:\n\n\n\n\nA container (JVM process) can host multiple operators.\n\n\nThe memory requirements for an operator can be specified via a properties file.\n\n\n\n\nFor reference, here is the application DAG:\n\n\n\nIf we look at the information displayed in the physical tab of \ndtManage\n for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContainer Id\n\n\nAllocated Memory\n\n\nHosted Operators\n\n\n\n\n\n\n1\n\n\n1 GB\n\n\nNone (AppMaster)\n\n\n\n\n\n\n2\n\n\n768 MB\n\n\nsnapshotServerGlobal, QueryGlobal\n\n\n\n\n\n\n3\n\n\n768 MB\n\n\nsnapshotServerFile, QueryFile\n\n\n\n\n\n\n4\n\n\n128 MB\n\n\nwsResultGlobal\n\n\n\n\n\n\n5\n\n\n640 MB\n\n\nwindowWordCount\n\n\n\n\n\n\n6\n\n\n128 MB\n\n\nConsole\n\n\n\n\n\n\n7\n\n\n128 MB\n\n\nwsResultFile\n\n\n\n\n\n\n8\n\n\n640 MB\n\n\nwordReader\n\n\n\n\n\n\n9\n\n\n128 MB\n\n\nwcWriter\n\n\n\n\n\n\n10\n\n\n1.4 GB\n\n\nlineReader\n\n\n\n\n\n\n11\n\n\n1.9 GB\n\n\nfileWordCount\n\n\n\n\n\n\n\n\n\nIf we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the \nproperties.xml\n file for\n\ndt.attr.MASTER_MEMORY_MB\n. The discrepancy is due to the fact that the file\n\n.dt/dt-site.xml\n in the home directory of user \ndtadmin\n has a value of 1024 for\nthis key which overrides the application specified value.\n\n\nLooking now at container 2, we notice that it hosts 2 operators: \nsnapshotServerGlobal\n\nand \nQueryGlobal\n and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator\n\nconnected to another operator outside the container\n\nalso has an associated \nbuffer-server\n which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in\n\nApplication Development\n.\nThe space allocated to the buffer server is governed by properties of the form:\n\n\ndt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB\n\n\n\nwhere \napp-name\n, \nop-name\n and \nport-name\n can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard (\n*\n). The\ndefault value is 512 MB. Of the two operators, only one (\nsnapshotServerGlobal\n)\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).\n\n\nThe values for \nwsResultGlobal\n, \nConsole\n, \nwsResultFile\n, \nwcWriter\n are, as\nexpected, 128 MB \n since no output ports are involved, there is no buffer-server.\nThe values for \nwindowWordCount\n and \nwordReader\n are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor \nfileWordCount\n is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for \nlineReader\n can be computed\nsimilarly.\n\n\nThe total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say, \nlow-mem.xml\n at \nsrc/site/conf/\n with this content:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n    \nvalue\n512\n/value\n\n  \n/property\n \nproperty\n\n    \nname\ndt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n    \nvalue\n128\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nWe will use this file at launch time.\n\n\nThe BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).\n\n\nThe MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in \nMETA-INF/properties.xml\n\nis actually overridden by the setting of 1024 MB for this parameter in\n\n~dtadmin/.dt/dt-site.xml\n; however, if we use a launch-time configuration file,\nvalues in it override those in \ndt-site.xml\n. A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see \nDebugging\n\nsection below):\n\n\njava.lang.OutOfMemoryError: GC overhead limit exceeded.\n\n\n\nRebuild the application, upload the package and use this file at launch time:\n\n\n\nThe allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.\n\n\nDebugging\n\n\nOn the sandbox, various log files generated by YARN and Hadoop are located at\n\n/sfw/hadoop/shared/logs\n; the \nnodemanager\n directory has application specific\ndirectories with names like this: \napplication_1448033276100_0001\n within\nwhich there are container specific directories with names like\n\ncontainer_1448033276100_0001_01_000001\n. The App Master container has the \n000001\n\nsuffix and the corresponding directory will have these files:\n\n\nAppMaster.stderr  AppMaster.stdout  dt.log\n\n\n\nThe remaining container directories will have files:\n\n\ndt.log  stderr  stdout\n\n\n\nWhen problems occur, all these log files should be carefully examined. For example, the\n\ndt.log\n file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:\n\n\n2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=\n com.datatorrent.stram.engine.StreamingContainer 1\n/stdout 2\n/stderr\n\n\nYou can provide your own \nlog4j\n configuration file called, say, \nlog4j.properties\n and place\nit in the directory \nsrc/main/resources\n as described in the\n\nconfiguration\n\npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say \nINFO\n to \nDEBUG\n while the application is running, you can click on the blue\n\nset logging level\n button in the \nApplication Overview\n panel of \ndtManage\n. It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:\n\n\n\n\nNormally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the \nlogs\n dropdown but sometimes using the commandline\nfrom a terminal window may be easier.\n\n\nPartitioning\n\n\nPartitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.\n\n\nWithout partitioning, the DAG shown in the \nlogical\n and \nphysical-dag-view\n tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a \nunifier\n. The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.\n\n\nFor our word counting example, we illustrate the technique by partitioning the \nwordReader\n\noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe \nStatelessPartitioner\n which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the \nlow-mem.xml\n configuration file\nwe created above\nto a new file named \nsimple-partition.xml\n in the same directory and add this stanza to it:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\nWhen you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:\n\n\n\n\nNotice the two copies of \nwordReader\n and the generated unifier. The \nphysical\n tab will\nalso show the containers for these additional operators and their characteristics as well.\n\n\nA slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators: \nwordReader\n and the next operator\n\nwindowWordCount\n. To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nIt enables the PARTITION_PARALLEL attribute on the \ninput port\n of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:\n\n\n\n\nNotice that both operators have been replicated and the unifier added.\n\n\nStreaming Windows and Application Windows\n\n\nOperators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a \nstreaming window\n. Its boundaries are marked by calls to \nbeginWindow\n and\n\nendWindow\n within which the platform repeatedly invokes either \nemitTuples\n (for input\nadapters) or \nprocess\n on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the\n\nOperatorGuide\n.\n\n\nFor flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:\n\n\nproperty\n\n  \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n  \nvalue\n5000\n/value\n\n\n/property\n\n\n\n\nThis is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.\n\n\nA second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before, \napp-name\n and \nop-name\n should be\nreplaced by either wildcards or names of a specific application and/or operator):\n\n\nproperty\n\n  \nname\ndt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n5\n/value\n\n\n/property\n\n\n\n\nBy default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the \nbeginWindow\n and \nendWindow\n are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.\n\n\nAppendix\n\n\nOperators in Top N words application\n\n\nThis section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nImplementing class\n\n\nDescription\n\n\n\n\n\n\nlineReader\n\n\nLineReader\n\n\nReads lines from input files.\n\n\n\n\n\n\nwordReader\n\n\nWordReader\n\n\nSplits a line into words.\n\n\n\n\n\n\nwindowWordCount\n\n\nWindowWordCount\n\n\nComputes word frequencies for a single window.\n\n\n\n\n\n\nfileWordCount\n\n\nFileWordCount\n\n\nMaintains per-file and global word frequencies.\n\n\n\n\n\n\nwcWriter\n\n\nWcWriter\n\n\nWrites top N words and their frequencies to output files.\n\n\n\n\n\n\nconsole\n\n\nConsoleOutputOperator\n\n\nWrites received tuples to console.\n\n\n\n\n\n\nsnapshotServerFile\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last data set for the current file, and returns it in response to queries.\n\n\n\n\n\n\nsnapshotServerGlobal\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last global data set, and returns it in response to queries.\n\n\n\n\n\n\nQueryFile\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for per-file data.\n\n\n\n\n\n\nQueryGlobal\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for global data.\n\n\n\n\n\n\nwsResultFile\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for per-file queries.\n\n\n\n\n\n\nwsResultGlobal\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for global queries.\n\n\n\n\n\n\n\n\n\nWe now describe the process of wiring these operators together in the\n\npopulateDAG()\n method of the main application class\n\nApplicationWithQuerySupport\n. First, the operators are created and added to\nthe DAG via the \naddOperator\n method:\n\n\nLineReader lineReader = dag.addOperator(\nlineReader\n,new LineReader());\n\n\n\n\nThe first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.\n\n\nNext, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the \naddStream\n function, for example:\n\n\ndag.addStream(\nlines\n, lineReader.output, wordReader.input);\n...\ndag.addStream(\nWordCountsFile\n, fileWordCount.outputPerFile, snapshotServerFile.input, console.input);\n\n\n\n\nNotice that the stream from \nfileWordCount.outputPerFile\n (which consists of\nthe top N words for the current file as the file is being read) goes to\n\nsnapshotServerFile.input\n (where it will be saved to respond to queries) and to\n\nconsole.input\n (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.\n\n\nThis section provides detailed information about each operator.\n\n\nLineReader\n\n\nThis class extends \nAbstractFileInputOperator\nString\n to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.\n\n\nThe base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods: \nopenFile\n, \ncloseFile\n, \nreadEntity\n, and \nemit\n. Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the \nFileWordCount\n operator.\n\n\nWordReader\n\n\nThis operator receives lines from \nLineReader\n on the input port and emits\nwords on the output port. It has a configurable property called \nnonWordStr\n\nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property\n\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n.\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe \nprocess\n method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.\n\n\nWindowWordCount\n\n\nThis operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when\n\nendWindow\n is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the\n\nendWindow\n call, but rather emit output tuples as each input tuple is\nprocessed.\n\n\nFileWordCount\n\n\nThis operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen \nLineReader\n reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next \nendWindow\n. The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we \ndo\n\nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing\n\nbeginWindow\n and \nendWindow\n calls by the upstream operator.\n\n\nThis operator also has three output ports: the \noutputPerFile\n port for the top\nN pairs for the current file as it is being read; the \noutputGlobal\n port for\nthe global top N pairs, and the \nfileOutput\n port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.\n\n\nFileWordCount\n also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.\n\n\nFileWordCount\n has a configurable property \ntopN\n for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name: \ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n\n\nIn the \nendWindow\n call, both maps are passed to the \ngetTopNList\n function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.\n\n\nWordCountWriter\n\n\nThis operator extends \nAbstractFileOutputOperator\nMap\nString,Object\n, and\nsimply writes the final top N pairs to the output file. As with \nLineReader\n,\nmost of the complexity of \nWordCountWriter\n is hidden in the base class. You must\nprovide implementations for 3 methods: \nendWindow\n, \ngetFileName\n, and\n\ngetBytesForTuple\n. The first method calls the base class method \nrequestFinalize\n.\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The \ngetFileName\n\nmethod retrieves the file name from the tuple, and the \ngetBytesForTuple\n\nmethod converts the list of pairs to a string in the desired format.\n\n\nConsoleOutputOperator\n\n\nThis is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.\n\n\nAppDataSnapshotServerMap\n\n\nThis operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots \n  one for a per-file top N snapshot and one for a\nglobal snapshot.\n\n\nPubSubWebSocketAppDataQuery\n\n\nThis is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:\n\n\nws://gateway-host:port/pubsub\n\n\n\n\nwhere \ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message to the URL where the value of the \ndata\n key\nis the desired message content. The JSON might look like this:\n\n\n{\ntype\n:\npublish\n, \ntopic\n:\nfoobar\n, \ndata\n: ...}\n\n\n\n\nCorrespondingly, subscribers send messages like this to retrieve published\nmessage data:\n\n\n{\ntype\n:\nsubscribe\n, \ntopic\n:\nfoobar\n}\n\n\n\n\nTopic names need not be pre-registered anywhere but the same topic\nname (for example, \nfoobar\n in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.\n\n\nFor this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values \nTopNWordsQueryFile\n and \nTopNWordsQueryGlobal\n under the\nrespective names:\n\n\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n\n\n\n\nPubSubWebSocketAppDataResult\n\n\nAnalogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values\n\nTopNWordsQueryFileResult\n and \nTopNWordsQueryGlobalResult\n corresponding to\nthe respective names:\n\n\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n\n\n\n\nFurther Exploration\n\n\nIn this tutorial, the property values in the \nproperties.xml\n file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.\n\n\nAnother aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the \nFileWordCount\n operator gets an EOF on the\ncontrol port, it waits for an \nendWindow\n call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.\n\n\nDataTorrent terminology\n\n\nOperators\n\n\nOperators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.\n\n\nStreams\n\n\nA stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.\n\n\nPorts\n\n\nPorts are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.\n\n\nDirected Acyclic Graph (DAG)\n\n\nA DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.\n\n\nLogical Plan or DAG\n\n\nLogical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.\n\n\nPhysical Plan or DAG\n\n\nA physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.\n\n\nData Tuples Processed\n\n\nThis is the number of data objects processed by real-time stream processing\napplications.\n\n\nData Tuples Emitted\n\n\nThis is the number of data objects emitted after real-time stream processing\napplications complete processing operations.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.\n\n\nStreaming Window\n\n\nA streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.\n\n\nSliding Application Window\n\n\nSliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.\n\n\nDemo Applications\n\n\nThe real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.\n\n\nCommand-line Interface\n\n\nCommand line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.\n\n\nWeb services\n\n\nDataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "Top N Words"
        }, 
        {
            "location": "/tutorials/topnwords/#top-n-words-application", 
            "text": "The Top N words application is a tutorial on building a word counting application using:   Apache Apex platform  Apache Apex Malhar, an associated library of operators  Other related tools   Note: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.  The Top N Words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words and computes word frequencies for both that specific file\nas well as across all files processed so far. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on  dtDashboard , the browser-based dashboard of DataTorrent RTS.  A simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:   The Apex platform  The operator library  The tools required for developing and deploying\n    applications on a cluster  apex    the command-line tool for managing\n    application packages and the constituent applications  dtManage    for monitoring the applications  dtDashboard    for visualizing the output   In the context of such an application, a number of questions arise:   What operators do we need ?  How many are present in the Malhar library ?  How many need to be written from scratch ?  How are operators wired together ?  How do we monitor the running application ?  How do we display the output data in an aesthetically pleasing way ?   The answers to these and other questions are explored in the sections below.  For this tutorial, use the DataTorrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and the latest version of DataTorrent RTS configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the licensed version of DataTorrent RTS, you\ncan use that setup instead.", 
            "title": "Top N Words Application"
        }, 
        {
            "location": "/tutorials/topnwords/#setting-up-your-development-environment", 
            "text": "To begin with, please follow the steps outlined in: Apache Apex Development Environment Setup \nto setup your development environment; you can skip the sandbox download\nand installation if you already have a Hadoop cluster with Datatorrent\nRTS installed where you can deploy applications.", 
            "title": "Setting up your development environment"
        }, 
        {
            "location": "/tutorials/topnwords/#sample-input-files", 
            "text": "For this tutorial, you need some sample text files to use as input to the application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,   Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as   div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:  Open a terminal and run the following commands to create a directory named data  under your home directory and download 3 files there:  cd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Sample input files"
        }, 
        {
            "location": "/tutorials/topnwords/#building-top-n-aords-application", 
            "text": "This chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on a cluster (or sandbox). We will use the  dtManage  GUI tool to launch the\napplication.", 
            "title": "Building Top N Aords Application"
        }, 
        {
            "location": "/tutorials/topnwords/#step-1-clone-the-apex-malhar-repository", 
            "text": "Clone the Malhar repository (we will use some of these source files in a later\nsection):    Open a terminal window and create a new directory where you want the code\n    to reside, for example:  cd ~/src; mkdir dt; cd dt    Download the code for Malhar:  git clone https://github.com/apache/incubator-apex-malhar  You should now see a directory named  incubator-apex-malhar .", 
            "title": "Step 1: Clone the Apex Malhar repository"
        }, 
        {
            "location": "/tutorials/topnwords/#step-2-create-a-new-application-project", 
            "text": "Create a new application project as described in (you can use either an IDE\nor the command line): Apache Apex Development Environment Setup", 
            "title": "Step 2: Create a new application project"
        }, 
        {
            "location": "/tutorials/topnwords/#step-3-copy-application-files-to-the-new-project", 
            "text": "We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.   Delete files  Application.java  and  RandomNumberGenerator.java \n   under  src/main/java/com/example/topnwordcount .  Delete file  ApplicationTest.java  file under\n    src/test/java/com/example/topnwordcount .   Copy the following files from:  incubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/  to  src/main/java/com/example/topnwordcount   ApplicationWithQuerySupport.java  FileWordCount.java  LineReader.java  WCPair.java  WindowWordCount.java  WordCountWriter.java  WordReader.java     Copy the file  WordDataSchema.json  from  incubator-apex-malhar/demos/wordcount/src/main/resources/  to  src/main/resources/  in the new project.  Note : This file defines the format of data sent to the visualization widgets within  dtDashboard .", 
            "title": "Step 3: Copy application files to the new project"
        }, 
        {
            "location": "/tutorials/topnwords/#step-4-configure-the-application-and-operators", 
            "text": "Next, we need to configure application properties. These\nproperties accomplish the following aims:   Limit the amount of memory used by most operators so that more memory can\n  be allocated for  fileWordCount  which maintains the frequency counts.  Set the locality of a couple of streams to  CONTAINER_LOCAL  to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).  Define the regular expression for matching the non-word string that\n  delimits words.  Define number of top (word, frequency) pairs we want output.  Define the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.  Define the topics for sending queries and retrieving data for visualization.   To do this:  Open the file  src/main/resources/META-INF/properties.xml , and replace its\ncontent with the following:  configuration \n  property \n    name dt.attr.MASTER_MEMORY_MB /name \n    value 500 /value \n  /property   property \n    name dt.application.*.operator.*.attr.MEMORY_MB /name \n    value 200 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB /name \n    value 512 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.lineReader.directory /name \n    value /tmp/test/input-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr /name \n    value [\\p{Punct}\\s]+ /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wcWriter.filePath /name \n    value /tmp/test/output-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.topN /name \n    value 10 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryFileStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryFile.topic /name \n    value TopNWordsQueryFile /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultFile.topic /name \n    value TopNWordsQueryFileResult /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryGlobal.topic /name \n    value TopNWordsQueryGlobal /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic /name \n    value TopNWordsQueryGlobalResult /value \n  /property   property \n    name dt.application.TwitterDemo.operator.wsResult.numRetries /name \n    value 2147483647 /value \n  /property  /configuration   Note :\nThe package name within the Java files we just copied currently reflects the\npackage from which they were copied. This may be flagged as an error by your IDE\nbut the application should build with no errors when built with maven on the\ncommand line. You can fix the errors in the IDE by changing the relevant line\nwithin each file from:  package com.datatorrent.demos.wordcount;  to reflect the current location of the file, for example:  package com.example.topnwordcount;", 
            "title": "Step 4: Configure the application and operators"
        }, 
        {
            "location": "/tutorials/topnwords/#step-5-build-the-top-n-words-count-application", 
            "text": "From your IDE build the application in the usual way  From the command line build it with:  cd topNwordcount; mvn clean package -DskipTests  In either case, if the build is successful, it should have created the\napplication package file topNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa .", 
            "title": "Step 5: Build the top N words count application"
        }, 
        {
            "location": "/tutorials/topnwords/#step-6-upload-the-top-n-words-application-package", 
            "text": "To upload the top N words application package   Log on to the DataTorrent Console using the default username and password\n   (both are  dtadmin ).  On the top navigation bar, click  Develop .  Click  Application Packages .  Under  Applications , click the  upload package  button.\n    Navigate to the location of the  topNwordcount-1.0-SNAPSHOT.apa \n   application package file is stored.  Wait till the package is successfully uploaded.", 
            "title": "Step 6: Upload the top N words application package"
        }, 
        {
            "location": "/tutorials/topnwords/#step-7-launch-the-top-n-words-application", 
            "text": "Note : If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.   Log on to the DataTorrent Console (the default username and password are\n   both  dtadmin ).  In the top navigation bar, click  Launch .   Under the  Applications  tab, locate the top N word count application, and click\n   the  launch  button.  Note : To configure the application using a configuration file, use the dropdown\nnext to the  launch  button and select a file under the  launch with xml  section.    A message indicating success of the launch operation should appear along with\nthe application ID.  Note : After a successful launch, monitor the top N words application following\ninstructions in the chapter  Monitoring with dtManage .", 
            "title": "Step 7: Launch the top N words application"
        }, 
        {
            "location": "/tutorials/topnwords/#monitoring-the-application", 
            "text": "dtManage  is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.", 
            "title": "Monitoring the Application"
        }, 
        {
            "location": "/tutorials/topnwords/#monitor-the-application", 
            "text": "To monitor the top N words application   Log on to the Datatorrent Console (the default username and password\n   are both  dtadmin ).  On the top navigation bar, click  Monitor .  Under  Datatorrent Applications , check if the application started.  Wait till the state entry changes to  RUNNING .  Click  TopNWordsWithQueries  to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and  metric-view .  Under  StramEvents , ensure that all the operators have started.", 
            "title": "Monitor the Application"
        }, 
        {
            "location": "/tutorials/topnwords/#dags-and-widgets", 
            "text": "When monitoring an application, the logical view is selected by default, with\nthe following panels, also called widgets:   Application\nOverview ,  Stram Events ,  Logical DAG ,  Logical Operators ,  Streams ,  Associated Services , and  Metrics Chart .\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).", 
            "title": "DAGs and widgets"
        }, 
        {
            "location": "/tutorials/topnwords/#logical-view-and-associated-widgets-panels", 
            "text": "This section describes the widgets that you see when you select the logical\ntab.  Application Overview  This panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The  visualize  button allows you to\ncreate one or more custom dashboards to visualize the application output.  Stram Events  As shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled  details \nappears next to the event; click on it for additional details about the\nfailure.  Logical DAG  The logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.  To customize these properties   Click an operator for which you want to display additional details.  To display a detail on the top of this operator, click the Top list and\n   select a metric.  To display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.    Logical Operators  This panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.   Streams  This panel displays details of each stream such as the name, locality, source,\nand sinks.   Associated Services  This panel displays the services associated with the applications and their running statuses.   Metrics Chart  This panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:", 
            "title": "Logical view and associated widgets (panels)"
        }, 
        {
            "location": "/tutorials/topnwords/#physical-view-and-associated-widgets", 
            "text": "The physical tab displays the  Application Overview  and  Metrics Chart \ndiscussed above along with additional panels:  Physical Operators  and Containers . The  Physical Operators  table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.  Physical Operators   Containers  For each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the  Containers  table.   If the state of all the physical operators and containers is  ACTIVE \nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.", 
            "title": "Physical view and associated widgets"
        }, 
        {
            "location": "/tutorials/topnwords/#the-physical-dag-view", 
            "text": "The  physical-dag-view  tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.", 
            "title": "The physical-dag-view"
        }, 
        {
            "location": "/tutorials/topnwords/#the-metric-view", 
            "text": "The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "The metric-view"
        }, 
        {
            "location": "/tutorials/topnwords/#view-application-logs", 
            "text": "When debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines.  dtManage  simplifies\nthis task by making all relevant logs accessible from the console.  For example, to examine logs for the  FileWordCount  operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.  The numeric values in the  container  column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the  Container Overview  panel, you should see a blue  logs  dropdown\nbutton; click on it to see a menu containing three entries:  dt.log ,  stderr ,\nand  stdout .   All messages output using  log4j  classes will appear in  dt.log \nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "View application logs"
        }, 
        {
            "location": "/tutorials/topnwords/#visualizing-the-application-output", 
            "text": "This chapter covers how to add input files to the monitored input directory and\nvisualize the output.  When adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled  Further Explorations .   Note: If you are have trouble with any of the following steps, or have not\ncomplete the preceding sections in this tutorial, you can import  Word Count Demo \nfrom AppFactory. The  Word Count Demo  Application Package contains the\nTopNWordsWithQueries Application.", 
            "title": "Visualizing the Application Output"
        }, 
        {
            "location": "/tutorials/topnwords/#step-1-add-files-to-the-monitored-directory", 
            "text": "To add the files to the monitored input directory   Log on to the Datatorrent Console (the default username and password are\n   both  dtadmin ).  On the top navigation bar, click  Monitor .  Click TopNWordsWithQueries to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and   metric-view .  Click the  logical  tab and make sure that the DAG is visible.  Create the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands: hdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir    You should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.  You can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:  hdfs dfs -cat /tmp/test/output-dir/rfc4844.txt  For operating on these input and output directories, you may find the following\nshell aliases and functions useful:  in=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}  Put them in a file called, say,  aliases  and read them into your shell with: source aliases .  Thereafter, you can list contents of the input and output directories with ls-input  and  ls-output , remove all files from them with  clean-input  and clean-output , drop an input file  foo.txt  into the input directory with put-file foo.txt  and finally, retrieve the corresponding output file with get-file foo.txt .  Note : When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.", 
            "title": "Step 1: Add files to the monitored directory"
        }, 
        {
            "location": "/tutorials/topnwords/#step-2-visualize-the-results-by-generating-dashboards", 
            "text": "To generate dashboards   Perform step I above.  Make sure that the logical tab is selected and the  Application Overview \n  panel is visible.   Click  visualize  to see a dropdown containing previously created dashboards\n (if any), as well as the  generate new dashboard  entry.    Select the  generate new dashboard  entry.  You should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.    Add more files, one at a time, to the input directory as described in\n  step I above.   Observe the charts changing to reflect the new data.   You can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.", 
            "title": "Step 2: Visualize the results by generating dashboards"
        }, 
        {
            "location": "/tutorials/topnwords/#step-3-add-widgets", 
            "text": "To derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets:  bar chart , pie chart ,  horizontal bar chart ,  table , and  note .  To add a widget   Generate a dashboard by following instructions of Step II above.  Click the  add widget  button below the name of the dashboard.  In the  Data Source  list, select a data source for your widget.   Select a widget type under  Available Widgets .     Click  add widget .    The widget is added to your dashboard.", 
            "title": "Step 3: Add widgets"
        }, 
        {
            "location": "/tutorials/topnwords/#step-4-configure-a-widget", 
            "text": "After you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To configure a widget   To change the size of the widget, click the border of the widget, and\n  resize it.  To move the widget around, click the widget, and drag it to the desired\n  position.  To change the title and other properties, click the  edit  button in the\n  top-right corner of the widget.\n     \n  You can now enter a new title in the  Title  box or configure the rest of the\n  options in any suitable way.  Click  OK .  To remove a widget, click the delete (x) button in the top-right corner of\n  the widget.", 
            "title": "Step 4: Configure a widget"
        }, 
        {
            "location": "/tutorials/topnwords/#perform-additional-tasks-on-dashboards", 
            "text": "At any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.  To perform additional tasks   Ensure that you generated a dashboard as described in Step II above and\n   select it.  Click  settings  button (next to buttons named  add widget ,\n    auto generate , and  save settings ), below the name of the dashboard to see the  Dashboard Settings  dialog:\n      Type a new name for the dashboard in the  Name of dashboard  box.  Type a suitable description in the box below.  Make sure that  TopNWordsWithQueries  is selected under  Choose apps to\n    visualize .  Click  Save .", 
            "title": "Perform additional tasks on dashboards"
        }, 
        {
            "location": "/tutorials/topnwords/#delete-a-dashboard", 
            "text": "You can delete a dashboard at any time.   Log on to the DataTorrent Console (default username and password are both\n   dtadmin )  On the top navigation bar, click  Visualize .   Select a dashboard.     Click delete.     Note: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Delete a dashboard"
        }, 
        {
            "location": "/tutorials/topnwords/#advanced-rts-features", 
            "text": "This section touches on some advanced features of the RTS platform in the context of the Top N Words  application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.  The first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.", 
            "title": "Advanced RTS Features"
        }, 
        {
            "location": "/tutorials/topnwords/#managing-memory-allocation-for-containers", 
            "text": "In this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.  Recall the following facts from the earlier sections:   A container (JVM process) can host multiple operators.  The memory requirements for an operator can be specified via a properties file.   For reference, here is the application DAG:  If we look at the information displayed in the physical tab of  dtManage  for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):          Container Id  Allocated Memory  Hosted Operators    1  1 GB  None (AppMaster)    2  768 MB  snapshotServerGlobal, QueryGlobal    3  768 MB  snapshotServerFile, QueryFile    4  128 MB  wsResultGlobal    5  640 MB  windowWordCount    6  128 MB  Console    7  128 MB  wsResultFile    8  640 MB  wordReader    9  128 MB  wcWriter    10  1.4 GB  lineReader    11  1.9 GB  fileWordCount     If we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the  properties.xml  file for dt.attr.MASTER_MEMORY_MB . The discrepancy is due to the fact that the file .dt/dt-site.xml  in the home directory of user  dtadmin  has a value of 1024 for\nthis key which overrides the application specified value.  Looking now at container 2, we notice that it hosts 2 operators:  snapshotServerGlobal \nand  QueryGlobal  and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator connected to another operator outside the container \nalso has an associated  buffer-server  which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in Application Development .\nThe space allocated to the buffer server is governed by properties of the form:  dt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB  where  app-name ,  op-name  and  port-name  can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard ( * ). The\ndefault value is 512 MB. Of the two operators, only one ( snapshotServerGlobal )\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).  The values for  wsResultGlobal ,  Console ,  wsResultFile ,  wcWriter  are, as\nexpected, 128 MB   since no output ports are involved, there is no buffer-server.\nThe values for  windowWordCount  and  wordReader  are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor  fileWordCount  is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for  lineReader  can be computed\nsimilarly.  The total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say,  low-mem.xml  at  src/site/conf/  with this content:  configuration \n   property \n     name dt.attr.MASTER_MEMORY_MB /name \n     value 512 /value \n   /property   property \n     name dt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n     value 128 /value \n   /property  /configuration   We will use this file at launch time.  The BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).  The MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in  META-INF/properties.xml \nis actually overridden by the setting of 1024 MB for this parameter in ~dtadmin/.dt/dt-site.xml ; however, if we use a launch-time configuration file,\nvalues in it override those in  dt-site.xml . A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see  Debugging \nsection below):  java.lang.OutOfMemoryError: GC overhead limit exceeded.  Rebuild the application, upload the package and use this file at launch time:  The allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.", 
            "title": "Managing Memory Allocation for Containers"
        }, 
        {
            "location": "/tutorials/topnwords/#debugging", 
            "text": "On the sandbox, various log files generated by YARN and Hadoop are located at /sfw/hadoop/shared/logs ; the  nodemanager  directory has application specific\ndirectories with names like this:  application_1448033276100_0001  within\nwhich there are container specific directories with names like container_1448033276100_0001_01_000001 . The App Master container has the  000001 \nsuffix and the corresponding directory will have these files:  AppMaster.stderr  AppMaster.stdout  dt.log  The remaining container directories will have files:  dt.log  stderr  stdout  When problems occur, all these log files should be carefully examined. For example, the dt.log  file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:  2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=  com.datatorrent.stram.engine.StreamingContainer 1 /stdout 2 /stderr  You can provide your own  log4j  configuration file called, say,  log4j.properties  and place\nit in the directory  src/main/resources  as described in the configuration \npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say  INFO  to  DEBUG  while the application is running, you can click on the blue set logging level  button in the  Application Overview  panel of  dtManage . It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:   Normally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the  logs  dropdown but sometimes using the commandline\nfrom a terminal window may be easier.", 
            "title": "Debugging"
        }, 
        {
            "location": "/tutorials/topnwords/#partitioning", 
            "text": "Partitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.  Without partitioning, the DAG shown in the  logical  and  physical-dag-view  tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a  unifier . The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.  For our word counting example, we illustrate the technique by partitioning the  wordReader \noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe  StatelessPartitioner  which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the  low-mem.xml  configuration file\nwe created above\nto a new file named  simple-partition.xml  in the same directory and add this stanza to it:  property \n   name dt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   When you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:   Notice the two copies of  wordReader  and the generated unifier. The  physical  tab will\nalso show the containers for these additional operators and their characteristics as well.  A slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators:  wordReader  and the next operator windowWordCount . To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:  property \n   name dt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL /name \n   value true /value  /property   It enables the PARTITION_PARALLEL attribute on the  input port  of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:   Notice that both operators have been replicated and the unifier added.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/tutorials/topnwords/#streaming-windows-and-application-windows", 
            "text": "Operators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a  streaming window . Its boundaries are marked by calls to  beginWindow  and endWindow  within which the platform repeatedly invokes either  emitTuples  (for input\nadapters) or  process  on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the OperatorGuide .  For flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:  property \n   name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n   value 5000 /value  /property   This is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.  A second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before,  app-name  and  op-name  should be\nreplaced by either wildcards or names of a specific application and/or operator):  property \n   name dt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT /name \n   value 5 /value  /property   By default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the  beginWindow  and  endWindow  are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.", 
            "title": "Streaming Windows and Application Windows"
        }, 
        {
            "location": "/tutorials/topnwords/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords/#operators-in-top-n-words-application", 
            "text": "This section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.          Operator  Implementing class  Description    lineReader  LineReader  Reads lines from input files.    wordReader  WordReader  Splits a line into words.    windowWordCount  WindowWordCount  Computes word frequencies for a single window.    fileWordCount  FileWordCount  Maintains per-file and global word frequencies.    wcWriter  WcWriter  Writes top N words and their frequencies to output files.    console  ConsoleOutputOperator  Writes received tuples to console.    snapshotServerFile  AppDataSnapshotServerMap  Caches the last data set for the current file, and returns it in response to queries.    snapshotServerGlobal  AppDataSnapshotServerMap  Caches the last global data set, and returns it in response to queries.    QueryFile  PubSubWebSocketAppDataQuery  Receives queries for per-file data.    QueryGlobal  PubSubWebSocketAppDataQuery  Receives queries for global data.    wsResultFile  PubSubWebSocketAppDataResult  Returns results for per-file queries.    wsResultGlobal  PubSubWebSocketAppDataResult  Returns results for global queries.     We now describe the process of wiring these operators together in the populateDAG()  method of the main application class ApplicationWithQuerySupport . First, the operators are created and added to\nthe DAG via the  addOperator  method:  LineReader lineReader = dag.addOperator( lineReader ,new LineReader());  The first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.  Next, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the  addStream  function, for example:  dag.addStream( lines , lineReader.output, wordReader.input);\n...\ndag.addStream( WordCountsFile , fileWordCount.outputPerFile, snapshotServerFile.input, console.input);  Notice that the stream from  fileWordCount.outputPerFile  (which consists of\nthe top N words for the current file as the file is being read) goes to snapshotServerFile.input  (where it will be saved to respond to queries) and to console.input  (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.  This section provides detailed information about each operator.  LineReader  This class extends  AbstractFileInputOperator String  to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.  The base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods:  openFile ,  closeFile ,  readEntity , and  emit . Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the  FileWordCount  operator.  WordReader  This operator receives lines from  LineReader  on the input port and emits\nwords on the output port. It has a configurable property called  nonWordStr \nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr .\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe  process  method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.  WindowWordCount  This operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when endWindow  is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the endWindow  call, but rather emit output tuples as each input tuple is\nprocessed.  FileWordCount  This operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen  LineReader  reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next  endWindow . The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we  do \nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing beginWindow  and  endWindow  calls by the upstream operator.  This operator also has three output ports: the  outputPerFile  port for the top\nN pairs for the current file as it is being read; the  outputGlobal  port for\nthe global top N pairs, and the  fileOutput  port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.  FileWordCount  also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.  FileWordCount  has a configurable property  topN  for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name:  dt.application.TopNWordsWithQueries.operator.fileWordCount.topN  In the  endWindow  call, both maps are passed to the  getTopNList  function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.  WordCountWriter  This operator extends  AbstractFileOutputOperator Map String,Object , and\nsimply writes the final top N pairs to the output file. As with  LineReader ,\nmost of the complexity of  WordCountWriter  is hidden in the base class. You must\nprovide implementations for 3 methods:  endWindow ,  getFileName , and getBytesForTuple . The first method calls the base class method  requestFinalize .\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The  getFileName \nmethod retrieves the file name from the tuple, and the  getBytesForTuple \nmethod converts the list of pairs to a string in the desired format.  ConsoleOutputOperator  This is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.  AppDataSnapshotServerMap  This operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots    one for a per-file top N snapshot and one for a\nglobal snapshot.  PubSubWebSocketAppDataQuery  This is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:  ws://gateway-host:port/pubsub  where  gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message to the URL where the value of the  data  key\nis the desired message content. The JSON might look like this:  { type : publish ,  topic : foobar ,  data : ...}  Correspondingly, subscribers send messages like this to retrieve published\nmessage data:  { type : subscribe ,  topic : foobar }  Topic names need not be pre-registered anywhere but the same topic\nname (for example,  foobar  in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.  For this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values  TopNWordsQueryFile  and  TopNWordsQueryGlobal  under the\nrespective names:  dt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic  PubSubWebSocketAppDataResult  Analogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values TopNWordsQueryFileResult  and  TopNWordsQueryGlobalResult  corresponding to\nthe respective names:  dt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic", 
            "title": "Operators in Top N words application"
        }, 
        {
            "location": "/tutorials/topnwords/#further-exploration", 
            "text": "In this tutorial, the property values in the  properties.xml  file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.  Another aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the  FileWordCount  operator gets an EOF on the\ncontrol port, it waits for an  endWindow  call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.", 
            "title": "Further Exploration"
        }, 
        {
            "location": "/tutorials/topnwords/#datatorrent-terminology", 
            "text": "Operators  Operators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.  Streams  A stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.  Ports  Ports are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.  Directed Acyclic Graph (DAG)  A DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.  Logical Plan or DAG  Logical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.  Physical Plan or DAG  A physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.  Data Tuples Processed  This is the number of data objects processed by real-time stream processing\napplications.  Data Tuples Emitted  This is the number of data objects emitted after real-time stream processing\napplications complete processing operations.  Streaming Application Manager (STRAM)  Streaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.  Streaming Window  A streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.  Sliding Application Window  Sliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.  Demo Applications  The real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.  Command-line Interface  Command line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.  Web services  DataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "DataTorrent terminology"
        }, 
        {
            "location": "/tutorials/salesdemo/", 
            "text": "Sales Dimension Application\n\n\nThe Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.\n\n\nExample Scenario\n\n\nA large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.\n\n\nIn order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.\n\n\nThe application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.\n\n\nThe application setup for this retailer requires:\n\n\n\n\nInput \n For receiving individual sales transactions\n\n\nTransform \n For converting incoming records into a consumable format\n\n\nEnrich \n For providing additional information for each record by\n    performing additional lookups\n\n\nCompute \n For performing aggregate computations on all possible\n    key field combinations\n\n\nStore \n For storing computed results for further\n    analysis and visualizations\n\n\nAnalyze, Alert \n Visualize \n For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.\n\n\n\n\nApplication Development\n\n\nTo save time we will use some source and data files that are available online.\nWe will create a new maven project using the maven archetype, add the source\nand data files to the project, modify them suitable and finally build and\ndeploy application.\n\n\nStep 1: Build the Sales Dimension application\n\n\nTo build an application\n\n\n\n\n\n\nCreate a new application project named, say \nsalesapp\n, as described in:\n   \nApache Apex Development Environment Setup\n\n\n\n\n\n\nDelete the following generated JAVA files: \nApplication.java\n and\n    \nRandomNumberGenerator.java\n under \nsrc/main/java/com/example/salesapp\n\n    and \nApplicationTest.java\n under \nsrc/test/java/com/example/salesapp\n.\n\n\n\n\n\n\nCheckout the \nexamples\n git repository in a suitable location, for example:\n\n\ncd; git checkout https://github.com/datatorrent/examples\n\n\n\n\n\n\n\nCopy the following files from that repository at\n    \nexamples/dt-demo/dimensions/src/main/java/com/datatorrent/demos/dimensions/sales/generic\n\n    to the main source directory of the new project at \nsrc/main/java/com/example/salesapp\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrichmentOperator.java\n\n\nJsonSalesGenerator.java\n\n\n\n\n\n\nJsonToMapConverter.java\n\n\nRandomWeightedMovableGenerator.java\n\n\n\n\n\n\nSalesDemo.java\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso copy these text files from the examples repository at\n    \nexamples/dt-demo/dimensions/src/main/resources\n:\n    \nsalesGenericDataSchema.json\n, \nsalesGenericEventSchema.json\n,\n    \nproducts.txt\n to the new project at \nsrc/main/resources\n. The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.\n\n\n\n\n\n\nChange the package location in each Java file to reflect\n    its current location by changing the line\n\n\npackage com.datatorrent.demos.dimensions.sales.generic;\n\n\n\nto\n\n\npackage com.example.salesapp;\n\n\n\n\n\n\n\nAdd a new file called \nInputGenerator.java\n to the same location\n    containing this block of code:\n\n\npackage com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator\nT\n extends InputOperator {\n    public OutputPort\nT\n getOutputPort();\n}\n\n\n\n\n\n\n\nRemove these lines from \nJsonSalesGenerator.java\n (the first is\n    unused, while the second is now package local):\n\n\nimport com.datatorrent.demos.dimensions.InputGenerator;\nimport com.datatorrent.demos.dimensions.ads.AdInfo;\n\n\n\nAlso remove the first import from \nSalesDemo.java\n.\n\n\n\n\n\n\nAdd the following two lines to \nSalesDemo.java\n (if it does not exist already).\n\n\nPubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\nwsIn.setTopic(\"SalesDimensionsQuery\");      // 1. Add this line\nstore.setEmbeddableQueryInfoProvider(wsIn);\n\nPubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\nwsOut.setTopic(\"SalesDimensionResult\");     // 2. Add this line\n\ndag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n...\n\n\n\n\n\n\n\nMake the following changes to pom.xml:\n\n\n\n\n\n\nChange the artifactId to something that is likely to be unique to\n   this application, for example: \nartifactId\nsalesapp\n/artifactId\n.\n   This step is optional but is recommended since uploading a second\n   package with the same artifact id will overwrite the first. Similarly,\n   change the \nname\n and \ndescription\n elements to something meaningful\n   for this application.\n\n\n\n\n\n\nAdd the following \nrepositories\n element at the top level (i.e. as a\n   child of the \nproject\n element):\n\n\n!-- repository to provide the DataTorrent artifacts --\n\n\nrepositories\n\n  \nrepository\n\n    \nid\ndatatorrent\n/id\n\n    \nname\nDataTorrent Release Repository\n/name\n\n    \nurl\nhttps://www.datatorrent.com/maven/content/repositories/releases/\n/url\n\n    \nsnapshots\n\n      \nenabled\nfalse\n/enabled\n\n    \n/snapshots\n\n  \n/repository\n\n\n/repositories\n\n\n\n\n\n\n\n\nAdd these lines to the dependencies section at the end of the \npom.xml\n\nfile (the version number might need to change as new releases come out):\n\n\ndependency\n\n  \ngroupId\ncom.datatorrent\n/groupId\n\n  \nartifactId\ndt-contrib\n/artifactId\n\n  \nversion\n3.5.0\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\n*\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\ndependency\n\n  \ngroupId\ncom.datatorrent\n/groupId\n\n  \nartifactId\ndt-library\n/artifactId\n\n  \nversion\n3.5.0\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\n*\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\n\n\n\n\n\n\nFinally change \napex.version\n to \n3.6.0-SNAPSHOT\n. To recapitulate, we are\n   using versions \n3.5.0\n for \ndt-contrib\n and \ndt-library\n, \n3.6.0\n\n   for \nmalhar-library\n and \n3.6.0-SNAPSHOT\n for Apex.\n\n\n\n\n\n\n\n\n\n\nBuild the project as usual:\n\n\nmvn clean package -DskipTests\n\n\n\n\n\n\n\nAssuming the build is successful, you should see the package file named\n\nsalesApp-1.0-SNAPSHOT.jar\n under the target directory. The next step\nshows you how to use the \ndtManage\n GUI to upload the package and launch the\napplication from there.\n\n\nStep 2: Upload the Sales Dimension application package\n\n\nTo upload the Sales Dimension application package\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n    both \ndtadmin\n).\n\n\nOn the menu bar, click \nDevelop\n.\n\n\n\n\nUnder \nApp Packages\n, click on \nupload package\n.\n\n\n\n\n\n\n\n\nNavigate to the location of \nsalesApp-1.0-SNAPSHOT.apa\n and select it.\n\n\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep 3: Launch the Sales Dimension application\n\n\nNote\n: If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.\n\n\n\n\nIn the menu bar, click \nLaunch\n.\n\n\n\n\nUnder the \nApplications\n tab, locate the Sales Dimension application, and click the\n   \nlaunch\n button. In the confirmation modal, click the \nlaunch\n button.\n\n\nNote\n: To configure the application using a configuration file, use the dropdown next to\nthe \nlaunch\n button and select a file under the \nlaunch with xml\n section.\n\n\n\n\n\n\nIf the launch is successful, a notification will appear on the top-right corner with the application ID and a hyperlink to monitor the running application.\n\n\nOperator base classes and interfaces\n\n\nThis section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.\n\n\nOperators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The \nOperator\n interface extends the \nComponent\n\ninterface:\n\n\npublic interface Component \nCONTEXT extends Context\n {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}\n\n\n\nThe \nOperator\n interface defines \nPort\n, \nInputPort\n, and \nOutputPort\n as inner interfaces with\n\nInputPort\n, and \nOutputPort\n extending \nPort\n.\n\n\npublic interface Operator extends Component\nContext.OperatorContext\n {\n\n  public static interface Port extends Component\nContext.PortContext\n {}\n\n  public static interface InputPort\nT extends Object\n extends Port {\n    public Sink\nT\n getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec\nT\n getStreamCodec();\n  }\n\n  public static interface OutputPort\nT extends Object\n extends Port {\n    public void setSink(Sink\nObject\n sink);\n    public Unifier\nT\n getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}\n\n\n\nOperators typically extend the \nBaseOperator\n class which simply\ndefines empty methods for \nsetup\n, \nteardown\n, \nbeginWindow\n, and\n\nendWindow\n. Derived classes only need to define those functions for\nwhich they want to perform an action. For example the\n\nConsoleOutputOperator\n class, which is often used during testing and\ndebugging, does not override any of these methods.\n\n\nInput operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).\n\n\nInput ports must implement the \nInputOperator\n interface.\n\n\npublic interface InputOperator extends Operator {\n  public void emitTuples();\n}\n\n\n\nThe \nemitTuples\n method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named \nRandomNumberGenerator\n,\nwhich is defined like this:\n\n\npublic class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort\nDouble\n out = new DefaultOutputPort\nDouble\n();\n\n  public void emitTuples()  {\n    if (count++ \n 100) {\n      out.emit(Math.random());\n    }\n  }\n}\n\n\n\nFinally, the \nDefaultInputPort\n and \nDefaultOutputPort\n classes are\nvery useful as base classes that can be extended when defining ports\nin operators.\n\n\npublic abstract class DefaultInputPort\nT\n implements InputPort\nT\n, Sink\nT\n {\n  private int count;\n\n  public Sink\nT\n getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort\nT\n implements Operator.OutputPort\nT\n {\n  private transient Sink\nObject\n sink;\n\n  final public void setSink(Sink\nObject\n s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}\n\n\n\nThe \nDefaultInputPort\n class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract \nprocess\n method needs to be\nimplemented by any concrete derived class; it will be invoked via the\n\nSink.put\n override.\n\n\nThe \nDefaultOutputPort\n class also supports a sink and forwards calls\nto \nemit\n to the sink. The \nsetSink\n method is called by the \nStrAM\n\nexecution engine to inject a suitable sink at deployment time.\n\n\nOutput operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending \nBaseOperator\n for\nconvenience. For example, the \nConsoleOutputOperator\n mentioned earlier\nis defined like this:\n\n\npublic class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort\nObject\n input = new DefaultInputPort\nObject\n() {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}\n\n\n\nNotice that the implementation of the abstract method\n\nDefaultInputPort.process\n simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).\n\n\nOperators in the Sales Dimensions application\n\n\nThe application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the\n\ndag.addoperator\n call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.\n\n\nThis diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.\n\n\n\nJsonSalesGenerator (InputGenerator)\n\n\nThis class (new operator) is an input operator that generates a single\nsales event defined by a class like this:\n\n\nclass SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}\n\n\n\nJsonToMapConverter (Converter)\n\n\nThis operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:\n\n\npublic class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort\nbyte\\[\\]\n input = new DefaultInputPort\nbyte[]\n() {\n    public void process(byte\\[\\] message) {\n      Map\nString, Object\n tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort\nMap\nString, Object\n outputMap\n     = new DefaultOutputPort\nMap\nString, Object\n();\n\n}\n\n\n\nEnrichmentOperator (Enrichment)\n\n\nThis operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file \nproducts.txt\n that\nwe encountered earlier while building the application. It contains\ndata like this:\n\n\n{\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}\n\n\n\nThe core functionality of this operator is in the \nprocess\n function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.\n\n\npublic class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort\nMap\nString, Object\n\n    outputPort = new DefaultOutputPort\nMap\nString, Object\n();\n\n  public transient DefaultInputPort\nMap\nString, Object\n\n    inputPort = new DefaultInputPort\nMap\nString, Object\n() {\n\n    public void process(Map\nString, Object\n tuple) {\n      ...\n    }\n  }\n}\n\n\n\nDimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)\n\n\nThis operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.\n\n\nAppDataSingleDimensionStoreHDHT (Store)\n\n\nThis operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.\n\n\nPubSubWebSocketAppDataQuery (Query)\n\n\nThis is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike \nws://\ngateway-host\n:\nport\n/pubsub\n where\n\ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message that looks like this to the URL\nwhere the value of the \ndata\n key is the desired message content:\n\n\n{\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}\n\n\n\nCorrespondingly, subscribers send messages like this\nto retrieve published message data:\n\n\n{\"type\":\"subscribe\", \"topic\":\"foobar\"}\n\n\n\nTopic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g. \nfoobar\n in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.\n\n\nThis query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:\n\n\npublic class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}\n\n\n\nThe important method here is \nconvertMessage\n to convert the input\nstring to a JSON object, get the value of the \ndata\n key from the object\nand return it. The base classes look like this:\n\n\npublic class PubSubWebSocketInputOperator\nT\n extends WebSocketInputOperator\nT\n {\n  ...\n}\n\n\n\nThis class simply converts a JSON event into Java maps via the\n\nconvertMessage\n method.\n\n\npublic class WebSocketInputOperator\nT\n extends\nSimpleSinglePortInputOperator\nT\n implements Runnable {\n  ...\n}\n\n\n\nThis code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.\n\n\npublic abstract class SimpleSinglePortInputOperator\nT\n extends BaseOperator\nimplements InputOperator, Operator.ActivationListener\nOperatorContext\n {\n\n  final public transient BufferingOutputPort\nT\n outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort\nT\n extends DefaultOutputPort\nT\n {\n    public void flush(int count) { ... }\n  }\n\n}\n\n\n\nThe class starts a separate thread which retrieves source events and\ninvokes the \nemit\n method of the output port; the output port buffers\nevents until the \nflush\n method is called at which point all buffered\nevents are emitted.\n\n\nPubSubWebSocketAppDataResult (QueryResult)\n\n\nThis is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:\n\n\npublic class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n}\n\n\n\nThis class merely overrides the generic \nconvertMapToMessage\n method of the\nbase class to generate the required JSON publish message.\n\n\npublic class PubSubWebSocketOutputOperator\nT\n extends WebSocketOutputOperator\nT\n {\n  ...\n}\n\n\n\nThis class, similarly, doesn't do much \n the \nconvertMapToMessage\n\nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.\n\n\npublic class WebSocketOutputOperator\nT\n extends BaseOperator {\n  public final transient DefaultInputPort\nT\n input = new DefaultInputPort\nT\n() {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}\n\n\n\nThe key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is \nprocess\n\nwhich converts the incoming event to a JSON message and sends it\nacross the connection.\n\n\nConnecting the operators\n\n\nNow that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the\n\nStreamingApplication\n interface:\n\n\npublic class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    ...\n\n    PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\n    wsIn.setTopic(\"SalesDimensionsQuery\");\n    store.setEmbeddableQueryInfoProvider(wsIn);\n\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setTopic(\"SalesDimensionsResult\");\n\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, wsOut.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}\n\n\n\nThe key method to implement in an application is \npopulateDAG\n; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the \nDAG.StreamMeta\n\ninterface and is created via \nDAG.addStream()\n. The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the\n\npopulateDAG\n function.\n\n\nThese two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.\n\n\nNote\n: You can also find these instructions in the UI console. Click \nLearn\n in the menu\nbar, and then click the first link in the left panel: \nTransform, Analyze, Alert\n.\n\n\nStep 1: Open the Application Builder interface\n\n\n\n\nOn the DataTorrent RTS console, navigate to \nApp Packages\n.\n\n\nMake sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).\n\n\nClick the green \nCreate new application\n button, and name the application\n    Sales Dimensions. The Application Canvas window should open.\n    \n\n\n\n\nStep 2: Add and connect operators\n\n\n\n\n\n\nUnder \nOperator Library\n in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.\n\n\n\n\nJSON Sales Event Generator (Input)\n \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.\n\n\nEnrichment (Enrich)\n \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n \n This operator\n   stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n \n The dashboard connector for\n   visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n \n The dashboard connector for\n   visualization data results.\n\n\n\n\n\n\n\n\nTo connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:\n    \n\n\n\n\n\n\nStep 3: Customize application and operator settings\n\n\nCustomize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the \nOperator Inspector\n panel\non the bottom to edit the operator and stream settings as described in the item:\n\n\n\n\n\n\nCopy this Sales schema below into the \nEvent Schema JSON\n field of \nInput\n\n    operator, and the \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n\n    operators.\n\n\n{\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to\n    \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\nOptional\n: In the \nBuilding with Java\n section, the \nApp Data Pub Sub Query (PubSubWebSocketAppDataQuery)\n operator was not added to the DAG. Instead, it was embedded into the \nstore\n operator to avoid query delays which may happen when the operator is blocked upstream.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.\n    Set \nBase Path\n value to \nSalesDimensionsDemoStore\n. This sets the HDHT\n    storage path to write dimensions computation results to\n    \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n    \n\n\n\n\nClick the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.\n\n\n\n\nNote\n: Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.\n\n\nStep 4: Launch the application\n\n\nOnce the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the\n\nApplication Canvas\n window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.\n\n\nTo launch the Sales Dimension application\n\n\n\n\nClick the launch button at the top left of the application canvas screen.\n\n\nType a name for the application in the \nName this application\n box.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a configuration file\n checkbox.\n\n\n(Optional) To specify individual properties, select\n    \nSpecify Launch Properties\n checkbox.\n\n\nClick Launch.\n\n\n\n\n\n\nOnce the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled \nMonitoring the Sales\nDimensions Application with dtManage\n.\n\n\n\n\nGo to the Sales Dimensions application operations page under the \nMonitor\n tab.\n\n\nConfirm that the application is launched successfully by validating that\n    the state of the application under the \nApplication Overview\n section\n    is \nRUNNING\n.\n\n\nMake sure that all the operators are successfully started under the\n    \nStramEvents\n widget.\n\n\nNavigate to the \nphysical\n tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.\n    \n\n\n\n\nNote\n: This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.\n\n\nVisualizing Application Data\n\n\nDataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled \ndtDashboard\n- Application Data Visualization\n at \nhttps://docs.datatorrent.com\n.\n\n\nAfter the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.\n\n\nGenerate dashboards\n\n\n\n\nIf you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.\n\n\nAfter the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:\n\n\n\n\nAdding widgets\n\n\nTo derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets: \nbar chart\n, \npie chart\n, \nhorizontal bar chart\n, \ntable\n, and\n\nnote\n.\n\n\nTo add a widget\n\n\n\n\nClick the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n    \n\n\nIn the Data Source list, click a data source for your widget.\n\n\nSelect a widget type under \nAvailable Widgets\n.\n    \n\n\nClick \nadd widget\n button.\n\n\n\n\nThe widget is added to your dashboard.\n\n\nEdit a widget\n\n\nAfter you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo edit a widget\n\n\n\n\n\n\nChange the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.\n\n\n\n\n\n\nEdit the widget:\n    a.  In the top-right corner of the widget, click \nedit\n.\n    b.  Type a new title in the \nTitle\n box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click \nOK\n.\n    \n\n\n\n\n\n\nTo remove a widget, in the top-right corner, click the \ndelete\n button.\n\n\n\n\n\n\nMonitoring the Application\n\n\nRecall that after the application is built and validated, it can be\nlaunched from the \nApp Packages\n page as described in an earlier chapter. \nThis section describes how you can monitor the running Sales Dimension application\nusing \ndtManage\n.\n\n\nThe Monitor menu option\n\n\nYou can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click \nMonitor\n, you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.\n\n\nlogical\n\n\nThis image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the \n+\n button at the top-left corner and choosing\nfrom the resulting dropdown list.\n\n\n\n\n\n\n\n\nApplication Overview\n\n\nThis widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.\n\n\n\n\n\n\nStramEvents\n\n\nThis widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.\n\n\n\n\n\n\nLogical DAG\n\n\nThis widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.\n\n\nTo include additional details\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator representation,\n    click the Top list, and select a metric.\n\n\nTo display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.\n\n\n\n\n\n\n\n\nLogical Operators\n\n\nThis widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.\n\n\n\n\n\n\nStreams\n\n\nThis operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.\n\n\n\n\n\n\nMetrics Chart\n\n\nThis widget displays moving averages of tuples processed and latencies.\n\n\n\n\n\n\nphysical\n\n\nThe physical tab displays, in addition to \nApplication Overview\n\nand \nMetrics Chart\n, 2 more widgets:\n\n\n\n\n\n\n\n\nPhysical Operators\n\n\nThis widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.\n\n\n\n\n\n\nContainers\n\n\nThis widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.\n\n\n\n\n\n\nphysical-dag-view\n\n\nThe physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:\n\n\n\n\nmetric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nMonitor Sales Dimension using the Monitor menu\n\n\nTo monitor the application\n\n\n\n\nClick \nMonitor\n on the menu bar to open the logical view of the DAG.\n    \n\n\nEnsure that the \nState\n is \nRunning\n, indicating that the application\n    is launched successfully.\n\n\nUnder \nStramEvents\n, ensure that the operators from within the\n    application have started.\n\n\nClick \nphysical\n tab to open the physical view.\n\n\n\n\nEnsure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n    \n\n\nNote: This is because we set the corresponding stream locality to\n\nCONTAINER_LOCAL\n earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.\n\n\n\n\n\n\nCreate additional tabs\n\n\nYou can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the \nApplication\nOverview\n widget.\n\n\nTo create additional tabs\n\n\n\n\nNext to the \nmetric-view\n tab, look for the plus sign (+) button.\n\n\nClick this button to create an additional tab.\n\n\nProvide a name for your tab.\n\n\nAdd widgets to your tab.", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/tutorials/salesdemo/#sales-dimension-application", 
            "text": "The Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.", 
            "title": "Sales Dimension Application"
        }, 
        {
            "location": "/tutorials/salesdemo/#example-scenario", 
            "text": "A large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.  In order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.  The application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.  The application setup for this retailer requires:   Input   For receiving individual sales transactions  Transform   For converting incoming records into a consumable format  Enrich   For providing additional information for each record by\n    performing additional lookups  Compute   For performing aggregate computations on all possible\n    key field combinations  Store   For storing computed results for further\n    analysis and visualizations  Analyze, Alert   Visualize   For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.", 
            "title": "Example Scenario"
        }, 
        {
            "location": "/tutorials/salesdemo/#application-development", 
            "text": "To save time we will use some source and data files that are available online.\nWe will create a new maven project using the maven archetype, add the source\nand data files to the project, modify them suitable and finally build and\ndeploy application.", 
            "title": "Application Development"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-1-build-the-sales-dimension-application", 
            "text": "To build an application    Create a new application project named, say  salesapp , as described in:\n    Apache Apex Development Environment Setup    Delete the following generated JAVA files:  Application.java  and\n     RandomNumberGenerator.java  under  src/main/java/com/example/salesapp \n    and  ApplicationTest.java  under  src/test/java/com/example/salesapp .    Checkout the  examples  git repository in a suitable location, for example:  cd; git checkout https://github.com/datatorrent/examples    Copy the following files from that repository at\n     examples/dt-demo/dimensions/src/main/java/com/datatorrent/demos/dimensions/sales/generic \n    to the main source directory of the new project at  src/main/java/com/example/salesapp .         EnrichmentOperator.java  JsonSalesGenerator.java    JsonToMapConverter.java  RandomWeightedMovableGenerator.java    SalesDemo.java        Also copy these text files from the examples repository at\n     examples/dt-demo/dimensions/src/main/resources :\n     salesGenericDataSchema.json ,  salesGenericEventSchema.json ,\n     products.txt  to the new project at  src/main/resources . The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.    Change the package location in each Java file to reflect\n    its current location by changing the line  package com.datatorrent.demos.dimensions.sales.generic;  to  package com.example.salesapp;    Add a new file called  InputGenerator.java  to the same location\n    containing this block of code:  package com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator T  extends InputOperator {\n    public OutputPort T  getOutputPort();\n}    Remove these lines from  JsonSalesGenerator.java  (the first is\n    unused, while the second is now package local):  import com.datatorrent.demos.dimensions.InputGenerator;\nimport com.datatorrent.demos.dimensions.ads.AdInfo;  Also remove the first import from  SalesDemo.java .    Add the following two lines to  SalesDemo.java  (if it does not exist already).  PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\nwsIn.setTopic(\"SalesDimensionsQuery\");      // 1. Add this line\nstore.setEmbeddableQueryInfoProvider(wsIn);\n\nPubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\nwsOut.setTopic(\"SalesDimensionResult\");     // 2. Add this line\n\ndag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n...    Make the following changes to pom.xml:    Change the artifactId to something that is likely to be unique to\n   this application, for example:  artifactId salesapp /artifactId .\n   This step is optional but is recommended since uploading a second\n   package with the same artifact id will overwrite the first. Similarly,\n   change the  name  and  description  elements to something meaningful\n   for this application.    Add the following  repositories  element at the top level (i.e. as a\n   child of the  project  element):  !-- repository to provide the DataTorrent artifacts --  repositories \n   repository \n     id datatorrent /id \n     name DataTorrent Release Repository /name \n     url https://www.datatorrent.com/maven/content/repositories/releases/ /url \n     snapshots \n       enabled false /enabled \n     /snapshots \n   /repository  /repositories     Add these lines to the dependencies section at the end of the  pom.xml \nfile (the version number might need to change as new releases come out):  dependency \n   groupId com.datatorrent /groupId \n   artifactId dt-contrib /artifactId \n   version 3.5.0 /version \n   exclusions \n     exclusion \n       groupId * /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency  dependency \n   groupId com.datatorrent /groupId \n   artifactId dt-library /artifactId \n   version 3.5.0 /version \n   exclusions \n     exclusion \n       groupId * /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency     Finally change  apex.version  to  3.6.0-SNAPSHOT . To recapitulate, we are\n   using versions  3.5.0  for  dt-contrib  and  dt-library ,  3.6.0 \n   for  malhar-library  and  3.6.0-SNAPSHOT  for Apex.      Build the project as usual:  mvn clean package -DskipTests    Assuming the build is successful, you should see the package file named salesApp-1.0-SNAPSHOT.jar  under the target directory. The next step\nshows you how to use the  dtManage  GUI to upload the package and launch the\napplication from there.", 
            "title": "Step 1: Build the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-2-upload-the-sales-dimension-application-package", 
            "text": "To upload the Sales Dimension application package   Log on to the DataTorrent Console (the default username and password are\n    both  dtadmin ).  On the menu bar, click  Develop .   Under  App Packages , click on  upload package .     Navigate to the location of  salesApp-1.0-SNAPSHOT.apa  and select it.   Wait till the package is successfully uploaded.", 
            "title": "Step 2: Upload the Sales Dimension application package"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-3-launch-the-sales-dimension-application", 
            "text": "Note : If you are launching the application on the sandbox, make sure that\nan IDE is not running on it at the same time; otherwise, the sandbox might\nhang due to resource exhaustion.   In the menu bar, click  Launch .   Under the  Applications  tab, locate the Sales Dimension application, and click the\n    launch  button. In the confirmation modal, click the  launch  button.  Note : To configure the application using a configuration file, use the dropdown next to\nthe  launch  button and select a file under the  launch with xml  section.    If the launch is successful, a notification will appear on the top-right corner with the application ID and a hyperlink to monitor the running application.", 
            "title": "Step 3: Launch the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdemo/#operator-base-classes-and-interfaces", 
            "text": "This section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.  Operators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The  Operator  interface extends the  Component \ninterface:  public interface Component  CONTEXT extends Context  {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}  The  Operator  interface defines  Port ,  InputPort , and  OutputPort  as inner interfaces with InputPort , and  OutputPort  extending  Port .  public interface Operator extends Component Context.OperatorContext  {\n\n  public static interface Port extends Component Context.PortContext  {}\n\n  public static interface InputPort T extends Object  extends Port {\n    public Sink T  getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec T  getStreamCodec();\n  }\n\n  public static interface OutputPort T extends Object  extends Port {\n    public void setSink(Sink Object  sink);\n    public Unifier T  getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}  Operators typically extend the  BaseOperator  class which simply\ndefines empty methods for  setup ,  teardown ,  beginWindow , and endWindow . Derived classes only need to define those functions for\nwhich they want to perform an action. For example the ConsoleOutputOperator  class, which is often used during testing and\ndebugging, does not override any of these methods.  Input operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).  Input ports must implement the  InputOperator  interface.  public interface InputOperator extends Operator {\n  public void emitTuples();\n}  The  emitTuples  method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named  RandomNumberGenerator ,\nwhich is defined like this:  public class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort Double  out = new DefaultOutputPort Double ();\n\n  public void emitTuples()  {\n    if (count++   100) {\n      out.emit(Math.random());\n    }\n  }\n}  Finally, the  DefaultInputPort  and  DefaultOutputPort  classes are\nvery useful as base classes that can be extended when defining ports\nin operators.  public abstract class DefaultInputPort T  implements InputPort T , Sink T  {\n  private int count;\n\n  public Sink T  getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort T  implements Operator.OutputPort T  {\n  private transient Sink Object  sink;\n\n  final public void setSink(Sink Object  s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}  The  DefaultInputPort  class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract  process  method needs to be\nimplemented by any concrete derived class; it will be invoked via the Sink.put  override.  The  DefaultOutputPort  class also supports a sink and forwards calls\nto  emit  to the sink. The  setSink  method is called by the  StrAM \nexecution engine to inject a suitable sink at deployment time.  Output operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending  BaseOperator  for\nconvenience. For example, the  ConsoleOutputOperator  mentioned earlier\nis defined like this:  public class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort Object  input = new DefaultInputPort Object () {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}  Notice that the implementation of the abstract method DefaultInputPort.process  simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).", 
            "title": "Operator base classes and interfaces"
        }, 
        {
            "location": "/tutorials/salesdemo/#operators-in-the-sales-dimensions-application", 
            "text": "The application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the dag.addoperator  call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.  This diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.  JsonSalesGenerator (InputGenerator)  This class (new operator) is an input operator that generates a single\nsales event defined by a class like this:  class SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}  JsonToMapConverter (Converter)  This operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:  public class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort byte\\[\\]  input = new DefaultInputPort byte[] () {\n    public void process(byte\\[\\] message) {\n      Map String, Object  tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort Map String, Object  outputMap\n     = new DefaultOutputPort Map String, Object ();\n\n}  EnrichmentOperator (Enrichment)  This operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file  products.txt  that\nwe encountered earlier while building the application. It contains\ndata like this:  {\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}  The core functionality of this operator is in the  process  function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.  public class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort Map String, Object \n    outputPort = new DefaultOutputPort Map String, Object ();\n\n  public transient DefaultInputPort Map String, Object \n    inputPort = new DefaultInputPort Map String, Object () {\n\n    public void process(Map String, Object  tuple) {\n      ...\n    }\n  }\n}  DimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)  This operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.  AppDataSingleDimensionStoreHDHT (Store)  This operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.  PubSubWebSocketAppDataQuery (Query)  This is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike  ws:// gateway-host : port /pubsub  where gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message that looks like this to the URL\nwhere the value of the  data  key is the desired message content:  {\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}  Correspondingly, subscribers send messages like this\nto retrieve published message data:  {\"type\":\"subscribe\", \"topic\":\"foobar\"}  Topic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g.  foobar  in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.  This query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:  public class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}  The important method here is  convertMessage  to convert the input\nstring to a JSON object, get the value of the  data  key from the object\nand return it. The base classes look like this:  public class PubSubWebSocketInputOperator T  extends WebSocketInputOperator T  {\n  ...\n}  This class simply converts a JSON event into Java maps via the convertMessage  method.  public class WebSocketInputOperator T  extends\nSimpleSinglePortInputOperator T  implements Runnable {\n  ...\n}  This code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.  public abstract class SimpleSinglePortInputOperator T  extends BaseOperator\nimplements InputOperator, Operator.ActivationListener OperatorContext  {\n\n  final public transient BufferingOutputPort T  outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort T  extends DefaultOutputPort T  {\n    public void flush(int count) { ... }\n  }\n\n}  The class starts a separate thread which retrieves source events and\ninvokes the  emit  method of the output port; the output port buffers\nevents until the  flush  method is called at which point all buffered\nevents are emitted.  PubSubWebSocketAppDataResult (QueryResult)  This is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:  public class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n}  This class merely overrides the generic  convertMapToMessage  method of the\nbase class to generate the required JSON publish message.  public class PubSubWebSocketOutputOperator T  extends WebSocketOutputOperator T  {\n  ...\n}  This class, similarly, doesn't do much   the  convertMapToMessage \nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.  public class WebSocketOutputOperator T  extends BaseOperator {\n  public final transient DefaultInputPort T  input = new DefaultInputPort T () {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}  The key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is  process \nwhich converts the incoming event to a JSON message and sends it\nacross the connection.", 
            "title": "Operators in the Sales Dimensions application"
        }, 
        {
            "location": "/tutorials/salesdemo/#connecting-the-operators", 
            "text": "Now that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the StreamingApplication  interface:  public class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    ...\n\n    PubSubWebSocketAppDataQuery wsIn = new PubSubWebSocketAppDataQuery();\n    wsIn.setTopic(\"SalesDimensionsQuery\");\n    store.setEmbeddableQueryInfoProvider(wsIn);\n\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setTopic(\"SalesDimensionsResult\");\n\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, wsOut.input).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}  The key method to implement in an application is  populateDAG ; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the  DAG.StreamMeta \ninterface and is created via  DAG.addStream() . The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the populateDAG  function.  These two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.  Note : You can also find these instructions in the UI console. Click  Learn  in the menu\nbar, and then click the first link in the left panel:  Transform, Analyze, Alert .", 
            "title": "Connecting the operators"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-1-open-the-application-builder-interface", 
            "text": "On the DataTorrent RTS console, navigate to  App Packages .  Make sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).  Click the green  Create new application  button, and name the application\n    Sales Dimensions. The Application Canvas window should open.", 
            "title": "Step 1: Open the Application Builder interface"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-2-add-and-connect-operators", 
            "text": "Under  Operator Library  in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.   JSON Sales Event Generator (Input)  \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.  Enrichment (Enrich)  \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.  Simple App Data Dimensions Store (Store)    This operator\n   stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)    The dashboard connector for\n   visualization queries.  App Data Pub Sub Result (Result)    The dashboard connector for\n   visualization data results.     To connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:", 
            "title": "Step 2: Add and connect operators"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-3-customize-application-and-operator-settings", 
            "text": "Customize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the  Operator Inspector  panel\non the bottom to edit the operator and stream settings as described in the item:    Copy this Sales schema below into the  Event Schema JSON  field of  Input \n    operator, and the  Configuration Schema JSON  of the  Compute  and  Store \n    operators.  {\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to\n     SalesDimensionsQuery  and  SalesDimensionsResult  respectively.  Optional : In the  Building with Java  section, the  App Data Pub Sub Query (PubSubWebSocketAppDataQuery)  operator was not added to the DAG. Instead, it was embedded into the  store  operator to avoid query delays which may happen when the operator is blocked upstream.    Select the  Store  operator, and edit the  File Store  property.\n    Set  Base Path  value to  SalesDimensionsDemoStore . This sets the HDHT\n    storage path to write dimensions computation results to\n     /user/ username /SalesDimensionsDemoStore  on HDFS.\n       Click the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.   Note : Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.", 
            "title": "Step 3: Customize application and operator settings"
        }, 
        {
            "location": "/tutorials/salesdemo/#step-4-launch-the-application", 
            "text": "Once the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the Application Canvas  window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.  To launch the Sales Dimension application   Click the launch button at the top left of the application canvas screen.  Type a name for the application in the  Name this application  box.  (Optional) To configure the application using a configuration file, select\n     Use a configuration file  checkbox.  (Optional) To specify individual properties, select\n     Specify Launch Properties  checkbox.  Click Launch.    Once the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled  Monitoring the Sales\nDimensions Application with dtManage .   Go to the Sales Dimensions application operations page under the  Monitor  tab.  Confirm that the application is launched successfully by validating that\n    the state of the application under the  Application Overview  section\n    is  RUNNING .  Make sure that all the operators are successfully started under the\n     StramEvents  widget.  Navigate to the  physical  tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.\n       Note : This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.", 
            "title": "Step 4: Launch the application"
        }, 
        {
            "location": "/tutorials/salesdemo/#visualizing-application-data", 
            "text": "DataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled  dtDashboard\n- Application Data Visualization  at  https://docs.datatorrent.com .  After the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.", 
            "title": "Visualizing Application Data"
        }, 
        {
            "location": "/tutorials/salesdemo/#generate-dashboards", 
            "text": "If you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.  After the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:", 
            "title": "Generate dashboards"
        }, 
        {
            "location": "/tutorials/salesdemo/#adding-widgets", 
            "text": "To derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets:  bar chart ,  pie chart ,  horizontal bar chart ,  table , and note .  To add a widget   Click the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n      In the Data Source list, click a data source for your widget.  Select a widget type under  Available Widgets .\n      Click  add widget  button.   The widget is added to your dashboard.", 
            "title": "Adding widgets"
        }, 
        {
            "location": "/tutorials/salesdemo/#edit-a-widget", 
            "text": "After you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To edit a widget    Change the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.    Edit the widget:\n    a.  In the top-right corner of the widget, click  edit .\n    b.  Type a new title in the  Title  box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click  OK .\n        To remove a widget, in the top-right corner, click the  delete  button.", 
            "title": "Edit a widget"
        }, 
        {
            "location": "/tutorials/salesdemo/#monitoring-the-application", 
            "text": "Recall that after the application is built and validated, it can be\nlaunched from the  App Packages  page as described in an earlier chapter. \nThis section describes how you can monitor the running Sales Dimension application\nusing  dtManage .", 
            "title": "Monitoring the Application"
        }, 
        {
            "location": "/tutorials/salesdemo/#the-monitor-menu-option", 
            "text": "You can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click  Monitor , you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.  logical  This image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the  +  button at the top-left corner and choosing\nfrom the resulting dropdown list.     Application Overview  This widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.    StramEvents  This widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.    Logical DAG  This widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.  To include additional details   Click an operator for which you want to display additional details.  To display a detail on the top of this operator representation,\n    click the Top list, and select a metric.  To display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.     Logical Operators  This widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.    Streams  This operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.    Metrics Chart  This widget displays moving averages of tuples processed and latencies.    physical  The physical tab displays, in addition to  Application Overview \nand  Metrics Chart , 2 more widgets:     Physical Operators  This widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.    Containers  This widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.    physical-dag-view  The physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:   metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "The Monitor menu option"
        }, 
        {
            "location": "/tutorials/salesdemo/#monitor-sales-dimension-using-the-monitor-menu", 
            "text": "To monitor the application   Click  Monitor  on the menu bar to open the logical view of the DAG.\n      Ensure that the  State  is  Running , indicating that the application\n    is launched successfully.  Under  StramEvents , ensure that the operators from within the\n    application have started.  Click  physical  tab to open the physical view.   Ensure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n      Note: This is because we set the corresponding stream locality to CONTAINER_LOCAL  earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.", 
            "title": "Monitor Sales Dimension using the Monitor menu"
        }, 
        {
            "location": "/tutorials/salesdemo/#create-additional-tabs", 
            "text": "You can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the  Application\nOverview  widget.  To create additional tabs   Next to the  metric-view  tab, look for the plus sign (+) button.  Click this button to create an additional tab.  Provide a name for your tab.  Add widgets to your tab.", 
            "title": "Create additional tabs"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.\n\n\nMicrosoft Windows\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:\n\n\n\n\n\n\ngit\n -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nVirtualBox\n -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from \nhttps://www.virtualbox.org/wiki/Downloads\n. It is needed to run the DataTorrent Sandbox.\n\n\n\n\n\n\nDataTorrent Sandbox\n -- The sandbox can be downloaded from \nhttps://www.datatorrent.com/download\n. It is useful for testing simple applications since it contains Apache Hadoop and DataTorrent RTS pre-installed. If you have already installed a licensed version of DataTorrent RTS on a cluster, you can use that setup for deployment and testing instead of the sandbox.\n\n\n\n\n\n\n(Optional) If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\Program Files\\Java\\jdk1.7.0\\_80\\bin\n; for \ngit\n it might be \nC:\\Program Files\\Git\\bin\n; and for maven it might be \nC:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\n. Open a console window and enter the command:\n\n\necho %PATH%\n\n\n\nto see the value of the \nPATH\n variable and verify that the above directories are present. If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nNow run the following commands and ensure that the output is something similar to that shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\nMaven home: C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\\..\n\n\nJava version: 1.7.0_80, vendor: Oracle Corporation\n\n\nJava home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre\n\n\nDefault locale: en_US, platform encoding: Cp1252\n\n\nOS name: \nwindows 8\n, version: \n6.2\n, arch: \namd64\n, family: \nwindows\n\n\n\n\n\n\n\n\n\nInstalling the Sandbox\n\n\nThe sandbox includes, as noted above, a complete, stand-alone, instance of the\nDatatorrent RTS Enterprise Edition configured as a single-node Hadoop cluster. Please\nsee \nDataTorrent RTS Sandbox\n for details on setting up the sandbox.\n\n\nYou can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g. \njdk\n, \ngit\n, \nmaven\n) are pre-installed and also the package files created by your project are directly available to the DataTorrent tools such as  \ndtManage\n and \nApex CLI\n. The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.\n\n\nCreating a new Project\n\n\nYou can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example, \nnewapp.cmd\n and run it (the\nvalue for \narchetypeVersion\n can be a more recent version if available):\n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn -B archetype:generate ^\n  -DarchetypeGroupId=org.apache.apex ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.6.0-SNAPSHOT ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\nThe caret (^) at the end of some lines indicates that a continuation line follows.\n\n\nThis command will eventually become outdated, check the \napex-app-archetype README\n\nfor the latest version, which includes the most recent archetypeVersion.\n\n\nYou can also, if you prefer, use an IDE to generate the project as described in\n\nCreating a New Apache Apex Project with your IDE\n.\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the appropriate maven command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa\n. This file can then be uploaded to the DataTorrent GUI tool (\ndtManage\n) on your cluster if you have DataTorrent RTS installed there, or on the sandbox and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string  \nhello world:\n.\n\n\nIf you built this package on the host, you can transfer it to the sandbox using either a shared folder or the \npscp\n tool bundled with \nPuTTY\n mentioned earlier.\n\n\nYou can also run this application from the generated unit test file as described in the\nnext section.\n\n\nRunning Unit Tests\n\n\nTo run unit tests on Linux or OSX, simply run the usual maven command, for example: \nmvn test\n\nto run all tests or \nmvn -Dcom.example.myapexapp.ApplicationTest#testApplication test\n to run\na selected test. For the default application generated from the archetype, it should\nprint output like this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\nOn Windows, an additional file, \nwinutils.exe\n, is required; download it from\n\nhttps://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip\n\nand unpack the archive to, say, \nC:\\hadoop\n; this file should be present under\n\nhadoop-common-2.2.0-bin-master\\bin\n within it.\n\n\nSet the \nHADOOP_HOME\n environment variable system-wide to\n\nc:\\hadoop\\hadoop-common-2.2.0-bin-master\n as described at:\n\nhttps://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true\n. You should now be able to run unit tests normally.\n\n\nIf you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty \nhadoop.home.dir\n:\n\n\nmvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test\n\n\n\nor set the environment variable separately:\n\n\nset HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test\n\n\n\nWithin your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:\n\n\nEnv.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master\n\n\n\nat \nProperties \n Actions \n Run project \n Set Properties\n.\n\n\nSimilarly, in Eclipse (Mars) add it to the\nproject properties at \nProperties\n \n \nRun/Debug Settings\n \n \nApplicationTest\n\n\n \nEnvironment\n tab.\n\n\nBuilding the Sources\n\n\nThe Apache Apex source code is useful to have locally for a variety of reasons:\n- It has more substantial demo applications which can serve as models for new\n  applications in the same or similar domain.\n- When extending a class, it is helpful to refer to the base class implementation\n  of overrideable methods.\n- The maven build file \npom.xml\n can be a useful model when you need to add or delete\n  plugins, dependencies, profiles, etc.\n- Browsing the code is a good way to gain a deeper understanding of the platform.\n\n\nYou can download and build the source repositories by running the script \nbuild-apex.cmd\n\nlocated in the same place in the examples repository described above. Alternatively, if\nyou do not want to use the script, you can follow these simple manual steps:\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\npushd incubator-apex-core\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\nmvn clean install -DskipTests\npopd\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your\nlocal maven repository (typically \n.m2/repository\n under your home directory), and\n\nnot\n to the system directories, so \nAdministrator\n privileges are not required.\nThe  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see application package files in the \ntarget\n\ndirectories under each module; for example, the \nPi Demo\n package file\n\npi-demo-3.2.1-incubating-SNAPSHOT.apa\n should be under\n\nincubator-apex-malhar\\demos\\pi\\target\n.\n\n\nLinux\n\n\nMost of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.\n\n\nThe pre-requisites (such as \ngit\n, \nmaven\n, etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is: \necho $PATH\n).\n\n\nThe maven archetype command is the same except that continuation lines use a backslash (\n\\\n) instead of caret (\n^\n); the script for it is available in the same location and is named \nnewapp\n (without the \n.cmd\n extension). The script to checkout and build the Apache Apex repositories is named \nbuild-apex\n.", 
            "title": "Apex Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#microsoft-windows", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:    git  -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    VirtualBox  -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from  https://www.virtualbox.org/wiki/Downloads . It is needed to run the DataTorrent Sandbox.    DataTorrent Sandbox  -- The sandbox can be downloaded from  https://www.datatorrent.com/download . It is useful for testing simple applications since it contains Apache Hadoop and DataTorrent RTS pre-installed. If you have already installed a licensed version of DataTorrent RTS on a cluster, you can use that setup for deployment and testing instead of the sandbox.    (Optional) If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like  java  and  javac , the directory might be something like  C:\\Program Files\\Java\\jdk1.7.0\\_80\\bin ; for  git  it might be  C:\\Program Files\\Git\\bin ; and for maven it might be  C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin . Open a console window and enter the command:  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories are present. If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Now run the following commands and ensure that the output is something similar to that shown in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  Maven home: C:\\Users\\user\\Software\\apache-maven-3.3.3\\bin\\..  Java version: 1.7.0_80, vendor: Oracle Corporation  Java home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre  Default locale: en_US, platform encoding: Cp1252  OS name:  windows 8 , version:  6.2 , arch:  amd64 , family:  windows", 
            "title": "Microsoft Windows"
        }, 
        {
            "location": "/apex_development_setup/#installing-the-sandbox", 
            "text": "The sandbox includes, as noted above, a complete, stand-alone, instance of the\nDatatorrent RTS Enterprise Edition configured as a single-node Hadoop cluster. Please\nsee  DataTorrent RTS Sandbox  for details on setting up the sandbox.  You can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g.  jdk ,  git ,  maven ) are pre-installed and also the package files created by your project are directly available to the DataTorrent tools such as   dtManage  and  Apex CLI . The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.", 
            "title": "Installing the Sandbox"
        }, 
        {
            "location": "/apex_development_setup/#creating-a-new-project", 
            "text": "You can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example,  newapp.cmd  and run it (the\nvalue for  archetypeVersion  can be a more recent version if available):  @echo off\n@rem Script for creating a new application\nsetlocal\nmvn -B archetype:generate ^\n  -DarchetypeGroupId=org.apache.apex ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.6.0-SNAPSHOT ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal  The caret (^) at the end of some lines indicates that a continuation line follows.  This command will eventually become outdated, check the  apex-app-archetype README \nfor the latest version, which includes the most recent archetypeVersion.  You can also, if you prefer, use an IDE to generate the project as described in Creating a New Apache Apex Project with your IDE .  When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the appropriate maven command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa . This file can then be uploaded to the DataTorrent GUI tool ( dtManage ) on your cluster if you have DataTorrent RTS installed there, or on the sandbox and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string   hello world: .  If you built this package on the host, you can transfer it to the sandbox using either a shared folder or the  pscp  tool bundled with  PuTTY  mentioned earlier.  You can also run this application from the generated unit test file as described in the\nnext section.", 
            "title": "Creating a new Project"
        }, 
        {
            "location": "/apex_development_setup/#running-unit-tests", 
            "text": "To run unit tests on Linux or OSX, simply run the usual maven command, for example:  mvn test \nto run all tests or  mvn -Dcom.example.myapexapp.ApplicationTest#testApplication test  to run\na selected test. For the default application generated from the archetype, it should\nprint output like this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  On Windows, an additional file,  winutils.exe , is required; download it from https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip \nand unpack the archive to, say,  C:\\hadoop ; this file should be present under hadoop-common-2.2.0-bin-master\\bin  within it.  Set the  HADOOP_HOME  environment variable system-wide to c:\\hadoop\\hadoop-common-2.2.0-bin-master  as described at: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx?mfr=true . You should now be able to run unit tests normally.  If you prefer not to set the variable globally, you can set it on the command line or within\nyour IDE. For example, on the command line, specify the maven\nproperty  hadoop.home.dir :  mvn -Dhadoop.home.dir=c:\\hadoop\\hadoop-common-2.2.0-bin-master test  or set the environment variable separately:  set HADOOP_HOME=c:\\hadoop\\hadoop-common-2.2.0-bin-master\nmvn test  Within your IDE, set the environment variable and then run the desired\nunit test in the usual way. For example, with NetBeans you can add:  Env.HADOOP_HOME=c:/hadoop/hadoop-common-2.2.0-bin-master  at  Properties   Actions   Run project   Set Properties .  Similarly, in Eclipse (Mars) add it to the\nproject properties at  Properties     Run/Debug Settings     ApplicationTest    Environment  tab.", 
            "title": "Running Unit Tests"
        }, 
        {
            "location": "/apex_development_setup/#building-the-sources", 
            "text": "The Apache Apex source code is useful to have locally for a variety of reasons:\n- It has more substantial demo applications which can serve as models for new\n  applications in the same or similar domain.\n- When extending a class, it is helpful to refer to the base class implementation\n  of overrideable methods.\n- The maven build file  pom.xml  can be a useful model when you need to add or delete\n  plugins, dependencies, profiles, etc.\n- Browsing the code is a good way to gain a deeper understanding of the platform.  You can download and build the source repositories by running the script  build-apex.cmd \nlocated in the same place in the examples repository described above. Alternatively, if\nyou do not want to use the script, you can follow these simple manual steps:    Check out the source code repositories:  git clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar    Switch to the appropriate release branch and build each repository:  pushd incubator-apex-core\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\nmvn clean install -DskipTests\npopd    The  install  argument to the  mvn  command installs resources from each project to your\nlocal maven repository (typically  .m2/repository  under your home directory), and not  to the system directories, so  Administrator  privileges are not required.\nThe   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see application package files in the  target \ndirectories under each module; for example, the  Pi Demo  package file pi-demo-3.2.1-incubating-SNAPSHOT.apa  should be under incubator-apex-malhar\\demos\\pi\\target .", 
            "title": "Building the Sources"
        }, 
        {
            "location": "/apex_development_setup/#linux", 
            "text": "Most of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.  The pre-requisites (such as  git ,  maven , etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is:  echo $PATH ).  The maven archetype command is the same except that continuation lines use a backslash ( \\ ) instead of caret ( ^ ); the script for it is available in the same location and is named  newapp  (without the  .cmd  extension). The script to checkout and build the Apache Apex repositories is named  build-apex .", 
            "title": "Linux"
        }, 
        {
            "location": "/configure_IDE/", 
            "text": "Creating a New Apache Apex Project with your IDE\n\n\nWe describe the process for creating a new Apache Apex project for three\ncommon IDEs: \nIntelliJ IDEA\n, \nEclipse\n and \nNetBeans\n\n\nIntelliJ IDEA\n\n\nThe \nIntelliJ IDEA\n is available at \nhttps://www.jetbrains.com/idea/\n.\n\n\nFirst make sure you have the \nMaven Integration\n plugin enabled in the list at\n\nFile\n \n  Settings \n \nPlugins\n.\n\n\nNow, select \nFile\n \n \nNew\n \n \nProject\n. Choose \nMaven\n in the left pane\nand check \nCreate from archetype\n in the dialog box; at this point, you should be\nable to expand the \norg.apache.apex:apex-app-archetype\n element in the center pane and\nselect a suitable version as shown below:\n\n\n\n\nIf the \norg.apache.apex:apex-app-archetype\n element in not present in the center pane,\nyou can click the \nAdd Archetype...\n button and fill out the \nGroup ID\n, \nArtifact ID\n,\nand \nVersion\n entries, (leave \nRepository\n blank), as shown below:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nGroup ID\n\n\norg.apache.apex\n\n\n\n\n\n\nArtifact ID\n\n\napex-app-archetype\n\n\n\n\n\n\nVersion\n\n\n3.2.0-incubating (or any later version)\n\n\n\n\n\n\n\n\nClick \nOK\n. The archetype will appear in the list, selected. Note that this\n\nAdd Archetype...\n step is only required the first time you use the archetype; thereafter,\nyou can select the archetype directly.\n\n\nClick \nNext\n, and fill out the rest of the required information. For example:\n\n\n\n\nClick \nNext\n, and verify the information shown on the next screen (if you have a more\nrecent version of Maven installed, enter its home directory):\n\n\n\n\nClick \nNext\n, and fill out the project name and location: \n\n\n\n\nClick \nFinish\n, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within the IDEA.\n\n\nEclipse\n\n\nThe \nEclipse\n IDE is downloadable from \nhttps://eclipse.org/downloads/\n.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen Eclipse.\n\n\nSelect \nFile\n \n \nNew\n \n \nProject...\n \n\n    \nMaven\n \n \nMaven Project\n and click \nNext\n.\n    \n\n\nClick \nNext\n on the next dialog as well; you should now see a window\n    where you can configure archetype catalogs:\n    \n\n\nFrom the \nCatalog\n dropdown select a suitable remote catalog if one is present\n    and enter \napex\n in the \nFilter\n input box; you should see one or more entries\n    in the center pane with \nGroup Id\n of \norg.apache.apex\n and an \nArtifact Id\n\n    of \napex-app-archetype\n:\n    \n\n\nIf a suitable remote catalog is not present, you'll need to add it by clicking\n    the \nConfigure\n button to see a new dialog that shows a list of catalogs in\n    the middle pane and a \nAdd Remote Catalog\n button on the right:\n    \n\n\nClick that button to get a dialog where you can enter details of a new\n    catalog and enter \nhttp://repo.maven.apache.org/maven2/archetype-catalog.xml\n\n    for the \nCatalog File\n entry and suitable text (such as \nApache Catalog\n)\n    for the \nDescription\n entry:\n    \n\n\nIn either case, you should now be able to select the \napex-app-archetype\n\n    entry, click \nNext\n to see a window where you can enter details of the new\n    project and enter values similar to those in the table below (you'll need\n    to replace the default value \n${archetypeVersion}\n of \narchetypeVersion\n\n    with a suitable concrete version number like \n3.3.0-incubating\n):\n    \n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \ncom.example\n\n  \n\n  \n\n  \nArtifact ID\n\n  \nTestApex\n\n  \n\n  \n\n  \nVersion\n\n  \n0.0.1-SNAPSHOT\n\n  \n\n  \n\n  \nPackage\n\n  \ncom.example.TestApex\n\n  \n\n  \n\n  \narchetypeVersion\n\n  \n3.3.0-incubating\n\n  \n\n  \n\n  \n\n\n\n\nClick Finish; you should see the new project in your Package Explorer\n\n\n\n\nNetBeans\n\n\nThe \nNetBeans\n IDE is downloadable from \nhttps://netbeans.org/downloads/\n.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen NetBeans.\n\n\nClick \nFile\n \n \nNew Project\n.\n\n\n\n\nFrom the \nCategories\n column select \nMaven\n and from the \nProjects\n column,\n    select \nProject from Archetype\n, and click \nNext\n.\n    \n\n\n\n\n\n\nOn the Maven Archetype window, type \napex\n in the \nSearch\n box, and\n     from the list of \nKnown Archetypes\n, select \napex-app-archetype\n.\n     \n\n\n\n\nMake sure that the values for the fields match the values shown in this\n     table (except that the archetype version may be more recent):\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \norg.apache.apex\n\n  \n\n  \n\n  \nArtifact ID\n\n  \napex-app-archetype\n\n  \n\n  \n\n  \nVersion\n\n  \n3.3.0-incubating\n\n  \n\n  \n\n  \nRepository\n\n  \n/maven/content/repositories/releases\n\n  \n\n  \n\n  \n\n\n\n\nClick Next.\n\n\n\n\nOn the \nName and Location\n window, do the following:\n\n\n\n\nEnter a name for this project in the \nProject Name\n box, for example,\n    \nTestApex\n.\n\n\nEnter a location for this project in the \nProject Location\n box, for\n     example, \n/home/dtadmin/NetBeansProjects\n.\n\n\nEnter an ID in the \nGroup Id\n box, for example, \ncom.example\n.\n\n\nEnter a version for this project in the \nVersion\n box, for example,\n     \n1.0-SNAPSHOT\n.\n\n\nEnter the package name in the \nPackage\n box, for example,\n      \ncom.example.testapex\n.\n\n\n\n\n\n\n\n\n\n\nClick Finish.\n\n\n\n\n\n\nThe project is generated at the specified location and should be visible in\nthe left panel with the name \nMy Apex Application\n. You can right-click the\nproject and choose \nRename\n to provide a more descriptive name if you wish.", 
            "title": "Generate New Project in IDE"
        }, 
        {
            "location": "/configure_IDE/#creating-a-new-apache-apex-project-with-your-ide", 
            "text": "We describe the process for creating a new Apache Apex project for three\ncommon IDEs:  IntelliJ IDEA ,  Eclipse  and  NetBeans", 
            "title": "Creating a New Apache Apex Project with your IDE"
        }, 
        {
            "location": "/configure_IDE/#intellij-idea", 
            "text": "The  IntelliJ IDEA  is available at  https://www.jetbrains.com/idea/ .  First make sure you have the  Maven Integration  plugin enabled in the list at File     Settings    Plugins .  Now, select  File     New     Project . Choose  Maven  in the left pane\nand check  Create from archetype  in the dialog box; at this point, you should be\nable to expand the  org.apache.apex:apex-app-archetype  element in the center pane and\nselect a suitable version as shown below:   If the  org.apache.apex:apex-app-archetype  element in not present in the center pane,\nyou can click the  Add Archetype...  button and fill out the  Group ID ,  Artifact ID ,\nand  Version  entries, (leave  Repository  blank), as shown below:      Field  Value      Group ID  org.apache.apex    Artifact ID  apex-app-archetype    Version  3.2.0-incubating (or any later version)     Click  OK . The archetype will appear in the list, selected. Note that this Add Archetype...  step is only required the first time you use the archetype; thereafter,\nyou can select the archetype directly.  Click  Next , and fill out the rest of the required information. For example:   Click  Next , and verify the information shown on the next screen (if you have a more\nrecent version of Maven installed, enter its home directory):   Click  Next , and fill out the project name and location:    Click  Finish , and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within the IDEA.", 
            "title": "IntelliJ IDEA"
        }, 
        {
            "location": "/configure_IDE/#eclipse", 
            "text": "The  Eclipse  IDE is downloadable from  https://eclipse.org/downloads/ .  Generate a new Maven archetype project as follows:   Open Eclipse.  Select  File     New     Project...   \n     Maven     Maven Project  and click  Next .\n      Click  Next  on the next dialog as well; you should now see a window\n    where you can configure archetype catalogs:\n      From the  Catalog  dropdown select a suitable remote catalog if one is present\n    and enter  apex  in the  Filter  input box; you should see one or more entries\n    in the center pane with  Group Id  of  org.apache.apex  and an  Artifact Id \n    of  apex-app-archetype :\n      If a suitable remote catalog is not present, you'll need to add it by clicking\n    the  Configure  button to see a new dialog that shows a list of catalogs in\n    the middle pane and a  Add Remote Catalog  button on the right:\n      Click that button to get a dialog where you can enter details of a new\n    catalog and enter  http://repo.maven.apache.org/maven2/archetype-catalog.xml \n    for the  Catalog File  entry and suitable text (such as  Apache Catalog )\n    for the  Description  entry:\n      In either case, you should now be able to select the  apex-app-archetype \n    entry, click  Next  to see a window where you can enter details of the new\n    project and enter values similar to those in the table below (you'll need\n    to replace the default value  ${archetypeVersion}  of  archetypeVersion \n    with a suitable concrete version number like  3.3.0-incubating ):\n       \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   com.example \n   \n   \n   Artifact ID \n   TestApex \n   \n   \n   Version \n   0.0.1-SNAPSHOT \n   \n   \n   Package \n   com.example.TestApex \n   \n   \n   archetypeVersion \n   3.3.0-incubating \n   \n   \n     Click Finish; you should see the new project in your Package Explorer", 
            "title": "Eclipse"
        }, 
        {
            "location": "/configure_IDE/#netbeans", 
            "text": "The  NetBeans  IDE is downloadable from  https://netbeans.org/downloads/ .  Generate a new Maven archetype project as follows:   Open NetBeans.  Click  File     New Project .   From the  Categories  column select  Maven  and from the  Projects  column,\n    select  Project from Archetype , and click  Next .\n        On the Maven Archetype window, type  apex  in the  Search  box, and\n     from the list of  Known Archetypes , select  apex-app-archetype .\n        Make sure that the values for the fields match the values shown in this\n     table (except that the archetype version may be more recent):   \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   org.apache.apex \n   \n   \n   Artifact ID \n   apex-app-archetype \n   \n   \n   Version \n   3.3.0-incubating \n   \n   \n   Repository \n   /maven/content/repositories/releases \n   \n   \n     Click Next.   On the  Name and Location  window, do the following:   Enter a name for this project in the  Project Name  box, for example,\n     TestApex .  Enter a location for this project in the  Project Location  box, for\n     example,  /home/dtadmin/NetBeansProjects .  Enter an ID in the  Group Id  box, for example,  com.example .  Enter a version for this project in the  Version  box, for example,\n      1.0-SNAPSHOT .  Enter the package name in the  Package  box, for example,\n       com.example.testapex .      Click Finish.    The project is generated at the specified location and should be visible in\nthe left panel with the name  My Apex Application . You can right-click the\nproject and choose  Rename  to provide a more descriptive name if you wish.", 
            "title": "NetBeans"
        }, 
        {
            "location": "/application_development/", 
            "text": "Application Developer Guide\n\n\nReal-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.\n\n\nThe DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.\n\n\nDataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.\n\n\nIn the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called \nOperators\n interconnected\nby the data-flow edges called  \nStreams\n.\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the \nOperator Development Guide\n.\n\n\nRunning A Test Application\n\n\nThis chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:\n\n\n\n\nOpen up platform files in your IDE (for example NetBeans, or Eclipse)\n\n\nOpen Demos project\n\n\nOpen Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package\n\n\nSee the results in your system console\n\n\n\n\nCongratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.\n\n\n// Generates random numbers\nRandomEventGenerator rand = dag.addOperator(\nrand\n, new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of \nx\n and \ny\n\nRoundRobinHashMap\nString,Object\n rrhm = dag.addOperator(\nrrhm\n, new RoundRobinHashMap\nString, Object\n());\nrrhm.setKeys(new String[] { \nx\n, \ny\n });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator(\npicalc\n, new Script());\ncalc.setPassThru(false);\ncalc.put(\ni\n,0);\ncalc.put(\ncount\n,0);\ncalc.addSetupScript(\nfunction pi() { if (x*x+y*y \n= \n+maxValue*maxValue+\n) { i++; } count++; return i / count * 4; }\n);\ncalc.setInvoke(\npi\n);\ndag.addStream(\nrand_rrhm\n, rand.integer_data, rrhm.data);\ndag.addStream(\nrrhm_calc\n, rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\ndag.addStream(\nrand_console\n,calc.result, console.input);\n\n\n\n\nYou can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.\n\n\nTest Application: Yahoo! Finance Quotes\n\n\nThe PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from  \nYahoo! Finance\n \u00a0and computes the\nfollowing for four tickers, namely \nIBM\n,\n\nGOOG\n, \nYHOO\n.\n\n\n\n\nQuote: Consisting of last trade price, last trade time, and\n    total volume for the day\n\n\nPer-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute\n\n\nSimple Moving Average: trade price over 5 minutes\n\n\n\n\nTotal volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.\n\n\n\n\nThe operator StockTickerInput:\u00a0StockTickerInput\n\u00a0\nis\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:\n\n\n$ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1'\n\nIBM\n,203.966,1513041,\n1:43pm\n\n\nGOOG\n,762.68,1879741,\n1:43pm\n\n\nAAPL\n,444.3385,11738366,\n1:43pm\n\n\nYHOO\n,19.3681,14707163,\n1:43pm\n\n\n\n\n\nAmong all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.\n\n\nHere is the class implementation for StockTickInput:\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap\nString, Long\n lastVolume = new HashMap\nString, Long\n();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Double\n price = new DefaultOutputPort\nKeyValPair\nString, Double\n();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Long\n volume = new DefaultOutputPort\nKeyValPair\nString, Long\n();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, String\n time = new DefaultOutputPort\nKeyValPair\nString, String\n();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str = \nhttp://download.finance.yahoo.com/d/quotes.csv?s=\n;\n    for (int i = 0; i \n symbols.length; i++) {\n      if (i != 0) {\n        str += \n,\n;\n      }\n      str += symbols[i];\n    }\n    str += \nf=sl1vt1\ne=.csv\n;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter(\nhttp.protocol.cookie-policy\n, CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println(\nMethod failed: \n + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List\nString[]\n myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList\nString\n tuple = new ArrayList\nString\n(Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is \nSymbol\n,\nPrice\n,\nVolume\n,\nTime\n\n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol \n 0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair\nString, Double\n(symbol, currentPrice));\n            volume.emit(new KeyValPair\nString, Long\n(symbol, vol));\n            time.emit(new KeyValPair\nString, String\n(symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}\n\n\n\n\nThe operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.\n\n\nImportant: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.\n\n\nThe method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.\n\n\nMethod\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.\n\n\nNote that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.\n\n\nThe operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal\nK,V\n\u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal\nString,Long\n, where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if cumulative was set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.\n\n\nThe operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal\nK\n\u00a0from the\nstream\u00a0package.\n\n\nThe operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal\nK,V\n\u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal\nString,Double\n.\n\n\nThe operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal\nString,Long\n, but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.\n\n\nThe operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.\n\n\nThe operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage\nString,Double\n, which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.\n\n\nThe operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.\n\n\nConnecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo. \np\n\n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal\nString, Long\n getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal\nString, Long\n getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal\nString, Double\n getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal\nString, Double\n oper = dag.addOperator(name, new RangeKeyVal\nString, Double\n());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal\nString,Double,Long,String,?,?\n getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,Double,Long,String,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,HighLow,Long,Object,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage\nString, Double\n getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage\nString, Double\n oper = dag.addOperator(name, new SimpleMovingAverage\nString, Double\n());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort\nObject\n getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix + \n: %s\n);\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n       DefaultPartitionCodec\nString, Double\n codec = new DefaultPartitionCodec\nString, Double\n();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n\n}\n\n\n\n\nNote that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.\n\n\nIn the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the \nInstallation Guide\n.\n\n\nRunning a Test Application\n\n\nWe will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).\n\n\nThe platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.\n\n\nThe instructions below assume that the platform was installed in a\ndirectory \nINSTALL_DIR\n and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\n\nlocal mode\n\u00a0\n(in IDE or from command line) or on a  \nHadoop cluster\n \n.\n\n\nTo start the Apex CLI run\n\n\nINSTALL_DIR\n/bin/apex\n\n\n\nThe command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)\n\n\napex\n launch -local \nINSTALL_DIR\n/yahoo-finance-demo-3.4.0.apa\n\n\n\nTo terminate the application in local mode, enter Ctrl-C\n\n\nTu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)\n\n\napex\n launch \nINSTALL_DIR\n/yahoo-finance-demo-3.4.0.apa\n\n\n\nTo stop the application running in Hadoop, terminate it in the Apex CLI:\n\n\napex\n kill-app\n\n\n\nExecuting the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.\n\n\nLocal Mode\n\n\nIn local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.\n\n\nHadoop Cluster\n\n\nIn this section we discuss various Hadoop cluster setups.\n\n\nSingle Node Cluster\n\n\nIn a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.\n\n\nIn this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.\n\n\nMulti-Node Cluster\n\n\nIn a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.\n\n\nBefore you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.\n\n\n\n\nApache Apex Platform Overview\n\n\nStreaming Computational Model\n\n\nIn this chapter, we describe the the basics of the real-time streaming platform and its computational model.\n\n\nThe platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .\n\n\nApplications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.\n\n\nThe streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.\n\n\nA fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0\n\n\nThis atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.\n\n\nThe platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.\n\n\nNote that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.\n\n\n\n\nAlongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave  \nproperties\n\n\n\u00a0\nthat can be set to specify the\ndesired computation. Those interested in details, should refer to\n\nApex Malhar Operator Library\n\n.\n\n\nThe platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.\n\n\nA streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.\n\n\nAn operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include\n\n\n\n\n\n\nRunning the Application\n\n\n\n\nRead the\u00a0logical plan\u00a0of the application (DAG) submitted by the client\n\n\nValidate the logical plan\n\n\nTranslate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.\n\n\nRequest resources (Hadoop containers) from Resource Manager,\n    per physical plan\n\n\nBased on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.\n\n\nExecutes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.\n\n\n\n\n\n\n\n\nContinually monitoring the application via heartbeats from each StreamingContainer\n\n\n\n\nCollecting Application System Statistics and Logs\n\n\nLogging all application-wide decisions taken\n\n\nProviding system data on the state of the application via a  Web Service.\n\n\n\n\nSupporting \nFault Tolerance\n\n\na.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper\n\n\n\n\n\n\nSupporting \nDynamic\n    Partitioning\n:\n\u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).\n\n\n\n\nEnabling \nSecurity\n:\n\u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.\n\n\nEnabling \nDynamic  modification\n\u00a0\nof\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.\n\n\n\n\nAn example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.\n\n\n\n\nAn example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.\n\n\n\n\nHadoop Components\n\n\nIn this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.\n\n\nA streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future\n\n\nAll investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.\n\n\nYARN\n\n\nYARN\n is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through YARN's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).\n\n\nResource Manager (RM)\n\n\nResourceManager\n(RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.\n\n\nApplication Master (AM)\n\n\nThe AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.\n\n\nNode Managers (NM)\n\n\nThere is one \nNodeManager\n(NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.\n\n\nRPC Protocol\n\n\nCommunication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.\n\n\nHDFS\n\n\nHadoop includes a highly fault tolerant, high throughput\ndistributed file system (\nHDFS\n).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.\n\n\nDeveloping An Application\n\n\nIn this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.\n\n\nDevelopment Process\n\n\nWhile the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.\n\n\nDesign\n\n\n\n\nIdentify common, reusable operators. Use a library\n    if possible.\n\n\nIdentify scalability and performance requirements before\n    designing the DAG.\n\n\nLeverage attributes that the platform supports for scalability\n    and performance.\n\n\nUse operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.\n\n\nUse THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completely. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.\n\n\nThe overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.\n\n\nDo not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.\n\n\nPersist key information to HDFS if possible; it may be useful\n    for debugging later.\n\n\nDecide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.\n\n\n\n\nCreating New Project\n\n\nPlease refer to the \nApex Application Packages\n\u00a0for\nthe basic steps for creating a new project.\n\n\nWriting the application code\n\n\nPreferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.\n\n\nTesting\n\n\nWrite test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.\n\n\nGood test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)\n\n\nRunning an application\n\n\nThe platform provides a command line tool called Apex CLI (apex)\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.\n\n\nApex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.\n\n\nFor more details on CLI please refer to the \nApex CLI Guide\n.\n\n\nApplication API\n\n\nThis section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are\n\n\n\n\n\n\nInstantiate an application (DAG)\n\n\n\n\n\n\n(Optional) Set Attributes\n\n\n\n\nAssign application name\n\n\nSet any other attributes as per application requirements\n\n\n\n\n\n\n\n\nCreate/re-use and instantiate operators\n\n\n\n\nAssign operator name that is unique within the  application\n\n\nDeclare schema upfront for each operator (and thereby its  \nports\n)\n\n\n(Optional) Set \nproperties\n\u00a0\n and \nattributes\n\u00a0\n on the dag as per specification\n\n\nConnect ports of operators via streams\n\n\nEach stream connects one output port of an operator to one or  more input ports of other operators.\n\n\n(Optional) Set attributes on the streams\n\n\n\n\n\n\n\n\n\n\n\n\nTest the application.\n\n\n\n\n\n\nThere are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.\n\n\nJava API\n\n\nThe Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.\n\n\nThe developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.\n\n\nLet us revisit how the Yahoo! Finance test application constructs the DAG:\n\n\npublic class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n}\n\n\n\n\nProperty File API\n\n\nThe platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.\n\n\nCreate an application (DAG): myApplication.properties\n\n\n# input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort\n\n\n\n\nAbove snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.\n\n\nAttributes\n\n\nAttributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.\n\n\nOperators\n\n\nOperators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the  \nOperator Developer Guide\n. As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.\n\n\nAll operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.\n\n\nEach operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith Apex CLI\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.\n\n\nOperator Interface\n\n\nOperator interface in a DAG consists of \nports\n,\n\u00a0\nproperties\n,\n\u00a0and\n \nattributes\n\n\n.\n\u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.\n\n\nPorts\n\n\nPorts are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.\n\n\nHere are examples of an input and an output port from the operator\nSum.\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort\nV\n data = new DefaultInputPort\nV\n() {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort\nV\n sum = new DefaultOutputPort\nV\n(){ \u2026 };\n\n\n\n\nThe process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.\n\n\nThere is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.\n\n\nPort connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.\n\n\nAttributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe \nParallel\nPartitions\n\u00a0\nsection.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in \nConfiguration\n.\n\n\nProperties\n\n\nProperties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.\n\n\nAll non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.\n\n\nAttributes\n\n\nAttributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in  \nConfiguration Guide\n.\n\n\nOperator State\n\n\nThe state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe \ncheckpointed\n\u00a0\nevery\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.\n\n\nThe distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.\n\n\nStateless\n\n\nA Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.\n\n\nStateful\n\n\nA Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.\n\n\nOperator API\n\n\nThe Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.\n\n\nThe APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.\n\n\nIn the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.\n\n\nStreaming Window\n\n\nStreaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows\n\n\npublic void process(\ntuple_type\n tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown\n\n\n\n\nA tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.\n\n\nAggregate Application Window\n\n\nAn operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.\n\n\nSliding Application Window\n\n\nA sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.\n\n\nSingle vs Multi-Input Operator\n\n\nA single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.\n\n\nRecovery Mechanisms\n\n\nApplication developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely\n\n\n\n\nAt-least-once: All atomic batches are processed at least once.\n    No data loss occurs.\n\n\nAt-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.\n\n\nExactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.\n\n\n\n\nAt-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.\n\n\nRecovery mechanisms can be specified per Operator while writing\nthe application as shown below.\n\n\nOperator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);\n\n\n\n\nAlso note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.\n\n\nDetails are explained in the chapter on Fault Tolerance\nbelow\n.\n\n\nStreams\n\n\nA stream\u00a0is a connector (edge) abstraction and a fundamental building block of the platform. A stream consists of data tuples that flow from one port, which can be the output port of an operator, to one or more ports on other operators, which can be the input ports of those operators. Tuples enter a stream through its output port and leave via one or more input ports. \nA stream has the following characteristics\n\n\n\n\nTuples are always delivered in the same order in which they\n    were emitted.\n\n\nConsists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.\n\n\nA stream that connects two containers passes through a\n    buffer server.\n\n\nAll streams can be persisted (by default in HDFS).\n\n\nExactly one output port writes to the stream.\n\n\nCan be read by one or more input ports.\n\n\nConnects operators within an application, not outside\n    an application.\n\n\nHas an unique name within an application.\n\n\nHas attributes which act as hints to STRAM.\n\n\n\n\nStreams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:\n\n\n\n\nTHREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.\n\n\nCONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.\n\n\nNODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.\n\n\nRACK_LOCAL: On nodes in the same rack; also called\n    in-rack.\n\n\nunspecified: No guarantee. Could be anywhere within the\n    cluster\n\n\n\n\n\n\n\n\nAn example of a stream declaration is given below\n\n\nDAG dag = new DAG();\n \u2026\ndag.addStream(\nviews\n, viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality\n\n\n\n\nThe platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.\n\n\nIn a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of  \nstandard operator template\nlibrary\n \n\u00a0\nfollow\nthese principles.\n\n\nA logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.\n\n\nModes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.\n\n\nTHREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.\n\n\nValidating an Application\n\n\nThe platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely\n\n\n\n\nCompile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.\n\n\nInitialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.\n\n\nRun Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.\n\n\n\n\nCompile Time\n\n\nCompile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include\n\n\n\n\nSchema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.\n\n\nStream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream\n\n\nNaming: Compile time checks ensures that applications\n    components operators, streams are named\n\n\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.\n\n\nExamples include\n\n\n\n\n\n\nJavaBeans Validation\n:\n    Examples include\n\n\n\n\n@Max(): Value must be less than or equal to the number\n\n\n@Min(): Value must be greater than or equal to the\n    number\n\n\n@NotNull: The value of the field or property must not be\n    null\n\n\n@Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression\n\n\nInput port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)\n\n\nOutput Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)\n\n\n@Valid: For nested property validation \n a property should have this\n    annotation if its value  is itself an object whose properties\n    need to be validated.\n\n\n\n\n\n\n\n\nUnique names in application scope: Operators, streams, must have\n    unique names.\n\n\n\n\nCycles in the dag: DAG cannot have a cycle.\n\n\nUnique names in operator scope: Ports, properties, annotations\n    must have unique names.\n\n\nOne stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.\n\n\nApplication Window Period: Has to be an integral multiple the\n    streaming window period.\n\n\n\n\nRun Time\n\n\nRun time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to  \ndemos\n \nt\no\nillustrate these.\n\n\nError ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.\n\n\n\n\nMulti-Tenancy and Security\n\n\nHadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the\n\nConfiguration\n\n.\n\n\nSecurity\n\n\nThe platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.\n\n\nResource Limits\n\n\nHadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.\n\n\n\n\nScalability and Partitioning\n\n\nScalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.\n\n\nDaily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.\n\n\nThe platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.\n\n\nPartitioning\n\n\nIf all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.\n\n\nTo address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition\n\n\n\n\nLoad balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.\n\n\nSticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.\n\n\n\n\nWe plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.\n\n\nSticky Partition vs Round Robin\n\n\nAs noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.\n\n\nStream Codec\n\n\nThe platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.\n\n\nStatic Partitioning\n\n\nDAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.\n\n\nDynamic Partitioning\n\n\nIn streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.\n\n\nSince partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.\n\n\nDefault Partitioning\n\n\nThe platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.\n\n\nTypically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.\n\n\nDefault Dynamic Partitioning\n\n\nTriggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.\n\n\nThe default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0 and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\na partition split\u00a0occurs, resulting in 00\nand 10 with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the 01 partition, leading to a split into 001 and 101\nwith mask 111, etc.\n\n\nShould load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.\n\n\nNxM Partitions\n\n\nWhen two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.\n\n\nFigure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.\n\n\n\n\nParallel\n\n\nIn cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.\n\n\nIn Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.\n\n\nSince operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.\n\n\n\n\nThe following code shows an example of creating a parallel partition.\n\n\ndag.addStream(\nDenormalizedUserId\n, idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);\n\n\n\n\nParallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.\n\n\nParallel Partitions with Streams Modes\n\n\nParallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1-\n2 and 2-\n3 significantly impacts the performance.\n\n\nCONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.\n\n\nA NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.\n\n\nA RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.\n\n\nParallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.\n\n\n\n\nParallel-Partition\n\n\nParallel-Partition with THREAD_LOCAL stream\n\n\nParallel-Partition with CONTAINER_LOCAL stream\n\n\nParallel-Partition with NODE_LOCAL stream\n\n\nParallel-Partition with RACK_LOCAL stream\n\n\n\n\nThese attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.\n\n\n\n\nSkew Balancing Partition\n\n\nSkew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.\n\n\nFigure 7 shows an example of skew balancing partition. An example\nof 3x1 partition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.\n\n\nLet's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.\n\n\n\n\nSkew Unifier Partition\n\n\nIn this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.\n\n\nTo trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.\n\n\nFigure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.\n\n\nIn the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.\n\n\n\n\nCascading Unifier\n\n\nLet's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.\n\n\nCascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.\n\n\n\n\nFigure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1\n\n F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk) \n F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following\n\n\n\n\nI/O limit on containers to allow proper behavior in an\n    multi-tenant environment\n\n\nLoad on oprD instance\n\n\nBuffer server limits on fan-in, fan-out\n\n\nSize of reservoir buffer for inbound fan-in\n\n\n\n\nA more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.\n\n\nSLA\n\n\nA Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.\n\n\n\n\nFault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.\n\n\nState of the Application\n\n\nThe state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).\n\n\nOperators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.\n\n\nRecovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.\n\n\nCheckpointing\n\n\nSTRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).\n\n\nThe only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.\n\n\nIn case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.\n\n\nIf an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.\n\n\nCheckpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.\n\n\nAn operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.\n\n\nThe serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.\n\n\nA complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.\n\n\nIn general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.\n\n\nAt Least Once\n\n\nAt least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.\n\n\nIn general for this recovery mode, the average time lag on a node\noutage is\n\n\n= (CP/2*SW)*T + HC\n\n\nwhere\n\n\n\n\nCP\n\u00a0\u00a0- Checkpointing period (default value is 30 seconds)\n\n\nSW\n\u00a0\u00a0- Streaming window period (default value is 0.5 seconds)\n\n\nT\n\u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory\n\n\nHC\n\u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones\n\n\n\n\nA lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.\n\n\nAt Most Once\n\n\nThis recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.\n\n\nFor multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.\n\n\nIn general, in this recovery mode, the average time lag on a node\noutage is\n\n\n= SW/2 + HC\n\n\nwhere\n\n\n\n\n\n\nSW\n\u00a0- Streaming window period (default value is 0.5\nseconds)\n\n\n\n\n\n\nHC\n\u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones\n\n\n\n\n\n\nExactly Once\n\n\nThis recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.\n\n\nSpeculative Execution\n\n\nIn future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.\n\n\n\n\n\n\nAt an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways\n\n\n\n\nStatically as dictated by STRAM\n\n\nDynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality\n\n\n\n\n\n\n\n\nAt a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner\n\n\n\n\nEntire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.\n\n\n\n\nIn all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.\n\n\n\n\nDynamic Application Modifications\n\n\nDynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.\n\n\nSome examples are\n\n\n\n\nDynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.\n\n\nModification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.\n\n\nModification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.\n\n\nModification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.\n\n\nQuery Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).\n\n\n\n\nDynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to  \nConfiguration Guide\n\n.\n\n\n\n\nUser Interface\n\n\nThe platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to  \nUI Console Guide\n.\n\n\nDemos\n\n\nIn this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source\n\nApache Apex-Malhar repository\n.\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.\n\n\n\n\nComputation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).\n\n\nYahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).\n\n\nEchoserver Reads messages from a\n    network connection and echoes them back out.\n\n\nTwitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes\n\n\nTwitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes\n\n\nTwitter top N frequent words:\n    Computes top N frequent words in a sliding window\n\n\nWord count: Computes word count for\n    all words within a large file\n\n\nMobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).\n\n\nFrauddetect: Analyzes a stream of\n    credit card merchant transactions.\n\n\nMroperator:Contains several\n    map-reduce applications.\n\n\nR: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).\n\n\nMachinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Applications"
        }, 
        {
            "location": "/application_development/#application-developer-guide", 
            "text": "Real-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.  The DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.  DataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.  In the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called  Operators  interconnected\nby the data-flow edges called   Streams .\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the  Operator Development Guide .", 
            "title": "Application Developer Guide"
        }, 
        {
            "location": "/application_development/#running-a-test-application", 
            "text": "This chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:   Open up platform files in your IDE (for example NetBeans, or Eclipse)  Open Demos project  Open Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package  See the results in your system console   Congratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.  // Generates random numbers\nRandomEventGenerator rand = dag.addOperator( rand , new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of  x  and  y \nRoundRobinHashMap String,Object  rrhm = dag.addOperator( rrhm , new RoundRobinHashMap String, Object ());\nrrhm.setKeys(new String[] {  x ,  y  });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator( picalc , new Script());\ncalc.setPassThru(false);\ncalc.put( i ,0);\ncalc.put( count ,0);\ncalc.addSetupScript( function pi() { if (x*x+y*y  =  +maxValue*maxValue+ ) { i++; } count++; return i / count * 4; } );\ncalc.setInvoke( pi );\ndag.addStream( rand_rrhm , rand.integer_data, rrhm.data);\ndag.addStream( rrhm_calc , rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator( console , new ConsoleOutputOperator());\ndag.addStream( rand_console ,calc.result, console.input);  You can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.", 
            "title": "Running A Test Application"
        }, 
        {
            "location": "/application_development/#test-application-yahoo-finance-quotes", 
            "text": "The PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from   Yahoo! Finance  \u00a0and computes the\nfollowing for four tickers, namely  IBM , GOOG ,  YHOO .   Quote: Consisting of last trade price, last trade time, and\n    total volume for the day  Per-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute  Simple Moving Average: trade price over 5 minutes   Total volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.   The operator StockTickerInput:\u00a0StockTickerInput \u00a0 is\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:  $ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1' IBM ,203.966,1513041, 1:43pm  GOOG ,762.68,1879741, 1:43pm  AAPL ,444.3385,11738366, 1:43pm  YHOO ,19.3681,14707163, 1:43pm   Among all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.  Here is the class implementation for StockTickInput:  package com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap String, Long  lastVolume = new HashMap String, Long ();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Double  price = new DefaultOutputPort KeyValPair String, Double ();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Long  volume = new DefaultOutputPort KeyValPair String, Long ();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, String  time = new DefaultOutputPort KeyValPair String, String ();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str =  http://download.finance.yahoo.com/d/quotes.csv?s= ;\n    for (int i = 0; i   symbols.length; i++) {\n      if (i != 0) {\n        str +=  , ;\n      }\n      str += symbols[i];\n    }\n    str +=  f=sl1vt1 e=.csv ;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter( http.protocol.cookie-policy , CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println( Method failed:   + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List String[]  myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList String  tuple = new ArrayList String (Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is  Symbol , Price , Volume , Time \n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol   0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair String, Double (symbol, currentPrice));\n            volume.emit(new KeyValPair String, Long (symbol, vol));\n            time.emit(new KeyValPair String, String (symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}  The operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.  Important: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.  The method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.  Method\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.  Note that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.  The operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal K,V \u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal String,Long , where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if cumulative was set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.  The operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal K \u00a0from the\nstream\u00a0package.  The operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal K,V \u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal String,Double .  The operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal String,Long , but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.  The operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.  The operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage String,Double , which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.  The operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.  Connecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.  package com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo.  p \n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal String, Long  getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal String, Long  getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal String, Double  getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal String, Double  oper = dag.addOperator(name, new RangeKeyVal String, Double ());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal String,Double,Long,String,?,?  getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,Double,Long,String,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,Double,Long,String,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal String,HighLow,Long,?,?,?  getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,HighLow,Long,Object,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage String, Double  getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage String, Double  oper = dag.addOperator(name, new SimpleMovingAverage String, Double ());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort Object  getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix +  : %s );\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n       DefaultPartitionCodec String, Double  codec = new DefaultPartitionCodec String, Double ();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n\n}  Note that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.  In the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the  Installation Guide .", 
            "title": "Test Application: Yahoo! Finance Quotes"
        }, 
        {
            "location": "/application_development/#running-a-test-application_1", 
            "text": "We will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).  The platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.  The instructions below assume that the platform was installed in a\ndirectory  INSTALL_DIR  and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in local mode \u00a0 (in IDE or from command line) or on a   Hadoop cluster   .  To start the Apex CLI run  INSTALL_DIR /bin/apex  The command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)  apex  launch -local  INSTALL_DIR /yahoo-finance-demo-3.4.0.apa  To terminate the application in local mode, enter Ctrl-C  Tu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)  apex  launch  INSTALL_DIR /yahoo-finance-demo-3.4.0.apa  To stop the application running in Hadoop, terminate it in the Apex CLI:  apex  kill-app  Executing the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.", 
            "title": "Running a Test Application"
        }, 
        {
            "location": "/application_development/#local-mode", 
            "text": "In local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/application_development/#hadoop-cluster", 
            "text": "In this section we discuss various Hadoop cluster setups.", 
            "title": "Hadoop Cluster"
        }, 
        {
            "location": "/application_development/#single-node-cluster", 
            "text": "In a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.  In this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.", 
            "title": "Single Node Cluster"
        }, 
        {
            "location": "/application_development/#multi-node-cluster", 
            "text": "In a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.  Before you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.", 
            "title": "Multi-Node Cluster"
        }, 
        {
            "location": "/application_development/#apache-apex-platform-overview", 
            "text": "", 
            "title": "Apache Apex Platform Overview"
        }, 
        {
            "location": "/application_development/#streaming-computational-model", 
            "text": "In this chapter, we describe the the basics of the real-time streaming platform and its computational model.  The platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .  Applications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.  The streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.  A fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0  This atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.  The platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.  Note that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.   Alongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave   properties  \u00a0 that can be set to specify the\ndesired computation. Those interested in details, should refer to Apex Malhar Operator Library \n.  The platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.  A streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.  An operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.", 
            "title": "Streaming Computational Model"
        }, 
        {
            "location": "/application_development/#streaming-application-manager-stram", 
            "text": "Streaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include    Running the Application   Read the\u00a0logical plan\u00a0of the application (DAG) submitted by the client  Validate the logical plan  Translate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.  Request resources (Hadoop containers) from Resource Manager,\n    per physical plan  Based on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.  Executes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.     Continually monitoring the application via heartbeats from each StreamingContainer   Collecting Application System Statistics and Logs  Logging all application-wide decisions taken  Providing system data on the state of the application via a  Web Service.   Supporting  Fault Tolerance  a.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper    Supporting  Dynamic\n    Partitioning : \u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).   Enabling  Security : \u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.  Enabling  Dynamic  modification \u00a0 of\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.   An example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.   An example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.", 
            "title": "Streaming Application Manager (STRAM)"
        }, 
        {
            "location": "/application_development/#hadoop-components", 
            "text": "In this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.  A streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future  All investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.", 
            "title": "Hadoop Components"
        }, 
        {
            "location": "/application_development/#yarn", 
            "text": "YARN  is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through YARN's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).", 
            "title": "YARN"
        }, 
        {
            "location": "/application_development/#resource-manager-rm", 
            "text": "ResourceManager (RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.", 
            "title": "Resource Manager (RM)"
        }, 
        {
            "location": "/application_development/#application-master-am", 
            "text": "The AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.", 
            "title": "Application Master (AM)"
        }, 
        {
            "location": "/application_development/#node-managers-nm", 
            "text": "There is one  NodeManager (NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.", 
            "title": "Node Managers (NM)"
        }, 
        {
            "location": "/application_development/#rpc-protocol", 
            "text": "Communication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.", 
            "title": "RPC Protocol"
        }, 
        {
            "location": "/application_development/#hdfs", 
            "text": "Hadoop includes a highly fault tolerant, high throughput\ndistributed file system ( HDFS ).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.", 
            "title": "HDFS"
        }, 
        {
            "location": "/application_development/#developing-an-application", 
            "text": "In this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.", 
            "title": "Developing An Application"
        }, 
        {
            "location": "/application_development/#development-process", 
            "text": "While the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.", 
            "title": "Development Process"
        }, 
        {
            "location": "/application_development/#design", 
            "text": "Identify common, reusable operators. Use a library\n    if possible.  Identify scalability and performance requirements before\n    designing the DAG.  Leverage attributes that the platform supports for scalability\n    and performance.  Use operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.  Use THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completely. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.  The overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.  Do not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.  Persist key information to HDFS if possible; it may be useful\n    for debugging later.  Decide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.", 
            "title": "Design"
        }, 
        {
            "location": "/application_development/#creating-new-project", 
            "text": "Please refer to the  Apex Application Packages \u00a0for\nthe basic steps for creating a new project.", 
            "title": "Creating New Project"
        }, 
        {
            "location": "/application_development/#writing-the-application-code", 
            "text": "Preferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.", 
            "title": "Writing the application code"
        }, 
        {
            "location": "/application_development/#testing", 
            "text": "Write test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.  Good test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)", 
            "title": "Testing"
        }, 
        {
            "location": "/application_development/#running-an-application", 
            "text": "The platform provides a command line tool called Apex CLI (apex)\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.  Apex CLI can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.  For more details on CLI please refer to the  Apex CLI Guide .", 
            "title": "Running an application"
        }, 
        {
            "location": "/application_development/#application-api", 
            "text": "This section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are    Instantiate an application (DAG)    (Optional) Set Attributes   Assign application name  Set any other attributes as per application requirements     Create/re-use and instantiate operators   Assign operator name that is unique within the  application  Declare schema upfront for each operator (and thereby its   ports )  (Optional) Set  properties \u00a0  and  attributes \u00a0  on the dag as per specification  Connect ports of operators via streams  Each stream connects one output port of an operator to one or  more input ports of other operators.  (Optional) Set attributes on the streams       Test the application.    There are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.", 
            "title": "Application API"
        }, 
        {
            "location": "/application_development/#java-api", 
            "text": "The Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.  The developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.  Let us revisit how the Yahoo! Finance test application constructs the DAG:  public class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n}", 
            "title": "Java API"
        }, 
        {
            "location": "/application_development/#property-file-api", 
            "text": "The platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.  Create an application (DAG): myApplication.properties  # input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort  Above snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.", 
            "title": "Property File API"
        }, 
        {
            "location": "/application_development/#attributes", 
            "text": "Attributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operators", 
            "text": "Operators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the   Operator Developer Guide . As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.  All operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.  Each operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith Apex CLI\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.", 
            "title": "Operators"
        }, 
        {
            "location": "/application_development/#operator-interface", 
            "text": "Operator interface in a DAG consists of  ports , \u00a0 properties , \u00a0and\n  attributes  . \u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.", 
            "title": "Operator Interface"
        }, 
        {
            "location": "/application_development/#ports", 
            "text": "Ports are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.  Here are examples of an input and an output port from the operator\nSum.  @InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort V  data = new DefaultInputPort V () {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort V  sum = new DefaultOutputPort V (){ \u2026 };  The process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.  There is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.  Port connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.  Attributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe  Parallel\nPartitions \u00a0 section.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Configuration .", 
            "title": "Ports"
        }, 
        {
            "location": "/application_development/#properties", 
            "text": "Properties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.  All non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.", 
            "title": "Properties"
        }, 
        {
            "location": "/application_development/#attributes_1", 
            "text": "Attributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in   Configuration Guide .", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operator-state", 
            "text": "The state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe  checkpointed \u00a0 every\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.  The distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.", 
            "title": "Operator State"
        }, 
        {
            "location": "/application_development/#stateless", 
            "text": "A Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.", 
            "title": "Stateless"
        }, 
        {
            "location": "/application_development/#stateful", 
            "text": "A Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.", 
            "title": "Stateful"
        }, 
        {
            "location": "/application_development/#operator-api", 
            "text": "The Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.  The APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.  In the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.", 
            "title": "Operator API"
        }, 
        {
            "location": "/application_development/#streaming-window", 
            "text": "Streaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows  public void process( tuple_type  tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown  A tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.", 
            "title": "Streaming Window"
        }, 
        {
            "location": "/application_development/#aggregate-application-window", 
            "text": "An operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.", 
            "title": "Aggregate Application Window"
        }, 
        {
            "location": "/application_development/#sliding-application-window", 
            "text": "A sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.", 
            "title": "Sliding Application Window"
        }, 
        {
            "location": "/application_development/#single-vs-multi-input-operator", 
            "text": "A single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.", 
            "title": "Single vs Multi-Input Operator"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms", 
            "text": "Application developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely   At-least-once: All atomic batches are processed at least once.\n    No data loss occurs.  At-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.  Exactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.   At-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.  Recovery mechanisms can be specified per Operator while writing\nthe application as shown below.  Operator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);  Also note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.  Details are explained in the chapter on Fault Tolerance\nbelow .", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#streams", 
            "text": "A stream\u00a0is a connector (edge) abstraction and a fundamental building block of the platform. A stream consists of data tuples that flow from one port, which can be the output port of an operator, to one or more ports on other operators, which can be the input ports of those operators. Tuples enter a stream through its output port and leave via one or more input ports. \nA stream has the following characteristics   Tuples are always delivered in the same order in which they\n    were emitted.  Consists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.  A stream that connects two containers passes through a\n    buffer server.  All streams can be persisted (by default in HDFS).  Exactly one output port writes to the stream.  Can be read by one or more input ports.  Connects operators within an application, not outside\n    an application.  Has an unique name within an application.  Has attributes which act as hints to STRAM.   Streams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:   THREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.  CONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.  NODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.  RACK_LOCAL: On nodes in the same rack; also called\n    in-rack.  unspecified: No guarantee. Could be anywhere within the\n    cluster     An example of a stream declaration is given below  DAG dag = new DAG();\n \u2026\ndag.addStream( views , viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality  The platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.  In a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of   standard operator template\nlibrary   \u00a0 follow\nthese principles.  A logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.  Modes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.  THREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.", 
            "title": "Streams"
        }, 
        {
            "location": "/application_development/#validating-an-application", 
            "text": "The platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely   Compile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.  Initialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.  Run Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.", 
            "title": "Validating an Application"
        }, 
        {
            "location": "/application_development/#compile-time", 
            "text": "Compile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include   Schema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.  Stream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream  Naming: Compile time checks ensures that applications\n    components operators, streams are named", 
            "title": "Compile Time"
        }, 
        {
            "location": "/application_development/#initializationinstantiation-time", 
            "text": "Initialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.  Examples include    JavaBeans Validation :\n    Examples include   @Max(): Value must be less than or equal to the number  @Min(): Value must be greater than or equal to the\n    number  @NotNull: The value of the field or property must not be\n    null  @Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression  Input port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)  Output Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)  @Valid: For nested property validation   a property should have this\n    annotation if its value  is itself an object whose properties\n    need to be validated.     Unique names in application scope: Operators, streams, must have\n    unique names.   Cycles in the dag: DAG cannot have a cycle.  Unique names in operator scope: Ports, properties, annotations\n    must have unique names.  One stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.  Application Window Period: Has to be an integral multiple the\n    streaming window period.", 
            "title": "Initialization/Instantiation Time"
        }, 
        {
            "location": "/application_development/#run-time", 
            "text": "Run time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to   demos   t o\nillustrate these.  Error ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.", 
            "title": "Run Time"
        }, 
        {
            "location": "/application_development/#multi-tenancy-and-security", 
            "text": "Hadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the Configuration \n.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/application_development/#security", 
            "text": "The platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.", 
            "title": "Security"
        }, 
        {
            "location": "/application_development/#resource-limits", 
            "text": "Hadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.", 
            "title": "Resource Limits"
        }, 
        {
            "location": "/application_development/#scalability-and-partitioning", 
            "text": "Scalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.  Daily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.  The platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.", 
            "title": "Scalability and Partitioning"
        }, 
        {
            "location": "/application_development/#partitioning", 
            "text": "If all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.  To address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition   Load balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.  Sticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.   We plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/application_development/#sticky-partition-vs-round-robin", 
            "text": "As noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.", 
            "title": "Sticky Partition vs Round Robin"
        }, 
        {
            "location": "/application_development/#stream-codec", 
            "text": "The platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.", 
            "title": "Stream Codec"
        }, 
        {
            "location": "/application_development/#static-partitioning", 
            "text": "DAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.", 
            "title": "Static Partitioning"
        }, 
        {
            "location": "/application_development/#dynamic-partitioning", 
            "text": "In streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.  Since partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.", 
            "title": "Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#default-partitioning", 
            "text": "The platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.  Typically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.", 
            "title": "Default Partitioning"
        }, 
        {
            "location": "/application_development/#default-dynamic-partitioning", 
            "text": "Triggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.  The default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0 and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\na partition split\u00a0occurs, resulting in 00\nand 10 with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the 01 partition, leading to a split into 001 and 101\nwith mask 111, etc.  Should load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.", 
            "title": "Default Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#nxm-partitions", 
            "text": "When two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.  Figure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.", 
            "title": "NxM Partitions"
        }, 
        {
            "location": "/application_development/#parallel", 
            "text": "In cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.  In Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.  Since operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.   The following code shows an example of creating a parallel partition.  dag.addStream( DenormalizedUserId , idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);  Parallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.", 
            "title": "Parallel"
        }, 
        {
            "location": "/application_development/#parallel-partitions-with-streams-modes", 
            "text": "Parallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1- 2 and 2- 3 significantly impacts the performance.  CONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.  A NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.  A RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.  Parallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.   Parallel-Partition  Parallel-Partition with THREAD_LOCAL stream  Parallel-Partition with CONTAINER_LOCAL stream  Parallel-Partition with NODE_LOCAL stream  Parallel-Partition with RACK_LOCAL stream   These attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.", 
            "title": "Parallel Partitions with Streams Modes"
        }, 
        {
            "location": "/application_development/#skew-balancing-partition", 
            "text": "Skew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.  Figure 7 shows an example of skew balancing partition. An example\nof 3x1 partition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.  Let's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.", 
            "title": "Skew Balancing Partition"
        }, 
        {
            "location": "/application_development/#skew-unifier-partition", 
            "text": "In this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.  To trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.  Figure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.  In the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.", 
            "title": "Skew Unifier Partition"
        }, 
        {
            "location": "/application_development/#cascading-unifier", 
            "text": "Let's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.  Cascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.   Figure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1  F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk)   F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following   I/O limit on containers to allow proper behavior in an\n    multi-tenant environment  Load on oprD instance  Buffer server limits on fan-in, fan-out  Size of reservoir buffer for inbound fan-in   A more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.", 
            "title": "Cascading Unifier"
        }, 
        {
            "location": "/application_development/#sla", 
            "text": "A Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.", 
            "title": "SLA"
        }, 
        {
            "location": "/application_development/#fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/application_development/#state-of-the-application", 
            "text": "The state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).  Operators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.  Recovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.", 
            "title": "State of the Application"
        }, 
        {
            "location": "/application_development/#checkpointing", 
            "text": "STRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).  The only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.  In case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.  If an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.  Checkpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.  An operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.  The serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.  A complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms_1", 
            "text": "Recovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.  In general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#at-least-once", 
            "text": "At least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.  In general for this recovery mode, the average time lag on a node\noutage is  = (CP/2*SW)*T + HC  where   CP \u00a0\u00a0- Checkpointing period (default value is 30 seconds)  SW \u00a0\u00a0- Streaming window period (default value is 0.5 seconds)  T \u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory  HC \u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones   A lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.", 
            "title": "At Least Once"
        }, 
        {
            "location": "/application_development/#at-most-once", 
            "text": "This recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.  For multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.  In general, in this recovery mode, the average time lag on a node\noutage is  = SW/2 + HC  where    SW \u00a0- Streaming window period (default value is 0.5\nseconds)    HC \u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones", 
            "title": "At Most Once"
        }, 
        {
            "location": "/application_development/#exactly-once", 
            "text": "This recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.", 
            "title": "Exactly Once"
        }, 
        {
            "location": "/application_development/#speculative-execution", 
            "text": "In future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.    At an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways   Statically as dictated by STRAM  Dynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality     At a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner   Entire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.   In all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.", 
            "title": "Speculative Execution"
        }, 
        {
            "location": "/application_development/#dynamic-application-modifications", 
            "text": "Dynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.  Some examples are   Dynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.  Modification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.  Modification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.  Modification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.  Query Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).   Dynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to   Configuration Guide \n.", 
            "title": "Dynamic Application Modifications"
        }, 
        {
            "location": "/application_development/#user-interface", 
            "text": "The platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to   UI Console Guide .", 
            "title": "User Interface"
        }, 
        {
            "location": "/application_development/#demos", 
            "text": "In this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source Apache Apex-Malhar repository .\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.   Computation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).  Yahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).  Echoserver Reads messages from a\n    network connection and echoes them back out.  Twitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes  Twitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes  Twitter top N frequent words:\n    Computes top N frequent words in a sliding window  Word count: Computes word count for\n    all words within a large file  Mobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).  Frauddetect: Analyzes a stream of\n    credit card merchant transactions.  Mroperator:Contains several\n    map-reduce applications.  R: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).  Machinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Demos"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Application Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nPreliminaries\n\n\nPlease read \nApache Apex Development Environment Setup\n\nfor detailed instructions on:\n\n\n\n\nPre-requisite tools such as JDK, Maven, Git, etc.\n\n\nSetting up the Datatorrent sandbox, if necessary.\n\n\nCreating a new Apache Apex project using either the command line or one of the\n  common IDEs.\n\n\nRunning the unit test from the newly created project.\n\n\n\n\nWriting Your Own App Package\n\n\nPlease refer to the \nBeginner's Guide\n on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to \nOperator Development\n ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \ndt\n is a constant, \nattr\n is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\n\ncom.datatorrent.api.Context.DAGContext\n and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \ndt\n is a\nconstant, \noperator\n is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \nattr\n is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \ninput\n to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\n\ncom.datatorrent.api.Context.OperatorContext\n and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \nprop\n is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \nhost\n of the\nredis server for a \nredis\n output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \nset\n and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\n\nsetHost\n. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method \nsetHost\n\nwill be called on the \nredis\n operator with \n127.0.0.1\n as the argument.\n\n\nA property that is a simple collection like a list or a map can also be initialized\nin this way but it needs a couple of additional steps: (a) The property\nmust be initialized with an empty collection of the appropriate type.\n(b) An additional setter method for initializing items of the collection\nmust be present. For example, suppose we have properties named \nlist\n\nand \nmap\n in an operator named \nopA\n initialized with empty collections:\n\n\n  private List\nString\n list = new ArrayList\n();\n  private Map\nString, String\n map = new HashMap\n();\n\n  public List\nString\n getList() { return list; }\n  public void setList(List\nString\n v) { list = v; }\n\n  public Map\nString, String\n getMap() { return map; }\n  public void setMap(Map\nString, String\n v) { map = v; }\n\n\n\n\nYou can add items to those empty collections by using fragments like this in\nyour properties file provided you also add the setters shown below:\n\n\n  \nproperty\n\n    \nname\ndt.operator.opA.mapItem(abc)\n/name\n\n    \nvalue\n123\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.mapItem(pqr)\n/name\n\n    \nvalue\n567\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[0]\n/name\n\n    \nvalue\n1000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[1]\n/name\n\n    \nvalue\n2000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.opA.listItem[3]\n/name\n\n    \nvalue\n3000\n/value\n\n  \n/property\n\n\n\n\n\nThe required additional setter methods in operator \nopA\n:\n\n\n  public void setListItem(int index, String value) {\n    final int need = index - list.size() + 1;\n    for (int i = 0; i \n need; i++) list.add(null);\n    list.set(index, value);\n  }\n\n  public void setMapItem(String key, String value) {\n    map.put(key, value);\n  }\n\n\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \ninputport\n is used to denote an input port\nand \noutputport\n to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \ninput\n of an operator named \nrange\n to\nbe 4000.\n\n\nproperty\n\n  \nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cinputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nConfiguration\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\n in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.version\n3.2.0-incubating\n/apex.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\nManaging Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages\n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages\n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}\n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}\n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/applications/MyFirstApplication/launch\n\n\n\n\nPlease refer to the \nGateway API\n for the complete specification of the REST API.\n\n\nExamining and Launching Application Packages Through Apex CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (apex).  Please refer to the \nGateway API\n\nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n apex\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n apex\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\napex\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\n apex\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Apache Apex Application Packages"
        }, 
        {
            "location": "/application_packages/#preliminaries", 
            "text": "Please read  Apache Apex Development Environment Setup \nfor detailed instructions on:   Pre-requisite tools such as JDK, Maven, Git, etc.  Setting up the Datatorrent sandbox, if necessary.  Creating a new Apache Apex project using either the command line or one of the\n  common IDEs.  Running the unit test from the newly created project.", 
            "title": "Preliminaries"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the  Beginner's Guide  on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to  Operator Development  ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, apex-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix  dt  is a constant,  attr  is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in com.datatorrent.api.Context.DAGContext  and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix  dt  is a\nconstant,  operator  is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator,  attr  is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named  input  to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in com.datatorrent.api.Context.OperatorContext  and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword  prop  is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property  host  of the\nredis server for a  redis  output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword  set  and the property name with the first character of the name\ncapitalized. In the above example the setter method would become setHost . The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method  setHost \nwill be called on the  redis  operator with  127.0.0.1  as the argument.  A property that is a simple collection like a list or a map can also be initialized\nin this way but it needs a couple of additional steps: (a) The property\nmust be initialized with an empty collection of the appropriate type.\n(b) An additional setter method for initializing items of the collection\nmust be present. For example, suppose we have properties named  list \nand  map  in an operator named  opA  initialized with empty collections:    private List String  list = new ArrayList ();\n  private Map String, String  map = new HashMap ();\n\n  public List String  getList() { return list; }\n  public void setList(List String  v) { list = v; }\n\n  public Map String, String  getMap() { return map; }\n  public void setMap(Map String, String  v) { map = v; }  You can add items to those empty collections by using fragments like this in\nyour properties file provided you also add the setters shown below:     property \n     name dt.operator.opA.mapItem(abc) /name \n     value 123 /value \n   /property \n   property \n     name dt.operator.opA.mapItem(pqr) /name \n     value 567 /value \n   /property \n   property \n     name dt.operator.opA.listItem[0] /name \n     value 1000 /value \n   /property \n   property \n     name dt.operator.opA.listItem[1] /name \n     value 2000 /value \n   /property \n   property \n     name dt.operator.opA.listItem[3] /name \n     value 3000 /value \n   /property   The required additional setter methods in operator  opA :    public void setListItem(int index, String value) {\n    final int need = index - list.size() + 1;\n    for (int i = 0; i   need; i++) list.add(null);\n    list.set(index, value);\n  }\n\n  public void setMapItem(String key, String value) {\n    map.put(key, value);\n  }", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword  inputport  is used to denote an input port\nand  outputport  to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named  input  of an operator named  range  to\nbe 4000.  property \n   name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cinputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Configuration .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml  in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.version 3.2.0-incubating /apex.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/application_packages/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages  The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/application_packages/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}  The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}  The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/applications/MyFirstApplication/launch  Please refer to the  Gateway API  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-apex-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (apex).  Please refer to the  Gateway API \nto see samples for these commands.", 
            "title": "Examining and Launching Application Packages Through Apex CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   apex  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   apex  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  apex  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.   apex  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/configuration_packages/", 
            "text": "Apache Apex Configuration Packages\n\n\nAn Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nApex CLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.4.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.4.0 \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File-\nNew Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.4.0 (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of \n\n\n\n\ndatatorrent.apppackage.name\n\n\ndatatorrent.apppackage.minversion\n\n\ndatatorrent.apppackage.maxversion\n\n\n\n\nto reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\napex\n launch mydtapp-1.0.0.apa -conf mydtconfig.apc\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\n\n\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n\n\n$ curl -XPOST -T {name}.apc http://{yourhost:port}/ws/v2/configPackages\n\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName. \n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n\n\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \n xyz.apc\n$ unzip -t xyz.apc\n\n\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration package stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#apache-apex-configuration-packages", 
            "text": "An Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nApex CLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Apache Apex Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.4.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuration_packages/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/configuration_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.4.0 \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/configuration_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File- New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.4.0 (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/configuration_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/configuration_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of    datatorrent.apppackage.name  datatorrent.apppackage.minversion  datatorrent.apppackage.maxversion   to reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/configuration_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/configuration_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/configuration_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  apex  launch mydtapp-1.0.0.apa -conf mydtconfig.apc  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/configuration_packages/#related-rest-api", 
            "text": "", 
            "title": "Related REST API"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2configpackages", 
            "text": "Payload: Raw content of configuration package zip  Function: Creates or replace a configuration package zip file in HDFS  Curl example:  $ curl -XPOST -T {name}.apc http://{yourhost:port}/ws/v2/configPackages", 
            "title": "POST /ws/v2/configPackages"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesuserapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusername", 
            "text": "Function: Returns the information of the specified configuration package", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusernamedownload", 
            "text": "Function: Returns the raw config package file  Curl example:  $ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download   xyz.apc\n$ unzip -t xyz.apc", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;/download"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2apppackagesuserapp-pkg-nameapp-pkg-versionapplicationsapp-namelaunchconfigpackageuserconfpkgname", 
            "text": "Function: Launches the app package with the specified configuration package stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "POST /ws/v2/appPackages/&lt;user&gt;/&lt;app-pkg-name&gt;/&lt;app-pkg-version&gt;/applications/{app-name}/launch?configPackage=&lt;user&gt;/&lt;confpkgname&gt;"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nin parallel. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nin parallel.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. An operator is thus a piece of code that runs in a machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nThere are three types of operators based on function: \n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nOperators Position in a DAG\n\n\nWe may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote:\n There are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nOperators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nHow Operator Works\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nDeveloping Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found \nhere\n.\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n\n\nsetup(OperatorContext context)\n\n\nbeginWindow(long windowId)\n\n\nendWindow()\n\n\ntearDown()\n\n\n\n\nIn order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nApex Operators\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Configures the output frequency from the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.\n\n\nThe following tasks are executed as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nAs part of testing our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.\n\n\n\n\nAdvanced Features \n\n\nControl Tuple Support\n\n\nOperators now also have the capability to emit control tuples. These control tuples are different from the control tuples used by the engine like BEGIN_WINDOW and END_WINDOW tuples. Operators can create and emit their own control tuples which can be used to communicate to the down stream operators regarding some event. Examples of such events can be BEGIN_FILE, or END_FILE.\nMore details can be found at \nControl Tuples\n\n\nOperator Development Reference\n\n\nFor more details about operator development see \nOperator Developmnet Reference", 
            "title": "Guide"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nin parallel. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nin parallel.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. An operator is thus a piece of code that runs in a machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nThere are three types of operators based on function:    Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#operators-position-in-a-dag", 
            "text": "We may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note:  There are no cycles formed in the application\u00a0DAG.", 
            "title": "Operators Position in a DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "Operators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#how-operator-works", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "How Operator Works"
        }, 
        {
            "location": "/operator_development/#developing-custom-operators", 
            "text": "", 
            "title": "Developing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#functionality", 
            "text": "We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.", 
            "title": "Functionality"
        }, 
        {
            "location": "/operator_development/#inputs", 
            "text": "As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.", 
            "title": "Inputs"
        }, 
        {
            "location": "/operator_development/#outputs", 
            "text": "We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Outputs"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found  here .", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#the-operator-class", 
            "text": "The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:   setup(OperatorContext context)  beginWindow(long windowId)  endWindow()  tearDown()   In order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Apex Operators \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}", 
            "title": "The Operator Class"
        }, 
        {
            "location": "/operator_development/#class-operator-properties", 
            "text": "We define the following class variables:   sendPerTuple \u00a0- Configures the output frequency from the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();", 
            "title": "Class (Operator) properties"
        }, 
        {
            "location": "/operator_development/#the-constructor", 
            "text": "The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.  globalCounts = Maps.newHashMap();", 
            "title": "The Constructor"
        }, 
        {
            "location": "/operator_development/#setup-call", 
            "text": "The setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.  The following tasks are executed as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.", 
            "title": "Setup call"
        }, 
        {
            "location": "/operator_development/#begin-window-call", 
            "text": "The begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.", 
            "title": "Begin Window call"
        }, 
        {
            "location": "/operator_development/#process-tuple-call", 
            "text": "The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.", 
            "title": "Process Tuple call"
        }, 
        {
            "location": "/operator_development/#end-window-call", 
            "text": "This call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.", 
            "title": "End Window call"
        }, 
        {
            "location": "/operator_development/#teardown-call", 
            "text": "This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Teardown call"
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "As part of testing our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/operator_development/#advanced-features", 
            "text": "", 
            "title": "Advanced Features "
        }, 
        {
            "location": "/operator_development/#control-tuple-support", 
            "text": "Operators now also have the capability to emit control tuples. These control tuples are different from the control tuples used by the engine like BEGIN_WINDOW and END_WINDOW tuples. Operators can create and emit their own control tuples which can be used to communicate to the down stream operators regarding some event. Examples of such events can be BEGIN_FILE, or END_FILE.\nMore details can be found at  Control Tuples", 
            "title": "Control Tuple Support"
        }, 
        {
            "location": "/operator_development/#operator-development-reference", 
            "text": "For more details about operator development see  Operator Developmnet Reference", 
            "title": "Operator Development Reference"
        }, 
        {
            "location": "/operator_development_ref/", 
            "text": "Operator Development Reference\n\n\n1: Introduction\n\n\nA streaming application is a DAG that consists of computations (called\noperators) and data flow (called streams). In this document we will\ndiscuss details on how an operator works and its internals. This\ndocument aims to enable the reader to write efficient operators and make\ninformed design choices.\n\n\n2: Operators\n\n\nOperators are basic computation units of the application. They are\ninterconnected via streams to form an application. Operators are classes\nthat implement the Operator interface. They read from incoming streams\nof tuples and write to other streams. Reading and writing to streams is\ndone through connection points called ports. An operator may have no\nports (if the operator is an Input Adapter and the only operator in the\nDAG) and there is no limit to the number of ports an operator can have.\nOperators also\nhave \nproperties\n and \nattributes\n.\u00a0Properties\ncustomize the functional definition of the operator, while attributes\ncustomize the operational behavior of the operator.\n\n\nOperators are designed to be simple and easy to use and develop. Their\njob is to process tuples one at a time and emit a tuple as per business\nlogic. Ports that read tuples from a stream are input ports as they\nimplement the InputPort interface. Ports that write tuples to a stream\nare output ports which implement the OutputPort interface. Given\noperators A and B where an input port of B is connected to an output\nport of A, we say that A is an upstream operator of B and B is a\ndownstream operator of A. The platform ensures that tuples emitted by A\nreach B. Thus the operator solely focuses on the business logic\nprocessing. The engine, i.e. Streaming Application Master (STRAM) is\ncognizant of the connection order of the operators. This connectivity\nand the dataflow is an intrinsic part of engine\u2019s decision-making. All\nguarantees of data delivery upon outage is done via attributes that can\nvary per application. The same operator can be reused in different\napplications. This means that the same operator code can be used in\nvarious recovery mechanisms by different applications. Two instances of\nthe same operator within an application can also have different recovery\nmechanisms (attributes).\n\n\nOperators can have properties that are used to customize functional\nbehavior of the operator. Within an application attributes can be\nassigned to an operator that impact the operability of the operator.\nLater sections of this chapter cover the internal details of operators.\n\n\nAPI\n\n\nTo write your own operator you need to implement the Operator interface.\nThis interface provides the basic API for an operator developer. It\nextends the Component interface; important parts of both interfaces are\ndiscussed below.\n\n\nInterface - Component\n\n\n\n\nsetup (OperatorContext context)\n This is part of \u00a0Component. This is\n    invoked as part of initialization of the operator. This is part of\n    initial pre-runtime setup of the operator that is called during\n    initialization time. Care should be taken to not use this method to\n    set parameters. It is strongly advocated that users use setter\n    functions to set parameters. The setter functions work for both the\n    Java API as well as the properties file based API of an application.\n    Setter functions can also be invoked during runtime via CLI. Setup\n    is useful for validation checks that can only be done during\n    initialization time. Setup is also very useful for initializing\n    transient objects like connections with outside systems (e.g.\n    sockets or database sessions). These objects should not be part of\n    the state of the operator as they are in session, i.e. upon recovery\n    from a node outage, the connection has to be reestablished in setup.\n    All objects that are not serializable must be transient and\n    initialized during setup or at the declaration site.\n\n\nteardown()\n This too is part of Component and is called as part of\n    terminating the operator. Any session related objects should be shut\n    down or deallocated in this callback. Graceful termination of\n    outside connections (for example sockets, database sessions, etc.)\n    should be done in this method\n\n\n\n\nInterface Operator\n\n\n\n\nbeginWindow(long windowId)\n Invoked at the start of a window. The\n    windowId parameter identifies the window. All tuples received and\n    emitted in that window belong to this windowId. A window gets\n    started upon receipt of the first\u00a0beginWindow tuple on any\u00a0input\n    port. The window computation starts at the first line of the\n    beginWindow().\n\n\nendWindow()\n Invoked at the end of window. This call is only made\n    after all\u00a0the input ports receive an end_window tuple. All tuples\n    emitted in endWindow belong to the same windowId (passed during\n    beginWindow). The window computation ends with the last line in\n    endWindow().\n\n\n\n\nClass DefaultInputPort\n\n\n\n\nprocess(T tuple)\n Invoked within an input port for every tuple\n    received from the port of an upstream operator. This method is part\n    of the input port interface. The schema of the tuple is same as the\n    schema of the input port. This callback is an abstract call that\n    must be implemented by the operator developer.\n\n\n\n\nClass DefaultOutputPort\n\n\n\n\nemit(T tuple)\n To be called by the operator developer when a tuple\n    has to be emitted on the output port. Tuples can be emitted in\n    beginWindow, endWindow, or process callbacks. The schema of the\n    tuple is the same as the schema of the output port.\n\n\n\n\nInterface InputPort\n\n\n\n\nStreamCodec \n T \n getStreamCodec()\n A stream codec serializes or\n    deserializes the data that can be received on the port. If null,\n    STRAM uses the generic codec.\n\n\n\n\nInterface OutputPort\n\n\n\n\nUnifier \n T \n getUnifier()\n When operators are partitioned via round\n    robin partitioning they may need to merge the outputs of the\n    partitions. For example MaxMap operator may have N partitions, each\n    would emit its maximum value. The maximum value is computed by\n    finding the maximum of these N values. Thus another operator is\n    needed to unify these tuples. getUnifier() returns such an operator.\n    Developers need to implement the process() api. The schema of the\n    input tuples sent to merge() and the output port are identical to\n    the schema of the output port of the operator. If the operator has\n    only one input and one output port, and if both have identical\n    schema, then the operator itself can act as its unifier (see MaxMap\n    as an example). The default unifier is a passthrough operator and it\n    just passes the tuples from each partition to the downstream\n    operators.\n\n\n\n\nInterface ActivationListener\n\n\nAn operator may be subjected to activate/deactivate cycle multiple times\nduring its lifetime which is bounded by setup/teardown method pair. So\nit's advised that all the operations which need to be done right before\nthe first window is delivered to the operator be done during activate\nand opposite be done in the deactivate.\n\n\nAn example of where one would consider implementing ActivationListener\nis an input operator which wants to consume a high throughput stream.\nSince there is typically at least a few hundreds of milliseconds between\nthe time the setup method is called and the first window, the operator\ndeveloper would want to place the code to activate the stream inside\nactivate instead of setup.\n\n\n\n\nactivate(CONTEXT context)\n Gets called just before the first\n    beginWindow() call of the operator activation. Any processing that\n    needs to be done, just before the operator starts processing tuples,\n    must be done here.\n\n\ndeactivate()\n Gets called just before the operator is deactivated.\n    Opposite of the operations done in activate() must be done in\n    deactivate().\n\n\n\n\nInterface CheckpointListener\n\n\nOperators which need to be notified as soon as they are checkpointed or\ncommitted, must implement this interface.\n\n\n\n\ncheckpointed(long windowId)\n - Called when the operator is\n    checkpointed. The windowId parameter contains the window id after\n    which the operator was checkpointed.\n\n\ncommitted(long windowId)\n - Called when the operator is committed. A\n    commit operation is performed when all the operators in the DAG have\n    successfully checkpointed a particular window id. This window id is\n    passed as a parameter to the call back.\n\n\n\n\nInterface IdleTimeHandeler\n\n\nAn operator must implement this interface if it is interested in being\nnotified when it's idling. An operator can be said to be idling when\n\n\n\n\nAn operator which is an Input Adaptor, is not emitting any tuple or\n\n\n\n\nA generic operator or an Output Adapter is not processing any inputs\n\n\n\n\n\n\nhandleIdleTime()\n - When the operator is idling, it is explicitly\n    notified of such a state. The operators which implement this\n    interface should make use of this idle time to do any auxiliary\n    processing they may want to do when operator is idling. If the\n    operator has no need to do such auxiliary processing, they should\n    not\u00a0implement this interface. In such a case, the engine will put\n    the operator in scaled back processing mode to better utilize CPU.\n    It resumes its normal processing as soon as it detects tuples being\n    received or generated. If this interface is implemented, care should\n    be taken to ensure that it will not result in busy loop because the\n    engine keeps calling handleIdleTime until it does not have tuples\n    which it can give to the operator.\n\n\n\n\n\n\nAll the above callbacks happen in a single dedicated thread, i.e. at any\none point of time only one of these callbacks is being executed. No\nthread locking issues exist when writing code that process tuples.\nTuples are queued in a buffer and await their turn for process() calls\non their respective ports. Tuples are always in-order within a stream,\ni.e. an input port receives them in the same order that they were\nemitted. If an operator has two input ports then the order between\ntuples on two different streams is not guaranteed.\n\n\nFigure 1 shows an operator with two ports. It has a guarantee that\nprocess() method on input port i1 will be invoked for tuples in the\nfollowing order: t11, t12, t13, t14; and process() method on input port\ni2 will be invoked for tuples in the following order: t21, t22, t23,\nt24, t25. But whether i2::process( t21) would happen before\ni1:process(t11) is entirely dependent on when they arrive. Users are\nstrongly advised to not depend on the order of tuples in two different\nstreams as in a distributed application this order cannot be guaranteed.\nSuch operators would not be idempotent.\n\n\nTuples always belong to a specific window (identified by its window id).\nAll tuples emitted by the upstream operator, whether in beginWindow(),\nendWindow(), or process(), are part of the same window and hence each of\nthese tuples would invoke process() on the input port of a downstream\noperator in the same window. Output ports need not be connected for\ntuples to be emitted. This leniency allows STRAM to monitor throughput\non an unconnected port, and information that is useful for future\nchanges, including dynamic modification of the DAG. For more details and\nspecific interfaces please refer to their API classes.\n\n\n\n\nPorts\n\n\nPorts are connection points of an operator and are transient objects\ndeclared in the Operator class. Tuples flow in and out through these\nports. Input ports read from the stream while output port write to one.\nInput ports are implementation of the InputPort interface, while output\nports are implementations of the OutputPort interface. An output port\ncan be tagged as an error port by placing an annotation on it (error =\ntrue). Later in this section we would discuss the implications of\ntagging an output port as an error port. Ports have schemas as part of\ntheir declaration. The schema of a port is useful for compile time error\nchecking as well as runtime checks required for dynamically inserting\noperators at runtime.\n\n\nWe classify the operators in 3 buckets by observing the number of input\nand output ports.\n\n\n\n\nInput Adapters (Having no input ports)\n: The operator with no input\n    ports is called an Input Adapter. (An Adapter is a special term used\n    to denote an operator which interacts with the systems external to\n    the DAG. We\u2019ll use the terms Input Adapter and Input Operator\n    interchangeably.) An Input Adapter is useful for reading data from a\n    source external to the DAG. Some examples of external data sources\n    are HDFS, HBase, Sockets, Message Busses, Cassandra, MySql, Oracle,\n    Redis, Memcache, HTTP (GET), RSS feed, etc. It ingests the data from\n    one or more external sources, creates a stream, introduces control\n    tuples in the stream and emits them via its output ports. The\n    streams at the output ports of the Input Operator are ready to be\n    consumed by downstream operators. The Apache Apex Malhar library\n    implements quite a few Input Operators named after the data sources\n    they interface with. Input adapters have a window generator that\n    generates begin_window and end_window tuples periodically. Since\n    the STRAM initializes all window generators, they all start with\n    same window id and the application is synchronized on these\n    windows.\n\n\n\n\nInput Operators have two modes, namely Synchronous and Asynchronous\n\n\n\n\nSynchronous (Pull mode)\n:\u00a0Examples of synchronous data intake is an\n    HDFS file reader. The operator can thus decide the intake rate, i.e.\n    \"how much data to read per window?\". This operator can be\n    implemented with a single thread. Scaling such an operator is\n    determinable at static/compile time.\n\n\n\n\nAsynchronous (Push mode)\n:\u00a0Examples of asynchronous data is typical\n    message busses, where data is pushed to the reader (the Input\n    Operator). In such a scenario, the operator does not know the\n    inbound rate and hence is asynchronous with respect to the platform\n    windows in nature. Asynchronous input operators create another\n    thread to receive the incoming data. The basic operator task of\n    taking the incoming data and converting to/emitting a tuple is done\n    by the main thread of the operator. A connection buffer is provided\n    to ensure that the two threads communicate effectively. Users are\n    advised to pay particular attention to the inbound rate and design\n    an asynchronous operator carefully.\n\n\n\n\n\n\nOutput Adapter (Having no output port)\n:\u00a0The operator having no\n    output ports is called an Output Adapter. An Output Operator is\n    useful for handing over the data to the systems outside of the DAG.\n    Typically output operators remove the control tuples from the\n    stream, transform it, and write the resulting data to external\n    sources like HDFS, HBASE, Sockets, Message Busses, Cassandra, MySql,\n    Oracle, HTTP Server (POST), and others. The Apache Apex Malhar\n    library has a few Output Operators also available ready for use in a\n    DAG.\n\n\n\n\n\n\nGeneric Operator (Having both input and output ports)\n:\u00a0 These are\n    the most generic operators. They use one or more input ports to read\n    the input tuples and use one or more output ports to emit the\n    computed tuples. Unifiers are examples of generic operators where\n    process() is used for all input ports instead of process() per\u00a0input\n    port.\n\n\n\n\nSingleton Operator (Having no input or output ports)\n:\u00a0These\n    operators are self-sufficient in implementing the business logic and\n    have no input or output ports. An application with such an operator\n    will not have any other operator in the DAG. Such an operator\n    effectively acts as both an Input Adapter as well as an Output\n    Adapter.\n\n\n\n\nBoth output adapters and generic operators process tuples on the input\nport and look alike, the difference is that a general operator emits\ntuples while output adapter has to ensure tight integration with outside\nsources. As far as the interface is concerned, both have to implement\nthe\nOperator\n\u00a0\ninterface.\nBoth input adapters and generic operators emit tuples, but input\nadapters generate the streams by talking to external systems and at\ntimes need two threads. Input operators do not have a process API, and\nimplement the InputOperator interface. Only input adapters have window\ngenerators which insert begin_window and end_window events (control\ntuples) in the stream. The details of window generators are discussed in\na later Chapter.\n\n\nPorts have to be transient objects within the Operator as they should\nnot be part of a checkpointed state. The importance of the transient\nkeyword is explained in the section on checkpointing. Ports are objects\nin the operator class that the platform recognizes. All objects that\nimplement the interface InputPort and OutputPort make this list. A\nspecial kind of output port is also supported, namely a port for\nemitting error tuples. Though this port is in essence an output port,\ni.e. it writes tuples to a stream, it is identified as error port via an\nannotation (error = true). The STRAM can then leverage this information\nfor better reporting, statistics, tools (for example UI), monitoring\netc. The port need not be connected for the STRAM to gather information\non it, as emitted tuples can be counted even if no stream is connected\nto it.\n\n\nExamples\n\n\nInput Port\n\n\nAn example of an input port implementing the abstract class\nDefaultInputPort is\n\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort \nKeyValPair \n K,V \n \n data = new DefaultInputPort \nKeyValPair \n K,V \n \n(this); {\n  @Override\n/p\n\n  public void process(KeyValPair \n K,V \n tuple) {\n    // code for processing the tuple\n/p\n\n  }\n};\n\n\n\n\n\nprocess() is an abstract method and must be provided by the developer of\nthe operator. This method is responsible for implementing the processing\nlogic for the incoming tuple.\n\n\nOutput Port\n\n\nHere is an example of an output port implementing the abstract class\nDefaultOutputPort:\n\n\n\n@OutputPortFieldAnnotation(name = \nsum\n, optional = true)\npublic final transient DefaultOutputPort \n HashMap \n K, V \n \n sum = new DefaultOutputPort \n HashMap \n K, V \n \n(this) {\n  @Override\n  public Unifier \n HashMap \n K, V \n \n getUnifier()\n  { // The unifier is also an operator, that is initialized in the container of the downstream operator\n/p\n\n    return new UnifierHashMapSumKeys \n K, V \n();\n  }\n;\n\n\n\n\nError Port\n\n\nAn example of an error port is:\n\n\n\n@OutputPortFieldAnnotation(name = \nsum\n, optional = true, error=true)\npublic final transient DefaultOutputPort \nKeyValPair \nString,Integer \n \n monitor = new DefaultOutputPort \n KeyValPair \n String,Integer \n \n(this);\n\n\n\n\n\nUnifiers\n\n\nAn operator that uses round robin partitioning may need a unifier to\nmerge data back. An example of this is a sum operator that takes a\nHashMap as its input tuple. In the above example of an output port, the\nkey-value pairs in the HashMap consist of keys and their values that are\nto be added. If this operator is partitioned in N ways, each partition\nwould emit its own sum for a key. So the unifier (merge operator) needs\nto unify them and compute the final result per key as sum of all the\nvalues from every partition for that key. An operator that uses a sticky\nkey partition does not need to provide its own merge as the default\nmerge put in by the platform is a passthrough merge (i.e. just emits the\nincoming tuple on the output port).\n\n\nPort Declarations\n\n\nAnnotations should be added to the ports, as these are very useful in\nvalidating them. Annotations are declared with\n\n@InputPortFieldAnnotation()\n for input ports and\n\n@OutputPortFieldAnnotation()\n for output ports. Currently the following\nannotations are supported on the ports\n\n\n\n\nname = \u201cportname\u201d\n\n\noptional = true (or false)\n\n\nerror = true (or false)\n\n\n\n\nOver time mode annotations would be added as per needs of our customers.\n\n\nThe schema of the tuple emitted on a port is part of the port\ndeclaration and thus is defined as compile time. These types may be\ndefined using Java generics. The schema declaration on the ports allow\ncompile time validation when an output port of an operator is connected\nto input port of another operator. On IDEs like Eclipse or NetBeans,\nthis check is done as you type. The application creator thus experiences\nproductivity gains by leveraging type safety alerts generated by the\nIDEs. The port schema also enables the STRAM to do schema validation\nbefore run time addition of an operator. The added sub-query can do a\ncheck (instanceof) before accepting a tuple.\n\n\nOperator Properties\n\n\nOperator can have properties which are used to customize the\nfunctionality of the operator. Operator properties should be accessed\nusing standard Java beans convention (read via getter method, and write\nvia setter method) so that values can be injected and validated as\nper\n\u00a0\njavax\nannotation\n\u00a0.\nThrough properties is a broad definition of anything instantiated inside\nan operator, as a generic definition we count variables of the object\nthat can be read and written to by the user via getter and setter\nmethods.\n\n\nThe DAG (including all the operators) is initialized on the client side\nand then passed to the STRAM. This means that the setter method is not\ncalled during initialization in the container. So during node recovery\nthe checkpointed state needs to have these values. In order for the\nproperties to be saved in the checkpoint, they cannot be transient. This\nis irrespective of whether they are set only during initialization and\nwould have same values in every checkpoint. The platform enables runtime\nchanges to property values via setter functions. If a setter function is\ncalled during runtime, STRAM would call the setter function after\ncheckpoint is loaded, so runtime changes are remembered irrespective of\nwhether the checkpoint happened between property set and operator\noutage.\n\n\nOperator Attributes\n\n\nOperators can have attributes. Attributes are provided by the platform\nand do not belong to user code. Attributes do not change the basic\nfunctionality of an operator, and most often the operator code would be\noblivious to these attributes. The platform recognizes these and takes\ndecisions (compile time or run time) that mainly impacts the performance\nof an operator. A very common example of an attribute is\nAPPLICATION_WINDOW_COUNT. This attribute is by default equal to the\nstreaming window (count = 1), and an user can set this on any operator.\nAttributes also exist at the application level and on a stream. If an\noperator can only work with certain values of an attribute, they can be\naccessed during setup call on an operator. During development of the\noperator the values of these attributes could be used to provide better\noperability for the operator. Details of attributes are covered in\n\nConfiguration\n.\n\n\nTemplates\n\n\nSince operators are java classes they can be templatized (generic types)\nfor reuse. The streaming platform leverages all features in Java for\ntype safety and parameterization. Templates have no performance impact\nas they exist only at compile time. Templates greatly reduce the number\nof operators that need to be maintained, and enable operators for lot\nmore reuse. Apache Apex Malhar Library Templates are examples of\noperator templates. In general, it is advisable to templatize operators\nwherever possible. This allows the core computation to be used for\nderived types. For example Sum operator can be used for any tuple type\nthat\nextends Number.\nIn general an operator should set the schema of the tuple to the minimum\nrequired for its functionality. Apache Apex Malhar Library package\nsample code has examples of a lot of such operators.\n\n\nValidations\n\n\nThe platform provides various ways of validating application\nspecifications and data input. Validation of an application is done in\nthree phases, namely:\n\n\n\n\nCompile Time\n:\u00a0Caught during application development, and is most\n    cost effective. These checks are mainly done on declarative objects\n    and leverage the Java compiler. For example \"schema of two ports on\n    the same stream should match\".\n\n\nInitialization Time\n:\u00a0When the application is being initialized,\n    before submitting to Hadoop. These checks are related to\n    configuration/context of an application, and are done by the logical\n    DAG builder implementation. For example, check that \"a particular\n    port must be connected\"\n\n\nRuntime\n:\u00a0Validations done when the application is running. This is\n    the costliest of all checks. These are checks that can only be done\n    at runtime as they involve data integrity. For example divide by 0\n    check, or negative age value.\n\n\n\n\nOperator properties and ports are validated at compile time (IDE) as\nwell as launch time (STRAM). Java annotations are used extensively to\ncheck validations such as ranges for the properties, string formatting,\netc. For example, a port, by default has to be connected. This check is\ndone during compile time in your IDE, as well as during launch time\n(CLI->STRAM). If you have an port whose connectivity is optional, it\nneeds to be mentioned via annotation given below:\n\n\n\n@InputPortFieldAnnotation(name = \ndata\n, optional=true)\npublic transient DefaultInputPort data = new DefaultInputPort(this) { }\n\n\n\n\n\nCompile Time Validations\n\n\nCompile time validations apply when an application is specified in Java\ncode and include all checks that can be done by Java compiler in the\ndevelopment environment (including IDEs like NetBeans or Eclipse).\n\n\nExamples include the following\n\n\n\n\nSchema Validation\n: The tuples on ports are POJO (plain old java\n    objects) and java compiler checks to ensure that all the ports on a\n    stream have the same schema. This check works in any IDE as well as\n    with a command line java compiler.\n\n\nStream Check\n: Single Output port and at least one Input port per\n    stream. A stream can only have one output port writer. This is part\n    of the addStream API. This check ensures that developers only\n    connect one output port to a stream. The same signature also ensures\n    that there is at least one input port for a stream\n\n\nNaming\n: Compile time checks ensures that applications components\n    like operators and streams are named\n\n\n\n\nWe will continue to add as many checks as possible in compile time. But\nthe Java compiler cannot do 100% of the checks, and validations have to\nbe done during initialization time. Further, for the errors that can\nhappen only during run time, run time checks are supported.\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validates include various checks that are done post\ncompile, and before the application starts running in a cluster (or\nlocal mode). These are mainly configuration/contextual in nature. This\nchecks are as critical to proper functionality of the application as the\ncompile time validations.\n\n\nExamples include the following\n\n\n\n\nJavaBeans\n    Validation\n\n\n\n\nThe most common checks for properties are listed below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstraint\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n@Max\n\n\nThe value of the field or property must be an integer value lower than or equal to the number in the value element\n\n\n@Max(100)\n\n\nint success_ratio;\n\n\n\n\n\n\n@Min\n\n\nThe value of the field or property must be an integer value greater than or equal to the number in the value element\n\n\n@Min(0)\n\n\nint mass;\n\n\n\n\n\n\n@NotNull\n\n\nThe value of the field or property must not be null\n\n\n@NotNull\n\n\nString username;\n\n\n\n\n\n\n@Pattern\n\n\nThe value of the field or property must match the regular expression defined in the regexp element\n\n\n@Pattern(regexp = \nlte|lt|eq|ne|gt|gte\n, message = \nValue has to be one of lte, lt, eq, ne, gt, gte\n)\n\n\nString cmp;\n\n\n\n\n\n\n\n\n\n\n\nInput port connectivity\n:\u00a0An input port must be connected, unless\n    optional = true. For example:\n\n\n\n\n@InputPortFieldAnnotation(name = \"data\", optional = true)\n\n\n\n\nOutput port connectivity\n:\u00a0At least one output port must be\n    connected, unless optional = true.\n\n\n\n\n@OutputPortFieldAnnotation(name = \"result\", optional = true)\n\n\n\n\n\n\nUnique names for operators and streams\n:\u00a0Check is done to ensure that\n    operators and streams are named uniquely within the application.\n\n\n\n\n\n\nCycles in the dag\n:\u00a0Check is done to ensure that the application is a\n    DAG and no cycle exists.\n\n\n\n\nPort annotations with the same name\n:\u00a0Checked to ensure that the port\n    annotations are one per port\n\n\nOne stream per port\n:\u00a0A port can connect to only one stream. This\n    check is very clear for an input port as by definition only one\n    stream can input to a port. This check is also done for an output\n    port, even though an output port can technically write to two\n    streams. This is done as the message queue infrastructure does a\n    much better job and allows STRAM to better do run time\n    optimizations. If you must have two streams originating from a\n    single output port, use a StreamDuplicator \u00a0operator.\n\n\n\n\nRuntime Validations\n\n\nRun time checks are those that are done when the application is running.\nThe\n\u00a0\nApache\nApex platform provides rich run time error handling mechanisms. These\nchecks also include those done by application business logic. In these\ncases the platform allows applications to count and audit these.\n\n\nExamples include the following:\n\n\n\n\nError Tuples\n:\u00a0Applications can log error tuples during computation.\n    These are counted on a per window basis and are part of application\n    statistics. This allows for an automated way to integrate error\n    catching into a monitoring system. These tuples can be written into\n    a file. The default storage engine is HDFS. This allows application\n    developers to debug and analyse data.\n\n\nTest Framework\n:\u00a0The ability to test the application at run time\n    includes the need to verify the functionality against a previous\n    run. Users can do that via a test framework that allows an\n    application to process parallel sets of streams, get the\n    computations on a selected list of output ports and compare the\n    results. The test would be user driven, would verify that the atomic\n    micro-batches generated are identical and certify an application.\n    This test allows users to validate any change to the application\n    operator logic in run time.\n\n\nError Ports\n:\u00a0Ports that emit error tuples are supported by the\n    \n@OutputPortFieldAnnotation\n. This means that users have to create\n    these ports just like any other ports with an annotation\n    \n@OutputPortFieldAnnotation (error = true)\n. The platform can then\n    support a \u201cnumber of error tuples\u201d computation just by counting the\n    errors emitted on this port and aid in persistence.\n\n\nBucket Testing\n:\u00a0When any new application logic needs to be tried\n    out, it is useful to first try it on a very small sample of a\n    stream. The concept of bucket testing will be available in a future\n    release. In this mode, the incoming streams (input adapters) would\n    have a Sampler operator inserted on appropriate streams in the\n    application. Users would thus be able to test a modification to the\n    application on a small subset of data. This feature will be\n    available in a later release.\n\n\nUnifier Output Port\n:\u00a0A unifier has to have only one output port as\n    the platform needs to know which port to connect the input port of\n    the downstream operator to. This is achieved by checking that\n    unifier operator has only one output port.\n\n\n\n\nRegarding the stream schema and the ability to validate dynamic DAG\nmodifications, the engine has only partial knowledge of the port types\nsince type information may be erased by the Java compiler and the exact\ntype used to instantiate a parameterized operator may not be known\n(HashMap, ArrayList, Object, etc.). Many validations are performed\nbefore an application is fully launched (compile time, startup). There\nare cases that need runtime checks, including connectivity, error\ntuples, operator uptime, container uptime, and others. Run time schema\nchecks can still be performed using the instanceof() function. Object\noriented design and usage of base class APIs is very critical to enable\nrun time schema checks. If the port of the operator to be inserted on\nthe stream only accesses the base class API, then run time insertion can\nbe done via usage of the instanceof() function. In later versions,\nstronger support would be provided by the platform for run time check\nfor dynamic insertion of objects.\n\n\nTransient Fields\n\n\nDuring the application lifecycle, operators are serialized as part of\ncheckpointing in the distributed environment. The platform uses Kryo for\nserialization. If a field is marked \"transient\" it will not be\nserialized. A number of guidelines need to be followed to avoid error or\ninefficiencies due to serialization:\n\n\n\n\nPort objects need to be declared as public static final. The port\n    objects are non-static inner classes that cannot be deserialized due\n    to absence of a default constructor. There is no need to serialize\n    port objects as port objects are stateless. State can be maintained\n    by the enclosing operator instance.\n\n\nUse non-transient fields for properties that configure/customize the\n    operator (and are set at initialization time). These properties are\n    required when operator is setup in the executing container and need\n    to be serialized. You can allow runtime changes to properties via\n    setter functions. This means that such objects/properties must be\n    non-transient as you now rely on the set value being serialized.\n\n\nUse non-transient for objects that are required across window\n    boundaries. This will ensure that the state is checkpointed and\n    recovered when the operator is transferred between containers as\n    result of rescheduling or an outage. An operator whose functionality\n    depends on tuples from previous windows must have those objects as\n    non-transient.\n\n\nUse transient objects for state of the operator in the following two\n    cases:\n\n\nIf the computation and resultant output from an incoming tuple does\n    not depend on any previous tuples. This means that the operator is\n    completely stateless. For such an operator, all serializable objects\n    should be declared as transient to reduce the size of the checkpoint\n    state.\n\n\nIf the computation and resultant output from an incoming tuple only\n    depends on other tuples in the same window. Such an operator can\n    technically be made stateless. The down side is that you would need\n    to ensure that application window is always equal to the streaming\n    window. If the application window is not equal (i.e. more than) the\n    streaming window, then the checkpoint will still happen on streaming\n    window boundaries, and therefore the state would include the state\n    of the operator within a window. This is because once the\n    application developer sets an application window, the endWindow call\n    is skipped until the application window boundary. As an operator\n    developer you can force this operator to be stateless by checking\n    for the application window value during the setup call.\n\n\n\n\nTo force the checkpoint of such operators to align with the application\nwindow boundary set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This will ensure more efficient execution\nas it avoids unnecessary serialization. Currently, by default\ncheckpointing happens at the checkpoint period, and that is most likely\nmore frequent than an application window. For long application windows\nit is more efficient to checkpoint more often and thereby avoid a very\nlong replay queue. For such an operator the parameters including those\nthat only exist within a window are part of the state of the operator as\nthe checkpoint would happen on an intermediate streaming window\nboundary. As an operator developer, if you are using transient objects\nfor state within a window it is very critical that you ensure that the\nApplication window is equal to streaming window. This can be done either\nduring the setup call, or you can set the checkpointing window count to\napplication window count.\n\n\nStateless vs Stateful\n\n\nThe platform intends to discern stateless vs. stateful without direct\nspecification from the developer. This depends on declaring objects as\ntransient. Therefore, care should be taken to ensure that objects that\nmay form state of the operator are not declared as transient. This\naspect is a critical part of the operator design.\n\n\nThe state of an operator is defined as all the non-transient fields of\nthe operator. To exclude a variable from being included in the state,\nthe developer must declare that variable as transient. Since the\ncomputing model of the platform is to treat windows as atomic\nmicro-batches, the operator state is checkpointed after an endWindow and\nbefore the next beginWindow event. In a future version, we will be\nadding an attribute that would allow an operator developer to force the\ncheckpoint to align with the application window boundary.\n\n\nCheckpointing is a process of serializing the operator object to disk\n(HDFS). It is a costly procedure and it blocks computations. To avoid\nthis cost checkpointing is done every Nth window, or every T time\nperiod, where T is significantly greater than the streaming window\nperiod. A stateless operator (all variables are transient) can recover\nmuch quicker than a stateful one and pay a far lower checkpointing\npenalty. In the future, the platform may interpret an operator as\nstateless and remove the checkpointing penalty. The needed windows are\nkept by the upstream buffer server and are used to recompute the lost\nwindows, and also rebuild the buffer server in the current container. A\npassthrough operator is an example of a stateless operator. For example,\nan operator that takes a line, splits it into key/val pairs, and sends\none HashMap\\\nkey,val> per line is stateless as the outbound tuple is\nderived solely from the in-bound tuple.\n\n\nThe Stateless vs. Stateful distinction of an operator does not impact\nthe core platform engine, i.e. the platform does not distinguish between\nthese. However it is a very good distinction to learn for those who want\nto write operators. The data that has to be transferred to the next\nwindow must be stored in non-transient objects, as a node recovery\nbetween these two windows must restore its state. If the outbound tuple\ndepends on tuples before the current tuple, then the operator has a\nstate. These objects must be non-transient. The state of an operator is\nindependent of the number of ports (ports are transient objects).\n\n\nA stateless operator is defined as one where no data is needed to be\nkept at the end of any API call (e.g. beginWindow, process, or\nendWindow). In other words, all variables are transient and one where\nthe outbound tuple solely depends on the current in-bound tuple. Such an\noperator is completely stateless. Another stateless operator is one\nwhere all the computations of a window can be derived from all the\ntuples the operator receives within that window. This guarantees that\nthe output of any window can be reconstructed by simply replaying the\ntuples that arrived in that window. But for such an operator the\noperator developer needs a mechanism to ensure that no checkpointing\nmust be done within an application window. The downside of such an\noperator is that if it is used for a very large application window, e.g.\na few hours, then the upstream buffer server must maintain that many\ntuples. Stateless operators have much more efficient recovery, however\noperator developers must take into account the cost of maintaining the\nbuffers of an application window in the upstream bufferserver.\n\n\nA stateful operator is defined as one where data needs to be stored at\nthe end of a window for computations, i.e some variables are\nnon-transient. Stateful operators are also those where outbound tuples\ndepends on more than one incoming tuple (for example aggregates), and\nthe operator developer has allowed checkpointing within an application\nwindow. Stateful operators have costlier recovery as compared to\nstateless operators.\n\n\nIf a container object has to be cleared after every window, it is better\nto clear it in endWindow as compared to beginWindow. Since checkpointing\nis done after endWindow, in cases where the checkpointing is done only\nafter application window or streaming window, this object is empty. If\nthe operator developer is not sure about application developer not\nasking for checkpointing within an application window, clearing\ncontainer objects in endWindow is more efficient as the object in most\ncases does not become part of the checkpoint state.\n\n\nSingle vs Multiple Inputs\n\n\nA single-input operator by definition has a single upstream operator,\nsince there can only be one writing port for a stream. \u00a0If an operator\nhas a single upstream operator, then the beginWindow on the upstream\nalso blocks the beginWindow of the single-input operator. For a window\nto start processing on any operator at least one upstream operator has\nto start processing that window. The platform supports \u201cat least one\nupstream operator should start processing\u201d model to allow processing to\nstart as soon as possible. For a single input operator this is very\nefficient as setting up internal objects for processing can be done in\nparallel and before the first tuple arrives.\n\n\nA multi-input operator can have more than one upstream operator. Each\ninput port has an upstream operator. In some cases all the input ports\nmay be connected to output ports of the same upstream operator. In\neither case the multi-input operator will not close a window until all\nthe upstream operators close this window. Thus the closing of a window\nis a blocking event. A multi-input operator is also the point in the DAG\nwhere windows of all upstream operators are synchronized. The windows\n(atomic micro-batches) from a faster (or just ahead in processing)\nupstream operators are queued up until the slower upstream operator\ncatches up. The STRAM monitors and guarantees these conditions. These\nmay occur dynamically due to changes in throughputs on various streams,\ncaused by internal or external events.\n\n\nHierarchical Operators\n\n\nHierarchical operators are those whose functional logic it itself a DAG.\nThe difference between an application and a hierarchical operator is\nthat the later has ports and thus can be inserted in other applications.\nHierarchical operators are very useful for reuse, and enforcing common\ndesign practices. The development for hierarchical operator is underway\nand will be available in a future version.\n\n\nMacros\n\n\nMacros are sets of instructions that run via the CLI to insert a\nsub-query (sub-DAG) into the application. They have a similar result as\nthe hierarchical operators, except they are executed at run time. In the\nfuture when the CLI supports application creation, macros can be used\nduring application creation time. Macros would still differ from a\nhierarchical operators as the operator would have a scope that the macro\nmay not.\n\n\n3: Computation Model\n\n\nIn this section we discuss details of the computation model of an\noperator. It is very important for an operator developer to understand\nthe nuances of the operator computational model to be able to leverage\nall the rich features provided by the platform.\n\n\nSingle Dedicated Thread Execution\n\n\nAll code of an operator always executes in a single dedicated thread\nwithin a Hadoop container. This is a design principle of the platform,\nand that ensures that all the calls on the operator are invoked in the\nsame thread. This frees the user from synchronization considerations in\nthe operator logic. This makes coding very easy for the operator, as\nonly one tuple is processed at any given time and no locking has to be\ndone. Arrival of two tuples on the same stream is not an issue as they\nalways arrive in order. However, for operators that process multiple\ninput streams, the platform serializes (de-parallelizes) the multiple\nparallel streams into one so that the processing of individual tuples\ndoes not overlap with that of another tuple on the same stream or any\nother stream processed by the operator. Since the streams are coming\nfrom different sources in a distributed architecture, there is no\nguarantee when tuples in two different streams would arrive with respect\nto each other. Thus the order of tuples from two different streams is\nrandom and cannot be guaranteed. The only guarantee is that all tuples\nof the streams that the operator listens to that belong to a window id\nwould arrive in that window. Operator logic should thus be written in\nsuch a way that it does not depend on the order of tuples from two\ndifferent streams. An operator that has such a dependency is not\nidempotent. The serialization of streams also ensures that all the\nstreams are synchronized on window boundaries. The management of\nin-flowing tuples is handled by the platform to allow the operator code\nto run in a single thread execution, and frees the developer to focus\nsolely on the business logic.\n\n\nMutability of tuples\n\n\nTuples emitted and received from ports are POJO (plain old java\nobjects). Operator developers should be careful about the ownership\nsemantics of tuples that are mutable objects and passed through\nTHREAD_LOCAL or CONTAINER_LOCAL streams (shared object references).\nImmutable objects can be passed without regard to the type of stream\nbeing used. In general immutable objects are better as they free the\ndownstream tuple from having to worry about the life span of the tuple.\nFor performance reasons it may be okay to use mutable objects during\nprocessing (internal data of the objects), but emitted tuples should\nideally be immutable. Emitting mutable tuples may be ok if they are not\naccessed by the operator post emit.\n\n\nWithin the operator code, care should be taken to not change a tuple\nemitted by that operator. The example below may result in bad data\n(empty HashMap) for a downstream operator:\n\n\n// Bad example, as object 't' is being changed post emit\nHashMap \n String, Integer \n t = new HashMap \nString, Integer \n ();\nt.put(\ni\n, 2);\noutputport.emit(t);\nt.clear();\n\n\n\n\n\nPassthrough vs End of Window tuples\n\n\nTuples can be emitted during beginWindow(), process(), or endWindow().\nAll these tuples would have the windowId associated with the window\nmarked by beginWindow() and endWindow() calls. The same operator can\nemit a tuple in each of the above calls.\n\n\nA passthrough tuple is one that is emitted during beginWindow() or\nprocess(). The reason for emitting a tuple during beginWindow() is rare\nas the tuple would be emitted without receiving any data. The most usual\npassthrough tuple is the one that is emitted during an input port's\nprocess() call. In case of multiple input ports, care should be given\nthat a passthrough tuple is not dependent of the order of tuple arrival\non two different input ports, as this order is only guaranteed within a\nstream, i.e. on one input port. Passthrough tuples do not always imply\nthat the operator is stateless. For an operator to be stateless, an\noutbound tuple should depend only on one tuple of the process call.\n\n\nAn end of window tuple is the one that is emitted during endWindow()\ncall. These tuples are usually aggregates, are and the ones that wait\nfor all tuples to be received in the window before they are emitted.\nThey are thus by definition not impacted by the order in which tuples\nmay arrive on different input windows. However existence of an end of\nwindow tuple almost always means that the operator is stateful.\n\n\nA passthrough tuple has lower latency than an end of window tuple. The\nplatform does not differentiate between a \u201cpassthrough\u201d tuple or an \u201cend\nof window\u201d tuple, and in fact does not even recognize them as different.\nFor downstream operators there is no semantic difference between a tuple\nthat is emitted as passthrough or as end of window. All the tuples are\nalways received as part of the process() call on the input ports of\ndownstream operators. This means that the difference in latency of a\ntuple emitted during end of window as compared to that during process()\nis \"window period/2\". The rest of the downstream operators make no\ncontribution to latency other than their processing time, which would be\nidentical for both passthrough and end of window emission. This is true\nfor the entire DAG irrespective of which operator decides between\nemitting a tuple during process call or during endWindow() call. An end\nof window tuple also has one safety feature that it is easy to ensure\nthat the outbound tuple does not depend on the order in which tuples\narrived on different input ports. Operators that only emit during\nendWindow() can be clearly marked as \"independent of tuple order in two\ninput streams\". This is very useful as it allows a host of\noptimizations, re-partitioning, and other operations to be done. Even if\nthe STRAM is not able to dynamically figure this difference, there is a\npossibility of adding annotations on the operator to signal the STRAM\nabout their behavior in the future. In the future, the platform would\nleverage this data.\n\n\nStreaming Window vs Application Window\n\n\nThe platform supports two windowing concepts. The first one is the\nstreaming window, i.e. the smallest atomic micro-batch. All bookkeeping\noperations are done between an end of a window and the start of the next\none.\u00a0This includes checkpointing, inserting the\ndebugger, inserting charting, recovery from the start of such a window,\netc. The second is the application window. This is decided by the\nfunctionality of the application. For example if an application is\ncomputing revenue per hour, then the application window is one hour. For\nbetter operability, minimizing the streaming window is recommended as\nthat denotes the smallest loss of computations in the event of an\noutage. The platform supports both these natively. By default, the\ncheckpointing is aligned with the endWindow() call, and hence it aligns\nwith end of the application window. For large application windows this\nmay be a problem as the upstream bufferserver has to retain a very long\nqueue. The way around is to set \"checkpoint=true\" for within the\napplication window, and to write the operator\nin such a fashion that the state of the operator consists of both the\ndata that is passed between windows, as well as the dependency of an\noutbound tuple on all previous tuples. Such a state definition is safe.\nThe downside is that the state may be large and make checkpointing\ncostly. A way around this is to partition the operator based on the size\nof the state to ensure checkpoints are being saved equally by\npartitions.\n\n\nThere are two types of application windows: aggregate application\nwindows and sliding application windows. Aggregate application windows\nare for applications that do computations per fixed time period. It is\nspecified by an attribute since exactly the same code works for a\nstreaming window as application window. Once the aggregate application\nwindow flag is specified (by default it is equal to streaming window)\nthe the begin_window starts an application window and then the\nintermediate begin and end windows are skipped until the application\nwindow boundary is reached. Then the end_window is called. Sliding\napplication windows are for applications that do computations for past\nfixed time period. In such a case the operator code needs access to all\nthe state of the computations in a sliding fashion (e.g. the last 5\nminutes). The platform has a Java interface written for sliding windows\nthat allows operator developers to easily code a sliding window\noperator. For details refer to Real-Time Streaming Platform\nGuide.\n\n\n4: Commonly Used Operators\n\n\nThere are operators in the platform that support commonly needed\nfunctions. In this chapter we review them.\n\n\nWindow Generator\n\n\nA Window Generator is inserted for every input adapter. This operator is\ntasked with creating windows. The windows from two different window\ngenerators get synced on the first operator that listens to streams\nwhose windows originate from the window generators. Since the downstream\noperator syncs the streams on endWindow, all the tuples from a window\ngenerator that started early would wait until the lagging window\ngenerator starts sending window events.\n\n\nAs much as possible, window generators should not be directly used by\nusers in their designs. The API of the window generator class is not\npublic and therefore should not be relied upon. An application that uses\nwindow generators directly also risks complications and compatibility\nissues with future releases of the platform.\n\n\nDefault Unifier\n\n\nA default unifier is provided for merging partitions. The default\nunifier is a passthrough, i.e. it forwards all the tuples from\npartitions to downstream operators, thus enabling the downstream\noperators to connect to one upstream source (i.e. the unifier). The\nunifiers works the same in Nx1 partitions as well as NxM partitions; the\nonly difference is that a NxM partition would have M unifiers - one for\neach downstream operator partition.\n\n\nSinks\n\n\nSinks are objects that implement the Sink interface. The sink is the\nbasic interface that has the process(tuple) API. The platform\ntransparently connects output ports to sinks for needed functionality.\nThese sink objects are used inside the engine for various purposes.\nExamples include stats collection, debugging, chart data collection,\netc. We have also included a host of sinks in testbench library. These\nare very valuable as they help developers quickly run their tests. Using\nsinks in tests is recommended as they follow the same API concepts and\nare fully supported by the platform. All the sinks are included in unit\nand performance tests and their functionality and performance is\nguaranteed.\n\n\n\n\nCollectorTestSink\n\n\nPartitionAwareSink\n\n\nWindowIdActivatedSink\n\n\nArrayListTestSink\n\n\nCountAndLastTupleTestSink\n\n\nHashTestSink\n\n\nCountTestSink\n\n\n\n\nInputOperator\n\n\nThe InputOperator interface should be used to develop input adapters.\nThis is the interface that all input adapters implement. Operators that\nimplement this interface need to implement the following method:\n\n\npublic void emitTuples();\n\n\nThis method is called continuously between the beginWindow() and\nendWindow() calls and is supposed to fetch data from an external system\nand write it out to the output port(s).\n\n\nApache Apex Malhar Library Templates\n\n\nThe Util and Common libraries have a collection of operators that can be\nextended to create custom operators. These include operators for key/val\npairs, matching, filters, unifiers, and others.\n\n\nDatabase/Window Synchronization Operator\n\n\nDatabase adapters (input or output) need to be able to instrument the\nat-most-once mechanism. This is needed as that is the final output\nstate, or once-only processing of incoming data. The standard library\ntemplates for databases have such a mechanism built in. A base operator\nis provided for precisely such a behavior. These operators rely on last\ncompletely processed window being written to these outside system for\nouputAdapters, and retaining the last read event for inputAdapters.\n\n\n5: Fault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to recognize\nan outage in any part of the application, provision replacement\nresources, initialize the lost operators to a last-known state, and\nrecompute the lost data. The default method is to bring the failed part\nof the DAG back to a known checkpointed state and recompute atomic micro\nbatches from there on (also called the \u201cat-least-once\u201d recovery\nmechanism). Operators can be set for \"at-most-once\" recovery, in which\ncase the new operator starts from the next available window. Operators\ncan be set for an \u201cexactly-once\u201d recovery, in which case the operator\nonly recomputes the window it was processing when the outage happened.\nAt-most-once recovery as an attribute will be available in a later\nversion.\u00a0For now, \u201cexactly-once\u201d recovery is\nachieved by setting the checkpoint interval to 1. This mechanism is very\ncostly as checkpointing is done after every\nwindow. In the future when the platform\nprovides the ability to recognize a stateless operator, an exactly-once\nmechanism will be significantly less costly as checkpointing is not\nneeded for stateless operators. At-most-once recovery in most\napplications is an outbound need, i.e. the data being written out to a\nsystem outside the application needs to be written only once. \u00a0This is\ninstrumented for output adapters (where it really matters) by saving the\nprocessed window ID into the outbound system (database, files, etc.) to\nbe used as a GUID or primary key. The default output adapters provided\nin the Apache Apex Malhar Library Templates \u00a0include such\nmechanism.\n\n\nA choice of a recovery mechanism is decided both by the operator design\nas well as the application needs. If computations are data-loss\ntolerant, an at-most-once mechanism works. For computations that are\ndata-loss intolerant, an at-least-once mechanism works. For computations\nthat write to an outside state and cannot handle re-computations, an\nexactly-once model is needed.\n\n\nCheckpointing\n\n\nThe STRAM provides checkpointing parameters to StreamingContainer during\nintialization. A checkpoint period is given to StreamingContainer of the\ncontainers that have window generators. A control tuple is sent when the\ncheckpoint interval is completed. This tuple traverses through the data\npath via streams and triggers each StreamingContainer in the path to\ninstrument a checkpoint of the operator that receives this tuple. This\nensures that all the operators checkpoint at exactly the same window\nboundary. The only delay is the latency of the control tuple to reach\nall the operators. The checkpoint is thus done after the endWindow()\ncall of the current window and before the beginWindow() call of the next\nwindow. Since all the operators are computing in parallel (separate\nthreads) they each process the \u201ccheckpoint\u201d control tuple independently.\nThe asynchronous design of the platform means that there is no guarantee\nthat two operators would checkpoint at exactly the same time, but there\nis guarantee that they would checkpoint at the same window boundary.\nThis feature also ensures that purging old data can be done very\nefficiently, since when the checkpoint window tuple is done traversing\nthe DAG, the checkpoint state of the entire DAG increments to this\nwindow id.\n\n\nIn case of an operator that has an application window that is different\nfrom the streaming window, the checkpointing happens after the\napplication window is complete. Though this allows the operators to\ntreat the application window as an atomic unit, it does need the\nupstream bufferserver to keep tuples for the entire application window.\n\n\nBy default, checkpoints are not done inside of an application window.\nApplication developers can choose to override this and specify that\ncheckpoint windows be used. This is possible only if the operator is\ncompletely stateless, i.e. an outbound tuple is only emitted in process\ncall and only depends on the tuple of that call. If the operator is\nstateful within a window, the operator developer should disallow\ncheckpointing within the window as the atomic computation could be for\nan application window. If the application developer allows for\ncheckpointing within an application window, then the checkpoint window\nis followed by the STRAM. If the application window is not an exact\nmultiple of the checkpoint window, then the checkpoints get done a\nlittle early. For example, in an application with streaming window = 1\nsec, if the checkpoint window is 30 for the application, and application\nwindow is 100, then the operator will checkpoint at 30, 60, 90, 100,\n130, 160, 190, 200 secs. For such a case, STRAM purge process will take\nthese into account by keeping required tuples in the bufferservers.\n\n\nCheckpointing involves pausing an operator and serializing the object to\nHDFS. After the checkpoint state is saved, the operator may start\nprocessing events again. Thus, checkpointing has a latency cost\nimplications in the throughput. It is important to ensure that\ncheckpointing is done with minimal required objects. This means that all\ndata that is not part of an operator state must be declared as\ntransient. An operator developer can also create a stateless operator as\nlong as the life span is only within a streaming window (i.e not part of\nthe state of the operator). By default this would work and such data can\nbe declared as transient. The serialized data is stored as a file, and\nis the state that the operator can be brought back to. The ID of the\nlast completed window (per operator) is sent back to the STRAM in the\nnext heartbeat. The default implementation for serialization\nuses\n\u00a0\nKRYO\n\u00a0.\nMultiple past checkpoints are kept per operator. Depending on the\ndownstream checkpoint, one of these are chosen to start from. These are\npurged only after they are are no longer needed. STRAM takes the purge\ndecision and informs all bufferservers about these.\n\n\nA complete recovery of an operator needs that the operator be brought\nback to a checkpoint state and then all the lost atomic windows being\nreplayed by upstream buffer server. The above design keeps the\nbookkeeping cost very low, and still allowing rapid catch up of\nprocessing. In the next section we would see how this simple abstraction\nallows applications to recover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanisms are ways to recover from a container (or an\noperator) outage. In this section we explain a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container is\nalso down. If multiple operators are present in a container, then the\ncontainer recovery treats each operator as independent objects when\nfiguring out the recovery steps. Application developers can set any of\nthe below recovery mechanisms for node outage. In general, the cost of\nrecovery depends on the state of the operator and the recovery mechanism\nselected, while the data loss tolerance is specified by the application.\nFor example a data loss tolerant application would prefer \u201cat-most-once\u201d\nrecovery.\n\n\nAll recovery mechanisms treat a streaming window as an atomic\ncomputation unit. In all three recovery mechanisms the new operator\nconnects to the upstream bufferserver and asks for data from a\nparticular window onwards. Thus all recoveries translate to deciding\nwhich atomic units to re-compute, and which state the new operator\nshould start from. A partially computed micro-batch is always dropped.\nThey are re-computed in at-least-once or exactly-once mode. In\nat-most-once mode, they get skipped. Atomic micro-batches are a critical\nguiding principle as this allows for very low bookkeeping cost, high\nthroughput, low recovery time, and high scalability.\n\n\nWithin an application, each operator can have its own recovery\nmechanism. The operators can be developed oblivious to the recovery mode\nin which they will function. Yet, in the cases where they do need to\nknow, the processing mode can be obtained as:\n\n\nProcessingMode mode =\ncontext.attrValue(OperatorContext.PROCESSING\\_MODE, \u00a0 \u00a0 \u00a0\nProcessingMode.AT\\_LEAST\\_ONCE);\n\n\n\n\nAt-Least-Once\n\n\nAt-least-once recovery is the default recovery mechanism, i.e it is used\nif no mechanism is specified. In this method, the lost operator is\nbrought back to its latest checkpointed state and the upstream buffer\nserver is asked to replay all windows since the checkpointed window.\nThere is no data loss in this recovery mode. The viable checkpoint state\nis defined as the one whose window ID is in the past as compared to all\nthe checkpoints of all the downstream operators. All downstream\noperators are restarted at their checkpointed state in the same\ncontainer. They ignore the data until the stream catches up to their\nstate by subscribing after their checkpointed window. All the lost\natomic micro-batches are thus recomputed and the application catches up\nwith live incoming data. This is the at-least-once mechanism, as lost\nwindows are recomputed. For example, if the streaming window is 0.5\nseconds and checkpointing is being done every 30 seconds, then upon node\noutage all windows since the last checkpoint (up to 60 windows) need to\nbe re-processed. If the application can handle loss of data, then this\nis not the most optimal recovery mechanism.\n\n\nIn general in this recovery the average time lag on a node outage in at\nleast recovery is:\n\n\nRecovery time = (CP/2 \\* SW) \\* T + HC\n\nWhere:\n\nCP \u00a0 \u00a0 \u00a0 \u00a0Checkpointing period (default value is 30 seconds)\n\nSW \u00a0 \u00a0 \u00a0 \u00a0Streaming window period (default value is 0.5 seconds)\n\nT \u00a0 \u00a0 \u00a0 \u00a0 Time taken to re-compute one lost window\n\nHC \u00a0 \u00a0 \u00a0 Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s) to its checkpointed state\n\n\n\n\nA lower CP is a trade off between the cost of checkpointing and the need\nto have lower latency during node recovery. Input adapters cannot do\nat-least-once without the support from sources outside Hadoop. For an\noutput adapter, care needs to be taken if external systems cannot handle\nre-writing the same data.\n\n\nAt-Most-Once\n\n\nApplications that can tolerate data loss get the quickest recovery in\nreturn. The engine brings the operators to the most recent checkpointed\nstate and connects its ports to the upstream buffer server, subscribing\nto data from the start of the next window. It then starts processing\nthat window. The downstream operators realize some windows were lost,\nbut continue to process. Thus, an at-most-once mechanism forces all\ndownstream operators to follow. In the cases where an at-most-once\noperator has more than one input port, it\u2019s possible that they play\ndifferent windows. In this case, some of the windows get the data from\njust from a few of the input ports and some of the windows get lost - by\ndefinition of at-most-once. This is acceptable because we care about\ncatching up to a steady state as fast as possible, and once achieved not\nlosing any data.\n\n\nFor example, if the operator has ports in1 and in2 and a checkpointed\nwindow of 95, and their buffer server responds with window id 100 and\n102 respectively (window 100 was received before 102), \u00a0then the\noperator will work on the tuples from only that buffer server which is\nat window 100. At the completion of that window, if tuples from 101 were\nreceived before 102, then it will work with the one with the data\nreceived for window 101, and then it will go on to process window 102.\nBut if tuples from window 102 were received before window 101, then\nwindow 102 will be processed and window 101 will be skipped completely.\nBut from window 102 onwards the operator will resume regular processing\nunless one of the inputs starts skipping the windows.\n\n\nIn general in this recovery the average time lag on a node outage in\nat-most-once recovery is:\n\n\nRecovery time = SW/2 + HC\n\nWhere:\n\nSW \u00a0 \u00a0 \u00a0 \u00a0Streaming window period (default value is 0.5 seconds)\n\nHC \u00a0 \u00a0 \u00a0 Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s)\n\n\n\n\nExactly-Once\n\n\nThis recovery mechanism is for applications that need no data-loss as\nwell as no recomputation. Since a window is an atomic compute unit,\nexactly-once applies to the window as a whole.\u00a0In this recovery the\noperator is brought back to the start of the window in which the outage\nhappened, and the window is recomputed. The window is considered closed\nwhen all the data computations are done and end window tuple is emitted.\n\u00a0Exactly-once requires every window to be checkpointed. Hence, it\u2019s the\nmost expensive of the three recovery modes. The operator asks the\nupstream buffer server to send data after the most recent checkpoint.\nThe upstream node behaves the same as in at-most-once. Checkpointing\nafter every streaming window is very costly, but users will most often\ndo exactly-once per application window in which case the cost of running\nan operator in this recovery mode may not be as costly. In the current\nset, up exactly-once can be achieved by setting checkpointing window to\n1 on an operator.\n\n\n6: Partitioning\n\n\nPartitioning is the fundamental building block for scaling a streaming\napplication. Any of the logical units in an application may face a\nresource crunch. This could be an operator that does not have enough CPU\nor RAM to complete its computations, or it could be a stream that just\ncannot fit within a NIC bandwidth limit. The platform provides rich\npartitioning schemes to help scale an application. The platform is\ndesigned to scale at a Hadoop level and has several functionalities\nbuilt in that support scalability.\n\n\nStatic vs Dynamic\n\n\nIn general, partitioning can be done statically (during launch time) or\ndynamically (during run time). There is no other difference between\nthese. The same logic and schemes hold true to both launch time as well\nas run time partitioning. The advantage of dynamic partitioning is that\nthe application can respond to runtime demands and leverage resources\nfrom Hadoop to meet SLAs or other constraints. This helps to optimize\nresources, since they may be provisioned only when exactly needed.\n\n\nThe implementation of dynamic partitioning impacts runtime as the new\npartitions are rolled out and stream processing is restarted based on\nthe recovery mechanism of the operator. In the case of at-least-once\nprocessing, the operator partitions go back to a checkpointed state. In\nthe case of at-most-once processing, the new partitions simply start\nfrom the latest window (i.e. there is data loss). Finally, in the case\nof exactly-once processing, the operator restarts at the current window.\nThis becomes very tricky when the operator is stateful. In certain\ncases, the STRAM needs to ensure that all the partitions are at the same\nstate. Scaling up may be accomplished by splitting all the partitions\ninto N different partitions, or by splitting the largest partition into\ntwo, but scaling down needs to ensure that the partitions being merged\nare checkpointed at the same window id. Thus, dynamic partitioning is\ncostlier and impacts application performance. Static partitioning,\nhowever, is done at launch time and if correctly done may not need\nfurther partitioning. In general this means that extra resources, if\nany, stay idle and are wasted. A choice between (possibly) wasting\nresources, or impacting application performance is a decision for the\nuser to make. With commodity hardware this decision is a close one.\n\n\nPartitioner\n\n\nHow does the platform know what data an operator partition needs to\nreceive in order to implement its logic? It depends on the functionality\nof the operator. In a simplified example, an operator may want to\nprocess a range of data, let us say all names that start with A through\nC should be belong to partition 1 and the rest to partition 2 (a custom\nstream codec would hash A through C to \u00a01 and the rest to 2). The\noperator needs the ability to declare that there should be 2 partitions\n(one receives tuples with hash code 1, the other tuples with hash code\n2). We provide the interface Partitioner, which is designed to give\ncontrol over the distribution scheme to the operator developer. In this\ncase, the operator would provide a mapping that declares:\n\n\n\n\nOperator Instance one receives on port inputPortName, all tuples\n    with hash code 1\n\n\nOperator Instance two receives on port inputPortName, all tuples\n    with hash code 2\n\n\n\n\nWhile this may look complicated at first glance, it is necessary for\nmore complex scenarios.\n\n\nMultiple Ports\n\n\nOperators may have multiple input ports. The partitioning of incoming\nstreams on each of the ports depends entirely on the logic or the\noperator. One port may be connected to a high throughput stream that we\nwould like to partition, while the other port may deliver low throughput\nstream, but each tuple is required for the functionality of each\noperator instance, regardless which portion of the other stream it\nhandles. An example for this could be the processing of location\nupdates. In this case, the operator may receive (high speed) location\nupdates on one input port, and (slow) location queries on the other. We\nwould like to partition the first stream by location, and receive\nlocation query on the the other port. Only the operator instance that\nhandles the corresponding location will respond to the query. \u00a0A\ndifferent mapping would be needed for an operator with 2 ports that\njoins tuples. In this case, each of the streams would need to be\npartitioned in exactly the same way. Since the StreamCodec could be\ncustom, there is no way for the platform to check if two ports are\nlistening to streams that are partitioned the same way (or in a\nparticular manner).\n\n\nStreamCodec\n\n\nThe StreamCodec is responsible for serialization of data tuples into\nbytes (object to byte array) and deserialization (byte array to object)\nfor transmission through buffer server streams. The codec is defined on\ninput ports and is used by the platform to serialize the data when\nemitted through the upstream operator\u2019s output port before writing to\nthe stream and to deserialize after reading from the stream, before\nhanding over the tuple object to the input port.\n\n\nWhen partitioning is enabled for a stream, the codec is responsible to\nassign the partition key for a tuple before it is transmitted over the\nstream. The buffer server then uses the key to determine the partition\nthat will process the tuple. The default codec is using Object\u2019s\nhashCode function to calculate this value. If the hashCode function\ncannot compute the partition key (for example, when using data\nstructures such as HashMap or ArrayList) it is necessary to supply a\ncodec that understands the data and can compute the partition key.\n\n\nIf no codec is set on an input port explicitly, the default stream codec\nis used. The default implementation\nuses\n\u00a0\nKryo\n\u00a0for\nserialization. It generically supports most standard types and is used\nfor all port types of library operators. To customize how objects are\nserialized or the partition key is computed, the operator developer can\nsupply a custom stream codec by overriding InputPort.getStreamCodec().\n\n\nUnifier\n\n\nWhen an operator is partitioned into N physical operator instances, the\ndownstream operator needs to get streams from all the partitions. All\noperator instances however, have fixed number of ports, and hence have\nfixed number of streams they can read from or write to. This anomaly is\nsolved by having the STRAM insert a dummy merge operator before the\ndownstream operator. This merge operator is CONTAINER_LOCAL (intra jvm\nprocess) with the partitioned operator. Since the STRAM knows exactly\nhow many partitions exists at execution roll out time, it inserts a\nmerge operator with precisely that number of inputs. The default merge\noperator is a passthrough, i.e. it simply forwards all the tuples from\nits input ports onto the output port. It has a single output port whose\nschema is same as that of the output port of the partitioned operator.\nThe downstream operator thus does not notice any difference as the\noutputs of all partitions are combined.\n\n\nAn example of an operator that needs specific functionality for the\nmerge to work is SumMap operator. This operator takes a HashMap\\\nK,V>\nas input. Its output is the sum of all the values of a particular key.\nSince HashMap is a single tuple that could be a collection of various\nkeys, the input port has to be partitioned on a round-robin basis. This\nmeans that any particular key may appear in any of the partitions. So\nthe only way to get correct output is for the merge operator to do a\nkey-wise sum again. This can be implemented in the unifier logic.\n\n\nAnother example is this would be top N calculation, where top N results\nfrom each partition need to be consolidated into aggregate top N at the\nend of the processing window. For this, the output port of any operator\ncan define a \u201cnifier\u201d object, which is an operator that has no input\nports but instead a single method that accepts tuples emitted from\npartitions and implements the logic required to merge them. The outputs\nof each instance of a partitioned operator feeds to this unifier\noperator and thus intercepts the output stream in front of the\ndownstream operator.\n\n\nThe unifier is an operator that implements Unifier interface. This\ninterface needs one method to be implemented - process(\\\ntype> tuple).\nThe schema (\\\ntype>) of the process() method is same as the schema of\nthe output port in which the getUnifer method is implemented. Thus, for\nparent operators that have only one output port, proper coding habits\nallow usage of the parent operator itself to be used as the Unifier.\nSince a Unifier is an operator, it has access to the operator interface,\nnamely beginWindow(), and endWindow() in addition to process. The\nplatform automatically inserts a process call that allows developers to\ndo a proper merge as seen above. Any object that implements this API can\nbe used as a unifier. Code specific to the unifying functionality for\noutput streams of a particular operator should be coded in this class.\nSome examples of usage of a parent operator as an unifier are MinMap or\nMaxMap. This is the interface for a unifier operator:\n\n\n// \n T \n is the tuple schema for the output port for which\n// the unifier object would be used\npublic interface Unifier \n T \n extends Operator\n{\n\u00a0 public void process(T tuple);\n}\n\n\n\n\nWhen using a unifier that collects all the output tuples in one\noperator, users can get away from a sticky key\npartitioning\u00a0scheme, as long as the the unifier\ncan function within one Hadoop container (usually \\~ 1GB\nRAM), i.e. a single unifier object that has\nenough resources (CPU, RAM, and Network) to process the outputs of all\nthe partitions. Combining the output of all partitions allows the basic\npartitioning to be done by round-robin without a sticky key, and thus\navoids any skew. For operators which are partitioned by sticky key, the\ndefault merge (passthrough) works fine.\n\n\nCare should be taken in cases where partitioning is done mainly to\nmanage outbound throughput, especially if it is more than the NIC\ncapacity of a Hadoop node. In this case, a Unifier does not help as all\nthe tuples flow back into the same operator. In almost all cases, the\noutbound throughput problem should be resolved by partitioning both the\ncurrent operator and the downstream operator. One way to look at this is\nthat the stream in question has a network resource requirement that no\nsingle Hadoop node can provide, hence all operators on that stream must\nbe partitioned, and no partition should read/write the full stream.\n\n\n\n\n7: Library\n\n\nThe platform includes a set of operator templates, which are available\nunder Apache 2.0 license. These are open source operator library under\nApache Apex Malhar project.They are provided to enable quick application\ndevelopment. As an operator developer, you can leverage this by either\nextending them, or creating your own library. In addition to reducing\nthe development time, they also help reduce maintenance cost. The\nlibrary operators can be benchmarked, tested, and have data that the\nSTRAM can leverage for running applications optimally.\n\n\nCommon Operator Functions\n\n\nThere are several common operator functions that can be utilized. We\ndescribe a few categories below. You can find many more operators in the\nApache Apex Malhar project. These are provided for a developer to\nquickly create an application. They are not meant to replace or\nsubstitute custom operators. Developers should judiciously decide on\nwhich operators to use.\n\n\nFiltering-Selecting-Map\n\n\nFiltering is a very common operation. Filters can be employed in the\nfollowing ways:\n\n\n\n\nConversions of tuple schemas. Examples include selecting/extracting\n    a particular set of keys from the tuple (a collection) and drop the\n    rest. Changing the contents (lower/upper case, round-up/down, etc.).\n    Contents of input tuples and output tuples are different in this\n    computation.\n\n\nPassing through certain tuples. Examples include letting tuples that\n    meet certain conditions go through. Tuple content remains the same\n    in this computation\n\n\nComparison. This is similar to conversion, except that the tuple\n    (flag) is just an alert.\n\n\nMap can be done on file contents, or lines (word count). Combiner\n    (map-side reduce) can be done over the streaming window.\n\n\n\n\nFiltering operations usually do not increase the throughput. In most\ncases the throughput will decrease. These operators are also most likely\nstateless. The resource requirements are usually not directly impacted\nby the micro-batch size.\n\n\nAggregations\n\n\nAggregate computations are those that need the entire window (atomic\nmicro-batch) to be computed for the results to be known. A lot of\napplications need these to be computed over the application window.\nCommon aggregation examples include\n\n\n\n\nCounters like sum, average, unique count etc.\n\n\nRange of the incoming data. Compute maximum, minimum, median etc.\n\n\nMatch. The first or the last match in the micro-batch.\n\n\nFrequency. Most or least frequent\n\n\nReduce\n\n\n\n\nAggregate functions are very effective in ensuring that the micro batch\nis treated in an atomic way. Computations are dependent on the size of\nthe micro-batch and this size is usually decided between what the\napplication needs (application window) and how much micro-batch\nprocessing makes sense from an operability point of view\n\n\nJoins\n\n\nJoins are very common operations done on streams. These computations are\nmostly done over the application window. The concept of the atomic micro\nbatch is very critical for joins. The tuples in the two streams can\narrive at unpredictable times within the window and thus at the end of\nthe window, the computation can guarantee that join operation is done\nover all the tuples in that window.\n\n\nInput Adapters\n\n\nInput adapters enable the application to get data from outside sources.\nData read is done by the Input Adapter either by pulling from the source\nor by data being pushed to the adapter. Input adapters have no input\nports. The primary function of an Input Adapter is to emitdata from\noutside the DAG, as tuples for rest of the application. Once the data is\nemitted as tuples all the mechanisms and abstractions in the platform\napply. To enable a data flow to have a recovery mechanism that spans\noutside of the application, the outside source must have support for\nsuch mechanisms. For scalability (partitioning) the outside source must\nhave support or enabling features. As an example, if the application\nneeds at-least-once recovery then an input adapter must be able to ask\nfor data from a specific timestamp or frame number upon re-connect after\nan outage. The design of an Input Adapter needs very careful\nconsideration and needs application logic to be built correctly as\nstreaming platform infrastructure is sometimes not applicable to outside\nsources. Examples of outside sources include HDFS, HBase, HTTP, Messages\nbusses like ZeroMQ, RabbitMQ, ActiveMQ, Kafka, Rendezvous, Databases\nlike MongoDB, MySql, Oracle.\n\n\nOutput Adapters\n\n\nOutput adapters write out results of the application to outside sources.\nData is written to a message bus, a database, to files (HDFS), or sent\nover the network to a data collection service. The primary function of\nan Output Adapter is to write data to destinations outside the DAG, and\nmanage data integrity during node outages. Output adapters have no\noutput ports. The data received by the Output Adapter follows all the\nmechanisms supported by the streaming platform. For recovery mechanisms,\nthe Output Adapter has to store the state of the current written data.\nSince the platform is atomic on a streaming window, output adapters\nshould use end of window as a commit. This way, during a recomputation\ndue to operator outage, data integrity can be ensured.\n\n\nEvent Generators\n\n\nEvent generators are operators that generate events without an input\nport. They are needed for testing other operators, for functionality or\nload. These are different from input adapters as event generators do not\nconnect to any source. Various event adapters are available in the\ntestbench \u00a0library.\n\n\nStream Manipulation\n\n\nStream manipulation operators are those that do not change the content\nof the tuples, but may either change schema, or just merge or split the\nstream. Schema change operators are needed to allow data to flow between\ntwo sets of ports with different schemas. You can use merge and split\nstreams between streams with different execution attributes. For\nexample, you would use a split operator if you want one stream to be\nCONTAINER_LOCAL, while another to be across containers. Merge is also\nused to merge in streams from upstream partition nodes.\n\n\nUser Defined\n\n\nOperator developers can easily develop their own operators, allowing for\nquick turnaround in application development. Functionality-wise, any\nuser defined operator is the same as one provided as a bundled solution.\nStandard operators do get tested with each build, are benchmarked, and\nare supported. An operator developer should take on these\nresponsibilities for user defined operators.\n\n\nSample Code\n\n\nCode samples are included in the samples project. We will continually\nadd examples of various operator templates to this package. This package\nis part of the open source Malhar project, and users are encouraged to\nadd their examples to enable community reuse and verification. Here is the github link to the project: https://github.com/apache/incubator-apex-malhar/tree/master/samples.\n\n\nLatency and Throughput\n\n\nThe latency of an atomic window compute operation of an operator is\ndefined as the time between the first begin window received on any input\nport and the last of the end window tuple sent out by the operator on\nany output port. Since the streaming window period is not contributed to\nby the operator, the real latency contribution of the operator is the\nabove latency minus the streaming window period. Operator developers can\nleverage distributed computing by doing as much computations upfront as\npossible. This is very useful for an operator that only emits tuples in\nend of window call.\n\n\nThroughput of an operator is defined on a per port basis. The incoming\nthroughput is the number of tuples it processes per unit time on each\nport. The outgoing throughput is the number of tuples it emits per unit\ntime on each output port. Per-port data is found in the \u201cstream view\u201d of\nthe application, while the incoming and outgoing totals are found on the\noperator view of the application.\n\n\n\u00a9 2012-2018 DataTorrent Inc. \u00a0Patent pending", 
            "title": "Reference"
        }, 
        {
            "location": "/operator_development_ref/#operator-development-reference", 
            "text": "", 
            "title": "Operator Development Reference"
        }, 
        {
            "location": "/operator_development_ref/#1-introduction", 
            "text": "A streaming application is a DAG that consists of computations (called\noperators) and data flow (called streams). In this document we will\ndiscuss details on how an operator works and its internals. This\ndocument aims to enable the reader to write efficient operators and make\ninformed design choices.", 
            "title": "1: Introduction"
        }, 
        {
            "location": "/operator_development_ref/#2-operators", 
            "text": "Operators are basic computation units of the application. They are\ninterconnected via streams to form an application. Operators are classes\nthat implement the Operator interface. They read from incoming streams\nof tuples and write to other streams. Reading and writing to streams is\ndone through connection points called ports. An operator may have no\nports (if the operator is an Input Adapter and the only operator in the\nDAG) and there is no limit to the number of ports an operator can have.\nOperators also\nhave  properties  and  attributes .\u00a0Properties\ncustomize the functional definition of the operator, while attributes\ncustomize the operational behavior of the operator.  Operators are designed to be simple and easy to use and develop. Their\njob is to process tuples one at a time and emit a tuple as per business\nlogic. Ports that read tuples from a stream are input ports as they\nimplement the InputPort interface. Ports that write tuples to a stream\nare output ports which implement the OutputPort interface. Given\noperators A and B where an input port of B is connected to an output\nport of A, we say that A is an upstream operator of B and B is a\ndownstream operator of A. The platform ensures that tuples emitted by A\nreach B. Thus the operator solely focuses on the business logic\nprocessing. The engine, i.e. Streaming Application Master (STRAM) is\ncognizant of the connection order of the operators. This connectivity\nand the dataflow is an intrinsic part of engine\u2019s decision-making. All\nguarantees of data delivery upon outage is done via attributes that can\nvary per application. The same operator can be reused in different\napplications. This means that the same operator code can be used in\nvarious recovery mechanisms by different applications. Two instances of\nthe same operator within an application can also have different recovery\nmechanisms (attributes).  Operators can have properties that are used to customize functional\nbehavior of the operator. Within an application attributes can be\nassigned to an operator that impact the operability of the operator.\nLater sections of this chapter cover the internal details of operators.", 
            "title": "2: Operators"
        }, 
        {
            "location": "/operator_development_ref/#api", 
            "text": "To write your own operator you need to implement the Operator interface.\nThis interface provides the basic API for an operator developer. It\nextends the Component interface; important parts of both interfaces are\ndiscussed below.", 
            "title": "API"
        }, 
        {
            "location": "/operator_development_ref/#interface-component", 
            "text": "setup (OperatorContext context)  This is part of \u00a0Component. This is\n    invoked as part of initialization of the operator. This is part of\n    initial pre-runtime setup of the operator that is called during\n    initialization time. Care should be taken to not use this method to\n    set parameters. It is strongly advocated that users use setter\n    functions to set parameters. The setter functions work for both the\n    Java API as well as the properties file based API of an application.\n    Setter functions can also be invoked during runtime via CLI. Setup\n    is useful for validation checks that can only be done during\n    initialization time. Setup is also very useful for initializing\n    transient objects like connections with outside systems (e.g.\n    sockets or database sessions). These objects should not be part of\n    the state of the operator as they are in session, i.e. upon recovery\n    from a node outage, the connection has to be reestablished in setup.\n    All objects that are not serializable must be transient and\n    initialized during setup or at the declaration site.  teardown()  This too is part of Component and is called as part of\n    terminating the operator. Any session related objects should be shut\n    down or deallocated in this callback. Graceful termination of\n    outside connections (for example sockets, database sessions, etc.)\n    should be done in this method", 
            "title": "Interface - Component"
        }, 
        {
            "location": "/operator_development_ref/#interface-operator", 
            "text": "beginWindow(long windowId)  Invoked at the start of a window. The\n    windowId parameter identifies the window. All tuples received and\n    emitted in that window belong to this windowId. A window gets\n    started upon receipt of the first\u00a0beginWindow tuple on any\u00a0input\n    port. The window computation starts at the first line of the\n    beginWindow().  endWindow()  Invoked at the end of window. This call is only made\n    after all\u00a0the input ports receive an end_window tuple. All tuples\n    emitted in endWindow belong to the same windowId (passed during\n    beginWindow). The window computation ends with the last line in\n    endWindow().", 
            "title": "Interface Operator"
        }, 
        {
            "location": "/operator_development_ref/#class-defaultinputport", 
            "text": "process(T tuple)  Invoked within an input port for every tuple\n    received from the port of an upstream operator. This method is part\n    of the input port interface. The schema of the tuple is same as the\n    schema of the input port. This callback is an abstract call that\n    must be implemented by the operator developer.", 
            "title": "Class DefaultInputPort"
        }, 
        {
            "location": "/operator_development_ref/#class-defaultoutputport", 
            "text": "emit(T tuple)  To be called by the operator developer when a tuple\n    has to be emitted on the output port. Tuples can be emitted in\n    beginWindow, endWindow, or process callbacks. The schema of the\n    tuple is the same as the schema of the output port.", 
            "title": "Class DefaultOutputPort"
        }, 
        {
            "location": "/operator_development_ref/#interface-inputport", 
            "text": "StreamCodec   T   getStreamCodec()  A stream codec serializes or\n    deserializes the data that can be received on the port. If null,\n    STRAM uses the generic codec.", 
            "title": "Interface InputPort"
        }, 
        {
            "location": "/operator_development_ref/#interface-outputport", 
            "text": "Unifier   T   getUnifier()  When operators are partitioned via round\n    robin partitioning they may need to merge the outputs of the\n    partitions. For example MaxMap operator may have N partitions, each\n    would emit its maximum value. The maximum value is computed by\n    finding the maximum of these N values. Thus another operator is\n    needed to unify these tuples. getUnifier() returns such an operator.\n    Developers need to implement the process() api. The schema of the\n    input tuples sent to merge() and the output port are identical to\n    the schema of the output port of the operator. If the operator has\n    only one input and one output port, and if both have identical\n    schema, then the operator itself can act as its unifier (see MaxMap\n    as an example). The default unifier is a passthrough operator and it\n    just passes the tuples from each partition to the downstream\n    operators.", 
            "title": "Interface OutputPort"
        }, 
        {
            "location": "/operator_development_ref/#interface-activationlistener", 
            "text": "An operator may be subjected to activate/deactivate cycle multiple times\nduring its lifetime which is bounded by setup/teardown method pair. So\nit's advised that all the operations which need to be done right before\nthe first window is delivered to the operator be done during activate\nand opposite be done in the deactivate.  An example of where one would consider implementing ActivationListener\nis an input operator which wants to consume a high throughput stream.\nSince there is typically at least a few hundreds of milliseconds between\nthe time the setup method is called and the first window, the operator\ndeveloper would want to place the code to activate the stream inside\nactivate instead of setup.   activate(CONTEXT context)  Gets called just before the first\n    beginWindow() call of the operator activation. Any processing that\n    needs to be done, just before the operator starts processing tuples,\n    must be done here.  deactivate()  Gets called just before the operator is deactivated.\n    Opposite of the operations done in activate() must be done in\n    deactivate().", 
            "title": "Interface ActivationListener"
        }, 
        {
            "location": "/operator_development_ref/#interface-checkpointlistener", 
            "text": "Operators which need to be notified as soon as they are checkpointed or\ncommitted, must implement this interface.   checkpointed(long windowId)  - Called when the operator is\n    checkpointed. The windowId parameter contains the window id after\n    which the operator was checkpointed.  committed(long windowId)  - Called when the operator is committed. A\n    commit operation is performed when all the operators in the DAG have\n    successfully checkpointed a particular window id. This window id is\n    passed as a parameter to the call back.", 
            "title": "Interface CheckpointListener"
        }, 
        {
            "location": "/operator_development_ref/#interface-idletimehandeler", 
            "text": "An operator must implement this interface if it is interested in being\nnotified when it's idling. An operator can be said to be idling when   An operator which is an Input Adaptor, is not emitting any tuple or   A generic operator or an Output Adapter is not processing any inputs    handleIdleTime()  - When the operator is idling, it is explicitly\n    notified of such a state. The operators which implement this\n    interface should make use of this idle time to do any auxiliary\n    processing they may want to do when operator is idling. If the\n    operator has no need to do such auxiliary processing, they should\n    not\u00a0implement this interface. In such a case, the engine will put\n    the operator in scaled back processing mode to better utilize CPU.\n    It resumes its normal processing as soon as it detects tuples being\n    received or generated. If this interface is implemented, care should\n    be taken to ensure that it will not result in busy loop because the\n    engine keeps calling handleIdleTime until it does not have tuples\n    which it can give to the operator.    All the above callbacks happen in a single dedicated thread, i.e. at any\none point of time only one of these callbacks is being executed. No\nthread locking issues exist when writing code that process tuples.\nTuples are queued in a buffer and await their turn for process() calls\non their respective ports. Tuples are always in-order within a stream,\ni.e. an input port receives them in the same order that they were\nemitted. If an operator has two input ports then the order between\ntuples on two different streams is not guaranteed.  Figure 1 shows an operator with two ports. It has a guarantee that\nprocess() method on input port i1 will be invoked for tuples in the\nfollowing order: t11, t12, t13, t14; and process() method on input port\ni2 will be invoked for tuples in the following order: t21, t22, t23,\nt24, t25. But whether i2::process( t21) would happen before\ni1:process(t11) is entirely dependent on when they arrive. Users are\nstrongly advised to not depend on the order of tuples in two different\nstreams as in a distributed application this order cannot be guaranteed.\nSuch operators would not be idempotent.  Tuples always belong to a specific window (identified by its window id).\nAll tuples emitted by the upstream operator, whether in beginWindow(),\nendWindow(), or process(), are part of the same window and hence each of\nthese tuples would invoke process() on the input port of a downstream\noperator in the same window. Output ports need not be connected for\ntuples to be emitted. This leniency allows STRAM to monitor throughput\non an unconnected port, and information that is useful for future\nchanges, including dynamic modification of the DAG. For more details and\nspecific interfaces please refer to their API classes.", 
            "title": "Interface IdleTimeHandeler"
        }, 
        {
            "location": "/operator_development_ref/#ports", 
            "text": "Ports are connection points of an operator and are transient objects\ndeclared in the Operator class. Tuples flow in and out through these\nports. Input ports read from the stream while output port write to one.\nInput ports are implementation of the InputPort interface, while output\nports are implementations of the OutputPort interface. An output port\ncan be tagged as an error port by placing an annotation on it (error =\ntrue). Later in this section we would discuss the implications of\ntagging an output port as an error port. Ports have schemas as part of\ntheir declaration. The schema of a port is useful for compile time error\nchecking as well as runtime checks required for dynamically inserting\noperators at runtime.  We classify the operators in 3 buckets by observing the number of input\nand output ports.   Input Adapters (Having no input ports) : The operator with no input\n    ports is called an Input Adapter. (An Adapter is a special term used\n    to denote an operator which interacts with the systems external to\n    the DAG. We\u2019ll use the terms Input Adapter and Input Operator\n    interchangeably.) An Input Adapter is useful for reading data from a\n    source external to the DAG. Some examples of external data sources\n    are HDFS, HBase, Sockets, Message Busses, Cassandra, MySql, Oracle,\n    Redis, Memcache, HTTP (GET), RSS feed, etc. It ingests the data from\n    one or more external sources, creates a stream, introduces control\n    tuples in the stream and emits them via its output ports. The\n    streams at the output ports of the Input Operator are ready to be\n    consumed by downstream operators. The Apache Apex Malhar library\n    implements quite a few Input Operators named after the data sources\n    they interface with. Input adapters have a window generator that\n    generates begin_window and end_window tuples periodically. Since\n    the STRAM initializes all window generators, they all start with\n    same window id and the application is synchronized on these\n    windows.   Input Operators have two modes, namely Synchronous and Asynchronous   Synchronous (Pull mode) :\u00a0Examples of synchronous data intake is an\n    HDFS file reader. The operator can thus decide the intake rate, i.e.\n    \"how much data to read per window?\". This operator can be\n    implemented with a single thread. Scaling such an operator is\n    determinable at static/compile time.   Asynchronous (Push mode) :\u00a0Examples of asynchronous data is typical\n    message busses, where data is pushed to the reader (the Input\n    Operator). In such a scenario, the operator does not know the\n    inbound rate and hence is asynchronous with respect to the platform\n    windows in nature. Asynchronous input operators create another\n    thread to receive the incoming data. The basic operator task of\n    taking the incoming data and converting to/emitting a tuple is done\n    by the main thread of the operator. A connection buffer is provided\n    to ensure that the two threads communicate effectively. Users are\n    advised to pay particular attention to the inbound rate and design\n    an asynchronous operator carefully.    Output Adapter (Having no output port) :\u00a0The operator having no\n    output ports is called an Output Adapter. An Output Operator is\n    useful for handing over the data to the systems outside of the DAG.\n    Typically output operators remove the control tuples from the\n    stream, transform it, and write the resulting data to external\n    sources like HDFS, HBASE, Sockets, Message Busses, Cassandra, MySql,\n    Oracle, HTTP Server (POST), and others. The Apache Apex Malhar\n    library has a few Output Operators also available ready for use in a\n    DAG.    Generic Operator (Having both input and output ports) :\u00a0 These are\n    the most generic operators. They use one or more input ports to read\n    the input tuples and use one or more output ports to emit the\n    computed tuples. Unifiers are examples of generic operators where\n    process() is used for all input ports instead of process() per\u00a0input\n    port.   Singleton Operator (Having no input or output ports) :\u00a0These\n    operators are self-sufficient in implementing the business logic and\n    have no input or output ports. An application with such an operator\n    will not have any other operator in the DAG. Such an operator\n    effectively acts as both an Input Adapter as well as an Output\n    Adapter.   Both output adapters and generic operators process tuples on the input\nport and look alike, the difference is that a general operator emits\ntuples while output adapter has to ensure tight integration with outside\nsources. As far as the interface is concerned, both have to implement\nthe\nOperator \u00a0 interface.\nBoth input adapters and generic operators emit tuples, but input\nadapters generate the streams by talking to external systems and at\ntimes need two threads. Input operators do not have a process API, and\nimplement the InputOperator interface. Only input adapters have window\ngenerators which insert begin_window and end_window events (control\ntuples) in the stream. The details of window generators are discussed in\na later Chapter.  Ports have to be transient objects within the Operator as they should\nnot be part of a checkpointed state. The importance of the transient\nkeyword is explained in the section on checkpointing. Ports are objects\nin the operator class that the platform recognizes. All objects that\nimplement the interface InputPort and OutputPort make this list. A\nspecial kind of output port is also supported, namely a port for\nemitting error tuples. Though this port is in essence an output port,\ni.e. it writes tuples to a stream, it is identified as error port via an\nannotation (error = true). The STRAM can then leverage this information\nfor better reporting, statistics, tools (for example UI), monitoring\netc. The port need not be connected for the STRAM to gather information\non it, as emitted tuples can be counted even if no stream is connected\nto it.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development_ref/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/operator_development_ref/#input-port", 
            "text": "An example of an input port implementing the abstract class\nDefaultInputPort is  \n@InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort  KeyValPair   K,V     data = new DefaultInputPort  KeyValPair   K,V    (this); {\n  @Override /p \n  public void process(KeyValPair   K,V   tuple) {\n    // code for processing the tuple /p \n  }\n};  process() is an abstract method and must be provided by the developer of\nthe operator. This method is responsible for implementing the processing\nlogic for the incoming tuple.", 
            "title": "Input Port"
        }, 
        {
            "location": "/operator_development_ref/#output-port", 
            "text": "Here is an example of an output port implementing the abstract class\nDefaultOutputPort:  \n@OutputPortFieldAnnotation(name =  sum , optional = true)\npublic final transient DefaultOutputPort   HashMap   K, V     sum = new DefaultOutputPort   HashMap   K, V    (this) {\n  @Override\n  public Unifier   HashMap   K, V     getUnifier()\n  { // The unifier is also an operator, that is initialized in the container of the downstream operator /p \n    return new UnifierHashMapSumKeys   K, V  ();\n  }\n;", 
            "title": "Output Port"
        }, 
        {
            "location": "/operator_development_ref/#error-port", 
            "text": "An example of an error port is:  \n@OutputPortFieldAnnotation(name =  sum , optional = true, error=true)\npublic final transient DefaultOutputPort  KeyValPair  String,Integer     monitor = new DefaultOutputPort   KeyValPair   String,Integer    (this);", 
            "title": "Error Port"
        }, 
        {
            "location": "/operator_development_ref/#unifiers", 
            "text": "An operator that uses round robin partitioning may need a unifier to\nmerge data back. An example of this is a sum operator that takes a\nHashMap as its input tuple. In the above example of an output port, the\nkey-value pairs in the HashMap consist of keys and their values that are\nto be added. If this operator is partitioned in N ways, each partition\nwould emit its own sum for a key. So the unifier (merge operator) needs\nto unify them and compute the final result per key as sum of all the\nvalues from every partition for that key. An operator that uses a sticky\nkey partition does not need to provide its own merge as the default\nmerge put in by the platform is a passthrough merge (i.e. just emits the\nincoming tuple on the output port).", 
            "title": "Unifiers"
        }, 
        {
            "location": "/operator_development_ref/#port-declarations", 
            "text": "Annotations should be added to the ports, as these are very useful in\nvalidating them. Annotations are declared with @InputPortFieldAnnotation()  for input ports and @OutputPortFieldAnnotation()  for output ports. Currently the following\nannotations are supported on the ports   name = \u201cportname\u201d  optional = true (or false)  error = true (or false)   Over time mode annotations would be added as per needs of our customers.  The schema of the tuple emitted on a port is part of the port\ndeclaration and thus is defined as compile time. These types may be\ndefined using Java generics. The schema declaration on the ports allow\ncompile time validation when an output port of an operator is connected\nto input port of another operator. On IDEs like Eclipse or NetBeans,\nthis check is done as you type. The application creator thus experiences\nproductivity gains by leveraging type safety alerts generated by the\nIDEs. The port schema also enables the STRAM to do schema validation\nbefore run time addition of an operator. The added sub-query can do a\ncheck (instanceof) before accepting a tuple.", 
            "title": "Port Declarations"
        }, 
        {
            "location": "/operator_development_ref/#operator-properties", 
            "text": "Operator can have properties which are used to customize the\nfunctionality of the operator. Operator properties should be accessed\nusing standard Java beans convention (read via getter method, and write\nvia setter method) so that values can be injected and validated as\nper \u00a0 javax\nannotation \u00a0.\nThrough properties is a broad definition of anything instantiated inside\nan operator, as a generic definition we count variables of the object\nthat can be read and written to by the user via getter and setter\nmethods.  The DAG (including all the operators) is initialized on the client side\nand then passed to the STRAM. This means that the setter method is not\ncalled during initialization in the container. So during node recovery\nthe checkpointed state needs to have these values. In order for the\nproperties to be saved in the checkpoint, they cannot be transient. This\nis irrespective of whether they are set only during initialization and\nwould have same values in every checkpoint. The platform enables runtime\nchanges to property values via setter functions. If a setter function is\ncalled during runtime, STRAM would call the setter function after\ncheckpoint is loaded, so runtime changes are remembered irrespective of\nwhether the checkpoint happened between property set and operator\noutage.", 
            "title": "Operator Properties"
        }, 
        {
            "location": "/operator_development_ref/#operator-attributes", 
            "text": "Operators can have attributes. Attributes are provided by the platform\nand do not belong to user code. Attributes do not change the basic\nfunctionality of an operator, and most often the operator code would be\noblivious to these attributes. The platform recognizes these and takes\ndecisions (compile time or run time) that mainly impacts the performance\nof an operator. A very common example of an attribute is\nAPPLICATION_WINDOW_COUNT. This attribute is by default equal to the\nstreaming window (count = 1), and an user can set this on any operator.\nAttributes also exist at the application level and on a stream. If an\noperator can only work with certain values of an attribute, they can be\naccessed during setup call on an operator. During development of the\noperator the values of these attributes could be used to provide better\noperability for the operator. Details of attributes are covered in Configuration .", 
            "title": "Operator Attributes"
        }, 
        {
            "location": "/operator_development_ref/#templates", 
            "text": "Since operators are java classes they can be templatized (generic types)\nfor reuse. The streaming platform leverages all features in Java for\ntype safety and parameterization. Templates have no performance impact\nas they exist only at compile time. Templates greatly reduce the number\nof operators that need to be maintained, and enable operators for lot\nmore reuse. Apache Apex Malhar Library Templates are examples of\noperator templates. In general, it is advisable to templatize operators\nwherever possible. This allows the core computation to be used for\nderived types. For example Sum operator can be used for any tuple type\nthat\nextends Number.\nIn general an operator should set the schema of the tuple to the minimum\nrequired for its functionality. Apache Apex Malhar Library package\nsample code has examples of a lot of such operators.", 
            "title": "Templates"
        }, 
        {
            "location": "/operator_development_ref/#validations", 
            "text": "The platform provides various ways of validating application\nspecifications and data input. Validation of an application is done in\nthree phases, namely:   Compile Time :\u00a0Caught during application development, and is most\n    cost effective. These checks are mainly done on declarative objects\n    and leverage the Java compiler. For example \"schema of two ports on\n    the same stream should match\".  Initialization Time :\u00a0When the application is being initialized,\n    before submitting to Hadoop. These checks are related to\n    configuration/context of an application, and are done by the logical\n    DAG builder implementation. For example, check that \"a particular\n    port must be connected\"  Runtime :\u00a0Validations done when the application is running. This is\n    the costliest of all checks. These are checks that can only be done\n    at runtime as they involve data integrity. For example divide by 0\n    check, or negative age value.   Operator properties and ports are validated at compile time (IDE) as\nwell as launch time (STRAM). Java annotations are used extensively to\ncheck validations such as ranges for the properties, string formatting,\netc. For example, a port, by default has to be connected. This check is\ndone during compile time in your IDE, as well as during launch time\n(CLI->STRAM). If you have an port whose connectivity is optional, it\nneeds to be mentioned via annotation given below:  \n@InputPortFieldAnnotation(name =  data , optional=true)\npublic transient DefaultInputPort data = new DefaultInputPort(this) { }", 
            "title": "Validations"
        }, 
        {
            "location": "/operator_development_ref/#compile-time-validations", 
            "text": "Compile time validations apply when an application is specified in Java\ncode and include all checks that can be done by Java compiler in the\ndevelopment environment (including IDEs like NetBeans or Eclipse).  Examples include the following   Schema Validation : The tuples on ports are POJO (plain old java\n    objects) and java compiler checks to ensure that all the ports on a\n    stream have the same schema. This check works in any IDE as well as\n    with a command line java compiler.  Stream Check : Single Output port and at least one Input port per\n    stream. A stream can only have one output port writer. This is part\n    of the addStream API. This check ensures that developers only\n    connect one output port to a stream. The same signature also ensures\n    that there is at least one input port for a stream  Naming : Compile time checks ensures that applications components\n    like operators and streams are named   We will continue to add as many checks as possible in compile time. But\nthe Java compiler cannot do 100% of the checks, and validations have to\nbe done during initialization time. Further, for the errors that can\nhappen only during run time, run time checks are supported.", 
            "title": "Compile Time Validations"
        }, 
        {
            "location": "/operator_development_ref/#initializationinstantiation-time", 
            "text": "Initialization time validates include various checks that are done post\ncompile, and before the application starts running in a cluster (or\nlocal mode). These are mainly configuration/contextual in nature. This\nchecks are as critical to proper functionality of the application as the\ncompile time validations.  Examples include the following   JavaBeans\n    Validation   The most common checks for properties are listed below         Constraint  Description  Example    @Max  The value of the field or property must be an integer value lower than or equal to the number in the value element  @Max(100)  int success_ratio;    @Min  The value of the field or property must be an integer value greater than or equal to the number in the value element  @Min(0)  int mass;    @NotNull  The value of the field or property must not be null  @NotNull  String username;    @Pattern  The value of the field or property must match the regular expression defined in the regexp element  @Pattern(regexp =  lte|lt|eq|ne|gt|gte , message =  Value has to be one of lte, lt, eq, ne, gt, gte )  String cmp;      Input port connectivity :\u00a0An input port must be connected, unless\n    optional = true. For example:   @InputPortFieldAnnotation(name = \"data\", optional = true)   Output port connectivity :\u00a0At least one output port must be\n    connected, unless optional = true.   @OutputPortFieldAnnotation(name = \"result\", optional = true)    Unique names for operators and streams :\u00a0Check is done to ensure that\n    operators and streams are named uniquely within the application.    Cycles in the dag :\u00a0Check is done to ensure that the application is a\n    DAG and no cycle exists.   Port annotations with the same name :\u00a0Checked to ensure that the port\n    annotations are one per port  One stream per port :\u00a0A port can connect to only one stream. This\n    check is very clear for an input port as by definition only one\n    stream can input to a port. This check is also done for an output\n    port, even though an output port can technically write to two\n    streams. This is done as the message queue infrastructure does a\n    much better job and allows STRAM to better do run time\n    optimizations. If you must have two streams originating from a\n    single output port, use a StreamDuplicator \u00a0operator.", 
            "title": "Initialization/Instantiation Time"
        }, 
        {
            "location": "/operator_development_ref/#runtime-validations", 
            "text": "Run time checks are those that are done when the application is running.\nThe \u00a0 Apache\nApex platform provides rich run time error handling mechanisms. These\nchecks also include those done by application business logic. In these\ncases the platform allows applications to count and audit these.  Examples include the following:   Error Tuples :\u00a0Applications can log error tuples during computation.\n    These are counted on a per window basis and are part of application\n    statistics. This allows for an automated way to integrate error\n    catching into a monitoring system. These tuples can be written into\n    a file. The default storage engine is HDFS. This allows application\n    developers to debug and analyse data.  Test Framework :\u00a0The ability to test the application at run time\n    includes the need to verify the functionality against a previous\n    run. Users can do that via a test framework that allows an\n    application to process parallel sets of streams, get the\n    computations on a selected list of output ports and compare the\n    results. The test would be user driven, would verify that the atomic\n    micro-batches generated are identical and certify an application.\n    This test allows users to validate any change to the application\n    operator logic in run time.  Error Ports :\u00a0Ports that emit error tuples are supported by the\n     @OutputPortFieldAnnotation . This means that users have to create\n    these ports just like any other ports with an annotation\n     @OutputPortFieldAnnotation (error = true) . The platform can then\n    support a \u201cnumber of error tuples\u201d computation just by counting the\n    errors emitted on this port and aid in persistence.  Bucket Testing :\u00a0When any new application logic needs to be tried\n    out, it is useful to first try it on a very small sample of a\n    stream. The concept of bucket testing will be available in a future\n    release. In this mode, the incoming streams (input adapters) would\n    have a Sampler operator inserted on appropriate streams in the\n    application. Users would thus be able to test a modification to the\n    application on a small subset of data. This feature will be\n    available in a later release.  Unifier Output Port :\u00a0A unifier has to have only one output port as\n    the platform needs to know which port to connect the input port of\n    the downstream operator to. This is achieved by checking that\n    unifier operator has only one output port.   Regarding the stream schema and the ability to validate dynamic DAG\nmodifications, the engine has only partial knowledge of the port types\nsince type information may be erased by the Java compiler and the exact\ntype used to instantiate a parameterized operator may not be known\n(HashMap, ArrayList, Object, etc.). Many validations are performed\nbefore an application is fully launched (compile time, startup). There\nare cases that need runtime checks, including connectivity, error\ntuples, operator uptime, container uptime, and others. Run time schema\nchecks can still be performed using the instanceof() function. Object\noriented design and usage of base class APIs is very critical to enable\nrun time schema checks. If the port of the operator to be inserted on\nthe stream only accesses the base class API, then run time insertion can\nbe done via usage of the instanceof() function. In later versions,\nstronger support would be provided by the platform for run time check\nfor dynamic insertion of objects.", 
            "title": "Runtime Validations"
        }, 
        {
            "location": "/operator_development_ref/#transient-fields", 
            "text": "During the application lifecycle, operators are serialized as part of\ncheckpointing in the distributed environment. The platform uses Kryo for\nserialization. If a field is marked \"transient\" it will not be\nserialized. A number of guidelines need to be followed to avoid error or\ninefficiencies due to serialization:   Port objects need to be declared as public static final. The port\n    objects are non-static inner classes that cannot be deserialized due\n    to absence of a default constructor. There is no need to serialize\n    port objects as port objects are stateless. State can be maintained\n    by the enclosing operator instance.  Use non-transient fields for properties that configure/customize the\n    operator (and are set at initialization time). These properties are\n    required when operator is setup in the executing container and need\n    to be serialized. You can allow runtime changes to properties via\n    setter functions. This means that such objects/properties must be\n    non-transient as you now rely on the set value being serialized.  Use non-transient for objects that are required across window\n    boundaries. This will ensure that the state is checkpointed and\n    recovered when the operator is transferred between containers as\n    result of rescheduling or an outage. An operator whose functionality\n    depends on tuples from previous windows must have those objects as\n    non-transient.  Use transient objects for state of the operator in the following two\n    cases:  If the computation and resultant output from an incoming tuple does\n    not depend on any previous tuples. This means that the operator is\n    completely stateless. For such an operator, all serializable objects\n    should be declared as transient to reduce the size of the checkpoint\n    state.  If the computation and resultant output from an incoming tuple only\n    depends on other tuples in the same window. Such an operator can\n    technically be made stateless. The down side is that you would need\n    to ensure that application window is always equal to the streaming\n    window. If the application window is not equal (i.e. more than) the\n    streaming window, then the checkpoint will still happen on streaming\n    window boundaries, and therefore the state would include the state\n    of the operator within a window. This is because once the\n    application developer sets an application window, the endWindow call\n    is skipped until the application window boundary. As an operator\n    developer you can force this operator to be stateless by checking\n    for the application window value during the setup call.   To force the checkpoint of such operators to align with the application\nwindow boundary set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This will ensure more efficient execution\nas it avoids unnecessary serialization. Currently, by default\ncheckpointing happens at the checkpoint period, and that is most likely\nmore frequent than an application window. For long application windows\nit is more efficient to checkpoint more often and thereby avoid a very\nlong replay queue. For such an operator the parameters including those\nthat only exist within a window are part of the state of the operator as\nthe checkpoint would happen on an intermediate streaming window\nboundary. As an operator developer, if you are using transient objects\nfor state within a window it is very critical that you ensure that the\nApplication window is equal to streaming window. This can be done either\nduring the setup call, or you can set the checkpointing window count to\napplication window count.", 
            "title": "Transient Fields"
        }, 
        {
            "location": "/operator_development_ref/#stateless-vs-stateful", 
            "text": "The platform intends to discern stateless vs. stateful without direct\nspecification from the developer. This depends on declaring objects as\ntransient. Therefore, care should be taken to ensure that objects that\nmay form state of the operator are not declared as transient. This\naspect is a critical part of the operator design.  The state of an operator is defined as all the non-transient fields of\nthe operator. To exclude a variable from being included in the state,\nthe developer must declare that variable as transient. Since the\ncomputing model of the platform is to treat windows as atomic\nmicro-batches, the operator state is checkpointed after an endWindow and\nbefore the next beginWindow event. In a future version, we will be\nadding an attribute that would allow an operator developer to force the\ncheckpoint to align with the application window boundary.  Checkpointing is a process of serializing the operator object to disk\n(HDFS). It is a costly procedure and it blocks computations. To avoid\nthis cost checkpointing is done every Nth window, or every T time\nperiod, where T is significantly greater than the streaming window\nperiod. A stateless operator (all variables are transient) can recover\nmuch quicker than a stateful one and pay a far lower checkpointing\npenalty. In the future, the platform may interpret an operator as\nstateless and remove the checkpointing penalty. The needed windows are\nkept by the upstream buffer server and are used to recompute the lost\nwindows, and also rebuild the buffer server in the current container. A\npassthrough operator is an example of a stateless operator. For example,\nan operator that takes a line, splits it into key/val pairs, and sends\none HashMap\\ key,val> per line is stateless as the outbound tuple is\nderived solely from the in-bound tuple.  The Stateless vs. Stateful distinction of an operator does not impact\nthe core platform engine, i.e. the platform does not distinguish between\nthese. However it is a very good distinction to learn for those who want\nto write operators. The data that has to be transferred to the next\nwindow must be stored in non-transient objects, as a node recovery\nbetween these two windows must restore its state. If the outbound tuple\ndepends on tuples before the current tuple, then the operator has a\nstate. These objects must be non-transient. The state of an operator is\nindependent of the number of ports (ports are transient objects).  A stateless operator is defined as one where no data is needed to be\nkept at the end of any API call (e.g. beginWindow, process, or\nendWindow). In other words, all variables are transient and one where\nthe outbound tuple solely depends on the current in-bound tuple. Such an\noperator is completely stateless. Another stateless operator is one\nwhere all the computations of a window can be derived from all the\ntuples the operator receives within that window. This guarantees that\nthe output of any window can be reconstructed by simply replaying the\ntuples that arrived in that window. But for such an operator the\noperator developer needs a mechanism to ensure that no checkpointing\nmust be done within an application window. The downside of such an\noperator is that if it is used for a very large application window, e.g.\na few hours, then the upstream buffer server must maintain that many\ntuples. Stateless operators have much more efficient recovery, however\noperator developers must take into account the cost of maintaining the\nbuffers of an application window in the upstream bufferserver.  A stateful operator is defined as one where data needs to be stored at\nthe end of a window for computations, i.e some variables are\nnon-transient. Stateful operators are also those where outbound tuples\ndepends on more than one incoming tuple (for example aggregates), and\nthe operator developer has allowed checkpointing within an application\nwindow. Stateful operators have costlier recovery as compared to\nstateless operators.  If a container object has to be cleared after every window, it is better\nto clear it in endWindow as compared to beginWindow. Since checkpointing\nis done after endWindow, in cases where the checkpointing is done only\nafter application window or streaming window, this object is empty. If\nthe operator developer is not sure about application developer not\nasking for checkpointing within an application window, clearing\ncontainer objects in endWindow is more efficient as the object in most\ncases does not become part of the checkpoint state.", 
            "title": "Stateless vs Stateful"
        }, 
        {
            "location": "/operator_development_ref/#single-vs-multiple-inputs", 
            "text": "A single-input operator by definition has a single upstream operator,\nsince there can only be one writing port for a stream. \u00a0If an operator\nhas a single upstream operator, then the beginWindow on the upstream\nalso blocks the beginWindow of the single-input operator. For a window\nto start processing on any operator at least one upstream operator has\nto start processing that window. The platform supports \u201cat least one\nupstream operator should start processing\u201d model to allow processing to\nstart as soon as possible. For a single input operator this is very\nefficient as setting up internal objects for processing can be done in\nparallel and before the first tuple arrives.  A multi-input operator can have more than one upstream operator. Each\ninput port has an upstream operator. In some cases all the input ports\nmay be connected to output ports of the same upstream operator. In\neither case the multi-input operator will not close a window until all\nthe upstream operators close this window. Thus the closing of a window\nis a blocking event. A multi-input operator is also the point in the DAG\nwhere windows of all upstream operators are synchronized. The windows\n(atomic micro-batches) from a faster (or just ahead in processing)\nupstream operators are queued up until the slower upstream operator\ncatches up. The STRAM monitors and guarantees these conditions. These\nmay occur dynamically due to changes in throughputs on various streams,\ncaused by internal or external events.", 
            "title": "Single vs Multiple Inputs"
        }, 
        {
            "location": "/operator_development_ref/#hierarchical-operators", 
            "text": "Hierarchical operators are those whose functional logic it itself a DAG.\nThe difference between an application and a hierarchical operator is\nthat the later has ports and thus can be inserted in other applications.\nHierarchical operators are very useful for reuse, and enforcing common\ndesign practices. The development for hierarchical operator is underway\nand will be available in a future version.", 
            "title": "Hierarchical Operators"
        }, 
        {
            "location": "/operator_development_ref/#macros", 
            "text": "Macros are sets of instructions that run via the CLI to insert a\nsub-query (sub-DAG) into the application. They have a similar result as\nthe hierarchical operators, except they are executed at run time. In the\nfuture when the CLI supports application creation, macros can be used\nduring application creation time. Macros would still differ from a\nhierarchical operators as the operator would have a scope that the macro\nmay not.", 
            "title": "Macros"
        }, 
        {
            "location": "/operator_development_ref/#3-computation-model", 
            "text": "In this section we discuss details of the computation model of an\noperator. It is very important for an operator developer to understand\nthe nuances of the operator computational model to be able to leverage\nall the rich features provided by the platform.", 
            "title": "3: Computation Model"
        }, 
        {
            "location": "/operator_development_ref/#single-dedicated-thread-execution", 
            "text": "All code of an operator always executes in a single dedicated thread\nwithin a Hadoop container. This is a design principle of the platform,\nand that ensures that all the calls on the operator are invoked in the\nsame thread. This frees the user from synchronization considerations in\nthe operator logic. This makes coding very easy for the operator, as\nonly one tuple is processed at any given time and no locking has to be\ndone. Arrival of two tuples on the same stream is not an issue as they\nalways arrive in order. However, for operators that process multiple\ninput streams, the platform serializes (de-parallelizes) the multiple\nparallel streams into one so that the processing of individual tuples\ndoes not overlap with that of another tuple on the same stream or any\nother stream processed by the operator. Since the streams are coming\nfrom different sources in a distributed architecture, there is no\nguarantee when tuples in two different streams would arrive with respect\nto each other. Thus the order of tuples from two different streams is\nrandom and cannot be guaranteed. The only guarantee is that all tuples\nof the streams that the operator listens to that belong to a window id\nwould arrive in that window. Operator logic should thus be written in\nsuch a way that it does not depend on the order of tuples from two\ndifferent streams. An operator that has such a dependency is not\nidempotent. The serialization of streams also ensures that all the\nstreams are synchronized on window boundaries. The management of\nin-flowing tuples is handled by the platform to allow the operator code\nto run in a single thread execution, and frees the developer to focus\nsolely on the business logic.", 
            "title": "Single Dedicated Thread Execution"
        }, 
        {
            "location": "/operator_development_ref/#mutability-of-tuples", 
            "text": "Tuples emitted and received from ports are POJO (plain old java\nobjects). Operator developers should be careful about the ownership\nsemantics of tuples that are mutable objects and passed through\nTHREAD_LOCAL or CONTAINER_LOCAL streams (shared object references).\nImmutable objects can be passed without regard to the type of stream\nbeing used. In general immutable objects are better as they free the\ndownstream tuple from having to worry about the life span of the tuple.\nFor performance reasons it may be okay to use mutable objects during\nprocessing (internal data of the objects), but emitted tuples should\nideally be immutable. Emitting mutable tuples may be ok if they are not\naccessed by the operator post emit.  Within the operator code, care should be taken to not change a tuple\nemitted by that operator. The example below may result in bad data\n(empty HashMap) for a downstream operator:  // Bad example, as object 't' is being changed post emit\nHashMap   String, Integer   t = new HashMap  String, Integer   ();\nt.put( i , 2);\noutputport.emit(t);\nt.clear();", 
            "title": "Mutability of tuples"
        }, 
        {
            "location": "/operator_development_ref/#passthrough-vs-end-of-window-tuples", 
            "text": "Tuples can be emitted during beginWindow(), process(), or endWindow().\nAll these tuples would have the windowId associated with the window\nmarked by beginWindow() and endWindow() calls. The same operator can\nemit a tuple in each of the above calls.  A passthrough tuple is one that is emitted during beginWindow() or\nprocess(). The reason for emitting a tuple during beginWindow() is rare\nas the tuple would be emitted without receiving any data. The most usual\npassthrough tuple is the one that is emitted during an input port's\nprocess() call. In case of multiple input ports, care should be given\nthat a passthrough tuple is not dependent of the order of tuple arrival\non two different input ports, as this order is only guaranteed within a\nstream, i.e. on one input port. Passthrough tuples do not always imply\nthat the operator is stateless. For an operator to be stateless, an\noutbound tuple should depend only on one tuple of the process call.  An end of window tuple is the one that is emitted during endWindow()\ncall. These tuples are usually aggregates, are and the ones that wait\nfor all tuples to be received in the window before they are emitted.\nThey are thus by definition not impacted by the order in which tuples\nmay arrive on different input windows. However existence of an end of\nwindow tuple almost always means that the operator is stateful.  A passthrough tuple has lower latency than an end of window tuple. The\nplatform does not differentiate between a \u201cpassthrough\u201d tuple or an \u201cend\nof window\u201d tuple, and in fact does not even recognize them as different.\nFor downstream operators there is no semantic difference between a tuple\nthat is emitted as passthrough or as end of window. All the tuples are\nalways received as part of the process() call on the input ports of\ndownstream operators. This means that the difference in latency of a\ntuple emitted during end of window as compared to that during process()\nis \"window period/2\". The rest of the downstream operators make no\ncontribution to latency other than their processing time, which would be\nidentical for both passthrough and end of window emission. This is true\nfor the entire DAG irrespective of which operator decides between\nemitting a tuple during process call or during endWindow() call. An end\nof window tuple also has one safety feature that it is easy to ensure\nthat the outbound tuple does not depend on the order in which tuples\narrived on different input ports. Operators that only emit during\nendWindow() can be clearly marked as \"independent of tuple order in two\ninput streams\". This is very useful as it allows a host of\noptimizations, re-partitioning, and other operations to be done. Even if\nthe STRAM is not able to dynamically figure this difference, there is a\npossibility of adding annotations on the operator to signal the STRAM\nabout their behavior in the future. In the future, the platform would\nleverage this data.", 
            "title": "Passthrough vs End of Window tuples"
        }, 
        {
            "location": "/operator_development_ref/#streaming-window-vs-application-window", 
            "text": "The platform supports two windowing concepts. The first one is the\nstreaming window, i.e. the smallest atomic micro-batch. All bookkeeping\noperations are done between an end of a window and the start of the next\none.\u00a0This includes checkpointing, inserting the\ndebugger, inserting charting, recovery from the start of such a window,\netc. The second is the application window. This is decided by the\nfunctionality of the application. For example if an application is\ncomputing revenue per hour, then the application window is one hour. For\nbetter operability, minimizing the streaming window is recommended as\nthat denotes the smallest loss of computations in the event of an\noutage. The platform supports both these natively. By default, the\ncheckpointing is aligned with the endWindow() call, and hence it aligns\nwith end of the application window. For large application windows this\nmay be a problem as the upstream bufferserver has to retain a very long\nqueue. The way around is to set \"checkpoint=true\" for within the\napplication window, and to write the operator\nin such a fashion that the state of the operator consists of both the\ndata that is passed between windows, as well as the dependency of an\noutbound tuple on all previous tuples. Such a state definition is safe.\nThe downside is that the state may be large and make checkpointing\ncostly. A way around this is to partition the operator based on the size\nof the state to ensure checkpoints are being saved equally by\npartitions.  There are two types of application windows: aggregate application\nwindows and sliding application windows. Aggregate application windows\nare for applications that do computations per fixed time period. It is\nspecified by an attribute since exactly the same code works for a\nstreaming window as application window. Once the aggregate application\nwindow flag is specified (by default it is equal to streaming window)\nthe the begin_window starts an application window and then the\nintermediate begin and end windows are skipped until the application\nwindow boundary is reached. Then the end_window is called. Sliding\napplication windows are for applications that do computations for past\nfixed time period. In such a case the operator code needs access to all\nthe state of the computations in a sliding fashion (e.g. the last 5\nminutes). The platform has a Java interface written for sliding windows\nthat allows operator developers to easily code a sliding window\noperator. For details refer to Real-Time Streaming Platform\nGuide.", 
            "title": "Streaming Window vs Application Window"
        }, 
        {
            "location": "/operator_development_ref/#4-commonly-used-operators", 
            "text": "There are operators in the platform that support commonly needed\nfunctions. In this chapter we review them.", 
            "title": "4: Commonly Used Operators"
        }, 
        {
            "location": "/operator_development_ref/#window-generator", 
            "text": "A Window Generator is inserted for every input adapter. This operator is\ntasked with creating windows. The windows from two different window\ngenerators get synced on the first operator that listens to streams\nwhose windows originate from the window generators. Since the downstream\noperator syncs the streams on endWindow, all the tuples from a window\ngenerator that started early would wait until the lagging window\ngenerator starts sending window events.  As much as possible, window generators should not be directly used by\nusers in their designs. The API of the window generator class is not\npublic and therefore should not be relied upon. An application that uses\nwindow generators directly also risks complications and compatibility\nissues with future releases of the platform.", 
            "title": "Window Generator"
        }, 
        {
            "location": "/operator_development_ref/#default-unifier", 
            "text": "A default unifier is provided for merging partitions. The default\nunifier is a passthrough, i.e. it forwards all the tuples from\npartitions to downstream operators, thus enabling the downstream\noperators to connect to one upstream source (i.e. the unifier). The\nunifiers works the same in Nx1 partitions as well as NxM partitions; the\nonly difference is that a NxM partition would have M unifiers - one for\neach downstream operator partition.", 
            "title": "Default Unifier"
        }, 
        {
            "location": "/operator_development_ref/#sinks", 
            "text": "Sinks are objects that implement the Sink interface. The sink is the\nbasic interface that has the process(tuple) API. The platform\ntransparently connects output ports to sinks for needed functionality.\nThese sink objects are used inside the engine for various purposes.\nExamples include stats collection, debugging, chart data collection,\netc. We have also included a host of sinks in testbench library. These\nare very valuable as they help developers quickly run their tests. Using\nsinks in tests is recommended as they follow the same API concepts and\nare fully supported by the platform. All the sinks are included in unit\nand performance tests and their functionality and performance is\nguaranteed.   CollectorTestSink  PartitionAwareSink  WindowIdActivatedSink  ArrayListTestSink  CountAndLastTupleTestSink  HashTestSink  CountTestSink", 
            "title": "Sinks"
        }, 
        {
            "location": "/operator_development_ref/#inputoperator", 
            "text": "The InputOperator interface should be used to develop input adapters.\nThis is the interface that all input adapters implement. Operators that\nimplement this interface need to implement the following method:  public void emitTuples();  This method is called continuously between the beginWindow() and\nendWindow() calls and is supposed to fetch data from an external system\nand write it out to the output port(s).", 
            "title": "InputOperator"
        }, 
        {
            "location": "/operator_development_ref/#apache-apex-malhar-library-templates", 
            "text": "The Util and Common libraries have a collection of operators that can be\nextended to create custom operators. These include operators for key/val\npairs, matching, filters, unifiers, and others.", 
            "title": "Apache Apex Malhar Library Templates"
        }, 
        {
            "location": "/operator_development_ref/#databasewindow-synchronization-operator", 
            "text": "Database adapters (input or output) need to be able to instrument the\nat-most-once mechanism. This is needed as that is the final output\nstate, or once-only processing of incoming data. The standard library\ntemplates for databases have such a mechanism built in. A base operator\nis provided for precisely such a behavior. These operators rely on last\ncompletely processed window being written to these outside system for\nouputAdapters, and retaining the last read event for inputAdapters.", 
            "title": "Database/Window Synchronization Operator"
        }, 
        {
            "location": "/operator_development_ref/#5-fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to recognize\nan outage in any part of the application, provision replacement\nresources, initialize the lost operators to a last-known state, and\nrecompute the lost data. The default method is to bring the failed part\nof the DAG back to a known checkpointed state and recompute atomic micro\nbatches from there on (also called the \u201cat-least-once\u201d recovery\nmechanism). Operators can be set for \"at-most-once\" recovery, in which\ncase the new operator starts from the next available window. Operators\ncan be set for an \u201cexactly-once\u201d recovery, in which case the operator\nonly recomputes the window it was processing when the outage happened.\nAt-most-once recovery as an attribute will be available in a later\nversion.\u00a0For now, \u201cexactly-once\u201d recovery is\nachieved by setting the checkpoint interval to 1. This mechanism is very\ncostly as checkpointing is done after every\nwindow. In the future when the platform\nprovides the ability to recognize a stateless operator, an exactly-once\nmechanism will be significantly less costly as checkpointing is not\nneeded for stateless operators. At-most-once recovery in most\napplications is an outbound need, i.e. the data being written out to a\nsystem outside the application needs to be written only once. \u00a0This is\ninstrumented for output adapters (where it really matters) by saving the\nprocessed window ID into the outbound system (database, files, etc.) to\nbe used as a GUID or primary key. The default output adapters provided\nin the Apache Apex Malhar Library Templates \u00a0include such\nmechanism.  A choice of a recovery mechanism is decided both by the operator design\nas well as the application needs. If computations are data-loss\ntolerant, an at-most-once mechanism works. For computations that are\ndata-loss intolerant, an at-least-once mechanism works. For computations\nthat write to an outside state and cannot handle re-computations, an\nexactly-once model is needed.", 
            "title": "5: Fault Tolerance"
        }, 
        {
            "location": "/operator_development_ref/#checkpointing", 
            "text": "The STRAM provides checkpointing parameters to StreamingContainer during\nintialization. A checkpoint period is given to StreamingContainer of the\ncontainers that have window generators. A control tuple is sent when the\ncheckpoint interval is completed. This tuple traverses through the data\npath via streams and triggers each StreamingContainer in the path to\ninstrument a checkpoint of the operator that receives this tuple. This\nensures that all the operators checkpoint at exactly the same window\nboundary. The only delay is the latency of the control tuple to reach\nall the operators. The checkpoint is thus done after the endWindow()\ncall of the current window and before the beginWindow() call of the next\nwindow. Since all the operators are computing in parallel (separate\nthreads) they each process the \u201ccheckpoint\u201d control tuple independently.\nThe asynchronous design of the platform means that there is no guarantee\nthat two operators would checkpoint at exactly the same time, but there\nis guarantee that they would checkpoint at the same window boundary.\nThis feature also ensures that purging old data can be done very\nefficiently, since when the checkpoint window tuple is done traversing\nthe DAG, the checkpoint state of the entire DAG increments to this\nwindow id.  In case of an operator that has an application window that is different\nfrom the streaming window, the checkpointing happens after the\napplication window is complete. Though this allows the operators to\ntreat the application window as an atomic unit, it does need the\nupstream bufferserver to keep tuples for the entire application window.  By default, checkpoints are not done inside of an application window.\nApplication developers can choose to override this and specify that\ncheckpoint windows be used. This is possible only if the operator is\ncompletely stateless, i.e. an outbound tuple is only emitted in process\ncall and only depends on the tuple of that call. If the operator is\nstateful within a window, the operator developer should disallow\ncheckpointing within the window as the atomic computation could be for\nan application window. If the application developer allows for\ncheckpointing within an application window, then the checkpoint window\nis followed by the STRAM. If the application window is not an exact\nmultiple of the checkpoint window, then the checkpoints get done a\nlittle early. For example, in an application with streaming window = 1\nsec, if the checkpoint window is 30 for the application, and application\nwindow is 100, then the operator will checkpoint at 30, 60, 90, 100,\n130, 160, 190, 200 secs. For such a case, STRAM purge process will take\nthese into account by keeping required tuples in the bufferservers.  Checkpointing involves pausing an operator and serializing the object to\nHDFS. After the checkpoint state is saved, the operator may start\nprocessing events again. Thus, checkpointing has a latency cost\nimplications in the throughput. It is important to ensure that\ncheckpointing is done with minimal required objects. This means that all\ndata that is not part of an operator state must be declared as\ntransient. An operator developer can also create a stateless operator as\nlong as the life span is only within a streaming window (i.e not part of\nthe state of the operator). By default this would work and such data can\nbe declared as transient. The serialized data is stored as a file, and\nis the state that the operator can be brought back to. The ID of the\nlast completed window (per operator) is sent back to the STRAM in the\nnext heartbeat. The default implementation for serialization\nuses \u00a0 KRYO \u00a0.\nMultiple past checkpoints are kept per operator. Depending on the\ndownstream checkpoint, one of these are chosen to start from. These are\npurged only after they are are no longer needed. STRAM takes the purge\ndecision and informs all bufferservers about these.  A complete recovery of an operator needs that the operator be brought\nback to a checkpoint state and then all the lost atomic windows being\nreplayed by upstream buffer server. The above design keeps the\nbookkeeping cost very low, and still allowing rapid catch up of\nprocessing. In the next section we would see how this simple abstraction\nallows applications to recover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/operator_development_ref/#recovery-mechanisms", 
            "text": "Recovery mechanisms are ways to recover from a container (or an\noperator) outage. In this section we explain a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container is\nalso down. If multiple operators are present in a container, then the\ncontainer recovery treats each operator as independent objects when\nfiguring out the recovery steps. Application developers can set any of\nthe below recovery mechanisms for node outage. In general, the cost of\nrecovery depends on the state of the operator and the recovery mechanism\nselected, while the data loss tolerance is specified by the application.\nFor example a data loss tolerant application would prefer \u201cat-most-once\u201d\nrecovery.  All recovery mechanisms treat a streaming window as an atomic\ncomputation unit. In all three recovery mechanisms the new operator\nconnects to the upstream bufferserver and asks for data from a\nparticular window onwards. Thus all recoveries translate to deciding\nwhich atomic units to re-compute, and which state the new operator\nshould start from. A partially computed micro-batch is always dropped.\nThey are re-computed in at-least-once or exactly-once mode. In\nat-most-once mode, they get skipped. Atomic micro-batches are a critical\nguiding principle as this allows for very low bookkeeping cost, high\nthroughput, low recovery time, and high scalability.  Within an application, each operator can have its own recovery\nmechanism. The operators can be developed oblivious to the recovery mode\nin which they will function. Yet, in the cases where they do need to\nknow, the processing mode can be obtained as:  ProcessingMode mode =\ncontext.attrValue(OperatorContext.PROCESSING\\_MODE, \u00a0 \u00a0 \u00a0\nProcessingMode.AT\\_LEAST\\_ONCE);", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/operator_development_ref/#at-least-once", 
            "text": "At-least-once recovery is the default recovery mechanism, i.e it is used\nif no mechanism is specified. In this method, the lost operator is\nbrought back to its latest checkpointed state and the upstream buffer\nserver is asked to replay all windows since the checkpointed window.\nThere is no data loss in this recovery mode. The viable checkpoint state\nis defined as the one whose window ID is in the past as compared to all\nthe checkpoints of all the downstream operators. All downstream\noperators are restarted at their checkpointed state in the same\ncontainer. They ignore the data until the stream catches up to their\nstate by subscribing after their checkpointed window. All the lost\natomic micro-batches are thus recomputed and the application catches up\nwith live incoming data. This is the at-least-once mechanism, as lost\nwindows are recomputed. For example, if the streaming window is 0.5\nseconds and checkpointing is being done every 30 seconds, then upon node\noutage all windows since the last checkpoint (up to 60 windows) need to\nbe re-processed. If the application can handle loss of data, then this\nis not the most optimal recovery mechanism.  In general in this recovery the average time lag on a node outage in at\nleast recovery is:  Recovery time = (CP/2 \\* SW) \\* T + HC\n\nWhere:\n\nCP \u00a0 \u00a0 \u00a0 \u00a0Checkpointing period (default value is 30 seconds)\n\nSW \u00a0 \u00a0 \u00a0 \u00a0Streaming window period (default value is 0.5 seconds)\n\nT \u00a0 \u00a0 \u00a0 \u00a0 Time taken to re-compute one lost window\n\nHC \u00a0 \u00a0 \u00a0 Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s) to its checkpointed state  A lower CP is a trade off between the cost of checkpointing and the need\nto have lower latency during node recovery. Input adapters cannot do\nat-least-once without the support from sources outside Hadoop. For an\noutput adapter, care needs to be taken if external systems cannot handle\nre-writing the same data.", 
            "title": "At-Least-Once"
        }, 
        {
            "location": "/operator_development_ref/#at-most-once", 
            "text": "Applications that can tolerate data loss get the quickest recovery in\nreturn. The engine brings the operators to the most recent checkpointed\nstate and connects its ports to the upstream buffer server, subscribing\nto data from the start of the next window. It then starts processing\nthat window. The downstream operators realize some windows were lost,\nbut continue to process. Thus, an at-most-once mechanism forces all\ndownstream operators to follow. In the cases where an at-most-once\noperator has more than one input port, it\u2019s possible that they play\ndifferent windows. In this case, some of the windows get the data from\njust from a few of the input ports and some of the windows get lost - by\ndefinition of at-most-once. This is acceptable because we care about\ncatching up to a steady state as fast as possible, and once achieved not\nlosing any data.  For example, if the operator has ports in1 and in2 and a checkpointed\nwindow of 95, and their buffer server responds with window id 100 and\n102 respectively (window 100 was received before 102), \u00a0then the\noperator will work on the tuples from only that buffer server which is\nat window 100. At the completion of that window, if tuples from 101 were\nreceived before 102, then it will work with the one with the data\nreceived for window 101, and then it will go on to process window 102.\nBut if tuples from window 102 were received before window 101, then\nwindow 102 will be processed and window 101 will be skipped completely.\nBut from window 102 onwards the operator will resume regular processing\nunless one of the inputs starts skipping the windows.  In general in this recovery the average time lag on a node outage in\nat-most-once recovery is:  Recovery time = SW/2 + HC\n\nWhere:\n\nSW \u00a0 \u00a0 \u00a0 \u00a0Streaming window period (default value is 0.5 seconds)\n\nHC \u00a0 \u00a0 \u00a0 Time it takes to get a new Hadoop Container, or make do with the current ones, and initialize the operator(s)", 
            "title": "At-Most-Once"
        }, 
        {
            "location": "/operator_development_ref/#exactly-once", 
            "text": "This recovery mechanism is for applications that need no data-loss as\nwell as no recomputation. Since a window is an atomic compute unit,\nexactly-once applies to the window as a whole.\u00a0In this recovery the\noperator is brought back to the start of the window in which the outage\nhappened, and the window is recomputed. The window is considered closed\nwhen all the data computations are done and end window tuple is emitted.\n\u00a0Exactly-once requires every window to be checkpointed. Hence, it\u2019s the\nmost expensive of the three recovery modes. The operator asks the\nupstream buffer server to send data after the most recent checkpoint.\nThe upstream node behaves the same as in at-most-once. Checkpointing\nafter every streaming window is very costly, but users will most often\ndo exactly-once per application window in which case the cost of running\nan operator in this recovery mode may not be as costly. In the current\nset, up exactly-once can be achieved by setting checkpointing window to\n1 on an operator.", 
            "title": "Exactly-Once"
        }, 
        {
            "location": "/operator_development_ref/#6-partitioning", 
            "text": "Partitioning is the fundamental building block for scaling a streaming\napplication. Any of the logical units in an application may face a\nresource crunch. This could be an operator that does not have enough CPU\nor RAM to complete its computations, or it could be a stream that just\ncannot fit within a NIC bandwidth limit. The platform provides rich\npartitioning schemes to help scale an application. The platform is\ndesigned to scale at a Hadoop level and has several functionalities\nbuilt in that support scalability.", 
            "title": "6: Partitioning"
        }, 
        {
            "location": "/operator_development_ref/#static-vs-dynamic", 
            "text": "In general, partitioning can be done statically (during launch time) or\ndynamically (during run time). There is no other difference between\nthese. The same logic and schemes hold true to both launch time as well\nas run time partitioning. The advantage of dynamic partitioning is that\nthe application can respond to runtime demands and leverage resources\nfrom Hadoop to meet SLAs or other constraints. This helps to optimize\nresources, since they may be provisioned only when exactly needed.  The implementation of dynamic partitioning impacts runtime as the new\npartitions are rolled out and stream processing is restarted based on\nthe recovery mechanism of the operator. In the case of at-least-once\nprocessing, the operator partitions go back to a checkpointed state. In\nthe case of at-most-once processing, the new partitions simply start\nfrom the latest window (i.e. there is data loss). Finally, in the case\nof exactly-once processing, the operator restarts at the current window.\nThis becomes very tricky when the operator is stateful. In certain\ncases, the STRAM needs to ensure that all the partitions are at the same\nstate. Scaling up may be accomplished by splitting all the partitions\ninto N different partitions, or by splitting the largest partition into\ntwo, but scaling down needs to ensure that the partitions being merged\nare checkpointed at the same window id. Thus, dynamic partitioning is\ncostlier and impacts application performance. Static partitioning,\nhowever, is done at launch time and if correctly done may not need\nfurther partitioning. In general this means that extra resources, if\nany, stay idle and are wasted. A choice between (possibly) wasting\nresources, or impacting application performance is a decision for the\nuser to make. With commodity hardware this decision is a close one.", 
            "title": "Static vs Dynamic"
        }, 
        {
            "location": "/operator_development_ref/#partitioner", 
            "text": "How does the platform know what data an operator partition needs to\nreceive in order to implement its logic? It depends on the functionality\nof the operator. In a simplified example, an operator may want to\nprocess a range of data, let us say all names that start with A through\nC should be belong to partition 1 and the rest to partition 2 (a custom\nstream codec would hash A through C to \u00a01 and the rest to 2). The\noperator needs the ability to declare that there should be 2 partitions\n(one receives tuples with hash code 1, the other tuples with hash code\n2). We provide the interface Partitioner, which is designed to give\ncontrol over the distribution scheme to the operator developer. In this\ncase, the operator would provide a mapping that declares:   Operator Instance one receives on port inputPortName, all tuples\n    with hash code 1  Operator Instance two receives on port inputPortName, all tuples\n    with hash code 2   While this may look complicated at first glance, it is necessary for\nmore complex scenarios.", 
            "title": "Partitioner"
        }, 
        {
            "location": "/operator_development_ref/#multiple-ports", 
            "text": "Operators may have multiple input ports. The partitioning of incoming\nstreams on each of the ports depends entirely on the logic or the\noperator. One port may be connected to a high throughput stream that we\nwould like to partition, while the other port may deliver low throughput\nstream, but each tuple is required for the functionality of each\noperator instance, regardless which portion of the other stream it\nhandles. An example for this could be the processing of location\nupdates. In this case, the operator may receive (high speed) location\nupdates on one input port, and (slow) location queries on the other. We\nwould like to partition the first stream by location, and receive\nlocation query on the the other port. Only the operator instance that\nhandles the corresponding location will respond to the query. \u00a0A\ndifferent mapping would be needed for an operator with 2 ports that\njoins tuples. In this case, each of the streams would need to be\npartitioned in exactly the same way. Since the StreamCodec could be\ncustom, there is no way for the platform to check if two ports are\nlistening to streams that are partitioned the same way (or in a\nparticular manner).", 
            "title": "Multiple Ports"
        }, 
        {
            "location": "/operator_development_ref/#streamcodec", 
            "text": "The StreamCodec is responsible for serialization of data tuples into\nbytes (object to byte array) and deserialization (byte array to object)\nfor transmission through buffer server streams. The codec is defined on\ninput ports and is used by the platform to serialize the data when\nemitted through the upstream operator\u2019s output port before writing to\nthe stream and to deserialize after reading from the stream, before\nhanding over the tuple object to the input port.  When partitioning is enabled for a stream, the codec is responsible to\nassign the partition key for a tuple before it is transmitted over the\nstream. The buffer server then uses the key to determine the partition\nthat will process the tuple. The default codec is using Object\u2019s\nhashCode function to calculate this value. If the hashCode function\ncannot compute the partition key (for example, when using data\nstructures such as HashMap or ArrayList) it is necessary to supply a\ncodec that understands the data and can compute the partition key.  If no codec is set on an input port explicitly, the default stream codec\nis used. The default implementation\nuses \u00a0 Kryo \u00a0for\nserialization. It generically supports most standard types and is used\nfor all port types of library operators. To customize how objects are\nserialized or the partition key is computed, the operator developer can\nsupply a custom stream codec by overriding InputPort.getStreamCodec().", 
            "title": "StreamCodec"
        }, 
        {
            "location": "/operator_development_ref/#unifier", 
            "text": "When an operator is partitioned into N physical operator instances, the\ndownstream operator needs to get streams from all the partitions. All\noperator instances however, have fixed number of ports, and hence have\nfixed number of streams they can read from or write to. This anomaly is\nsolved by having the STRAM insert a dummy merge operator before the\ndownstream operator. This merge operator is CONTAINER_LOCAL (intra jvm\nprocess) with the partitioned operator. Since the STRAM knows exactly\nhow many partitions exists at execution roll out time, it inserts a\nmerge operator with precisely that number of inputs. The default merge\noperator is a passthrough, i.e. it simply forwards all the tuples from\nits input ports onto the output port. It has a single output port whose\nschema is same as that of the output port of the partitioned operator.\nThe downstream operator thus does not notice any difference as the\noutputs of all partitions are combined.  An example of an operator that needs specific functionality for the\nmerge to work is SumMap operator. This operator takes a HashMap\\ K,V>\nas input. Its output is the sum of all the values of a particular key.\nSince HashMap is a single tuple that could be a collection of various\nkeys, the input port has to be partitioned on a round-robin basis. This\nmeans that any particular key may appear in any of the partitions. So\nthe only way to get correct output is for the merge operator to do a\nkey-wise sum again. This can be implemented in the unifier logic.  Another example is this would be top N calculation, where top N results\nfrom each partition need to be consolidated into aggregate top N at the\nend of the processing window. For this, the output port of any operator\ncan define a \u201cnifier\u201d object, which is an operator that has no input\nports but instead a single method that accepts tuples emitted from\npartitions and implements the logic required to merge them. The outputs\nof each instance of a partitioned operator feeds to this unifier\noperator and thus intercepts the output stream in front of the\ndownstream operator.  The unifier is an operator that implements Unifier interface. This\ninterface needs one method to be implemented - process(\\ type> tuple).\nThe schema (\\ type>) of the process() method is same as the schema of\nthe output port in which the getUnifer method is implemented. Thus, for\nparent operators that have only one output port, proper coding habits\nallow usage of the parent operator itself to be used as the Unifier.\nSince a Unifier is an operator, it has access to the operator interface,\nnamely beginWindow(), and endWindow() in addition to process. The\nplatform automatically inserts a process call that allows developers to\ndo a proper merge as seen above. Any object that implements this API can\nbe used as a unifier. Code specific to the unifying functionality for\noutput streams of a particular operator should be coded in this class.\nSome examples of usage of a parent operator as an unifier are MinMap or\nMaxMap. This is the interface for a unifier operator:  //   T   is the tuple schema for the output port for which\n// the unifier object would be used\npublic interface Unifier   T   extends Operator\n{\n\u00a0 public void process(T tuple);\n}  When using a unifier that collects all the output tuples in one\noperator, users can get away from a sticky key\npartitioning\u00a0scheme, as long as the the unifier\ncan function within one Hadoop container (usually \\~ 1GB\nRAM), i.e. a single unifier object that has\nenough resources (CPU, RAM, and Network) to process the outputs of all\nthe partitions. Combining the output of all partitions allows the basic\npartitioning to be done by round-robin without a sticky key, and thus\navoids any skew. For operators which are partitioned by sticky key, the\ndefault merge (passthrough) works fine.  Care should be taken in cases where partitioning is done mainly to\nmanage outbound throughput, especially if it is more than the NIC\ncapacity of a Hadoop node. In this case, a Unifier does not help as all\nthe tuples flow back into the same operator. In almost all cases, the\noutbound throughput problem should be resolved by partitioning both the\ncurrent operator and the downstream operator. One way to look at this is\nthat the stream in question has a network resource requirement that no\nsingle Hadoop node can provide, hence all operators on that stream must\nbe partitioned, and no partition should read/write the full stream.", 
            "title": "Unifier"
        }, 
        {
            "location": "/operator_development_ref/#7-library", 
            "text": "The platform includes a set of operator templates, which are available\nunder Apache 2.0 license. These are open source operator library under\nApache Apex Malhar project.They are provided to enable quick application\ndevelopment. As an operator developer, you can leverage this by either\nextending them, or creating your own library. In addition to reducing\nthe development time, they also help reduce maintenance cost. The\nlibrary operators can be benchmarked, tested, and have data that the\nSTRAM can leverage for running applications optimally.", 
            "title": "7: Library"
        }, 
        {
            "location": "/operator_development_ref/#common-operator-functions", 
            "text": "There are several common operator functions that can be utilized. We\ndescribe a few categories below. You can find many more operators in the\nApache Apex Malhar project. These are provided for a developer to\nquickly create an application. They are not meant to replace or\nsubstitute custom operators. Developers should judiciously decide on\nwhich operators to use.", 
            "title": "Common Operator Functions"
        }, 
        {
            "location": "/operator_development_ref/#filtering-selecting-map", 
            "text": "Filtering is a very common operation. Filters can be employed in the\nfollowing ways:   Conversions of tuple schemas. Examples include selecting/extracting\n    a particular set of keys from the tuple (a collection) and drop the\n    rest. Changing the contents (lower/upper case, round-up/down, etc.).\n    Contents of input tuples and output tuples are different in this\n    computation.  Passing through certain tuples. Examples include letting tuples that\n    meet certain conditions go through. Tuple content remains the same\n    in this computation  Comparison. This is similar to conversion, except that the tuple\n    (flag) is just an alert.  Map can be done on file contents, or lines (word count). Combiner\n    (map-side reduce) can be done over the streaming window.   Filtering operations usually do not increase the throughput. In most\ncases the throughput will decrease. These operators are also most likely\nstateless. The resource requirements are usually not directly impacted\nby the micro-batch size.", 
            "title": "Filtering-Selecting-Map"
        }, 
        {
            "location": "/operator_development_ref/#aggregations", 
            "text": "Aggregate computations are those that need the entire window (atomic\nmicro-batch) to be computed for the results to be known. A lot of\napplications need these to be computed over the application window.\nCommon aggregation examples include   Counters like sum, average, unique count etc.  Range of the incoming data. Compute maximum, minimum, median etc.  Match. The first or the last match in the micro-batch.  Frequency. Most or least frequent  Reduce   Aggregate functions are very effective in ensuring that the micro batch\nis treated in an atomic way. Computations are dependent on the size of\nthe micro-batch and this size is usually decided between what the\napplication needs (application window) and how much micro-batch\nprocessing makes sense from an operability point of view", 
            "title": "Aggregations"
        }, 
        {
            "location": "/operator_development_ref/#joins", 
            "text": "Joins are very common operations done on streams. These computations are\nmostly done over the application window. The concept of the atomic micro\nbatch is very critical for joins. The tuples in the two streams can\narrive at unpredictable times within the window and thus at the end of\nthe window, the computation can guarantee that join operation is done\nover all the tuples in that window.", 
            "title": "Joins"
        }, 
        {
            "location": "/operator_development_ref/#input-adapters", 
            "text": "Input adapters enable the application to get data from outside sources.\nData read is done by the Input Adapter either by pulling from the source\nor by data being pushed to the adapter. Input adapters have no input\nports. The primary function of an Input Adapter is to emitdata from\noutside the DAG, as tuples for rest of the application. Once the data is\nemitted as tuples all the mechanisms and abstractions in the platform\napply. To enable a data flow to have a recovery mechanism that spans\noutside of the application, the outside source must have support for\nsuch mechanisms. For scalability (partitioning) the outside source must\nhave support or enabling features. As an example, if the application\nneeds at-least-once recovery then an input adapter must be able to ask\nfor data from a specific timestamp or frame number upon re-connect after\nan outage. The design of an Input Adapter needs very careful\nconsideration and needs application logic to be built correctly as\nstreaming platform infrastructure is sometimes not applicable to outside\nsources. Examples of outside sources include HDFS, HBase, HTTP, Messages\nbusses like ZeroMQ, RabbitMQ, ActiveMQ, Kafka, Rendezvous, Databases\nlike MongoDB, MySql, Oracle.", 
            "title": "Input Adapters"
        }, 
        {
            "location": "/operator_development_ref/#output-adapters", 
            "text": "Output adapters write out results of the application to outside sources.\nData is written to a message bus, a database, to files (HDFS), or sent\nover the network to a data collection service. The primary function of\nan Output Adapter is to write data to destinations outside the DAG, and\nmanage data integrity during node outages. Output adapters have no\noutput ports. The data received by the Output Adapter follows all the\nmechanisms supported by the streaming platform. For recovery mechanisms,\nthe Output Adapter has to store the state of the current written data.\nSince the platform is atomic on a streaming window, output adapters\nshould use end of window as a commit. This way, during a recomputation\ndue to operator outage, data integrity can be ensured.", 
            "title": "Output Adapters"
        }, 
        {
            "location": "/operator_development_ref/#event-generators", 
            "text": "Event generators are operators that generate events without an input\nport. They are needed for testing other operators, for functionality or\nload. These are different from input adapters as event generators do not\nconnect to any source. Various event adapters are available in the\ntestbench \u00a0library.", 
            "title": "Event Generators"
        }, 
        {
            "location": "/operator_development_ref/#stream-manipulation", 
            "text": "Stream manipulation operators are those that do not change the content\nof the tuples, but may either change schema, or just merge or split the\nstream. Schema change operators are needed to allow data to flow between\ntwo sets of ports with different schemas. You can use merge and split\nstreams between streams with different execution attributes. For\nexample, you would use a split operator if you want one stream to be\nCONTAINER_LOCAL, while another to be across containers. Merge is also\nused to merge in streams from upstream partition nodes.", 
            "title": "Stream Manipulation"
        }, 
        {
            "location": "/operator_development_ref/#user-defined", 
            "text": "Operator developers can easily develop their own operators, allowing for\nquick turnaround in application development. Functionality-wise, any\nuser defined operator is the same as one provided as a bundled solution.\nStandard operators do get tested with each build, are benchmarked, and\nare supported. An operator developer should take on these\nresponsibilities for user defined operators.", 
            "title": "User Defined"
        }, 
        {
            "location": "/operator_development_ref/#sample-code", 
            "text": "Code samples are included in the samples project. We will continually\nadd examples of various operator templates to this package. This package\nis part of the open source Malhar project, and users are encouraged to\nadd their examples to enable community reuse and verification. Here is the github link to the project: https://github.com/apache/incubator-apex-malhar/tree/master/samples.", 
            "title": "Sample Code"
        }, 
        {
            "location": "/operator_development_ref/#latency-and-throughput", 
            "text": "The latency of an atomic window compute operation of an operator is\ndefined as the time between the first begin window received on any input\nport and the last of the end window tuple sent out by the operator on\nany output port. Since the streaming window period is not contributed to\nby the operator, the real latency contribution of the operator is the\nabove latency minus the streaming window period. Operator developers can\nleverage distributed computing by doing as much computations upfront as\npossible. This is very useful for an operator that only emits tuples in\nend of window call.  Throughput of an operator is defined on a per port basis. The incoming\nthroughput is the number of tuples it processes per unit time on each\nport. The outgoing throughput is the number of tuples it emits per unit\ntime on each output port. Per-port data is found in the \u201cstream view\u201d of\nthe application, while the incoming and outgoing totals are found on the\noperator view of the application.  \u00a9 2012-2018 DataTorrent Inc. \u00a0Patent pending", 
            "title": "Latency and Throughput"
        }, 
        {
            "location": "/library_operators/", 
            "text": "Operator Library\n\n\nThe following operators, classified into four groups, are available:\n\n\n\n\n\n\nInput\n\n\n\n\nTCP Input Operator \nGuide\n\n\nEvent Hub Input Operator \nGuide\n\n\nKafka Input \nGuide\n\n  and \nJava Doc\n\n\nHDFS Input \nGuide\n\n  and \nJava Doc\n\n\nHDFS Input for large files \nGuide\n\n  and \nJava Doc\n\n\nJDBC Input \nGuide\n\n   and \nJava Doc\n\n\nJMS Input \nGuide\n\n  and \nJava Doc\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nPMML Operator \nGuide\n\n\nDrools Operator \nGuide\n\n\nBlock Reader \nGuide\n\n  and \nJava Doc\n\n\nCSV Parser \nGuide\n\n  and \nJava Doc\n\n\nJSON Parser \nGuide\n\n  and \nJava Doc\n\n\nJSON Formatter  \nGuide\n\n  and \nJava Doc\n\n\nDeduper  \nGuide\n\n  and \nJava Doc\n\n\nEnrich \nGuide\n\n  and \nJava Doc\n\n\nFilter  \nGuide\n\n  and \nJava Doc\n\n\nTransform  \nGuide\n\n  and \nJava Doc\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\nElasticsearch Output Operator \nGuide\n\n\nEvent Hub Output\u00a0Operator \nGuide\n\n\nAzure Blob Output Operator \nGuide\n\n\nHDFS Output \nGuide\n\n  and \nJava Doc\n\n\nJMS Output \nGuide\n\n  and \nJava Doc\n\n\n\n\n\n\n\n\nInput/Output\n\n\n\n\nAbstractHttpServer Operator \nGuide", 
            "title": "Operators List"
        }, 
        {
            "location": "/library_operators/#operator-library", 
            "text": "The following operators, classified into four groups, are available:    Input   TCP Input Operator  Guide  Event Hub Input Operator  Guide  Kafka Input  Guide \n  and  Java Doc  HDFS Input  Guide \n  and  Java Doc  HDFS Input for large files  Guide \n  and  Java Doc  JDBC Input  Guide \n   and  Java Doc  JMS Input  Guide \n  and  Java Doc     Process   PMML Operator  Guide  Drools Operator  Guide  Block Reader  Guide \n  and  Java Doc  CSV Parser  Guide \n  and  Java Doc  JSON Parser  Guide \n  and  Java Doc  JSON Formatter   Guide \n  and  Java Doc  Deduper   Guide \n  and  Java Doc  Enrich  Guide \n  and  Java Doc  Filter   Guide \n  and  Java Doc  Transform   Guide \n  and  Java Doc     Output   Elasticsearch Output Operator  Guide  Event Hub Output\u00a0Operator  Guide  Azure Blob Output Operator  Guide  HDFS Output  Guide \n  and  Java Doc  JMS Output  Guide \n  and  Java Doc     Input/Output   AbstractHttpServer Operator  Guide", 
            "title": "Operator Library"
        }, 
        {
            "location": "/operators/drools_operator/", 
            "text": "About Drools Operator\n\n\nDrools operator provides real-time complex event processing capability with the help of Drools engine.\n\n\nThe Drools operator embeds the Drools rule engine to simplify the rules configuration for a business apps such as the Fraud Prevention App.\n\n\nYou can write rules as per your business requirements and the execution aspect is handled by the Drools operator.\n\n\nThe Drools operator provides the following advantages:\n\n\n\n\nFacility to write rules in a rule file instead of programming them. This makes it easy for a domain expert (non-technical person) to configure rules.\n\n\nCentralization of knowledge-base in the form of rules. This results in easy maintenance.\n\n\nPre-defined algorithms in Drools engine provide efficient ways to match rule patterns with data.\n\n\nEasily modify the existing rules instead of modifying a program.\n\n\nSupports rule configuration in various file formats such as \n.drl\n , \n.xls\n , \n.dsl\n etc.\n\n\n\n\nThis operator is available under \nDT Premium\n license.\n\n\nTesting\n\n\nThe drools operator is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.7 and above\n\n\nDrools embedded version 6.5.0-final\n\n\n\n\nWorkflow of the Operator\n\n\n\n\n\n\nWrite and save the rules in the drools format.\n\n\nConfigure the location of this rule\ns file in the drools operator.\n\n\nDrools operator receives enriched data from upstream operators which is validated against the specified rules.\n\n\nDrools operator emits processed data with fields changed by applied rules data, details of rule name, and the number of times that the rule is applied.\n\n\n\n\nPorts\n\n\nThe following ports are available on the Drools operator:\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nInput Port\n\n\nfactsInput Port\n\n\nReceives incoming tuples from upstream operators.\n\n\n\n\n\n\nOutput Port\n\n\nfactsOutput Port\n\n\nEmits processed data with fields changed by applied rules.\n\n\n\n\n\n\n\n\nruleCountOutput Port\n\n\nEmits details of rule name and the number of times that the rule is applied.\n\n\n\n\n\n\n\n\nfiredRuleAndTransactionOutput Port\n\n\nEmits rules along with processed data matching a specific rule.\n\n\n\n\n\n\n\n\nfactAndFiredRulesOutput\n\n\nEmits processed data along with associated rules.\n\n\n\n\n\n\n\n\nPartitioning\n\n\nThe Drools operator can be partitioned using default partitioner provided by Apex. In case of stateful rules, you must ensure that during partitioning the associated data goes to the single partition, that is, if all the rules are for a customer, the partitioning should be done on the customer field in the record.\n\n\nDrools operator can be dynamically partitioned when rules are stateless. For stateful rules dynamic partitioning is not supported.\n\n\nFor partitioning, you must add the following property in the \nproperties.xml\n file. For example, if you add Drools operator with the name \nRuleExecutor\n in the DAG, then this property creates four partitions of the operator when the application starts.\n\n\nproperty\n\n     \nname\ndt.operator.RuleExecutor.attr.PARTITIONER\n/name\n\n     \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:4\n/value\n\n\n/property\n\n\n\n\n\nConfiguring Drools Operator\n\n\nThe following settings can be configured for Drools operator:\n\n\n\n\nConfiguring rules\n\n\nConfiguring Replay\n\n\nSetting the expiration of events\n\n\n\n\nConfiguring Rules\n\n\nFor the Drools operator, you can configure the rules using one of the following methods:\n\n\n\n\nHDFS Direct Deploy\n\n\nCEP Workbench \n Configuration Artifacts\n\n\n\n\nHDFS Direct Deploy\n\n\nTo configure rules from HDFS, do the following:\n\n\n\n\nWrite the rules in the one the format that is supported by Drools.\n\n\nAdd this rules file to a folder in HDFS.\n\n\nIn an application configuration, set the folder path using the following operator property:\n\n\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndt.operator.FraudRulesExecutor.prop.rulesDir\n\n\nThe path to HDFS from where to load the rules. If this path is set to null, then the operator loads the rules from the classpath.\n\n\n\n\n\n\n\n\nCEP Workbench \n JAR Artifacts\n\n\nRefer to \nCEP Workbench\n and \nJAR Artifacts\n for more details. \n\n\nSetting the Expiration Limit for Events\n\n\nIf rules are loaded from HDFS, Drools operator configures the session in streaming mode. In this mode the rule file declares the input data type as event, using annotations.\n\n\nEvery item that is added to the Drools operator is kept in the memory. Hence, you must set the expiration timeout of the events. The Drools operator, automatically removes those item from the Drools working memory.\n\n\nFor example, if you want to evaluate the rules on \nUserEventType\n objects, you must add the following code on top of the rules file \n(.drl file)\n:\n\n\ndeclare UserEventType\n  @role(event)\n  @expires(60m)\nend\n\n\n\n\nNote:\n For more details refer to the \nDrools documentation\n.\n\n\nSample Application\n\n\nThe following code illustrates how Drools Operator can be used within an application. The application reads JSON data from Kafka, parses it to Java object, and sends it to Drools Operator for evaluation of rules. The result of rule evaluation is then sent to another Kafka topic, which can be consumed by other applications.\n\n\npublic class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG, Configuration configuration)\n  {\n    KafkaSinglePortInputOperator kafkaInputOperator = dag.addOperator(\nInput\n, new KafkaSinglePortInputOperator());\n    JsonParser parser = dag.addOperator(\nTransactionParser\n, JsonParser.class);`\n\n    dag.setInputPortAttribute(parser.in, Context.PortContext.PARTITION_PARALLEL, true);\n    DroolsOperator ruleExecutor = dag.addOperator(\nRuleExecutor\n, new DroolsOperator());\n    KafkaSinglePortOutputOperator output = dag.addOperator(\nOutput\n, new KafkaSinglePortOutputOperator());\n\n    dag.addStream(\ninput\n, kafkaInputOperator.outputPort, parser.in);\n    dag.addStream(\ndata\n, parser.out, ruleExecutor.factsInput);\n    dag.addStream(\nprocessed\n, ruleExecutor.factsOutput, output.inputPort);\n  }\n\n}\n\n\n\n\nPerformance Benchmarking\n\n\nAs the Drools operator is configured in streaming mode, it keeps the data in memory till the configured expiration in the rule file. Memory required by operator depends on the following parameters:\n\n\n\n\nOperator input rate\n\n\nData tuple size in memory\n\n\nExpiration duration.\n\n\n\n\nDrools operator was tested with 1KB tuple size and various expiration rate, input rate, and memory configuration.\n\n\nThe result is shown as a graph where \nY-axis\n is the \nexpiration interval\n and \nX-axis\n is the \ninput rate\n.\n\n\n\nIf you want to process data at a specific input rate and expiration interval, you can choose a point on the plot with configured input rate and expiration interval and choose that line of memory configuration which is above that point.\n\n\nFor example, if user wants to process 240 tuples per second with 4000 seconds expiry, then container size should be 12 GB.", 
            "title": "Drools Operator"
        }, 
        {
            "location": "/operators/drools_operator/#about-drools-operator", 
            "text": "Drools operator provides real-time complex event processing capability with the help of Drools engine.  The Drools operator embeds the Drools rule engine to simplify the rules configuration for a business apps such as the Fraud Prevention App.  You can write rules as per your business requirements and the execution aspect is handled by the Drools operator.  The Drools operator provides the following advantages:   Facility to write rules in a rule file instead of programming them. This makes it easy for a domain expert (non-technical person) to configure rules.  Centralization of knowledge-base in the form of rules. This results in easy maintenance.  Pre-defined algorithms in Drools engine provide efficient ways to match rule patterns with data.  Easily modify the existing rules instead of modifying a program.  Supports rule configuration in various file formats such as  .drl  ,  .xls  ,  .dsl  etc.   This operator is available under  DT Premium  license.", 
            "title": "About Drools Operator"
        }, 
        {
            "location": "/operators/drools_operator/#testing", 
            "text": "The drools operator is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.7 and above  Drools embedded version 6.5.0-final", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/drools_operator/#workflow-of-the-operator", 
            "text": "Write and save the rules in the drools format.  Configure the location of this rule s file in the drools operator.  Drools operator receives enriched data from upstream operators which is validated against the specified rules.  Drools operator emits processed data with fields changed by applied rules data, details of rule name, and the number of times that the rule is applied.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/drools_operator/#ports", 
            "text": "The following ports are available on the Drools operator:     Port Type  Port Name  Details      Input Port  factsInput Port  Receives incoming tuples from upstream operators.    Output Port  factsOutput Port  Emits processed data with fields changed by applied rules.     ruleCountOutput Port  Emits details of rule name and the number of times that the rule is applied.     firedRuleAndTransactionOutput Port  Emits rules along with processed data matching a specific rule.     factAndFiredRulesOutput  Emits processed data along with associated rules.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/drools_operator/#partitioning", 
            "text": "The Drools operator can be partitioned using default partitioner provided by Apex. In case of stateful rules, you must ensure that during partitioning the associated data goes to the single partition, that is, if all the rules are for a customer, the partitioning should be done on the customer field in the record.  Drools operator can be dynamically partitioned when rules are stateless. For stateful rules dynamic partitioning is not supported.  For partitioning, you must add the following property in the  properties.xml  file. For example, if you add Drools operator with the name  RuleExecutor  in the DAG, then this property creates four partitions of the operator when the application starts.  property \n      name dt.operator.RuleExecutor.attr.PARTITIONER /name \n      value com.datatorrent.common.partitioner.StatelessPartitioner:4 /value  /property", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/drools_operator/#configuring-drools-operator", 
            "text": "The following settings can be configured for Drools operator:   Configuring rules  Configuring Replay  Setting the expiration of events", 
            "title": "Configuring Drools Operator"
        }, 
        {
            "location": "/operators/drools_operator/#configuring-rules", 
            "text": "For the Drools operator, you can configure the rules using one of the following methods:   HDFS Direct Deploy  CEP Workbench   Configuration Artifacts", 
            "title": "Configuring Rules"
        }, 
        {
            "location": "/operators/drools_operator/#hdfs-direct-deploy", 
            "text": "To configure rules from HDFS, do the following:   Write the rules in the one the format that is supported by Drools.  Add this rules file to a folder in HDFS.  In an application configuration, set the folder path using the following operator property:      Property Name  Description      dt.operator.FraudRulesExecutor.prop.rulesDir  The path to HDFS from where to load the rules. If this path is set to null, then the operator loads the rules from the classpath.", 
            "title": "HDFS Direct Deploy"
        }, 
        {
            "location": "/operators/drools_operator/#cep-workbench-jar-artifacts", 
            "text": "Refer to  CEP Workbench  and  JAR Artifacts  for more details.", 
            "title": "CEP Workbench &amp; JAR Artifacts"
        }, 
        {
            "location": "/operators/drools_operator/#setting-the-expiration-limit-for-events", 
            "text": "If rules are loaded from HDFS, Drools operator configures the session in streaming mode. In this mode the rule file declares the input data type as event, using annotations.  Every item that is added to the Drools operator is kept in the memory. Hence, you must set the expiration timeout of the events. The Drools operator, automatically removes those item from the Drools working memory.  For example, if you want to evaluate the rules on  UserEventType  objects, you must add the following code on top of the rules file  (.drl file) :  declare UserEventType\n  @role(event)\n  @expires(60m)\nend  Note:  For more details refer to the  Drools documentation .", 
            "title": "Setting the Expiration Limit for Events"
        }, 
        {
            "location": "/operators/drools_operator/#sample-application", 
            "text": "The following code illustrates how Drools Operator can be used within an application. The application reads JSON data from Kafka, parses it to Java object, and sends it to Drools Operator for evaluation of rules. The result of rule evaluation is then sent to another Kafka topic, which can be consumed by other applications.  public class Application implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG, Configuration configuration)\n  {\n    KafkaSinglePortInputOperator kafkaInputOperator = dag.addOperator( Input , new KafkaSinglePortInputOperator());\n    JsonParser parser = dag.addOperator( TransactionParser , JsonParser.class);`\n\n    dag.setInputPortAttribute(parser.in, Context.PortContext.PARTITION_PARALLEL, true);\n    DroolsOperator ruleExecutor = dag.addOperator( RuleExecutor , new DroolsOperator());\n    KafkaSinglePortOutputOperator output = dag.addOperator( Output , new KafkaSinglePortOutputOperator());\n\n    dag.addStream( input , kafkaInputOperator.outputPort, parser.in);\n    dag.addStream( data , parser.out, ruleExecutor.factsInput);\n    dag.addStream( processed , ruleExecutor.factsOutput, output.inputPort);\n  }\n\n}", 
            "title": "Sample Application"
        }, 
        {
            "location": "/operators/drools_operator/#performance-benchmarking", 
            "text": "As the Drools operator is configured in streaming mode, it keeps the data in memory till the configured expiration in the rule file. Memory required by operator depends on the following parameters:   Operator input rate  Data tuple size in memory  Expiration duration.   Drools operator was tested with 1KB tuple size and various expiration rate, input rate, and memory configuration.  The result is shown as a graph where  Y-axis  is the  expiration interval  and  X-axis  is the  input rate .  If you want to process data at a specific input rate and expiration interval, you can choose a point on the plot with configured input rate and expiration interval and choose that line of memory configuration which is above that point.  For example, if user wants to process 240 tuples per second with 4000 seconds expiry, then container size should be 12 GB.", 
            "title": "Performance Benchmarking"
        }, 
        {
            "location": "/operators/python_operator/", 
            "text": "About the Operator\n\n\nPython Operator support provides capability for Python developers to write DT Operators and use these operators in DT application. It ensures that developer can focus more on the actual business logic, just like the Java DT operators, and the execution aspect is handled by the framework with minimal configurations to the project.\n\n\nThis implementation can be used under DT Plus license.\n\n\nSupport includes tasks such as the follows:\n\n\n\n\nGeneration of Java operator that is to be used in DT application.\n\n\nDependency collection\n\n\nTransport and installation on Hadoop nodes\n\n\nFault-tolerance support for Python operator etc.\n\n\n\n\nTesting\n\n\nPython operator is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.7 and above\n\n\nPython 2.7\n\n\n\n\nRequirements \n\n\nThe following must be pre-installed before running the operator:\n\n\n\n\n\n\ndt-operators-python\n\n\nTo install dt-operators-python, do the following:\n\n\n\n\n\n\nGo to \n/saarang/plugins\nand run mvn install to create maven plugin required by Python implementation.\n\n\n\n\n\n\nGo to \nsaarang/operators/python\n and run mvn install to install dt-operators-python and dt-operators-python jar file to .m2 directory.\n\n\n\n\n\n\nPython 2.7\n\n\n\n\n\n\nWorkflow of the Operator\n\n\nThe following image depicts the workflow of the operator:\n\n\n\n\n\nCompile Python Operator Project\n\n\nAdd dependency on actual application project.\n\n\nAdd py-apa-package as goal to application project.\n\n\nCompile Project.\n\n\n\n\nThe following image depicts the steps in the operator execution:\n\n\n\nSupplementary Classes\n\n\nPythonAsyncStorageAgent\n\n\nPython Operator support also provides fault-tolerance as any other DT operator but as of now Python application developer will have to explicitly provide STORAGE_AGENT as DAG attribute. Please refer to the application example from Using Operator Section.\n\n\nPartitioning\n\n\nThe Python operator can be partitioned using the default partitioner provided by Apex.\n\n\nThis operator can be dynamically partitioned, when rules are stateless.\n\n\nFor partitioning, you must add the following property in the \nproperties.xml\n file. For example, if you add Python operator with the name \nPythonOperator\n in the DAG, then this property creates four partitions of the operator when the application starts.\n\n\nproperty\n\n     \nname\ndt.operator.PythonOperator.attr.PARTITIONER\n/name\n\n     \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/name\n\n\n/property\n\n\n\n\n\nUsing the Operator\n\n\nYou can create a directory named   \npython\n under \nsrc/main\n at same level as Java in any DT application maven project directory.\n\n\nIf you want to include any \nrequirements.txt\n for python dependencies, add it at the top level in the project directory so maven plugins can find those and add to final *.apa file.\n\n\nThe following code illustrates how the Python operator can be written for DT application:\n\n\nfrom dtbase.operator.Operator import *\nimport types\n@Properties(var1=types.IntType, var2=types.StringType, var3=types.FloatType)\n@OutputPort(\noutPort\n)\n@OutputPort(\noutPort1\n)\nclass TestOperator(BaseOperator):\n\n   def __init__(self):\n       self.counter=0\n\n   @InputPort('inPort')\n   def process1(self, tuple):\n       if tuple['amount'] \n= 200000:\n           tuple['fraud'] = True\n           self.counter=self.counter+1\n       self.outPort.emit(tuple)\n       print \nCurrent Counter\n, self.counter\n\n   @InputPort('inPort1')\n   def process2(self, tuple):\n       print 'Do Nothing'\n       print \nCurrent Counter\n, self.counter\n\n   def setup(self, context):\n       print \nsetup\n,context\n\n   def activate(self, context):\n       print \nactivate\n,context\n\n   def deactivate(self):\n       print \ndeactivate\n\n\n   def teardown(self):\n       print \nteardown\n\n\n   def beforeCheckpoint(self, windowId):\n       print \nOperator is checkpointing \n,windowId\n\n   def checkpointed(self, windowId):\n       print \nOperator is checkpointed \n,windowId\n\n   def committed(self, windowId):\n       print \nOperator is committed\n,windowId\n\n   def beginWindow(self, windowId):\n       print \nBegin Window\n,windowId\n\n   def endWindow(self):\n       print \nEnd Window\n\n\n\n\n\nThe above python operator examples after compilation generate equivalent Java operator with the same name TestOperator and input ports [inPort,inPort1] and output ports [outPort, outPort1].\n\n\nThese generated Java operators can be used in the DT applications to create the hybrid DAG.\n\n\nApplication Example\n\n\nThe following code illustrates  how the generated Java operator can be used in a DAG:\n\n\n@ApplicationAnnotation(name=\nPythonOperatorExample\n)\npublic class Application implements StreamingApplication\n{\n @Override\n public void populateDAG(DAG dag, Configuration conf)\n {\n   // Sample DAG with 2 operators\n   // Replace this code with the DAG you want to build\n   RandomTransactionGenerator randomGenerator = dag.addOperator(\nrandomGenerator\n, RandomTransactionGenerator.class);\n   randomGenerator.setNumTuples(500);\n   TestOperator pyOp = dag.addOperator(\ntestop\n,TestOperator.class);\n   ConsoleOutputOperator cons = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\n   dag.addStream(\nrandomData\n, randomGenerator.out, pyOp.inPort);\n   dag.addStream(\noutput\n, pyOp.outPort, cons.input);\n   dag.setAttribute(Context.DAGContext.HEARTBEAT_TIMEOUT_MILLIS,100000);\n   dag.setAttribute(Context.OperatorContext.STORAGE_AGENT, new PythonAsyncStorageAgent(\n.\n,conf));\n }\n}\n\n\n\n\nCompiling DT Application with Python Implementation\n\n\nTo compile DT application with Python, you must first add dt-operators-python as dependency to \npom.xml\n. Refer \nRequirements\n.\n\n\ndependency\n\n \ngroupId\ncom.datatorrent.operators\n/groupId\n\n \nartifactId\ndt-operators-python\n/artifactId\n\n \nversion\n1.4.0-SNAPSHOT\n/version\n\n\n/dependency\n\n\n\n\n\nLater, you must add the following plugin to \npom.xml\n for packaging the python code and required dependencies in *.apa.\n\n\nplugin\n\n \ngroupId\ncom.datatorrent.plugins\n/groupId\n\n \nartifactId\ndt-plugins-maven-python\n/artifactId\n\n \nversion\n1.4.0-SNAPSHOT\n/version\n\n \nexecutions\n\n   \nexecution\n\n     \nid\npyGenerate\n/id\n\n     \ngoals\n\n       \ngoal\npydependency\n/goal\n\n       \ngoal\ntransform\n/goal\n\n     \n/goals\n\n   \n/execution\n\n   \nexecution\n\n     \nid\npyCompile\n/id\n\n     \ngoals\n\n       \ngoal\nwhl-compile\n/goal\n\n     \n/goals\n\n   \n/execution\n\n   \nexecution\n\n     \nid\npyInstall\n/id\n\n     \ngoals\n\n       \ngoal\nwhl-install\n/goal\n\n     \n/goals\n\n   \n/execution\n\n   \nexecution\n\n     \nid\nbuild-python-apa\n/id\n\n     \n!--\nphase\nprepare-package\n/phase\n--\n\n     \ngoals\n\n       \ngoal\npy-apa-package\n/goal\n\n     \n/goals\n\n     \nconfiguration\n\n\n     \n/configuration\n\n   \n/execution\n\n \n/executions\n\n\n/plugin", 
            "title": "Python Operator"
        }, 
        {
            "location": "/operators/python_operator/#about-the-operator", 
            "text": "Python Operator support provides capability for Python developers to write DT Operators and use these operators in DT application. It ensures that developer can focus more on the actual business logic, just like the Java DT operators, and the execution aspect is handled by the framework with minimal configurations to the project.  This implementation can be used under DT Plus license.  Support includes tasks such as the follows:   Generation of Java operator that is to be used in DT application.  Dependency collection  Transport and installation on Hadoop nodes  Fault-tolerance support for Python operator etc.", 
            "title": "About the Operator"
        }, 
        {
            "location": "/operators/python_operator/#testing", 
            "text": "Python operator is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.7 and above  Python 2.7", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/python_operator/#requirements", 
            "text": "The following must be pre-installed before running the operator:    dt-operators-python  To install dt-operators-python, do the following:    Go to  /saarang/plugins and run mvn install to create maven plugin required by Python implementation.    Go to  saarang/operators/python  and run mvn install to install dt-operators-python and dt-operators-python jar file to .m2 directory.    Python 2.7", 
            "title": "Requirements "
        }, 
        {
            "location": "/operators/python_operator/#workflow-of-the-operator", 
            "text": "The following image depicts the workflow of the operator:   Compile Python Operator Project  Add dependency on actual application project.  Add py-apa-package as goal to application project.  Compile Project.   The following image depicts the steps in the operator execution:", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/python_operator/#supplementary-classes", 
            "text": "", 
            "title": "Supplementary Classes"
        }, 
        {
            "location": "/operators/python_operator/#pythonasyncstorageagent", 
            "text": "Python Operator support also provides fault-tolerance as any other DT operator but as of now Python application developer will have to explicitly provide STORAGE_AGENT as DAG attribute. Please refer to the application example from Using Operator Section.", 
            "title": "PythonAsyncStorageAgent"
        }, 
        {
            "location": "/operators/python_operator/#partitioning", 
            "text": "The Python operator can be partitioned using the default partitioner provided by Apex.  This operator can be dynamically partitioned, when rules are stateless.  For partitioning, you must add the following property in the  properties.xml  file. For example, if you add Python operator with the name  PythonOperator  in the DAG, then this property creates four partitions of the operator when the application starts.  property \n      name dt.operator.PythonOperator.attr.PARTITIONER /name \n      value com.datatorrent.common.partitioner.StatelessPartitioner:2 /name  /property", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/python_operator/#using-the-operator", 
            "text": "You can create a directory named    python  under  src/main  at same level as Java in any DT application maven project directory.  If you want to include any  requirements.txt  for python dependencies, add it at the top level in the project directory so maven plugins can find those and add to final *.apa file.  The following code illustrates how the Python operator can be written for DT application:  from dtbase.operator.Operator import *\nimport types\n@Properties(var1=types.IntType, var2=types.StringType, var3=types.FloatType)\n@OutputPort( outPort )\n@OutputPort( outPort1 )\nclass TestOperator(BaseOperator):\n\n   def __init__(self):\n       self.counter=0\n\n   @InputPort('inPort')\n   def process1(self, tuple):\n       if tuple['amount']  = 200000:\n           tuple['fraud'] = True\n           self.counter=self.counter+1\n       self.outPort.emit(tuple)\n       print  Current Counter , self.counter\n\n   @InputPort('inPort1')\n   def process2(self, tuple):\n       print 'Do Nothing'\n       print  Current Counter , self.counter\n\n   def setup(self, context):\n       print  setup ,context\n\n   def activate(self, context):\n       print  activate ,context\n\n   def deactivate(self):\n       print  deactivate \n\n   def teardown(self):\n       print  teardown \n\n   def beforeCheckpoint(self, windowId):\n       print  Operator is checkpointing  ,windowId\n\n   def checkpointed(self, windowId):\n       print  Operator is checkpointed  ,windowId\n\n   def committed(self, windowId):\n       print  Operator is committed ,windowId\n\n   def beginWindow(self, windowId):\n       print  Begin Window ,windowId\n\n   def endWindow(self):\n       print  End Window   The above python operator examples after compilation generate equivalent Java operator with the same name TestOperator and input ports [inPort,inPort1] and output ports [outPort, outPort1].  These generated Java operators can be used in the DT applications to create the hybrid DAG.", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/python_operator/#application-example", 
            "text": "The following code illustrates  how the generated Java operator can be used in a DAG:  @ApplicationAnnotation(name= PythonOperatorExample )\npublic class Application implements StreamingApplication\n{\n @Override\n public void populateDAG(DAG dag, Configuration conf)\n {\n   // Sample DAG with 2 operators\n   // Replace this code with the DAG you want to build\n   RandomTransactionGenerator randomGenerator = dag.addOperator( randomGenerator , RandomTransactionGenerator.class);\n   randomGenerator.setNumTuples(500);\n   TestOperator pyOp = dag.addOperator( testop ,TestOperator.class);\n   ConsoleOutputOperator cons = dag.addOperator( console , new ConsoleOutputOperator());\n   dag.addStream( randomData , randomGenerator.out, pyOp.inPort);\n   dag.addStream( output , pyOp.outPort, cons.input);\n   dag.setAttribute(Context.DAGContext.HEARTBEAT_TIMEOUT_MILLIS,100000);\n   dag.setAttribute(Context.OperatorContext.STORAGE_AGENT, new PythonAsyncStorageAgent( . ,conf));\n }\n}", 
            "title": "Application Example"
        }, 
        {
            "location": "/operators/python_operator/#compiling-dt-application-with-python-implementation", 
            "text": "To compile DT application with Python, you must first add dt-operators-python as dependency to  pom.xml . Refer  Requirements .  dependency \n  groupId com.datatorrent.operators /groupId \n  artifactId dt-operators-python /artifactId \n  version 1.4.0-SNAPSHOT /version  /dependency   Later, you must add the following plugin to  pom.xml  for packaging the python code and required dependencies in *.apa.  plugin \n  groupId com.datatorrent.plugins /groupId \n  artifactId dt-plugins-maven-python /artifactId \n  version 1.4.0-SNAPSHOT /version \n  executions \n    execution \n      id pyGenerate /id \n      goals \n        goal pydependency /goal \n        goal transform /goal \n      /goals \n    /execution \n    execution \n      id pyCompile /id \n      goals \n        goal whl-compile /goal \n      /goals \n    /execution \n    execution \n      id pyInstall /id \n      goals \n        goal whl-install /goal \n      /goals \n    /execution \n    execution \n      id build-python-apa /id \n      !-- phase prepare-package /phase -- \n      goals \n        goal py-apa-package /goal \n      /goals \n      configuration \n\n      /configuration \n    /execution \n  /executions  /plugin", 
            "title": "Compiling DT Application with Python Implementation"
        }, 
        {
            "location": "/operators/PMML_operator/", 
            "text": "About the PMML Operator\n\n\nPMML\n stands for Predictive Model Markup Language and is an XML-based model interchange format for predictive machine learning models. PMML provides a way for analytic applications to describe and exchange predictive models produced by data mining and machine learning algorithms.\n\n\nThe PMML operator allows persisting a model into an XML format that is given by the PMML standard.\n\n\nDataTorrent RTS provides scoring operator which can be used to score or  predict incoming data using a PMML model.\n\n\nThe only input that the operator takes is the PMML file. The operator identifies the algorithm, that is used to train the model, by processing the file and instantiates an appropriate scorer which understands the PMML format for that algorithm.\n\n\nFor example, the following image depicts how the PMML operator process the scoring data to predict about an admission to a graduate school.\n\n\n\n\nThe PMML operator is available under \nDT Premium\n license.\n\n\nTools Supporting Export of Models into PMML\n\n\nThe following tools support export of models into PMML:\n\n\n\n\nR\n\n\nApache Spark\n\n\nScikit-Learn\n\n\nBig ML\n\n\nZementis\n\n\nWeka\n\n\nKnime\n\n\nIBM\n\n\nLot more\u2026 \nhttp://dmg.org/pmml/products.html\n\n\n\n\nPorts\n\n\nThe following ports are available for the PMML operator:\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nInput port\n\n\ninput\n\n\nReceives incoming tuples from upstream operators\n\n\n\n\n\n\nOutput port\n\n\noutput\n\n\nEmits processed data in the form of clusters/classes \n distance of the tuple with them\n\n\n\n\n\n\nError port\n\n\nerror\n\n\nEmits errors if not able to process incoming tuple\n\n\n\n\n\n\n\n\nPartitioning\n\n\nPMML operator implementation supports static and parallel partitioning.\n\n\nSupported Algorithms\n\n\nMachine learning algorithms that are supported as part of release 3.10.0 are:\n\n\n\n\nNaive Bayes classification\n\n\nSupport Vector Machine\n\n\nK-Means Clustering\n\n\n\n\nTesting\n\n\nThe PMML Operator is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.7 and above\n\n\nPMML version 4.3\n\n\n\n\nWorkflow of the Operator\n\n\n\nThe PMML operator is preceded by a pre-processing operator, which adapts incoming tuple in the format that suits PMML standard.\n\n\nConfiguring for PMML Operator\n\n\nFor running the PMML operator, the following properties must be set in the \nproperties.xml\n file:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npmmlFilePath\n\n\nPath of XML file on HDFS\n\n\n\n\n\n\nmodelIndexInPmml\n\n\nIndex (number) of the model to be used from XML file.\n\n\n\n\n\n\n\n\nUsing the Operator\n\n\nFollowing is a sample code to use the PMML operator:\n\n\npackage com.datatorrent.Classification;\nimport org.apache.apex.malhar.lib.fs.GenericFileOutputOperator;\nimport org.apache.hadoop.conf.Configuration;\n\nimport com.datatorrent.api.annotation.ApplicationAnnotation;\nimport com.datatorrent.api.StreamingApplication;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.DAG.Locality;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.io.fs.AbstractFileOutputOperator;\nimport com.datatorrent.pmml.operator.ClassificationScoringOperator;\nimport com.datatorrent.pmml.scorer.ClassificationScorer;\n\n@ApplicationAnnotation(name = \nPMML-Classification-Scoring-App\n)\npublic class Application implements StreamingApplication\n{\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n\n    ClassificationInput inputOp = dag.addOperator(\ninputOp\n, ClassificationInput.class);\n    inputOp.setEmitBatchSize(1);\n    ClassificationScoringOperator scoring = dag.addOperator(\nclassificationOperator\n, ClassificationScoringOperator.class);\n    ScoringOutputOperator logger = dag.addOperator(\nLogger\n, ScoringOutputOperator.class);\n\n    dag.addStream(\ndata to scoring\n, inputOp.scoringOut, scoring.input);\n    dag.addStream(\nscoring to output\n, scoring.output, logger.input);\n  }\n}", 
            "title": "PMML Operator"
        }, 
        {
            "location": "/operators/PMML_operator/#about-the-pmml-operator", 
            "text": "PMML  stands for Predictive Model Markup Language and is an XML-based model interchange format for predictive machine learning models. PMML provides a way for analytic applications to describe and exchange predictive models produced by data mining and machine learning algorithms.  The PMML operator allows persisting a model into an XML format that is given by the PMML standard.  DataTorrent RTS provides scoring operator which can be used to score or  predict incoming data using a PMML model.  The only input that the operator takes is the PMML file. The operator identifies the algorithm, that is used to train the model, by processing the file and instantiates an appropriate scorer which understands the PMML format for that algorithm.  For example, the following image depicts how the PMML operator process the scoring data to predict about an admission to a graduate school.   The PMML operator is available under  DT Premium  license.", 
            "title": "About the PMML Operator"
        }, 
        {
            "location": "/operators/PMML_operator/#tools-supporting-export-of-models-into-pmml", 
            "text": "The following tools support export of models into PMML:   R  Apache Spark  Scikit-Learn  Big ML  Zementis  Weka  Knime  IBM  Lot more\u2026  http://dmg.org/pmml/products.html", 
            "title": "Tools Supporting Export of Models into PMML"
        }, 
        {
            "location": "/operators/PMML_operator/#ports", 
            "text": "The following ports are available for the PMML operator:     Port Type  Port Name  Details      Input port  input  Receives incoming tuples from upstream operators    Output port  output  Emits processed data in the form of clusters/classes   distance of the tuple with them    Error port  error  Emits errors if not able to process incoming tuple", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/PMML_operator/#partitioning", 
            "text": "PMML operator implementation supports static and parallel partitioning.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/PMML_operator/#supported-algorithms", 
            "text": "Machine learning algorithms that are supported as part of release 3.10.0 are:   Naive Bayes classification  Support Vector Machine  K-Means Clustering", 
            "title": "Supported Algorithms"
        }, 
        {
            "location": "/operators/PMML_operator/#testing", 
            "text": "The PMML Operator is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.7 and above  PMML version 4.3", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/PMML_operator/#workflow-of-the-operator", 
            "text": "The PMML operator is preceded by a pre-processing operator, which adapts incoming tuple in the format that suits PMML standard.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/PMML_operator/#configuring-for-pmml-operator", 
            "text": "For running the PMML operator, the following properties must be set in the  properties.xml  file:     Property Name  Description      pmmlFilePath  Path of XML file on HDFS    modelIndexInPmml  Index (number) of the model to be used from XML file.", 
            "title": "Configuring for PMML Operator"
        }, 
        {
            "location": "/operators/PMML_operator/#using-the-operator", 
            "text": "Following is a sample code to use the PMML operator:  package com.datatorrent.Classification;\nimport org.apache.apex.malhar.lib.fs.GenericFileOutputOperator;\nimport org.apache.hadoop.conf.Configuration;\n\nimport com.datatorrent.api.annotation.ApplicationAnnotation;\nimport com.datatorrent.api.StreamingApplication;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.DAG.Locality;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.io.fs.AbstractFileOutputOperator;\nimport com.datatorrent.pmml.operator.ClassificationScoringOperator;\nimport com.datatorrent.pmml.scorer.ClassificationScorer;\n\n@ApplicationAnnotation(name =  PMML-Classification-Scoring-App )\npublic class Application implements StreamingApplication\n{\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n\n    ClassificationInput inputOp = dag.addOperator( inputOp , ClassificationInput.class);\n    inputOp.setEmitBatchSize(1);\n    ClassificationScoringOperator scoring = dag.addOperator( classificationOperator , ClassificationScoringOperator.class);\n    ScoringOutputOperator logger = dag.addOperator( Logger , ScoringOutputOperator.class);\n\n    dag.addStream( data to scoring , inputOp.scoringOut, scoring.input);\n    dag.addStream( scoring to output , scoring.output, logger.input);\n  }\n}", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/tcpinputoperator/", 
            "text": "About TCP Input Operator\n\n\nTCP Input Operator provides a method to let the operator accept concurrent TCP connections. It also allows to parse the data that is received using more than one delimiters.\n\n\nThis operator can be used for accepting data directly from various IOT devices, any other TCP clients such as agents running on multiple nodes etc.\n\n\nThis operator is available under \nDT Plus\n license.\n\n\nPorts\n\n\nThe following port is available on the TCP Input operator:\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nOutput Port\n\n\noutputPort\n\n\nEmits tuples received on TCP connections.\n\n\n\n\n\n\n\n\nTesting\n\n\nTCP Input operator is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.7 and above\n\n\nNetty\n\n\n\n\nConfigurations\n\n\nThe following properties must be updated in the \nproperties.xml\n file for this operator:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nqueueSize\n\n\nDefines the queue size to hold messages before emitting tuples\n\n\ndt.operator.[OPNAME].prop.queueSize\n\n\n\n\n\n\nmaxFrameLength\n\n\nSpecifies the maximum size of the decoded frames.\n\n\ndt.operator.[OPNAME].prop.maxFrameLength\n\n\n\n\n\n\nport\n\n\nDefines the listening TCP port that is used by the server.\n\n\ndt.operator.[OPNAME].prop.port\n\n\n\n\n\n\nthreadCount\n\n\nSpecifies the number of parallel threads.\n\n\ndt.operator.[OPNAME].prop.threadCount\n\n\n\n\n\n\n\n\nPartitioning\n\n\nThe TCP Input operator can be partitioned using default partitioner provided by Apex. TCP Input operator can be dynamically partitioned when rules are stateless.\n\n\nFor partitioning, you must add the following property in the \nproperties.xml\n file. For example, if you add \nTcpInputOperator\n with the name \nTcpInputOperatorEx\n in the DAG, then this property creates four partitions of the operator when the application starts.\n\n\nproperty\n\n     \nname\ndt.operator.TcpInputOperatorEx.attr.PARTITIONER\n/name\n\n     \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner(4)\n/name\n\n\n/property\n\n\n\n\n\nUsing the Operator\n\n\nTCP Input operator starts TCP server on the defined port . When the operator is used with multiple partitions, partitions cannot be deployed on the same node. Hence you must use it with the correct anti-affinity rule for TCP Input operator.\n\n\nThe following code illustrates, how this operator can be used in an application:\n\n\nTcpInputOperator tcpInput = dag.addOperator(\nTCPInput\n, TcpInputOperator.class);\nAffinityRule tcpRule = new AffinityRule(AffinityRule.Type.ANTI_AFFINITY, DAG.Locality.NODE_LOCAL, false, \nTCPInput\n, \nTCPInput\n);", 
            "title": "TCP Input Operator"
        }, 
        {
            "location": "/operators/tcpinputoperator/#about-tcp-input-operator", 
            "text": "TCP Input Operator provides a method to let the operator accept concurrent TCP connections. It also allows to parse the data that is received using more than one delimiters.  This operator can be used for accepting data directly from various IOT devices, any other TCP clients such as agents running on multiple nodes etc.  This operator is available under  DT Plus  license.", 
            "title": "About TCP Input Operator"
        }, 
        {
            "location": "/operators/tcpinputoperator/#ports", 
            "text": "The following port is available on the TCP Input operator:     Port Type  Port Name  Details      Output Port  outputPort  Emits tuples received on TCP connections.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/tcpinputoperator/#testing", 
            "text": "TCP Input operator is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.7 and above  Netty", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/tcpinputoperator/#configurations", 
            "text": "The following properties must be updated in the  properties.xml  file for this operator:     Property Name  Description  Example      queueSize  Defines the queue size to hold messages before emitting tuples  dt.operator.[OPNAME].prop.queueSize    maxFrameLength  Specifies the maximum size of the decoded frames.  dt.operator.[OPNAME].prop.maxFrameLength    port  Defines the listening TCP port that is used by the server.  dt.operator.[OPNAME].prop.port    threadCount  Specifies the number of parallel threads.  dt.operator.[OPNAME].prop.threadCount", 
            "title": "Configurations"
        }, 
        {
            "location": "/operators/tcpinputoperator/#partitioning", 
            "text": "The TCP Input operator can be partitioned using default partitioner provided by Apex. TCP Input operator can be dynamically partitioned when rules are stateless.  For partitioning, you must add the following property in the  properties.xml  file. For example, if you add  TcpInputOperator  with the name  TcpInputOperatorEx  in the DAG, then this property creates four partitions of the operator when the application starts.  property \n      name dt.operator.TcpInputOperatorEx.attr.PARTITIONER /name \n      value com.datatorrent.common.partitioner.StatelessPartitioner(4) /name  /property", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/tcpinputoperator/#using-the-operator", 
            "text": "TCP Input operator starts TCP server on the defined port . When the operator is used with multiple partitions, partitions cannot be deployed on the same node. Hence you must use it with the correct anti-affinity rule for TCP Input operator.  The following code illustrates, how this operator can be used in an application:  TcpInputOperator tcpInput = dag.addOperator( TCPInput , TcpInputOperator.class);\nAffinityRule tcpRule = new AffinityRule(AffinityRule.Type.ANTI_AFFINITY, DAG.Locality.NODE_LOCAL, false,  TCPInput ,  TCPInput );", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/eventhubinput/", 
            "text": "About Event Hub Input Operator\n\n\nEvent Hub is part of Microsoft Azure cloud ecosystem. It is a highly scalable data streaming platform and event ingestion service which can receive and process millions of events per second. Event Hubs can process and store events, data, or telemetry produced by distributed software and devices. Data sent to an Event Hub can be transformed and stored using any real-time analytics provider or batching/storage adapters. Using Event Hub, you can also publish-subscribe with low latency and at a massive scale.\n\n\nEvent Hub input operator receives events from Azure Event Hub in the form of raw byte arrays. It processes them and converts into byte array tuples. These tuples are then processed by downstream operators.\n\n\nThis operator is available under DT Plus license.\n\n\nPorts\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nOutput port\n\n\noutput\n\n\nEmits tuples received from Event Hub in byte array format\n\n\n\n\n\n\n\n\nTesting\n\n\nTested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.8\n\n\nHDInsight: Hadoop 2.7.3 - HDI 3.6\n\n\nEventhub client library - com.microsoft.azure:azure-eventhubs:0.14.0\n\n\n\n\nRequirements\n\n\nJava 1.8 and above\n\n\nWorkflow of the Operator\n\n\n\n\nSupported Data Types\n\n\nThis operator reads events from the Event Hub as raw byte array and emits them as byte array tuples.\n\n\nConfiguring Events Hub Input Operator\n\n\nThe following properties must be configured in the \nproperties.xml\n file:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmaxTuplesPerWindow\n\n\nMaximum tuples that are  allowed to be emitted in each window.\n\n\n\n\n\n\nstrategy\n\n\nPartitioning strategy for the operator: \nONE_TO_ONE\n: Each operator partition connects to one event hub partition, \nMANY_TO_ONE\n: Each operator consumes from several partitions.\n\n\n\n\n\n\npartitionManager\n\n\nPartition manager for the operator.\n\n\n\n\n\n\ninitialPartitionCount\n\n\nDecide initial partition count based on partition strategy.\n\n\n\n\n\n\ninitialOffset\n\n\ninitialOffset could be either earliest or latest. Earliest is  beginning of partition. Latest is the current record to consume from partition.\n\n\n\n\n\n\nnamespaceName\n\n\nEvent Hub name space.\n\n\n\n\n\n\neventHubName\n\n\nEvent Hub name.\n\n\n\n\n\n\nsasKeyName\n\n\nSAS key name for accessing Event Hub.\n\n\n\n\n\n\n.sasKey\n\n\nSAS key for accessing Event Hub.\n\n\n\n\n\n\nconsumerGroup\n\n\nConsumer group for accessing Event Hub.\n\n\n\n\n\n\nwindowDataManager\n\n\nwindowDataManager value. Default will be FSWindowDataManager.\n\n\n\n\n\n\n\n\nSupplementary Classes\n\n\n\n\nWindowDataManager:\n Manages the state of an operator in every application window. Application developers can specify custom implementation of \nWindowDataManager\n if it requires special handling.\n\n\nPartitionManager:\n Used to manage offset positions for individual partitions. For example, you can write \nPartitionManager\n to HDFS and reload it when restarting the application.  If required, Application developers can also specify custom implementation of \nPartitionManager\n. There is no default implementation for this.\n\n\n\n\nPartitioning\n\n\nThis operator supports the following types of partitioning strategies:\n\n\n\n\nONE_TO_ONE\n : If this is enabled, the AppMaster creates one input operator instance per Event Hub partition. So, the number of Event Hub partitions equals the number of operator instances.\n\n\nMANY_TO_ONE\n: The AppMaster creates \nK = min(initialPartitionCount, N\n)\nEvent Hub input operator instances where N is the number of Event Hub partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Event Hub partition.\nFor example, if initialPartitionCount = 5 and number of Event Hub partitions(N) = 2 then AppMaster creates 2 Event Hub input operator instances.\nDefault Value = ONE_TO_ONE\n\n\n\n\nUsing the Operator\n\n\nThe following code illustrates how Events Hub Input operator can be used within an application:\n\n\n@ApplicationAnnotation(name = \nevent-hub-to-hdfs\n)\npublic class Application implements StreamingApplication\n{\n\n public void populateDAG(DAG dag, Configuration conf)\n {\n   EventHubByteArrayInputOperator eventHubInput =\n       dag.addOperator(\neventHubInput\n, EventHubByteArrayInputOperator.class);\n\n   BytesFileOutputOperator fileOutput = dag.addOperator(\nfileOutput\n, BytesFileOutputOperator.class);\n\n   dag.addStream(\ndata\n, eventHubInput.outputPort, fileOutput.input);\n }\n\n\n\n\nPerformance Benchmarking\n\n\nFollowing are the numbers for \nMANY_TO_ONE\n 30:3 (30 partitions on eventhub and 3 partitions on app-template) on morado.\n\n\n\n\n\n\n\n\nTime Taken\n\n\nrecordslimit\n\n\nQueue Size\n\n\nProcessed/s\n\n\nTotal Messages\n\n\nMessage size\n\n\n\n\n\n\n\n\n\n\n29 min 30 sec\n\n\nDefault (100recordslimit)\n\n\nDefault (1024)\n\n\n994,1003,951,977\n\n\n5000000 (5M)\n\n\n4 KB\n\n\n\n\n\n\n29 min 25 sec\n\n\n500\n\n\n2048\n\n\n1003,998,982,990\n\n\n5000000 (5M)\n\n\n4 KB\n\n\n\n\n\n\n29 min 21 sec\n\n\n999\n\n\n3072\n\n\n999,989,1001,982\n\n\n5000000 (5M)\n\n\n4 KB\n\n\n\n\n\n\n\n\nInitial Offset - Earliest for all above.\n\n\nProcessed/s is the average of all three input operators taken at 4 different times.", 
            "title": "Event Hub Input Operator"
        }, 
        {
            "location": "/operators/eventhubinput/#about-event-hub-input-operator", 
            "text": "Event Hub is part of Microsoft Azure cloud ecosystem. It is a highly scalable data streaming platform and event ingestion service which can receive and process millions of events per second. Event Hubs can process and store events, data, or telemetry produced by distributed software and devices. Data sent to an Event Hub can be transformed and stored using any real-time analytics provider or batching/storage adapters. Using Event Hub, you can also publish-subscribe with low latency and at a massive scale.  Event Hub input operator receives events from Azure Event Hub in the form of raw byte arrays. It processes them and converts into byte array tuples. These tuples are then processed by downstream operators.  This operator is available under DT Plus license.", 
            "title": "About Event Hub Input Operator"
        }, 
        {
            "location": "/operators/eventhubinput/#ports", 
            "text": "Port Type  Port Name  Details      Output port  output  Emits tuples received from Event Hub in byte array format", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/eventhubinput/#testing", 
            "text": "Tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.8  HDInsight: Hadoop 2.7.3 - HDI 3.6  Eventhub client library - com.microsoft.azure:azure-eventhubs:0.14.0", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/eventhubinput/#requirements", 
            "text": "Java 1.8 and above", 
            "title": "Requirements"
        }, 
        {
            "location": "/operators/eventhubinput/#workflow-of-the-operator", 
            "text": "", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/eventhubinput/#supported-data-types", 
            "text": "This operator reads events from the Event Hub as raw byte array and emits them as byte array tuples.", 
            "title": "Supported Data Types"
        }, 
        {
            "location": "/operators/eventhubinput/#configuring-events-hub-input-operator", 
            "text": "The following properties must be configured in the  properties.xml  file:     Property  Description      maxTuplesPerWindow  Maximum tuples that are  allowed to be emitted in each window.    strategy  Partitioning strategy for the operator:  ONE_TO_ONE : Each operator partition connects to one event hub partition,  MANY_TO_ONE : Each operator consumes from several partitions.    partitionManager  Partition manager for the operator.    initialPartitionCount  Decide initial partition count based on partition strategy.    initialOffset  initialOffset could be either earliest or latest. Earliest is  beginning of partition. Latest is the current record to consume from partition.    namespaceName  Event Hub name space.    eventHubName  Event Hub name.    sasKeyName  SAS key name for accessing Event Hub.    .sasKey  SAS key for accessing Event Hub.    consumerGroup  Consumer group for accessing Event Hub.    windowDataManager  windowDataManager value. Default will be FSWindowDataManager.", 
            "title": "Configuring Events Hub Input Operator"
        }, 
        {
            "location": "/operators/eventhubinput/#supplementary-classes", 
            "text": "WindowDataManager:  Manages the state of an operator in every application window. Application developers can specify custom implementation of  WindowDataManager  if it requires special handling.  PartitionManager:  Used to manage offset positions for individual partitions. For example, you can write  PartitionManager  to HDFS and reload it when restarting the application.  If required, Application developers can also specify custom implementation of  PartitionManager . There is no default implementation for this.", 
            "title": "Supplementary Classes"
        }, 
        {
            "location": "/operators/eventhubinput/#partitioning", 
            "text": "This operator supports the following types of partitioning strategies:   ONE_TO_ONE  : If this is enabled, the AppMaster creates one input operator instance per Event Hub partition. So, the number of Event Hub partitions equals the number of operator instances.  MANY_TO_ONE : The AppMaster creates  K = min(initialPartitionCount, N )\nEvent Hub input operator instances where N is the number of Event Hub partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Event Hub partition.\nFor example, if initialPartitionCount = 5 and number of Event Hub partitions(N) = 2 then AppMaster creates 2 Event Hub input operator instances.\nDefault Value = ONE_TO_ONE", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/eventhubinput/#using-the-operator", 
            "text": "The following code illustrates how Events Hub Input operator can be used within an application:  @ApplicationAnnotation(name =  event-hub-to-hdfs )\npublic class Application implements StreamingApplication\n{\n\n public void populateDAG(DAG dag, Configuration conf)\n {\n   EventHubByteArrayInputOperator eventHubInput =\n       dag.addOperator( eventHubInput , EventHubByteArrayInputOperator.class);\n\n   BytesFileOutputOperator fileOutput = dag.addOperator( fileOutput , BytesFileOutputOperator.class);\n\n   dag.addStream( data , eventHubInput.outputPort, fileOutput.input);\n }", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/eventhubinput/#performance-benchmarking", 
            "text": "Following are the numbers for  MANY_TO_ONE  30:3 (30 partitions on eventhub and 3 partitions on app-template) on morado.     Time Taken  recordslimit  Queue Size  Processed/s  Total Messages  Message size      29 min 30 sec  Default (100recordslimit)  Default (1024)  994,1003,951,977  5000000 (5M)  4 KB    29 min 25 sec  500  2048  1003,998,982,990  5000000 (5M)  4 KB    29 min 21 sec  999  3072  999,989,1001,982  5000000 (5M)  4 KB     Initial Offset - Earliest for all above.  Processed/s is the average of all three input operators taken at 4 different times.", 
            "title": "Performance Benchmarking"
        }, 
        {
            "location": "/operators/eventhuboutput/", 
            "text": "About the Event Hub Output Operator\n\n\nEvent Hub is part of Microsoft Azure cloud ecosystem. It is a highly scalable data streaming platform and event ingestion service which can receive and process millions of events per second. Event Hubs can process, and store events, data, or telemetry produced by distributed software and devices. Data sent to an Event Hub can be transformed and stored using any real-time analytics provider or batching/storage adapters. Using Event Hub, you can also publish-subscribe with low latency and at a massive scale.\n\n\nEvent Hub Output operator receives events from upstream operator(s) in the form of byte array or string tuples and converts them into raw byte array format. These tuples are then emitted to Event Hub in Azure cloud.\n\n\nThis operator is available under DT Plus license.\n\n\nPorts\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nInput port\n\n\ninput\n\n\nReceives tuples from upstream operators in byte / string array format\n\n\n\n\n\n\n\n\nTesting\n\n\nTested with following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.8\n\n\nHDInsight: Hadoop 2.7.3 - HDI 3.6\n\n\nEventhub client library - com.microsoft.azure:azure-eventhubs:0.14.0\n\n\n\n\nRequirements\n\n\nJava 1.8 and above\n\n\nWorkflow of the Operator\n\n\nThe following figure depicts the workflow of this operator:\n\n\nSupported Data Types\n\n\n\n\nThis operator receives events from upstream operator in the form of byte/string array tuples, and emits them as raw byte array Event Hub in Azure cloud.\n\n\nIf there are any other custom data types, you can extend the class \nAbstractEventHubOutputOperator\n by providing implementation for \ngetRecord()\n method. This converts the incoming tuple into byte array.\n\n\n\n\nProperties and Attributes\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnamespaceName\n\n\nEvent Hub name space\n\n\n\n\n\n\neventHubName\n\n\nEvent Hub name\n\n\n\n\n\n\nsasKeyName\n\n\nSAS key name for accessing Event Hub\n\n\n\n\n\n\nsasKey\n\n\nSAS key for accessing Event Hub\n\n\n\n\n\n\nisBatchProcessing\n\n\nWhether events should be clubbed together to send to Event Hub in batch or to send individually\n\n\n\n\n\n\nbatchSize\n\n\nNumber of events to be clubbed together in a single send request. Default value = 100\n\n\n\n\n\n\n\n\nUsing the Operator\n\n\nThe following code illustrates how Events Hub output operator can be used within an application:\n\n\npublic class Application implements StreamingApplication\n{\n\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    KafkaSinglePortInputOperator kafkaInput =\n        dag.addOperator(\nkafkaInput\n, KafkaSinglePortInputOperator.class);\n\n    EventHubByteArrayOutputOperator eventHubOutput = dag.addOperator(\neventHubOutput\n, EventHubByteArrayOutputOperator.class);\n\n    dag.addStream(\ndata\n, kafkaInput.outputPort, eventHubOutput.inputPort);\n  }\n}", 
            "title": "Event Hub Output Operator"
        }, 
        {
            "location": "/operators/eventhuboutput/#about-the-event-hub-output-operator", 
            "text": "Event Hub is part of Microsoft Azure cloud ecosystem. It is a highly scalable data streaming platform and event ingestion service which can receive and process millions of events per second. Event Hubs can process, and store events, data, or telemetry produced by distributed software and devices. Data sent to an Event Hub can be transformed and stored using any real-time analytics provider or batching/storage adapters. Using Event Hub, you can also publish-subscribe with low latency and at a massive scale.  Event Hub Output operator receives events from upstream operator(s) in the form of byte array or string tuples and converts them into raw byte array format. These tuples are then emitted to Event Hub in Azure cloud.  This operator is available under DT Plus license.", 
            "title": "About the Event Hub Output Operator"
        }, 
        {
            "location": "/operators/eventhuboutput/#ports", 
            "text": "Port Type  Port Name  Details      Input port  input  Receives tuples from upstream operators in byte / string array format", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/eventhuboutput/#testing", 
            "text": "Tested with following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.8  HDInsight: Hadoop 2.7.3 - HDI 3.6  Eventhub client library - com.microsoft.azure:azure-eventhubs:0.14.0", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/eventhuboutput/#requirements", 
            "text": "Java 1.8 and above", 
            "title": "Requirements"
        }, 
        {
            "location": "/operators/eventhuboutput/#workflow-of-the-operator", 
            "text": "The following figure depicts the workflow of this operator:", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/eventhuboutput/#supported-data-types", 
            "text": "This operator receives events from upstream operator in the form of byte/string array tuples, and emits them as raw byte array Event Hub in Azure cloud.  If there are any other custom data types, you can extend the class  AbstractEventHubOutputOperator  by providing implementation for  getRecord()  method. This converts the incoming tuple into byte array.", 
            "title": "Supported Data Types"
        }, 
        {
            "location": "/operators/eventhuboutput/#properties-and-attributes", 
            "text": "Property  Description      namespaceName  Event Hub name space    eventHubName  Event Hub name    sasKeyName  SAS key name for accessing Event Hub    sasKey  SAS key for accessing Event Hub    isBatchProcessing  Whether events should be clubbed together to send to Event Hub in batch or to send individually    batchSize  Number of events to be clubbed together in a single send request. Default value = 100", 
            "title": "Properties and Attributes"
        }, 
        {
            "location": "/operators/eventhuboutput/#using-the-operator", 
            "text": "The following code illustrates how Events Hub output operator can be used within an application:  public class Application implements StreamingApplication\n{\n\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    KafkaSinglePortInputOperator kafkaInput =\n        dag.addOperator( kafkaInput , KafkaSinglePortInputOperator.class);\n\n    EventHubByteArrayOutputOperator eventHubOutput = dag.addOperator( eventHubOutput , EventHubByteArrayOutputOperator.class);\n\n    dag.addStream( data , kafkaInput.outputPort, eventHubOutput.inputPort);\n  }\n}", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/abstracthttpserver/", 
            "text": "About AbstractHttpServer Operator\n\n\nAbstractHttpServer operator helps developer to implement HTTP services and run them as part of operator.  It provides a way to host Jetty HTTP Servlet server as an . You can develop custom Servlets and plug-in those using these classes: ServletContextHolder and ServletHolder.\n\n\nThis operator can be used as an input or output operator based on how you can extend it.  This operator can be used to receive data from Mobile/ Web clients as well as from clients using WebSocket connections.\n\n\nTo use this operator, you must extend it and implement the \ngetHandlers\n method to define the collection of handlers to handle HTTP requests.\n\n\nThis operator is available under DT Plus license.\n\n\nPorts\n\n\nThe following port is available for the AbstractHttpServerOperator operator:\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nOutput Port\n\n\ndeployBroadcastOutport\n\n\nEmits information about the current partition including, hostname, services started or not, partition mask, partitionId.  If the partition has more than one partitionId assigned, then that many tuples are emitted.  All this information is emitted using POJO object of the type ServerDeploymentConfig.\n\n\n\n\n\n\n\n\nTesting\n\n\nAbstractHttpServer  is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.7 and above\n\n\nEmbedded Jetty version 9.2.11.v20150529.\n\n\n\n\nWorkflow of the Operator\n\n\nThe workflow of the \nAbstrractHttpServer\n  is as follows:\n\n\n\n\nAt initialization, the operator loads the handlers that you have provided.\n\n\nStarts webserver with the defined port.\n\n\nIf the operator is partitioned and the anti-affinity rule is set for the partitions of this operator, partitions of the operator gets deployed on different nodes. This is needed to ensure that two partitions do not share the same TCP port.\n\n\n\n\nSupplementary Classes\n\n\nThe following classes are used by the \nAbstrractHttpServer\n operator:\n\n\nServerDeploymentConfig\n\n\n\n\n\n\n\n\nProperty Name\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhostName\n\n\nString\n\n\nThis property can be used as the address URL and is specified in the following format by default: \nprotocol://[hostname]:[port]/.\n\n\n\n\n\n\nstarted\n\n\nboolean\n\n\nSpecifies if the server has started or shut down for the clients.\n\n\n\n\n\n\nmask\n\n\nint\n\n\nSpecifies the partition mask of the partition of the operator\n\n\n\n\n\n\npartitionId\n\n\nint\n\n\nSpecifies the partition ID for the current partition of the operator.\n\n\n\n\n\n\n\n\nHandlerCollection\n\n\nYou must implement the \ngetHandlers()\n method on this operator, which returns the \nHandlerCollection\n object \norg.eclipse.jetty.server.handler.HandlerCollection\n. The following example depicts how you implement the \ngethandlers\n method:\n\n\n@Override\npublic HandlerCollection getHandlers() throws Exception\n{\n HandlerCollection handlers = new HandlerCollection(true);\n this.logHandler = new RequestLogHandler();\n DummyRequestLog requestLog = new DummyRequestLog();\n requestLog.setExtended(true);\n requestLog.setLogLatency(true);\n requestLog.setPreferProxiedForAddress(true);\n logHandler.setRequestLog(requestLog);\n handlers.addHandler(logHandler);\n ServletContextHandler handler = new ServletContextHandler();\n handler.addServlet(new ServletHolder(new HeartbeatServlet(\nHttpServer\n, this.getNodeUrl())), \n/health\n);\n handlers.addHandler(handler);\n return handlers;\n}\n\n\n\n\nYou can also use this operator to develop Websocket operators with some modifications. Two classes are implemented; one for handling data transfer for receiving and sending data over websocket connections and other for creating connections and configuring WebSocketServletFactory.\n\n\nThe following example depicts implementation of \ngetHandlers()\n method for WebsocketOutput operator.\n\n\npublic HandlerCollection getHandlers() throws UnknownHostException\n{\n HandlerCollection handlers = new HandlerCollection(true);\n ServletContextHandler handler = new ServletContextHandler();\n handler.addServlet(new ServletHolder(\nws-events\n, new DummyRequestWebsocketServlet(\n   180000, 180000, DummyRequestWS.class)), \n/ws/*\n);\n handlers.addHandler(handler);\n return handlers;\n\n}\n\n\n\n\nDummyRequestWS ( Example for WebsocketAdapter ) :\n\n\nObject of this class is instantiated for each established websocket connection. You can refer to details about WebSocketAdapter at http://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/websocket/api/WebSocketAdapter.html](http://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/websocket/api/WebSocketAdapter.html)\n\n\nimport java.io.IOException;\nimport org.eclipse.jetty.websocket.api.Session;\nimport org.eclipse.jetty.websocket.api.WebSocketAdapter;\n\npublic class DummyRequestWS extends WebSocketAdapter\n{\n  @Override\n  public void onWebSocketConnect(Session sess){}\n\n  @Override\n  public void onWebSocketText(String message){}\n\n  @Override\n  public void onWebSocketClose(int statusCode, String reason){}\n\n  @Override\n  public void onWebSocketError(Throwable cause){}\n}\n\n\n\n\nDummyRequestWebsocketServlet Implementation (Example of AbstractWebSocketServlet)\n\n\npublic class DummyRequestWebsocketServlet extends AbstractWebSocketServlet\n{\n\n public DummyRequestWebsocketServlet(long maxIdleTime, long maxAsyncWriteTimeout, Class wsClazzName)\n {\n   super(maxIdleTime, maxAsyncWriteTimeout, wsClazzName);\n }\n\n @Override\n public void configure(WebSocketServletFactory webSocketServletFactory)\n {\n   // set a 10 second timeout\n   super.configure(webSocketServletFactory);\n   int BUFFER_SIZE = 10240; // 10240 bytes\n   webSocketServletFactory.getPolicy().setInputBufferSize(BUFFER_SIZE / 10);\n   webSocketServletFactory.getPolicy().setMaxTextMessageSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setMaxTextMessageBufferSize(BUFFER_SIZE * 8);\n   webSocketServletFactory.getPolicy().setMaxBinaryMessageSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setMaxBinaryMessageBufferSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setIdleTimeout(getMaxIdleTime());\n   webSocketServletFactory.getPolicy().setAsyncWriteTimeout(getMaxAsyncWriteTimeout());\n\n }\n}\n\n\n\n\nConfigurations\n\n\nThe following properties must be updated in the \nproperties.xml\n file:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nthreads\n\n\nDefines number of threads for the server operator.\n\n\ndt.operator.[OPNAME].prop.threads\n\n\n\n\n\n\ncooldownSecs\n\n\nDefines the number of seconds to wait before you shut down the jetty server.\n\n\ndt.operator.[OPNAME].prop.cooldownSecs\n\n\n\n\n\n\nport\n\n\nDefines the listening tcp port used by the server.\n\n\ndt.operator.[OPNAME].prop.port\n\n\n\n\n\n\n\n\nWhen you are creating the DAG, you must specify the following configuration:\n\n\nDummyHttpServerOperator tcpInput = dag.addOperator(\nHttpInput\n, DummyHttpServerOperator.class);\nAffinityRule httpRule = new AffinityRule(AffinityRule.Type.ANTI_AFFINITY, DAG.Locality.NODE_LOCAL, false, \nHttpInput\n, \nHttpInput\n);\n\n\n\n\nPartitioning\n\n\nThis operator implements interface called WebServerPartitionable and requires WebServerPartitioner for static partitioning. Being stateless, this operator does not redistribute any state.\n\n\nThe following configuration must be set in \nProperties.xml\n for partitioning:\n\n\nproperty\n\n \nname\ndt.operator.[OperatorName].attr.PARTITIONER\n/name\n\n \nvalue\n[FQDN for WebServerPartitioner or One extended from it] :[PARTITION_COUNT]\n/value\n\n\n/property\n\n\n\n\n\nFor example, the following code depicts a scenario when an  is extended from AbstractHttpServerOperator as DummyServerOperator and DummyWebPartitioner and extends WebServerPartitioner, configuration for specifying 3 partitions for the operator.\n\n\nproperty\n\n \nname\ndt.operator.DummyHttpServer.attr.PARTITIONER\n/name\n\n \nvalue\ncom.example.app.DummyWebPartitioner :3\n/value\n\n\n/property\n\n\n\n\n\nUsing the Operator\n\n\nThe following code illustrates, how you can use the operator can be used within an application:\npublic class DummyHttpServer extends AbstractHttpServerOperator implements InputOperator\n\n\n{\n\n @Override\n public HandlerCollection getHandlers() throws Exception\n {\n   HandlerCollection handlers = new HandlerCollection(true);\n   this.logHandler = new RequestLogHandler();\n   DummyRequestLog requestLog = new DummyRequestLog();\n   requestLog.setExtended(true);\n   requestLog.setLogLatency(true);\n   requestLog.setPreferProxiedForAddress(true);\n   logHandler.setRequestLog(requestLog);\n   handlers.addHandler(logHandler);\n   ServletContextHandler handler = new ServletContextHandler();\n   handler.addServlet(new ServletHolder(new HeartbeatServlet(\nHttpServer\n, this.getNodeUrl())), \n/health\n);\n   handlers.addHandler(handler);\n   return handlers;\n }\n}", 
            "title": "Abstract HTTP Server Operator"
        }, 
        {
            "location": "/operators/abstracthttpserver/#about-abstracthttpserver-operator", 
            "text": "AbstractHttpServer operator helps developer to implement HTTP services and run them as part of operator.  It provides a way to host Jetty HTTP Servlet server as an . You can develop custom Servlets and plug-in those using these classes: ServletContextHolder and ServletHolder.  This operator can be used as an input or output operator based on how you can extend it.  This operator can be used to receive data from Mobile/ Web clients as well as from clients using WebSocket connections.  To use this operator, you must extend it and implement the  getHandlers  method to define the collection of handlers to handle HTTP requests.  This operator is available under DT Plus license.", 
            "title": "About AbstractHttpServer Operator"
        }, 
        {
            "location": "/operators/abstracthttpserver/#ports", 
            "text": "The following port is available for the AbstractHttpServerOperator operator:     Port Type  Port Name  Details      Output Port  deployBroadcastOutport  Emits information about the current partition including, hostname, services started or not, partition mask, partitionId.  If the partition has more than one partitionId assigned, then that many tuples are emitted.  All this information is emitted using POJO object of the type ServerDeploymentConfig.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/abstracthttpserver/#testing", 
            "text": "AbstractHttpServer  is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.7 and above  Embedded Jetty version 9.2.11.v20150529.", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/abstracthttpserver/#workflow-of-the-operator", 
            "text": "The workflow of the  AbstrractHttpServer   is as follows:   At initialization, the operator loads the handlers that you have provided.  Starts webserver with the defined port.  If the operator is partitioned and the anti-affinity rule is set for the partitions of this operator, partitions of the operator gets deployed on different nodes. This is needed to ensure that two partitions do not share the same TCP port.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/abstracthttpserver/#supplementary-classes", 
            "text": "The following classes are used by the  AbstrractHttpServer  operator:", 
            "title": "Supplementary Classes"
        }, 
        {
            "location": "/operators/abstracthttpserver/#serverdeploymentconfig", 
            "text": "Property Name  Type  Description      hostName  String  This property can be used as the address URL and is specified in the following format by default:  protocol://[hostname]:[port]/.    started  boolean  Specifies if the server has started or shut down for the clients.    mask  int  Specifies the partition mask of the partition of the operator    partitionId  int  Specifies the partition ID for the current partition of the operator.", 
            "title": "ServerDeploymentConfig"
        }, 
        {
            "location": "/operators/abstracthttpserver/#handlercollection", 
            "text": "You must implement the  getHandlers()  method on this operator, which returns the  HandlerCollection  object  org.eclipse.jetty.server.handler.HandlerCollection . The following example depicts how you implement the  gethandlers  method:  @Override\npublic HandlerCollection getHandlers() throws Exception\n{\n HandlerCollection handlers = new HandlerCollection(true);\n this.logHandler = new RequestLogHandler();\n DummyRequestLog requestLog = new DummyRequestLog();\n requestLog.setExtended(true);\n requestLog.setLogLatency(true);\n requestLog.setPreferProxiedForAddress(true);\n logHandler.setRequestLog(requestLog);\n handlers.addHandler(logHandler);\n ServletContextHandler handler = new ServletContextHandler();\n handler.addServlet(new ServletHolder(new HeartbeatServlet( HttpServer , this.getNodeUrl())),  /health );\n handlers.addHandler(handler);\n return handlers;\n}  You can also use this operator to develop Websocket operators with some modifications. Two classes are implemented; one for handling data transfer for receiving and sending data over websocket connections and other for creating connections and configuring WebSocketServletFactory.  The following example depicts implementation of  getHandlers()  method for WebsocketOutput operator.  public HandlerCollection getHandlers() throws UnknownHostException\n{\n HandlerCollection handlers = new HandlerCollection(true);\n ServletContextHandler handler = new ServletContextHandler();\n handler.addServlet(new ServletHolder( ws-events , new DummyRequestWebsocketServlet(\n   180000, 180000, DummyRequestWS.class)),  /ws/* );\n handlers.addHandler(handler);\n return handlers;\n\n}", 
            "title": "HandlerCollection"
        }, 
        {
            "location": "/operators/abstracthttpserver/#dummyrequestws-example-for-websocketadapter", 
            "text": "Object of this class is instantiated for each established websocket connection. You can refer to details about WebSocketAdapter at http://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/websocket/api/WebSocketAdapter.html](http://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/websocket/api/WebSocketAdapter.html)  import java.io.IOException;\nimport org.eclipse.jetty.websocket.api.Session;\nimport org.eclipse.jetty.websocket.api.WebSocketAdapter;\n\npublic class DummyRequestWS extends WebSocketAdapter\n{\n  @Override\n  public void onWebSocketConnect(Session sess){}\n\n  @Override\n  public void onWebSocketText(String message){}\n\n  @Override\n  public void onWebSocketClose(int statusCode, String reason){}\n\n  @Override\n  public void onWebSocketError(Throwable cause){}\n}", 
            "title": "DummyRequestWS ( Example for WebsocketAdapter ) :"
        }, 
        {
            "location": "/operators/abstracthttpserver/#dummyrequestwebsocketservlet-implementation-example-of-abstractwebsocketservlet", 
            "text": "public class DummyRequestWebsocketServlet extends AbstractWebSocketServlet\n{\n\n public DummyRequestWebsocketServlet(long maxIdleTime, long maxAsyncWriteTimeout, Class wsClazzName)\n {\n   super(maxIdleTime, maxAsyncWriteTimeout, wsClazzName);\n }\n\n @Override\n public void configure(WebSocketServletFactory webSocketServletFactory)\n {\n   // set a 10 second timeout\n   super.configure(webSocketServletFactory);\n   int BUFFER_SIZE = 10240; // 10240 bytes\n   webSocketServletFactory.getPolicy().setInputBufferSize(BUFFER_SIZE / 10);\n   webSocketServletFactory.getPolicy().setMaxTextMessageSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setMaxTextMessageBufferSize(BUFFER_SIZE * 8);\n   webSocketServletFactory.getPolicy().setMaxBinaryMessageSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setMaxBinaryMessageBufferSize(BUFFER_SIZE);\n   webSocketServletFactory.getPolicy().setIdleTimeout(getMaxIdleTime());\n   webSocketServletFactory.getPolicy().setAsyncWriteTimeout(getMaxAsyncWriteTimeout());\n\n }\n}", 
            "title": "DummyRequestWebsocketServlet Implementation (Example of AbstractWebSocketServlet)"
        }, 
        {
            "location": "/operators/abstracthttpserver/#configurations", 
            "text": "The following properties must be updated in the  properties.xml  file:     Property Name  Description  Example      threads  Defines number of threads for the server operator.  dt.operator.[OPNAME].prop.threads    cooldownSecs  Defines the number of seconds to wait before you shut down the jetty server.  dt.operator.[OPNAME].prop.cooldownSecs    port  Defines the listening tcp port used by the server.  dt.operator.[OPNAME].prop.port     When you are creating the DAG, you must specify the following configuration:  DummyHttpServerOperator tcpInput = dag.addOperator( HttpInput , DummyHttpServerOperator.class);\nAffinityRule httpRule = new AffinityRule(AffinityRule.Type.ANTI_AFFINITY, DAG.Locality.NODE_LOCAL, false,  HttpInput ,  HttpInput );", 
            "title": "Configurations"
        }, 
        {
            "location": "/operators/abstracthttpserver/#partitioning", 
            "text": "This operator implements interface called WebServerPartitionable and requires WebServerPartitioner for static partitioning. Being stateless, this operator does not redistribute any state.  The following configuration must be set in  Properties.xml  for partitioning:  property \n  name dt.operator.[OperatorName].attr.PARTITIONER /name \n  value [FQDN for WebServerPartitioner or One extended from it] :[PARTITION_COUNT] /value  /property   For example, the following code depicts a scenario when an  is extended from AbstractHttpServerOperator as DummyServerOperator and DummyWebPartitioner and extends WebServerPartitioner, configuration for specifying 3 partitions for the operator.  property \n  name dt.operator.DummyHttpServer.attr.PARTITIONER /name \n  value com.example.app.DummyWebPartitioner :3 /value  /property", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/abstracthttpserver/#using-the-operator", 
            "text": "The following code illustrates, how you can use the operator can be used within an application:\npublic class DummyHttpServer extends AbstractHttpServerOperator implements InputOperator  {\n\n @Override\n public HandlerCollection getHandlers() throws Exception\n {\n   HandlerCollection handlers = new HandlerCollection(true);\n   this.logHandler = new RequestLogHandler();\n   DummyRequestLog requestLog = new DummyRequestLog();\n   requestLog.setExtended(true);\n   requestLog.setLogLatency(true);\n   requestLog.setPreferProxiedForAddress(true);\n   logHandler.setRequestLog(requestLog);\n   handlers.addHandler(logHandler);\n   ServletContextHandler handler = new ServletContextHandler();\n   handler.addServlet(new ServletHolder(new HeartbeatServlet( HttpServer , this.getNodeUrl())),  /health );\n   handlers.addHandler(handler);\n   return handlers;\n }\n}", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/azure_blob/", 
            "text": "About Azure Blob Storage Operator\n\n\nAzure Blob storage is a service for storing large amounts of unstructured object data, such as text or binary data, that can be accessed from anywhere in the world through HTTP or HTTPS. Blob storage can be used to expose data publicly to the world or to store application data privately.\n\n\nCommon uses of Blob storage include:\n\n\n\n\nServing images or documents directly to a browser\n\n\nStoring files for distributed access\n\n\nStreaming video and audio\n\n\nStoring data for backup and restore, disaster recovery, and archiving\n\n\nStoring data for analysis by an on-premises or Azure-hosted service\n\n\n\n\nAzure Blob Output operator receives tuples from upstream operators in the form of byte array or string. These tuples are written to HDFS file. The files are rolled over based on user configuration. Rolled over files are then uploaded to Azure Blob storage.\n\n\nThis operator is available under DT Plus license.\n\n\nPorts\n\n\nThe following port is available for the Azure Blob Output operator\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nInput port\n\n\ninput\n\n\nReceives tuples from upstream operators in byte / string array format.\n\n\n\n\n\n\n\n\nTesting\n\n\nThe Azure Blob Output operator  is tested with the following components:\n\n\n\n\nCloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0\n\n\nJava version 1.8\n\n\nHDInsight : Hadoop 2.7.3 - HDI 3.6\n\n\nMicrosoft Azure Storage Client SDK - com.microsoft.azure:azure-storage:5.5.0\n\n\n\n\nRequirements\n\n\nJava 1.8 and above\n\n\nWorkflow of the Operator\n\n\nThe following image depicts the workflow of the Azure Blob Output operator:\n\n\n\nThe Blob Output module consists of the following two operators:\n\n\n\n\nRecord Compaction operator\n\nThis operator combines multiple incoming tuples that are written into the HDFS file. It also rolls over the files based on the user configuration values.\n\n\nBlob Upload operator\n\nThis operator uploads the rolled-over files to Blob storage.\n\n\n\n\nSupported Data Types\n\n\nPre-defined implementations are provided for the following data types:\n\n\n\n\nByte array (BlobOutputModule.BytesBlobOutputModule)\n\n\nString (BlobOutputModule.StringBlobOutputModule)\n\n\n\n\nIf there are any other custom data types, user can extend the class \nBlobOutputModule\n providing implementation for \ngetConverter()\n method, which converts the incoming tuple into byte array to write to HDFS file.\n\n\nConfiguring Azure Blob Storage Operator\n\n\nThe following properties must be configured in the \nproperties.xml\n file:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmaxIdleWindows\n\n\nThe maximum number of idle windows allowed before the files are rolled over.\n\n\n\n\n\n\nmaxLength\n\n\nThe permitted length of the file after which the file is rolled over (in bytes).\n\n\n\n\n\n\ntupleSeparator\n\n\nCharacter/string that separates 2 consecutive tuples.\n\n\n\n\n\n\nexpireStreamAfterAccessMillis\n\n\nThe Time interval allowed after which the file output stream is evicted from the cache.\n\n\n\n\n\n\nrotationWindows\n\n\nThe number of windows after which the file should be rolled over.\n\n\n\n\n\n\nstorageAccountName\n\n\nStorage account name for Azure blob storage.\n\n\n\n\n\n\nstorageAccountKey\n\n\nStorage account key for Azure blob storage.\n\n\n\n\n\n\nstorageContainer\n\n\nStorage container name for Azure blob storage.\n\n\n\n\n\n\nvirtualDirectory\n\n\nVirtual directory name.\n\n\n\n\n\n\nblobUploadPartitionCount\n\n\nPartition count for blob upload operator.\n\n\n\n\n\n\n\n\nPartitioning\n\n\n\n\nPartitioning for the RecordCompaction operator is decided based on number of partitions for the upstream operator (PARTITION_PARALLEL).\n\n\nPartionining for the BlobUpload operator can be controlled using the property blobUploadPartitionCount.\n\n\n\n\nUsing the Operator\n\n\nThe following code illustrates how Azure Blob output operator can be used within an application:\n\n\n@ApplicationAnnotation(name = \nKafka-to-Azure-blob\n)\npublic class Application implements StreamingApplication\n{\n public void populateDAG(DAG, Configuration conf)\n {\n   KafkaSinglePortInputOperator kafkaInput =\n       dag.addOperator(\nkafkaInput\n, KafkaSinglePortInputOperator.class);\n\n   BlobOutputModule\nbyte[]\n blobOutput = dag.addModule(\nblobOutput\n,\n       new BlobOutputModule.BytesBlobOutputModule());\n   dag.addStream(\ndata\n, kafkaInput.outputPort, blobOutput.input);\n }\n}", 
            "title": "Azure Blob Storage Operator"
        }, 
        {
            "location": "/operators/azure_blob/#about-azure-blob-storage-operator", 
            "text": "Azure Blob storage is a service for storing large amounts of unstructured object data, such as text or binary data, that can be accessed from anywhere in the world through HTTP or HTTPS. Blob storage can be used to expose data publicly to the world or to store application data privately.  Common uses of Blob storage include:   Serving images or documents directly to a browser  Storing files for distributed access  Streaming video and audio  Storing data for backup and restore, disaster recovery, and archiving  Storing data for analysis by an on-premises or Azure-hosted service   Azure Blob Output operator receives tuples from upstream operators in the form of byte array or string. These tuples are written to HDFS file. The files are rolled over based on user configuration. Rolled over files are then uploaded to Azure Blob storage.  This operator is available under DT Plus license.", 
            "title": "About Azure Blob Storage Operator"
        }, 
        {
            "location": "/operators/azure_blob/#ports", 
            "text": "The following port is available for the Azure Blob Output operator     Port Type  Port Name  Details      Input port  input  Receives tuples from upstream operators in byte / string array format.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/azure_blob/#testing", 
            "text": "The Azure Blob Output operator  is tested with the following components:   Cloudera Hadoop version 5.8.2 along with Apache Hadoop version 2.6.0  Java version 1.8  HDInsight : Hadoop 2.7.3 - HDI 3.6  Microsoft Azure Storage Client SDK - com.microsoft.azure:azure-storage:5.5.0", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/azure_blob/#requirements", 
            "text": "Java 1.8 and above", 
            "title": "Requirements"
        }, 
        {
            "location": "/operators/azure_blob/#workflow-of-the-operator", 
            "text": "The following image depicts the workflow of the Azure Blob Output operator:  \nThe Blob Output module consists of the following two operators:   Record Compaction operator \nThis operator combines multiple incoming tuples that are written into the HDFS file. It also rolls over the files based on the user configuration values.  Blob Upload operator \nThis operator uploads the rolled-over files to Blob storage.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/azure_blob/#supported-data-types", 
            "text": "Pre-defined implementations are provided for the following data types:   Byte array (BlobOutputModule.BytesBlobOutputModule)  String (BlobOutputModule.StringBlobOutputModule)   If there are any other custom data types, user can extend the class  BlobOutputModule  providing implementation for  getConverter()  method, which converts the incoming tuple into byte array to write to HDFS file.", 
            "title": "Supported Data Types"
        }, 
        {
            "location": "/operators/azure_blob/#configuring-azure-blob-storage-operator", 
            "text": "The following properties must be configured in the  properties.xml  file:     Property  Description      maxIdleWindows  The maximum number of idle windows allowed before the files are rolled over.    maxLength  The permitted length of the file after which the file is rolled over (in bytes).    tupleSeparator  Character/string that separates 2 consecutive tuples.    expireStreamAfterAccessMillis  The Time interval allowed after which the file output stream is evicted from the cache.    rotationWindows  The number of windows after which the file should be rolled over.    storageAccountName  Storage account name for Azure blob storage.    storageAccountKey  Storage account key for Azure blob storage.    storageContainer  Storage container name for Azure blob storage.    virtualDirectory  Virtual directory name.    blobUploadPartitionCount  Partition count for blob upload operator.", 
            "title": "Configuring Azure Blob Storage Operator"
        }, 
        {
            "location": "/operators/azure_blob/#partitioning", 
            "text": "Partitioning for the RecordCompaction operator is decided based on number of partitions for the upstream operator (PARTITION_PARALLEL).  Partionining for the BlobUpload operator can be controlled using the property blobUploadPartitionCount.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/azure_blob/#using-the-operator", 
            "text": "The following code illustrates how Azure Blob output operator can be used within an application:  @ApplicationAnnotation(name =  Kafka-to-Azure-blob )\npublic class Application implements StreamingApplication\n{\n public void populateDAG(DAG, Configuration conf)\n {\n   KafkaSinglePortInputOperator kafkaInput =\n       dag.addOperator( kafkaInput , KafkaSinglePortInputOperator.class);\n\n   BlobOutputModule byte[]  blobOutput = dag.addModule( blobOutput ,\n       new BlobOutputModule.BytesBlobOutputModule());\n   dag.addStream( data , kafkaInput.outputPort, blobOutput.input);\n }\n}", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/aoooperator/", 
            "text": "About Analytics Output Operator\n\n\nThis operator can be used to send data from a source application for processing in Online Analytics Service. \nAnalytics Output operator\n is essentially an extension to \nKafka exactly once output\n operator which does some additional work for seamlessly integrating with Online Analytics Service (OAS). This operator is extended from \nKafka Single Port Exactly Once Output operator\n from malhar-kafka library. The kafka client used in this operator is versioned at 0.9.\n\n\nAnalytics Output Operator\n (AOO for short) can be added to any pipeline for sending the data to OAS for OLAP processing. This operator can be referred as a thin client for OAS.\n\n\nAnalyticsOutputOperator receives a tuple of type Object from upstream Apex pipeline and sends it to a configured Kafka topic. The data sent to Kafka topic is in exactly once fashion. That means, if the AOO gets killed, after recovery it attempts for data deduplication.\n\n\nPorts\n\n\nAnalyticsOutputOperator has a single input port and no output port. The input port is used to receive the tuple from upstream and sending to downstream kafka topic.\n\n\nPartitioning\n\n\nThis operator is partitionable in a stateless manner and all partitioning capabilities of \nKafka Single Port Exactly Once Output\n operator are applicable to this operator.\n\n\nAccessing the Operator\n\n\nAnalytics Output Operator is present in \ndt-apoxi-oas-operator\n maven artifact. One can add following lines to project\ns maven \npom.xml\n file for making the operator available in their project:\n\n\ndependency\n\n            \ngroupId\ncom.datatorrent\n/groupId\n\n        \nartifactId\ndt-apoxi-oas-operator\n/artifactId\n\n         \nversion\n1.4.0\n/version\n\n\n/dependency\n\n\n\n\n\nCurrent version for DT Apoxi OAS operator is 1.4.0.\n\n\nTesting\n\n\nAnalytics Output operator is tested with the following configuration:\n\n\n\n\nJava 7 runtime\n\n\nKafka 0.9 and 0.10 clients\n\n\nOnline Analytics Service 1.4.0\n\n\n\n\nRequirements\n\n\nRequirements for using this operator are as follows:\n\n\n\n\nStandard system requirements of Apex Application are applicable for this operator.\n\n\nKafka version 0.9 or 0.10.\n\n\nDataTorrent RTS 3.10.0 platform\n\n\nUpstream pipeline should be idempotent. This is required for the purpose of maintaining exactly once semantics. You can choose to skip the exactly once semantics using a property \ndisablePartialWindowCheck\n described in the properties section and setting it to \ntrue\n.\n\n\n\n\nWorkflow of the Operator\n\n\nThe workflow of Analytics Output operator is as follows:\n\n\n\n\nAt the beginning of every checkpoint window, it send a druid schema (configured as a string property) tuple to the configured kafka topic  which is consumed by the Online Analytics Service to register the new datasource. This is a general step irrespective of any tuples being received from upstream.\n\n\nThe operator receives a tuple of type \nObject\n from upstream.\n\n\nIt parses, validates and serializes the tuple and forwards it to base class of \nKafka Single Port Exactly Once Output\n operator.\n\n\nKafka Single Port Exactly Once Output\n operator sends this tuple to the configured kafka topic in exactly once fashion.\n\n\nAt the end of every window operator it keeps track of the kafka offset for exactly once purpose.\n\n\n\n\nThe workflow of Analytics Output operator in case of recovery:\n\n\n\n\nAs per the apex semantics, the recovery of this operator occurs at the last commited window. This means the data is reprocessed by this operator.\n\n\nIf \ndisablePartialWindowCheck\n is \nfalse\n, this means exactly once semantics are enabled. In this case, the operator skips all the reprocessing till the window for which kafka offset is present. There can be an incomplete window for which some of the data is already sent to kafka. For this purpose, the operator reads the data from the last offset and skips all the data received in this window which is already sent.\n\n\nIf \ndisablePartialWindowCheck\n is true, this means exactly once semantics are disabled. In this case, the operator still skips the data reprocessing for all those window for which the kafka offset is present. In case of incomplete or partial window, the data is resent. This means there is a possible data duplication in kafka for maximum of one window. If the upstream pipeline is not idempotent, this option is suggested.\n\n\n\n\nSupported Data Types\n\n\nAnalytics Output operator receives data as an \nObject\n. There is a default serializer provided for serializing the received tuple to a JSON object. One can choose to override the serializer using the property \nserializerClass\n.\n\n\nProperties and Attributes\n\n\nYou can set the following properties for Analytics Output operator:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nIs required property?\n\n\nDefault value\n\n\n\n\n\n\n\n\n\n\nproperties\n\n\nThese are a map of kafka properties where key is a name of kafka property and value is property value. This can be used to specify the broker and other kafka consumer settings.Minimum kafka property that is required is: \nbootstrap.servers\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\ntopic\n\n\nName of the kafka topic to which the analytics data needs to be sent.\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nschema\n\n\nDruid schema (as a string) which defines the data present in the tuple as well as some druid specific properties for OLAP computation.For details on druid schema, refer \nDataSchema\n section of  following document: \nhttp://druid.io/docs/0.9.1/ingestion/index.html\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nserializerClass\n\n\nSerializer class which is used to serialize the tuples of type \nObject\n received from upstream.This class essentially is implementation of interface: \ncom.datatorrent.analytics.AnalyticsOutputOperator.AnalyticsPOJOSerializer\n Ensure that the given class is present in classpath.\n\n\nNo\n\n\ncom.datatorrent.analytics.AnalyticsOutputOperator.ObjectMapperPOJOSerializer\n\n\n\n\n\n\ndisablePartialWindowCheck\n\n\nUse this property to disable the exactly once semantics. A value set to \ntrue\n disables the exactly once semantics. Value set to \nfalse\n, enables the exactly once semantics.\n\n\nNo\n\n\nfalse\n\n\n\n\n\n\n\n\nNote:\n Other than above mentioned properties, all the properties and attributes of \nKafka Single Port Exactly Once Output\n operator are applicable to this operator.\n\n\nSupplementary Classes\n\n\nAnalytics Output Operator\n  can be supplied with a serializer class using \nserializerClass\n property to control how to serialize the tuples received from upstream before sending it to kafka. It is expected that the implementation of this class should provide a valid JSON string after serialization.\n\n\nThis is an optional property and the default implementation uses a ObjectMapper to serialize a POJO to a JSON string.\n\n\nUsing the Operator\n\n\nThe following is a sample Apex Application where the Analytics Output operator is used:\n\n\n@ApplicationAnnotation(name = \nSampleApplication\n)\npublic class SampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    DataGenOperator input = dag.addOperator(\nInput\n, new DataGenOperator());\n    AnalyticsOutputOperator output = dag.addOperator(\nAOO\n, new AnalyticsOutputOperator());\n\n    dag.addStream(\nio\n, input.dataOutput, output.inputPort);\n  }\n}\n\n\n\n\nSample properties\n\n\nproperty\n\n    \nname\ndt.operator.AOO.prop.topic\n/name\n\n    \nvalue\nanalytics\n/value\n\n\n/property\n\n\nproperty\n\n\nname\ndt.operator.AOO.prop.properties(bootstrap.servers)\n/name\n\n    \nvalue\nlocalhost:9092\n/value\n\n\n/property\n\n\nproperty\n\n    \nname\ndt.operator.AOO.prop.schema\n/name\n\n    \nvalue\n\n{\n  \ndataSource\n: \nsample\n,\n  \ngranularitySpec\n: {\ntype\n: \nuniform\n, \nsegmentGranularity\n: \nhour\n, \nqueryGranularity\n: \nminute\n},\n  \nparser\n: {\n    \ntype\n: \nstring\n,\n    \nparseSpec\n: {\n      \nformat\n: \njson\n,\n      \ndimensionsSpec\n: {\n        \ndimensions\n: [\nemail\n, \ncountry\n, \ncctype\n, \ncurrency\n]\n      },\n      \ntimestampSpec\n: {\nformat\n: \nmillis\n, \ncolumn\n: \ntimestamp\n}\n    }\n  },\n  \nmetricsSpec\n: [\n    {\nname\n: \nCOUNT\n, \ntype\n: \ncount\n},\n    {\nname\n: \nTRANSACTIONAMOUNT\n, \ntype\n: \nlongSum\n, \nfieldName\n: \namount\n},\n    {\nname\n: \nemail_unique\n, \ntype\n: \nhyperUnique\n, \nfieldName\n: \nemail\n},\n    {\nname\n: \ncountry_unique\n, \ntype\n: \nhyperUnique\n, \nfieldName\n: \ncountry\n},\n    {\nname\n: \ncctype_unique\n, \ntype\n: \nhyperUnique\n, \nfieldName\n: \ncctype\n},\n    {\nname\n: \ncurrency_unique\n, \ntype\n: \nhyperUnique\n, \nfieldName\n: \ncurrency\n},\n    {\nname\n: \namount_min\n, \ntype\n: \nlongMin\n, \nfieldName\n: \namount\n},\n    {\nname\n: \namount_max\n, \ntype\n: \nlongMax\n, \nfieldName\n: \namount\n}\n  ]\n}\n\n/value\n\n\n/property", 
            "title": "Analytics Output Operator"
        }, 
        {
            "location": "/operators/aoooperator/#about-analytics-output-operator", 
            "text": "This operator can be used to send data from a source application for processing in Online Analytics Service.  Analytics Output operator  is essentially an extension to  Kafka exactly once output  operator which does some additional work for seamlessly integrating with Online Analytics Service (OAS). This operator is extended from  Kafka Single Port Exactly Once Output operator  from malhar-kafka library. The kafka client used in this operator is versioned at 0.9.  Analytics Output Operator  (AOO for short) can be added to any pipeline for sending the data to OAS for OLAP processing. This operator can be referred as a thin client for OAS.  AnalyticsOutputOperator receives a tuple of type Object from upstream Apex pipeline and sends it to a configured Kafka topic. The data sent to Kafka topic is in exactly once fashion. That means, if the AOO gets killed, after recovery it attempts for data deduplication.", 
            "title": "About Analytics Output Operator"
        }, 
        {
            "location": "/operators/aoooperator/#ports", 
            "text": "AnalyticsOutputOperator has a single input port and no output port. The input port is used to receive the tuple from upstream and sending to downstream kafka topic.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/aoooperator/#partitioning", 
            "text": "This operator is partitionable in a stateless manner and all partitioning capabilities of  Kafka Single Port Exactly Once Output  operator are applicable to this operator.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/aoooperator/#accessing-the-operator", 
            "text": "Analytics Output Operator is present in  dt-apoxi-oas-operator  maven artifact. One can add following lines to project s maven  pom.xml  file for making the operator available in their project:  dependency \n             groupId com.datatorrent /groupId \n         artifactId dt-apoxi-oas-operator /artifactId \n          version 1.4.0 /version  /dependency   Current version for DT Apoxi OAS operator is 1.4.0.", 
            "title": "Accessing the Operator"
        }, 
        {
            "location": "/operators/aoooperator/#testing", 
            "text": "Analytics Output operator is tested with the following configuration:   Java 7 runtime  Kafka 0.9 and 0.10 clients  Online Analytics Service 1.4.0", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/aoooperator/#requirements", 
            "text": "Requirements for using this operator are as follows:   Standard system requirements of Apex Application are applicable for this operator.  Kafka version 0.9 or 0.10.  DataTorrent RTS 3.10.0 platform  Upstream pipeline should be idempotent. This is required for the purpose of maintaining exactly once semantics. You can choose to skip the exactly once semantics using a property  disablePartialWindowCheck  described in the properties section and setting it to  true .", 
            "title": "Requirements"
        }, 
        {
            "location": "/operators/aoooperator/#workflow-of-the-operator", 
            "text": "The workflow of Analytics Output operator is as follows:   At the beginning of every checkpoint window, it send a druid schema (configured as a string property) tuple to the configured kafka topic  which is consumed by the Online Analytics Service to register the new datasource. This is a general step irrespective of any tuples being received from upstream.  The operator receives a tuple of type  Object  from upstream.  It parses, validates and serializes the tuple and forwards it to base class of  Kafka Single Port Exactly Once Output  operator.  Kafka Single Port Exactly Once Output  operator sends this tuple to the configured kafka topic in exactly once fashion.  At the end of every window operator it keeps track of the kafka offset for exactly once purpose.   The workflow of Analytics Output operator in case of recovery:   As per the apex semantics, the recovery of this operator occurs at the last commited window. This means the data is reprocessed by this operator.  If  disablePartialWindowCheck  is  false , this means exactly once semantics are enabled. In this case, the operator skips all the reprocessing till the window for which kafka offset is present. There can be an incomplete window for which some of the data is already sent to kafka. For this purpose, the operator reads the data from the last offset and skips all the data received in this window which is already sent.  If  disablePartialWindowCheck  is true, this means exactly once semantics are disabled. In this case, the operator still skips the data reprocessing for all those window for which the kafka offset is present. In case of incomplete or partial window, the data is resent. This means there is a possible data duplication in kafka for maximum of one window. If the upstream pipeline is not idempotent, this option is suggested.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/aoooperator/#supported-data-types", 
            "text": "Analytics Output operator receives data as an  Object . There is a default serializer provided for serializing the received tuple to a JSON object. One can choose to override the serializer using the property  serializerClass .", 
            "title": "Supported Data Types"
        }, 
        {
            "location": "/operators/aoooperator/#properties-and-attributes", 
            "text": "You can set the following properties for Analytics Output operator:     Property  Description  Is required property?  Default value      properties  These are a map of kafka properties where key is a name of kafka property and value is property value. This can be used to specify the broker and other kafka consumer settings.Minimum kafka property that is required is:  bootstrap.servers  Yes  N/A    topic  Name of the kafka topic to which the analytics data needs to be sent.  Yes  N/A    schema  Druid schema (as a string) which defines the data present in the tuple as well as some druid specific properties for OLAP computation.For details on druid schema, refer  DataSchema  section of  following document:  http://druid.io/docs/0.9.1/ingestion/index.html  Yes  N/A    serializerClass  Serializer class which is used to serialize the tuples of type  Object  received from upstream.This class essentially is implementation of interface:  com.datatorrent.analytics.AnalyticsOutputOperator.AnalyticsPOJOSerializer  Ensure that the given class is present in classpath.  No  com.datatorrent.analytics.AnalyticsOutputOperator.ObjectMapperPOJOSerializer    disablePartialWindowCheck  Use this property to disable the exactly once semantics. A value set to  true  disables the exactly once semantics. Value set to  false , enables the exactly once semantics.  No  false     Note:  Other than above mentioned properties, all the properties and attributes of  Kafka Single Port Exactly Once Output  operator are applicable to this operator.", 
            "title": "Properties and Attributes"
        }, 
        {
            "location": "/operators/aoooperator/#supplementary-classes", 
            "text": "Analytics Output Operator   can be supplied with a serializer class using  serializerClass  property to control how to serialize the tuples received from upstream before sending it to kafka. It is expected that the implementation of this class should provide a valid JSON string after serialization.  This is an optional property and the default implementation uses a ObjectMapper to serialize a POJO to a JSON string.", 
            "title": "Supplementary Classes"
        }, 
        {
            "location": "/operators/aoooperator/#using-the-operator", 
            "text": "The following is a sample Apex Application where the Analytics Output operator is used:  @ApplicationAnnotation(name =  SampleApplication )\npublic class SampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    DataGenOperator input = dag.addOperator( Input , new DataGenOperator());\n    AnalyticsOutputOperator output = dag.addOperator( AOO , new AnalyticsOutputOperator());\n\n    dag.addStream( io , input.dataOutput, output.inputPort);\n  }\n}", 
            "title": "Using the Operator"
        }, 
        {
            "location": "/operators/aoooperator/#sample-properties", 
            "text": "property \n     name dt.operator.AOO.prop.topic /name \n     value analytics /value  /property  property  name dt.operator.AOO.prop.properties(bootstrap.servers) /name \n     value localhost:9092 /value  /property  property \n     name dt.operator.AOO.prop.schema /name \n     value \n{\n   dataSource :  sample ,\n   granularitySpec : { type :  uniform ,  segmentGranularity :  hour ,  queryGranularity :  minute },\n   parser : {\n     type :  string ,\n     parseSpec : {\n       format :  json ,\n       dimensionsSpec : {\n         dimensions : [ email ,  country ,  cctype ,  currency ]\n      },\n       timestampSpec : { format :  millis ,  column :  timestamp }\n    }\n  },\n   metricsSpec : [\n    { name :  COUNT ,  type :  count },\n    { name :  TRANSACTIONAMOUNT ,  type :  longSum ,  fieldName :  amount },\n    { name :  email_unique ,  type :  hyperUnique ,  fieldName :  email },\n    { name :  country_unique ,  type :  hyperUnique ,  fieldName :  country },\n    { name :  cctype_unique ,  type :  hyperUnique ,  fieldName :  cctype },\n    { name :  currency_unique ,  type :  hyperUnique ,  fieldName :  currency },\n    { name :  amount_min ,  type :  longMin ,  fieldName :  amount },\n    { name :  amount_max ,  type :  longMax ,  fieldName :  amount }\n  ]\n} /value  /property", 
            "title": "Sample properties"
        }, 
        {
            "location": "/operators/elasticsearch/", 
            "text": "About ElasticSearch Output Operator\n\n\nElasticsearch output operator along with Elastic Search engine provides data capability to store full-text search documents in real time.\n\n\nThe Elasticsearch operator embeds the transport client to simplify the connectivity and stores the documents to Elasticsearch cluster for a text search analysis.\n\n\nYou can configure the operator properties as per your business requirements and the execution aspect is handled by the Elasticsearch operator.\n\n\nThe Elasticsearch output operator provides the following advantages:\n\n\n\n\nHigh performant and can process data in bulk operations.\n\n\nThe application is also highly performant and can process as fast as the network allows.\n\n\nEase to modify the configuration of Elasticsearch indexes through properties as most of the functionality is exported as tuning parameters as per requirements.\n\n\nFlexible and configurable sharding (that is, it allows you to distribute and parallelize operations across shard) for Elasticsearch indexes based on APIs provided in operator for user provided document ID and index name.\n\n\nFlexible in setting input source data type formats such as JSON.\n\n\nBuilt-in metrics related to Elasticsearch, such as number of events, bytes written, bad events etc., are available.\n\n\n\n\nThis operator is available under \nDT Plus\n license.\n\n\nTesting\n\n\nThe Elasticsearch output operator is tested with the following components:\n\n\n\n\nApache Hadoop version 2.7.0\n\n\nJava version 1.8.\n\n\nElastic search version 5.4\n\n\n\n\nWorkflow of the Operator\n\n\n\n\nConfigure the Elasticsearch cluster node and port in the Elasticsearch operator.\n\n\nElasticsearch output operator receives enriched data from upstream operators which are stored on Elasticsearch cluster.\n\n\n\n\nPorts\n\n\nThe following ports are available on the Elasticsearch output operator:\n\n\n\n\n\n\n\n\nPort Type\n\n\nPort Name\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nInput Port\n\n\nInput Port\n\n\nReceives incoming tuples from upstream operators.\n\n\n\n\n\n\n\n\nPartitioning\n\n\nThe ElasticSearch operator can be partitioned using the default partitioner provided by Apex.\n\n\nElasticsearch operator can be dynamically partitioned as stateless. For stateful, dynamic partitioning is not supported. Also see\n\n\nConfiguring ElasticSearch Output Operator\n\n\nThe following settings can be configured for Elasticsearch output operator:\n\n\n\n\nMandatory properties for connectivity \n\n\nOptional properties for performance tuning\n\n\n\n\nMandatory Elasticsearch Cluster Properties \n\n\nThe following properties must be mandatorily updated in the \nproperties.xml\n file for connectivity:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nclusterName\n\n\nName of Elasticsearch cluster.\n\n\n\n\n\n\nhostNames\n\n\nName of hosts along with the port numbers where Elasticsearch cluster is configured. For e.g: localhost:9300\n\n\n\n\n\n\nindexName\n\n\nName of the Elasticsearch index.\n\n\n\n\n\n\n\n\nOptional Properties for Performance Tuning \n\n\nOperator can be optionally configured as per the performance and resources requirement. The following properties must be added to \nproperties.xml\n file.\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbatchSize\n\n\nThis controls the amount of data that is grouped in a bulk for storing in the Elasticsearch store.\n\n\n\n\n\n\nconcurrentRequests\n\n\nSets the number of concurrent requests that are allowed to be executed parallelly.\n\n\n\n\n\n\nbulkRequestTimeoutInSecs\n\n\nSets the timeout for bulk request in seconds. A value of zero indicates that only a single request can be executed. A value of one indicates that concurrent request can be executed while accumulating new bulk requests.\n\n\n\n\n\n\nbulkRequestBatchSizeInMb\n\n\nSets the batch size in MB for bulk request.\n\n\n\n\n\n\ncheckClusterStatus\n\n\nChecks the entire cluster status along with status for specified index.\n\n\n\n\n\n\nflushAfterWindowCompletion\n\n\nPushes the bulk request to elastic store after every end window.\n\n\n\n\n\n\nbackoffPolicyRetryCount\n\n\nSets a retry count for back off policy in case of a failure.\n\n\n\n\n\n\nbackoffPolicyRetryTimeInMSecs\n\n\nSets the time interval for back off policy in case of a failure.\n\n\n\n\n\n\n\n\nConfiguring the Partitioning\n\n\nFor partitioning, you must add the following property in the \nproperties.xml\n file. For example, if you add Elasticsearch operator with the name \nElasticOutputStore\n in the DAG, then this property creates four partitions of the operator when the application starts.\n\n\nproperty\n\n     \nname\ndt.operator.ElasticOutputStore.attr.PARTITIONER\n/name\n\n     \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner(4)\n/name\n\n\n/property\n\n\n\n\n\nSample Application\n\n\nThe following code illustrates how Elasticsearch output Operator can be used within an application.\n\n\nThis application reads JSON data from Kafka and parses it. After some transformation, this data is sent to Elasticsearch output Operator which is then stored on an Elasticsearch cluster.\n\n\npublic class Application implements StreamingApplication\n{\n\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    // This kafka input operator takes input from specified Kafka brokers.\n    KafkaSinglePortInputOperator kafkaInputOperator = dag.addOperator(\nkafkaInput\n, KafkaSinglePortInputOperator.class);\n\n    // Parses a json string tuple against a specified json schema and emits JSONObject.\n    JsonParser jsonParser = dag.addOperator(\nJsonParser\n, JsonParser.class);\n\n    // Filters the tuple as per specified condition by user.\n    FilterOperator filterOperator = dag.addOperator(\nfilter\n, new FilterOperator());\n\n    // Transforms the tuple value to user logic. Note logic may be modified.\n    TransformOperator transform = dag.addOperator(\ntransform\n, new TransformOperator());\n\n    // Format the transformed logic into JSON format.\n    JsonFormatter jsonFormatter = dag.addOperator(\nJsonFormatter\n, JsonFormatter.class);\n\n    // Use elasticsearch as a store.\n    ElasticSearchOutputOperator elasticSearchOutput = dag.addOperator(\nElasticStore\n, ElasticSearchOutputOperator.class);\n\n    // Now create the streams to complete the dag or application logic.\n    // Most of the operators are kept THREAD_LOCAL for optimizing the local resources. As latest elastic search supports java 1.8,\n    // so most of the clusters are not on java 1.8. If hadoop cluster is migrated to java 1.8, one can change the locality as\n    // per the requirement.\n    dag.addStream(\nKafkaToJsonParser\n, kafkaInputOperator.outputPort, jsonParser.in).setLocality(DAG.Locality.CONTAINER_LOCAL);\n    dag.addStream(\nJsonParserToFilter\n, jsonParser.out, filterOperator.input).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream(\nFilterToTransform\n, filterOperator.truePort, transform.input).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream(\nTransformToJsonFormatter\n, transform.output, jsonFormatter.in).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream(\nJsonToElasticStore\n, jsonFormatter.out, elasticSearchOutput.input);\n    dag.setAttribute(Context.DAGContext.METRICS_TRANSPORT, null);\n  }\n}", 
            "title": "Elasticsearch Output Operator"
        }, 
        {
            "location": "/operators/elasticsearch/#about-elasticsearch-output-operator", 
            "text": "Elasticsearch output operator along with Elastic Search engine provides data capability to store full-text search documents in real time.  The Elasticsearch operator embeds the transport client to simplify the connectivity and stores the documents to Elasticsearch cluster for a text search analysis.  You can configure the operator properties as per your business requirements and the execution aspect is handled by the Elasticsearch operator.  The Elasticsearch output operator provides the following advantages:   High performant and can process data in bulk operations.  The application is also highly performant and can process as fast as the network allows.  Ease to modify the configuration of Elasticsearch indexes through properties as most of the functionality is exported as tuning parameters as per requirements.  Flexible and configurable sharding (that is, it allows you to distribute and parallelize operations across shard) for Elasticsearch indexes based on APIs provided in operator for user provided document ID and index name.  Flexible in setting input source data type formats such as JSON.  Built-in metrics related to Elasticsearch, such as number of events, bytes written, bad events etc., are available.   This operator is available under  DT Plus  license.", 
            "title": "About ElasticSearch Output Operator"
        }, 
        {
            "location": "/operators/elasticsearch/#testing", 
            "text": "The Elasticsearch output operator is tested with the following components:   Apache Hadoop version 2.7.0  Java version 1.8.  Elastic search version 5.4", 
            "title": "Testing"
        }, 
        {
            "location": "/operators/elasticsearch/#workflow-of-the-operator", 
            "text": "Configure the Elasticsearch cluster node and port in the Elasticsearch operator.  Elasticsearch output operator receives enriched data from upstream operators which are stored on Elasticsearch cluster.", 
            "title": "Workflow of the Operator"
        }, 
        {
            "location": "/operators/elasticsearch/#ports", 
            "text": "The following ports are available on the Elasticsearch output operator:     Port Type  Port Name  Details      Input Port  Input Port  Receives incoming tuples from upstream operators.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/elasticsearch/#partitioning", 
            "text": "The ElasticSearch operator can be partitioned using the default partitioner provided by Apex.  Elasticsearch operator can be dynamically partitioned as stateless. For stateful, dynamic partitioning is not supported. Also see", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/elasticsearch/#configuring-elasticsearch-output-operator", 
            "text": "The following settings can be configured for Elasticsearch output operator:   Mandatory properties for connectivity   Optional properties for performance tuning", 
            "title": "Configuring ElasticSearch Output Operator"
        }, 
        {
            "location": "/operators/elasticsearch/#mandatory-elasticsearch-cluster-properties", 
            "text": "The following properties must be mandatorily updated in the  properties.xml  file for connectivity:     Property Name  Description      clusterName  Name of Elasticsearch cluster.    hostNames  Name of hosts along with the port numbers where Elasticsearch cluster is configured. For e.g: localhost:9300    indexName  Name of the Elasticsearch index.", 
            "title": "Mandatory Elasticsearch Cluster Properties "
        }, 
        {
            "location": "/operators/elasticsearch/#optional-properties-for-performance-tuning", 
            "text": "Operator can be optionally configured as per the performance and resources requirement. The following properties must be added to  properties.xml  file.     Property Name  Description      batchSize  This controls the amount of data that is grouped in a bulk for storing in the Elasticsearch store.    concurrentRequests  Sets the number of concurrent requests that are allowed to be executed parallelly.    bulkRequestTimeoutInSecs  Sets the timeout for bulk request in seconds. A value of zero indicates that only a single request can be executed. A value of one indicates that concurrent request can be executed while accumulating new bulk requests.    bulkRequestBatchSizeInMb  Sets the batch size in MB for bulk request.    checkClusterStatus  Checks the entire cluster status along with status for specified index.    flushAfterWindowCompletion  Pushes the bulk request to elastic store after every end window.    backoffPolicyRetryCount  Sets a retry count for back off policy in case of a failure.    backoffPolicyRetryTimeInMSecs  Sets the time interval for back off policy in case of a failure.", 
            "title": "Optional Properties for Performance Tuning "
        }, 
        {
            "location": "/operators/elasticsearch/#configuring-the-partitioning", 
            "text": "For partitioning, you must add the following property in the  properties.xml  file. For example, if you add Elasticsearch operator with the name  ElasticOutputStore  in the DAG, then this property creates four partitions of the operator when the application starts.  property \n      name dt.operator.ElasticOutputStore.attr.PARTITIONER /name \n      value com.datatorrent.common.partitioner.StatelessPartitioner(4) /name  /property", 
            "title": "Configuring the Partitioning"
        }, 
        {
            "location": "/operators/elasticsearch/#sample-application", 
            "text": "The following code illustrates how Elasticsearch output Operator can be used within an application.  This application reads JSON data from Kafka and parses it. After some transformation, this data is sent to Elasticsearch output Operator which is then stored on an Elasticsearch cluster.  public class Application implements StreamingApplication\n{\n\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    // This kafka input operator takes input from specified Kafka brokers.\n    KafkaSinglePortInputOperator kafkaInputOperator = dag.addOperator( kafkaInput , KafkaSinglePortInputOperator.class);\n\n    // Parses a json string tuple against a specified json schema and emits JSONObject.\n    JsonParser jsonParser = dag.addOperator( JsonParser , JsonParser.class);\n\n    // Filters the tuple as per specified condition by user.\n    FilterOperator filterOperator = dag.addOperator( filter , new FilterOperator());\n\n    // Transforms the tuple value to user logic. Note logic may be modified.\n    TransformOperator transform = dag.addOperator( transform , new TransformOperator());\n\n    // Format the transformed logic into JSON format.\n    JsonFormatter jsonFormatter = dag.addOperator( JsonFormatter , JsonFormatter.class);\n\n    // Use elasticsearch as a store.\n    ElasticSearchOutputOperator elasticSearchOutput = dag.addOperator( ElasticStore , ElasticSearchOutputOperator.class);\n\n    // Now create the streams to complete the dag or application logic.\n    // Most of the operators are kept THREAD_LOCAL for optimizing the local resources. As latest elastic search supports java 1.8,\n    // so most of the clusters are not on java 1.8. If hadoop cluster is migrated to java 1.8, one can change the locality as\n    // per the requirement.\n    dag.addStream( KafkaToJsonParser , kafkaInputOperator.outputPort, jsonParser.in).setLocality(DAG.Locality.CONTAINER_LOCAL);\n    dag.addStream( JsonParserToFilter , jsonParser.out, filterOperator.input).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream( FilterToTransform , filterOperator.truePort, transform.input).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream( TransformToJsonFormatter , transform.output, jsonFormatter.in).setLocality(DAG.Locality.THREAD_LOCAL);\n    dag.addStream( JsonToElasticStore , jsonFormatter.out, elasticSearchOutput.input);\n    dag.setAttribute(Context.DAGContext.METRICS_TRANSPORT, null);\n  }\n}", 
            "title": "Sample Application"
        }, 
        {
            "location": "/operators/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/block_reader/#ports", 
            "text": "blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/block_reader/#readercontext", 
            "text": "This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.", 
            "title": "readerContext"
        }, 
        {
            "location": "/operators/block_reader/#abstract-methods", 
            "text": "STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/block_reader/#auto-scalability", 
            "text": "Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .", 
            "title": "Auto-scalability"
        }, 
        {
            "location": "/operators/block_reader/#configuration", 
            "text": "maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#example-application", 
            "text": "This simple dag demonstrates how any concrete implementation of  AbstractFSBlockReader  can be plugged into an application.    In the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.  public class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( File-splitter , new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator( Block-reader , new LineReader());\n    Filter filter = dag.addOperator( Filter , new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator( Record-writer , new RecordOutputOperator());\n\n    dag.addStream( file-block metadata , input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream( records , blockReader.messages, filter.input);\n    dag.addStream( filtered-records , filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader String \n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort AbstractBlockReader.ReaderRecord String  output = new DefaultOutputPort ();\n    public final transient DefaultInputPort AbstractBlockReader.ReaderRecord String  input = new DefaultInputPort AbstractBlockReader.ReaderRecord String ()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord String  stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(),  . )) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator AbstractBlockReader.ReaderRecord String \n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}  Configuration to parallel partition block reader with its downstream operators.     property \n     name dt.operator.Filter.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property \n   property \n     name dt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property", 
            "title": "Example Application"
        }, 
        {
            "location": "/operators/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/block_reader/#readaheadlinereadercontext", 
            "text": "In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "ReadAheadLineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/block_reader/#linereadercontext", 
            "text": "This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "LineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/block_reader/#fixedbytesreadercontext", 
            "text": "This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.", 
            "title": "FixedBytesReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#configuration_1", 
            "text": "readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/operators/block_reader/#processstats", 
            "text": "The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.", 
            "title": "processStats "
        }, 
        {
            "location": "/operators/block_reader/#definepartitions", 
            "text": "Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "definePartitions"
        }, 
        {
            "location": "/operators/deduper/", 
            "text": "Dedup Operator\n\n\nThis document is intended as a guide for understanding and using the\nDedup operator/module.\n\n\nDedup - \u201cWhat\u201d in a Nutshell\n\n\nDedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.\n\n\n\n\nDedup - \u201cHow\u201d in a Nutshell\n\n\nIn order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.\n\n\n\n\nFollowing are the different components of the Deduper Operator\n\n\n\n\nDedup Operator\n - This is responsible for the overall functionality\n    of the operator. This in turn makes use of other components to\n    establish the end goal of deciding whether a tuple is a duplicate of\n    some earlier tuple, or is a unique tuple.\n\n\nBucket Store\n - This is responsible for storing the unique tuples as\n    supplied by the Deduper and storing them into Buckets in HDFS.\n\n\nBucket Manager\n - Since, all of the data cannot be stored in memory,\n    this component is responsible for loading and unloading of the\n    buckets to and from the memory as requested by the Deduper.\n\n\n\n\nThis was a very small introduction to the functioning of the Deduper.\nFollowing sections will go into more detail on each of the components.\n\n\nUse case - Basic Dedup\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.\n\n\nConsider an example schema and two sample tuples\n\n\n{Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nLet us assume that the Dedup Key is\n\n\n{Name, Phone}\n\n\n\n\nIn this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as\n\n\n{Phone, Email}\n\n\n\n\nthen in this case, the two are unique tuples as the keys of both tuples\ndo not match.\n\n\nUse case Details\n\n\nConsider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.\n\n\n{Name, Phone, Email, Date, City, Zip, Country}\n\n\n\n\nAlso consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.\n\n\nAs part of configuring the operator for this use case, we need to set\nthe following parameters:\n\n\nDedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to\n\n\n{Name,Email}\n\n\n\n\nThe above configuration is sufficient to resolve the use case.\n\n\n\n\nUse case - Dedup with Expiry\n\n\nMotivation\n\n\nThe Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.\n\n\nExpiry Key\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.\n\n\nUse case Details\n\n\nConsider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.\n\n\nThe expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.\n\n\nConfiguring the below parameters will solve the problem for this use\ncase:\n\n\n\n\nDedup Key\n - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.\n\n\nExpiry Key\n - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.\n\n\nExpiry Period\n - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.\n\n\n\n\nConfiguration of these parameters would resolve this use case.\n\n\nUse cases - Summary\n\n\n\n\nBasic Dedup\n - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.\n\n\nDedup with Expiry\n - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:\n\n\nTime based\n - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time\n\n\nWith respect to system time\n - Time progresses with system time. Any expiry criterions are executed with this notion of system time.\n\n\nWith respect to tuple time\n - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.\n\n\nAny Ordered Key\n - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.\n\n\nCategorical Key\n - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.\n\n\n\n\n\n\nTechnical Architecture\n\n\nBlock Diagram\n\n\n\n\nThe deduper has a single input port and multiple output ports.\n\n\n\n\ninput\n - This is the input port through which the tuples arrive at\n    the Deduper.\n\n\nunique\n\u00a0- This is the output port on which unique tuples are sent out\n    by the Deduper.\n\n\nduplicate\n\u00a0- This is the output port on which duplicate tuples are\n    sent out by the Deduper.\n\n\nexpired\n\u00a0- This is the output port on which expired tuples are sent\n    out by the Deduper.\n\n\nerror\n\u00a0- This is the output port on which the error tuples are sent\n    out by the Deduper.\n\n\n\n\n\n\nConcepts\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.\n\n\nExpiry Key\n\n\nA tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.\n\n\nAt the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.\n\n\nIn such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.\n\n\nIn order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.\n\n\n\n\nThe domain of the key must be ordered. Example - Timestamp field\n\n\nThe domain of the key must be categorical and sorted. Example - City\n    names grouped together\n\n\n\n\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.\n\n\nTime Points\n\n\nFor every dataset that the deduper processes, it maintains a set of time\npoints:\n\n\n\n\nLatest Point\n\u00a0- This is the maximum time point observed in all the\n    processed tuples.\n\n\nExpiry Point\n\u00a0- This is given by: \nExpiry Point = Latest Point -\n    Expiry Period\n\n\n\n\nThese points help the deduper to make decisions related to expiry of a\ntuple.\n\n\nExample - Expiry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTuple Id\n\n\nExpiry Key\n\n\n(Expiry Period = 10)\n\n\nLatest Point\n\n\nExpiry Point\n\n\nDecision for Tuple\n\n\n\n\n\n\n1\n\n\n10\n\n\n10\n\n\n1\n\n\nNot Expired\n\n\n\n\n\n\n2\n\n\n20\n\n\n20\n\n\n11\n\n\nNot Expired\n\n\n\n\n\n\n3\n\n\n25\n\n\n25\n\n\n16\n\n\nNot Expired\n\n\n\n\n\n\n4\n\n\n40\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n5\n\n\n21\n\n\n40\n\n\n31\n\n\nExpired\n\n\n\n\n\n\n6\n\n\n35\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n7\n\n\n45\n\n\n45\n\n\n36\n\n\nNot Expired\n\n\n\n\n\n\n8\n\n\n57\n\n\n57\n\n\n48\n\n\nNot Expired\n\n\n\n\n\n\n\n\n\nBuckets\n\n\nOne of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.\n\n\nBucket Span\n\n\nBucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.\n\n\nBucket Key\n\n\nBucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.\n\n\nWe define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.\n\n\nIn case of Basic Dedup:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with Expiry:\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nNumber of Buckets\n\n\nThe number of buckets can be given by\n\n\nNum Buckets = Expiry Period / Bucket Span.\n\n\n\n\nThis is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.\n\n\nBucket Index\n\n\nA Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).\n\n\nExample - Buckets\n\n\n\n\nAssumptions\n\n\nAssumption 1\n\n\nThis assumption is only applicable in case of Dedup with Expiry.\n\n\nFor any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2\n\n\n\n\nIn other words, there may never be\u00a0two tuples t1 and t2 such that\n\n\nTuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2\n\n\n\n\nThis assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.\n\n\nThe reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.\n\n\nFlow of a Tuple through Dedup Operator\n\n\nTuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.\n\n\nDeduper View\n\n\nA tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.\n\n\nIdentify Bucket Key\n\n\nIdentify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with expiry, we calculate the Bucket key as\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nCheck if tuple is Expired\n\n\nThis is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.\n\n\nif ( Latest Point - Expiry Key \n Expiry Point ) then Expired\n\n\n\n\nIf the tuple is expired, then put it out to the expired port.\n\n\nCheck if tuple is a Duplicate or Unique\n\n\nOnce a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed \nhere\n.\n\n\nDuplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the \nfirst\nstep\n. The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.\n\n\nCase I - Bucket available in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.\n\n\nCase II - Bucket not in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed\n\nhere\n. Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.\n\n\nHandling tuples after Buckets are loaded\n\n\nThe Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:\n\n\n\n\nThe bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.\n\n\nThe bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.\n\n\n\n\nAfter processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section \nCase II\n- Bucket not in memory\n. For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for \nBuckets available in memory\n.\n\n\nBucket Manager\n\n\nBucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.\n\n\nIdentify Bucket Index\n\n\nBucket index is discussed \nhere\n. Bucket index can be\ncalculated as follows:\n\n\nBucket Index = Requested Bucket Key % Number of Buckets,\n\n\n\n\nwhere Number of Buckets is as defined \nhere\n.\n\n\nRequest Bucket Load from Store\n\n\nOnce we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned \nhere\n. We detail the process of fetching the\nbucket data from the store in \nthis\n\u00a0section.\n\n\nBucket Eviction\n\n\nIt may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.\n\n\nBucket Store\n\n\nThe Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.\n\n\nData Format\n\n\nBucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.\n\n\n\n\nAll the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the \nData Structures\n\u00a0section.\n\n\nData Structures\n\n\nHDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.\n\n\n\n\nBucket Positions\n - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.\n\n\nWindow To Buckets\n - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.\n\n\nWindow to Timestamp\n - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.\n\n\n\n\nBucket Fetch\n\n\nFor fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Deduper"
        }, 
        {
            "location": "/operators/deduper/#dedup-operator", 
            "text": "This document is intended as a guide for understanding and using the\nDedup operator/module.", 
            "title": "Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#dedup-what-in-a-nutshell", 
            "text": "Dedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.", 
            "title": "Dedup - \u201cWhat\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#dedup-how-in-a-nutshell", 
            "text": "In order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.", 
            "title": "Dedup - \u201cHow\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#use-case-basic-dedup", 
            "text": "", 
            "title": "Use case - Basic Dedup"
        }, 
        {
            "location": "/operators/deduper/#dedup-key", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.  Consider an example schema and two sample tuples  {Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}  Let us assume that the Dedup Key is  {Name, Phone}  In this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as  {Phone, Email}  then in this case, the two are unique tuples as the keys of both tuples\ndo not match.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#use-case-details", 
            "text": "Consider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.  {Name, Phone, Email, Date, City, Zip, Country}  Also consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.  As part of configuring the operator for this use case, we need to set\nthe following parameters:  Dedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to  {Name,Email}  The above configuration is sufficient to resolve the use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-case-dedup-with-expiry", 
            "text": "", 
            "title": "Use case - Dedup with Expiry"
        }, 
        {
            "location": "/operators/deduper/#motivation", 
            "text": "The Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.", 
            "title": "Motivation"
        }, 
        {
            "location": "/operators/deduper/#expiry-key", 
            "text": "The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period", 
            "text": "The expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#use-case-details_1", 
            "text": "Consider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.  The expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.  Configuring the below parameters will solve the problem for this use\ncase:   Dedup Key  - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.  Expiry Key  - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.  Expiry Period  - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.   Configuration of these parameters would resolve this use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-cases-summary", 
            "text": "Basic Dedup  - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.  Dedup with Expiry  - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:  Time based  - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time  With respect to system time  - Time progresses with system time. Any expiry criterions are executed with this notion of system time.  With respect to tuple time  - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.  Any Ordered Key  - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.  Categorical Key  - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.", 
            "title": "Use cases - Summary"
        }, 
        {
            "location": "/operators/deduper/#technical-architecture", 
            "text": "", 
            "title": "Technical Architecture"
        }, 
        {
            "location": "/operators/deduper/#block-diagram", 
            "text": "", 
            "title": "Block Diagram"
        }, 
        {
            "location": "/operators/deduper/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/deduper/#dedup-key_1", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-key_1", 
            "text": "A tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.  At the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.  In such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.  In order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.  The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.   The domain of the key must be ordered. Example - Timestamp field  The domain of the key must be categorical and sorted. Example - City\n    names grouped together   This expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period_1", 
            "text": "The Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#time-points", 
            "text": "For every dataset that the deduper processes, it maintains a set of time\npoints:   Latest Point \u00a0- This is the maximum time point observed in all the\n    processed tuples.  Expiry Point \u00a0- This is given by:  Expiry Point = Latest Point -\n    Expiry Period   These points help the deduper to make decisions related to expiry of a\ntuple.", 
            "title": "Time Points"
        }, 
        {
            "location": "/operators/deduper/#example-expiry", 
            "text": "Tuple Id  Expiry Key  (Expiry Period = 10)  Latest Point  Expiry Point  Decision for Tuple    1  10  10  1  Not Expired    2  20  20  11  Not Expired    3  25  25  16  Not Expired    4  40  40  31  Not Expired    5  21  40  31  Expired    6  35  40  31  Not Expired    7  45  45  36  Not Expired    8  57  57  48  Not Expired", 
            "title": "Example - Expiry"
        }, 
        {
            "location": "/operators/deduper/#buckets", 
            "text": "One of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.", 
            "title": "Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-span", 
            "text": "Bucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.", 
            "title": "Bucket Span"
        }, 
        {
            "location": "/operators/deduper/#bucket-key", 
            "text": "Bucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.  We define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.  In case of Basic Dedup:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with Expiry:  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#number-of-buckets", 
            "text": "The number of buckets can be given by  Num Buckets = Expiry Period / Bucket Span.  This is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.", 
            "title": "Number of Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-index", 
            "text": "A Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).", 
            "title": "Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#example-buckets", 
            "text": "", 
            "title": "Example - Buckets"
        }, 
        {
            "location": "/operators/deduper/#assumptions", 
            "text": "", 
            "title": "Assumptions"
        }, 
        {
            "location": "/operators/deduper/#assumption-1", 
            "text": "This assumption is only applicable in case of Dedup with Expiry.  For any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2  In other words, there may never be\u00a0two tuples t1 and t2 such that  Tuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2  This assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.  The reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.", 
            "title": "Assumption 1"
        }, 
        {
            "location": "/operators/deduper/#flow-of-a-tuple-through-dedup-operator", 
            "text": "Tuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.", 
            "title": "Flow of a Tuple through Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#deduper-view", 
            "text": "A tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.", 
            "title": "Deduper View"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-key", 
            "text": "Identify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with expiry, we calculate the Bucket key as  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Identify Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-expired", 
            "text": "This is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.  if ( Latest Point - Expiry Key   Expiry Point ) then Expired  If the tuple is expired, then put it out to the expired port.", 
            "title": "Check if tuple is Expired"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-a-duplicate-or-unique", 
            "text": "Once a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed  here .  Duplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the  first\nstep . The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.", 
            "title": "Check if tuple is a Duplicate or Unique"
        }, 
        {
            "location": "/operators/deduper/#case-i-bucket-available-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.", 
            "title": "Case I - Bucket available in memory"
        }, 
        {
            "location": "/operators/deduper/#case-ii-bucket-not-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed here . Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.", 
            "title": "Case II - Bucket not in memory"
        }, 
        {
            "location": "/operators/deduper/#handling-tuples-after-buckets-are-loaded", 
            "text": "The Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:   The bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.  The bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.   After processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section  Case II\n- Bucket not in memory . For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for  Buckets available in memory .", 
            "title": "Handling tuples after Buckets are loaded"
        }, 
        {
            "location": "/operators/deduper/#bucket-manager", 
            "text": "Bucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.", 
            "title": "Bucket Manager"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-index", 
            "text": "Bucket index is discussed  here . Bucket index can be\ncalculated as follows:  Bucket Index = Requested Bucket Key % Number of Buckets,  where Number of Buckets is as defined  here .", 
            "title": "Identify Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#request-bucket-load-from-store", 
            "text": "Once we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned  here . We detail the process of fetching the\nbucket data from the store in  this \u00a0section.", 
            "title": "Request Bucket Load from Store"
        }, 
        {
            "location": "/operators/deduper/#bucket-eviction", 
            "text": "It may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.", 
            "title": "Bucket Eviction"
        }, 
        {
            "location": "/operators/deduper/#bucket-store", 
            "text": "The Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.", 
            "title": "Bucket Store"
        }, 
        {
            "location": "/operators/deduper/#data-format", 
            "text": "Bucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.   All the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the  Data Structures \u00a0section.", 
            "title": "Data Format"
        }, 
        {
            "location": "/operators/deduper/#data-structures", 
            "text": "HDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.   Bucket Positions  - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.  Window To Buckets  - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.  Window to Timestamp  - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.", 
            "title": "Data Structures"
        }, 
        {
            "location": "/operators/deduper/#bucket-fetch", 
            "text": "For fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Bucket Fetch"
        }, 
        {
            "location": "/operators/dimensions_computation/", 
            "text": "Dimensions Computation\n\n\nBig data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends.\n\nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as \nDimensions Computation\n.\n\n\nThis document provides an overview of \nDimensions Computation\n, as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.\n\n\nOverview\n\n\nWhat is Dimensions Computation?\n\n\nDimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.\n\n\nDimensions Computation\n provides a way for businesses to perform aggregations on configured numeric data. The \nDimensionsComputation\n operator works along with the \nDimensionStore\n operator, which provides the capability for applications to display historical data and trends.\n\n\nKey Concepts\n\n\nKey set\n\n\nA key set is a set of fields in the incoming tuple that is used to combine data for aggregation.\n\n\nValue set\n\n\nA value set is the set of fields in the incoming tuple on which \nAggregator\n(s)  are applied.\n\n\nAggregator\n\n\nAn aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.\n\n\nAggregates\n\n\nAggregates are objects containing the aggregated values for a configured value set and key combination.\n\n\nTime buckets\n\n\nTime buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12:\n01:34\n PM * and * 12:\n01:59\n PM * will be * 12:\n01:00\n PM\n. Similarly, for an hourly time bucket, floor time-value for * \n15\n:02:34 PM\n and * \n15\n:34:00 PM\n will be * \n15:00:00\n PM\n.\n\n\nAfter calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.\n\n\nCombinations\n\n\nCombinations indicate a group of the keys that are used for aggregate computations.\n\n\nIncremental \nAggregators\n\n\nIncremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,\n\n\nSUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value} \n {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}\n\n\n\n\nOn-the-fly \nAggregators\n (OTF Aggregators)\n\n\nOn-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,\n\n\nAVERAGE = {Current_SUM} / {Current_COUNT}\n\n\n\n\nDimensionsComputation operator\n\n\nThe DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.\n\n\nDimensionsStore operator\n\n\nThe DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.\n\n\nDimensions Computation use cases\n\n\nConsider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.\n\n\nArchitecture\n\n\nDimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given \nDAG\n is for a complete Dimensions Computation application.\n\n\nThe operators within Dimensions Computation are described in detail.\n\n\nDimensionsComputation\n\n\nThe DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the \nconfiguration\n, the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in \nDAG\n) for calculating cumulative aggregates.\n\n\nDimensionsStore\n\n\nThe DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:\n\n\nSUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2\n\n\n\n\nThe DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.\n\n\nQuery\n\n\nThe Query operator interfaces with the \npubsub server\n of \nDataTorrent Gateway\n. The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.\n\n\nQueryResult\n\n\nThe QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the \npubsub server\n for publishing to UI widgets.\n\n\nDAG \n\n\n\n\nDimensions Computation Configuration \n\n\nConfiguration Definitions\n\n\nThe configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:\n\n\nDimensions Computation Schema Configuration\n\n\nThe Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:\n\n\n{\nkeys\n:[{\nname\n:\npublisher\n,\ntype\n:\nstring\n,\nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n]},\n         {\nname\n:\nadvertiser\n,\ntype\n:\nstring\n,\nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n]},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n,\nenumValues\n:[\nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n]}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\npublisher\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nThe schema configuration is a JSON string that contains the following information:\n\n\n\n\n\n\nkeys:\n - This contains the set of keys derived from an input tuple. The \nname\n field stands for the name of the field from input tuple. The \ntype\n can be defined for individual keys. The probable values for individual keys can be provided using \nenumValues\n.\n\n\n\n\n\n\nvalues:\n - This contains the set of fields from an input tuple on which aggregates are calculated. The \nname\n field stands for the name of the field from an input tuple. The \ntype\n can be defined for individual keys. The \naggregators\n can be defined separately for individual values. Only configured aggregator functions are executed on values.\n\n\n\n\n\n\ntimeBuckets:\n - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are \n\"1m\", \"1h\", \"1d\"\n\n\n\n\n\n\ndimensions:\n - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in \ncombination\n with the JSON key. \nadditionalValues\n can be used for mentioning additional aggregators for any \nvalue\n. For example, \nimpressions:MIN\n indicates that for a given combination, calculate \"\nMIN\n\" for value \"\nimpression\n\" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.\n\n\n\n\n\n\nOperator Configurations\n\n\nOperator configurations is another set of configuration that can be used to configure individual operators.\n\n\nProperties\n\n\n\n\ndt.operator.QueryResult.topic:\n - This is the name of the topic on which UI widgets listen for results.\n\n\ndt.operator.Query.topic:\n - his is the name of the topic on which Query operator listen for queries.\n\n\ndt.operator.QueryResult.numRetries\n - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.\n\n\n\n\nAttributes\n\n\n\n\ndt.operator.DimensionsComputation.attr.PARTITIONER:\n - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the \nPartitioning\n section for details on partitioning.\n\n\ndt.operator.DimensionsComputation.attr.MEMORY_MB:\n - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.\n\n\ndt.operator.Store.attr.MEMORY_MB:\n - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.\n\n\ndt.port.*.attr.QUEUE_CAPACITY\n - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.\n\n\n\n\nVisualizing Dimensions Computation\n\n\nWhen Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.\n\n\nAnd example of a Dashboard UI Widget is as follows:\n\n\n\n\nCreating Dimensions Computation Application\n\n\nConsider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.\n\n\nSample publisher event\n\n\nAn event might look like this:\n\n\npublic class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}\n\n\n\n\nCreating an Application using out-of-the-box operators\n\n\nDimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:\n\n\n@ApplicationAnnotation(name=\nAdEventDemo\n)\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA = \nadsGenericEventSchema.json\n;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString(\neventSchema.json\n);\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver object.\n    AdEventReceiver receiver = dag.addOperator(\nEvent Receiver\n, AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map\nString, String\n keyToExpression = Maps.newHashMap();\n    keyToExpression.put(\nadvertiser\n, \ngetAdvertiser()\n);\n    keyToExpression.put(\nlocation\n, \ngetLocation()\n);\n    keyToExpression.put(\ntime\n, \ngetTime()\n);\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map\nString, String\n valueToExpression = Maps.newHashMap();\n    valueToExpression.put(\ncost\n, \ngetCost()\n);\n    valueToExpression.put(\nrevenue\n, \ngetRevenue()\n);\n    valueToExpression.put(\nimpressions\n, \ngetImpressions()\n);\n    valueToExpression.put(\nclicks\n, \ngetClicks()\n);\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\nStore\n, AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\ndataStorePath\n);\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator(\nQuery\n, PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\nQueryResult\n, PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream(\nQuery\n, wsIn.outputPort, store.query);\n    dag.addStream(\nQueryResult\n, store.queryResult, wsOut.input);\n    dag.addStream(\nInputStream\n, receiver.output, dimensions.input);\n    dag.addStream(\nDimensionalData\n, dimensions.output, store.input);\n  }\n}\n\n\n\n\nConfiguration for Sample Predefined Use Cases\n\n\nThe following configuration can be used for the predefined use case of the advertiser-publisher.\n\n\nDimensions Schema Configuration\n\n\n{\nkeys\n:[{\nname\n:\nadvertiser\n,\ntype\n:\nstring\n},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nOperator Configuration\n\n\nNote:\n This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.PARTITIONER\n/name\n\n    \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.MEMORY_MB\n/name\n\n    \nvalue\n16384\n/value\n\n  \n/property\n\n  \nproperty\n\n     \nname\ndt.port.*.attr.QUEUE_CAPACITY\n/name\n\n     \nvalue\n32000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Query.topic\n/name\n\n    \nvalue\nAdsEventQuery\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.QueryResult.topic\n/name\n\n    \nvalue\nAdsEventQueryResult\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.\n\n\nAdvanced Concepts\n\n\nPartitioning \n\n\nThe Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the \nproperties.xml\n file, or in the \ndt-site.xml\n file.\n\n\nproperty\n\n  \nname\ndt.operator.DimensionsComputations.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n\n/property\n\n\n\n\n\nThis adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of \n8\n is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.\n\n\nAlong with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:\n\n\nDimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n\n\n\nHere the unifier used is \nDimensionsComputationUnifierImpl\n which is an out-of-the-box operator present in the DataTorrent distribution.\n\n\nConclusion\n\n\nAggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Dimension Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation", 
            "text": "Big data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends. \nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as  Dimensions Computation .  This document provides an overview of  Dimensions Computation , as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.", 
            "title": "Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/operators/dimensions_computation/#what-is-dimensions-computation", 
            "text": "Dimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.  Dimensions Computation  provides a way for businesses to perform aggregations on configured numeric data. The  DimensionsComputation  operator works along with the  DimensionStore  operator, which provides the capability for applications to display historical data and trends.", 
            "title": "What is Dimensions Computation?"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-concepts", 
            "text": "", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-set", 
            "text": "A key set is a set of fields in the incoming tuple that is used to combine data for aggregation.", 
            "title": "Key set"
        }, 
        {
            "location": "/operators/dimensions_computation/#value-set", 
            "text": "A value set is the set of fields in the incoming tuple on which  Aggregator (s)  are applied.", 
            "title": "Value set"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregator", 
            "text": "An aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.", 
            "title": "Aggregator"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregates", 
            "text": "Aggregates are objects containing the aggregated values for a configured value set and key combination.", 
            "title": "Aggregates"
        }, 
        {
            "location": "/operators/dimensions_computation/#time-buckets", 
            "text": "Time buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12: 01:34  PM * and * 12: 01:59  PM * will be * 12: 01:00  PM . Similarly, for an hourly time bucket, floor time-value for *  15 :02:34 PM  and *  15 :34:00 PM  will be *  15:00:00  PM .  After calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.", 
            "title": "Time buckets"
        }, 
        {
            "location": "/operators/dimensions_computation/#combinations", 
            "text": "Combinations indicate a group of the keys that are used for aggregate computations.", 
            "title": "Combinations"
        }, 
        {
            "location": "/operators/dimensions_computation/#incremental-aggregators", 
            "text": "Incremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,  SUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value}   {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}", 
            "title": "Incremental Aggregators"
        }, 
        {
            "location": "/operators/dimensions_computation/#on-the-fly-aggregators-otf-aggregators", 
            "text": "On-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,  AVERAGE = {Current_SUM} / {Current_COUNT}", 
            "title": "On-the-fly Aggregators (OTF Aggregators)"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation-operator", 
            "text": "The DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.", 
            "title": "DimensionsComputation operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore-operator", 
            "text": "The DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.", 
            "title": "DimensionsStore operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-use-cases", 
            "text": "Consider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.", 
            "title": "Dimensions Computation use cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#architecture", 
            "text": "Dimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given  DAG  is for a complete Dimensions Computation application.  The operators within Dimensions Computation are described in detail.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation", 
            "text": "The DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the  configuration , the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in  DAG ) for calculating cumulative aggregates.", 
            "title": "DimensionsComputation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore", 
            "text": "The DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:  SUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2  The DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.", 
            "title": "DimensionsStore"
        }, 
        {
            "location": "/operators/dimensions_computation/#query", 
            "text": "The Query operator interfaces with the  pubsub server  of  DataTorrent Gateway . The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.", 
            "title": "Query"
        }, 
        {
            "location": "/operators/dimensions_computation/#queryresult", 
            "text": "The QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the  pubsub server  for publishing to UI widgets.", 
            "title": "QueryResult"
        }, 
        {
            "location": "/operators/dimensions_computation/#dag", 
            "text": "", 
            "title": "DAG "
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-configuration", 
            "text": "", 
            "title": "Dimensions Computation Configuration "
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-definitions", 
            "text": "The configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:", 
            "title": "Configuration Definitions"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-schema-configuration", 
            "text": "The Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:  { keys :[{ name : publisher , type : string , enumValues :[ twitter , facebook , yahoo ]},\n         { name : advertiser , type : string , enumValues :[ starbucks , safeway , mcdonalds ]},\n         { name : location , type : string , enumValues :[ N , LREC , SKY , AL , AK ]}],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : clicks , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : cost , type : double , aggregators :[ SUM , COUNT , AVG ]},\n   { name : revenue , type : double , aggregators :[ SUM , COUNT , AVG ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ publisher ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ advertiser , location ]},\n   { combination :[ publisher , location ]},\n   { combination :[ publisher , advertiser ]},\n   { combination :[ publisher , advertiser , location ]}]\n}  The schema configuration is a JSON string that contains the following information:    keys:  - This contains the set of keys derived from an input tuple. The  name  field stands for the name of the field from input tuple. The  type  can be defined for individual keys. The probable values for individual keys can be provided using  enumValues .    values:  - This contains the set of fields from an input tuple on which aggregates are calculated. The  name  field stands for the name of the field from an input tuple. The  type  can be defined for individual keys. The  aggregators  can be defined separately for individual values. Only configured aggregator functions are executed on values.    timeBuckets:  - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are  \"1m\", \"1h\", \"1d\"    dimensions:  - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in  combination  with the JSON key.  additionalValues  can be used for mentioning additional aggregators for any  value . For example,  impressions:MIN  indicates that for a given combination, calculate \" MIN \" for value \" impression \" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.", 
            "title": "Dimensions Computation Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configurations", 
            "text": "Operator configurations is another set of configuration that can be used to configure individual operators.", 
            "title": "Operator Configurations"
        }, 
        {
            "location": "/operators/dimensions_computation/#properties", 
            "text": "dt.operator.QueryResult.topic:  - This is the name of the topic on which UI widgets listen for results.  dt.operator.Query.topic:  - his is the name of the topic on which Query operator listen for queries.  dt.operator.QueryResult.numRetries  - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.", 
            "title": "Properties"
        }, 
        {
            "location": "/operators/dimensions_computation/#attributes", 
            "text": "dt.operator.DimensionsComputation.attr.PARTITIONER:  - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the  Partitioning  section for details on partitioning.  dt.operator.DimensionsComputation.attr.MEMORY_MB:  - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.  dt.operator.Store.attr.MEMORY_MB:  - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.  dt.port.*.attr.QUEUE_CAPACITY  - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.", 
            "title": "Attributes"
        }, 
        {
            "location": "/operators/dimensions_computation/#visualizing-dimensions-computation", 
            "text": "When Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.  And example of a Dashboard UI Widget is as follows:", 
            "title": "Visualizing Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-dimensions-computation-application", 
            "text": "Consider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.", 
            "title": "Creating Dimensions Computation Application"
        }, 
        {
            "location": "/operators/dimensions_computation/#sample-publisher-event", 
            "text": "An event might look like this:  public class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}", 
            "title": "Sample publisher event"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-an-application-using-out-of-the-box-operators", 
            "text": "Dimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:  @ApplicationAnnotation(name= AdEventDemo )\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA =  adsGenericEventSchema.json ;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString( eventSchema.json );\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver object.\n    AdEventReceiver receiver = dag.addOperator( Event Receiver , AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map String, String  keyToExpression = Maps.newHashMap();\n    keyToExpression.put( advertiser ,  getAdvertiser() );\n    keyToExpression.put( location ,  getLocation() );\n    keyToExpression.put( time ,  getTime() );\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map String, String  valueToExpression = Maps.newHashMap();\n    valueToExpression.put( cost ,  getCost() );\n    valueToExpression.put( revenue ,  getRevenue() );\n    valueToExpression.put( impressions ,  getImpressions() );\n    valueToExpression.put( clicks ,  getClicks() );\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator( Store , AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( dataStorePath );\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator( Query , PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator( QueryResult , PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream( Query , wsIn.outputPort, store.query);\n    dag.addStream( QueryResult , store.queryResult, wsOut.input);\n    dag.addStream( InputStream , receiver.output, dimensions.input);\n    dag.addStream( DimensionalData , dimensions.output, store.input);\n  }\n}", 
            "title": "Creating an Application using out-of-the-box operators"
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-for-sample-predefined-use-cases", 
            "text": "The following configuration can be used for the predefined use case of the advertiser-publisher.", 
            "title": "Configuration for Sample Predefined Use Cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-schema-configuration", 
            "text": "{ keys :[{ name : advertiser , type : string },\n         { name : location , type : string }],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : clicks , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : cost , type : double , aggregators :[ SUM , MAX , MIN ]},\n   { name : revenue , type : double , aggregators :[ SUM , MAX , MIN ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ]},\n   { combination :[ advertiser , location ]}]\n}", 
            "title": "Dimensions Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configuration", 
            "text": "Note:  This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.  ?xml version= 1.0  encoding= UTF-8  standalone= no ?  configuration \n   property \n     name dt.operator.DimensionsComputation.attr.PARTITIONER /name \n     value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value \n   /property \n   property \n     name dt.operator.DimensionsComputation.attr.MEMORY_MB /name \n     value 16384 /value \n   /property \n   property \n      name dt.port.*.attr.QUEUE_CAPACITY /name \n      value 32000 /value \n   /property \n   property \n     name dt.operator.Query.topic /name \n     value AdsEventQuery /value \n   /property \n   property \n     name dt.operator.QueryResult.topic /name \n     value AdsEventQueryResult /value \n   /property  /configuration   The above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.", 
            "title": "Operator Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#advanced-concepts", 
            "text": "", 
            "title": "Advanced Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#partitioning", 
            "text": "The Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the  properties.xml  file, or in the  dt-site.xml  file.  property \n   name dt.operator.DimensionsComputations.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value  /property   This adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of  8  is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.  Along with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:  DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());  Here the unifier used is  DimensionsComputationUnifierImpl  which is an out-of-the-box operator present in the DataTorrent distribution.", 
            "title": "Partitioning "
        }, 
        {
            "location": "/operators/dimensions_computation/#conclusion", 
            "text": "Aggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/operators/file_output/", 
            "text": "AbstractFileOutputOperator\n\n\nThe abstract file output operator in Apache Apex Malhar library \n \nAbstractFileOutputOperator\n writes streaming data to files. The main features of this operator are:\n\n\n\n\nPersisting data to files.\n\n\nAutomatic rotation of files based on:\n\n  a. maximum length of a file.\n\n  b. time-based rotation where time is specified using a count of application windows.\n\n\nFault-tolerance.\n\n\nCompression and encryption of data before it is persisted.\n\n\n\n\nIn this tutorial we will cover the details of the basic structure and implementation of all the above features in \nAbstractFileOutputOperator\n. Configuration items related to each feature are discussed as they are introduced in the section of that feature.\n\n\nPersisting data to files\n\n\nThe principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:\n\n\nfilePath\n: path specifying the directory where files are written.\n\n\nDifferent types of file system that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The file system instance which is used for creating streams is constructed from the \nfilePath\n URI.\n\n\nFileSystem.newInstance(new Path(filePath).toUri(), new Configuration())\n\n\n\n\nTuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.\n\n\nPorts\n\n\n\n\ninput\n: the input port on which tuples to be persisted are received.\n\n\n\n\nstreamsCache\n\n\nThis transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.\n\n\nstreamsCache\n is of type \ncom.google.common.cache.LoadingCache\n. A \nLoadingCache\n has an attached \nCacheLoader\n which is responsible to load value of a key when the key is not present in the cache. Details are explained here- \nCachesExplained\n.\n\n\nThe operator constructs this cache in \nsetup(...)\n. It is built with the following configuration items:\n\n\n\n\nmaxOpenFiles\n: maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit. \nDefault\n: 100\n\n\nexpireStreamAfterAcessMillis\n: expires streams after the specified duration has passed since the stream was last accessed. \nDefault\n: value of attribute- \nOperatorContext.SPIN_MILLIS\n.\n\n\n\n\nAn important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.\n\n\nCacheLoader\n\n\nstreamsCache\n is created with a \nCacheLoader\n that opens an \nFSDataOutputStream\n for a file which is not in the cache. The output stream is opened in either \nappend\n or \ncreate\n mode and the basic logic to determine this is explained by the simple diagram below.\n\n\n\n\nThis process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.\n\n\nFollowing are a few configuration items used for opening the streams:\n\n\n\n\nreplication\n: specifies the replication factor of the output files. \nDefault\n: \nfs.getDefaultReplication(new Path(filePath))\n\n\nfilePermission\n: specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command. \nDefault\n: 0777\n\n\n\n\nRemovalListener\n\n\nA \nGuava\n cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since \nstreamsCache\n is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to \nstreamsCache\n which closes the stream when it is evicted.\n\n\nsetup(OperatorContext context)\n\n\nDuring setup the following main tasks are performed:\n\n\n\n\nFileSystem instance is created.\n\n\nThe cache of streams is created.\n\n\nFiles are recovered (see Fault-tolerance section).\n\n\nStray part files are cleaned (see Automatic rotation section).\n\n\n\n\nprocessTuple(INPUT tuple)\n\n\nThe code snippet below highlights the basic steps of processing a tuple.\n\n\nprotected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}\n\n\n\n\nendWindow()\n\n\nIt should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's \nendWindow()\n.\n\n\nMap\nString, FSFilterStreamContext\n openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}\n\n\n\n\nFSFilterStreamContext\n will be explained with compression and encryption.\n\n\nteardown()\n\n\nWhen any operator in a DAG fails then the application master invokes \nteardown()\n for that operator and its downstream operators. In \nAbstractFileOutputOperator\n we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in \nteardown()\n.\n\n\nAutomatic rotation\n\n\nIn a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.\n\n\nTo help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.\n\n\nPart filename\n\n\nThe filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,\n\n\norigfile.partnum\n\n\nThis naming scheme can be changed by the user. It can be done so by overriding the following method\n\n\nprotected String getPartFileName(String fileName, int part)\n\n\n\n\nThis method is passed the original filename and part number as arguments and should return the part filename.\n\n\nMechanisms\n\n\nThe user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.\n\n\nSize Based\n\n\nWith size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property\n\n\nmaxLength\n\n\nLike any other property this can be set in Java application code or in the property file.\n\n\nTime Based\n\n\nIn time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property\n\n\nrotationWindows\n\n\nsetup(OperatorContext context)\n\n\nWhen an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process\n\n\n\n\nFault-tolerance\n\n\nThere are two issues that should be addressed in order to make the operator fault-tolerant:\n\n\n\n\n\n\nThe operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.\n\n\n\n\n\n\nWhile writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:  \n\n\n\n\nalwaysWriteToTmp\n: enables/disables writing to a temporary file. \nDefault\n: true.\n\n\n\n\nMost of the complexity in the code comes from making this operator fault-tolerant.\n\n\nCheckpointed states needed for fault-tolerance\n\n\n\n\n\n\nendOffsets\n: contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator \nsetup(...)\n and is also used while loading a stream to find out if the operator has seen a file before.\n\n\n\n\n\n\nfileNameToTmpName\n: contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.\n\n\n\n\n\n\nfinalizedFiles\n: contains set of files which were requested to be finalized per window id.\n\n\n\n\n\n\nfinalizedPart\n: contains the latest \npart\n of each file which was requested to be finalized.\n\n\n\n\n\n\nThe use of \nfinalizedFiles\n and \nfinalizedPart\n are explained in detail under \nrequestFinalize(...)\n method.\n\n\nRecovering files\n\n\nWhen the operator is re-deployed, it checks in its \nsetup(...)\n method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the \nendOffsets\n. When it doesn't the operator truncates the file.\n\n\nFor example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.\n\n\nrequestFinalize(String fileName)\n\n\nWhen the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is referred to as \nfinalization\n of files and the method allows the user code to specify when a file is ready for finalization.\n\n\nIn this method, the requested file (or in the case of rotation \n all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to \nfinalizedFiles\n state and updating \nfinalizedPart\n.\n\n\nThe process of \nfinalization\n of all the files which were requested till the window \nw\n is deferred till window \nw\n is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.\n\n\nWhen rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "File Output"
        }, 
        {
            "location": "/operators/file_output/#abstractfileoutputoperator", 
            "text": "The abstract file output operator in Apache Apex Malhar library    AbstractFileOutputOperator  writes streaming data to files. The main features of this operator are:   Persisting data to files.  Automatic rotation of files based on: \n  a. maximum length of a file. \n  b. time-based rotation where time is specified using a count of application windows.  Fault-tolerance.  Compression and encryption of data before it is persisted.   In this tutorial we will cover the details of the basic structure and implementation of all the above features in  AbstractFileOutputOperator . Configuration items related to each feature are discussed as they are introduced in the section of that feature.", 
            "title": "AbstractFileOutputOperator"
        }, 
        {
            "location": "/operators/file_output/#persisting-data-to-files", 
            "text": "The principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:  filePath : path specifying the directory where files are written.  Different types of file system that are implementations of  org.apache.hadoop.fs.FileSystem  are supported. The file system instance which is used for creating streams is constructed from the  filePath  URI.  FileSystem.newInstance(new Path(filePath).toUri(), new Configuration())  Tuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.", 
            "title": "Persisting data to files"
        }, 
        {
            "location": "/operators/file_output/#ports", 
            "text": "input : the input port on which tuples to be persisted are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_output/#streamscache", 
            "text": "This transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.  streamsCache  is of type  com.google.common.cache.LoadingCache . A  LoadingCache  has an attached  CacheLoader  which is responsible to load value of a key when the key is not present in the cache. Details are explained here-  CachesExplained .  The operator constructs this cache in  setup(...) . It is built with the following configuration items:   maxOpenFiles : maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit.  Default : 100  expireStreamAfterAcessMillis : expires streams after the specified duration has passed since the stream was last accessed.  Default : value of attribute-  OperatorContext.SPIN_MILLIS .   An important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.", 
            "title": "streamsCache"
        }, 
        {
            "location": "/operators/file_output/#cacheloader", 
            "text": "streamsCache  is created with a  CacheLoader  that opens an  FSDataOutputStream  for a file which is not in the cache. The output stream is opened in either  append  or  create  mode and the basic logic to determine this is explained by the simple diagram below.   This process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.  Following are a few configuration items used for opening the streams:   replication : specifies the replication factor of the output files.  Default :  fs.getDefaultReplication(new Path(filePath))  filePermission : specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command.  Default : 0777", 
            "title": "CacheLoader"
        }, 
        {
            "location": "/operators/file_output/#removallistener", 
            "text": "A  Guava  cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since  streamsCache  is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to  streamsCache  which closes the stream when it is evicted.", 
            "title": "RemovalListener"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context", 
            "text": "During setup the following main tasks are performed:   FileSystem instance is created.  The cache of streams is created.  Files are recovered (see Fault-tolerance section).  Stray part files are cleaned (see Automatic rotation section).", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#automatic-rotation", 
            "text": "In a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.  To help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.", 
            "title": "Automatic rotation"
        }, 
        {
            "location": "/operators/file_output/#part-filename", 
            "text": "The filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,  origfile.partnum  This naming scheme can be changed by the user. It can be done so by overriding the following method  protected String getPartFileName(String fileName, int part)  This method is passed the original filename and part number as arguments and should return the part filename.", 
            "title": "Part filename"
        }, 
        {
            "location": "/operators/file_output/#mechanisms", 
            "text": "The user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.", 
            "title": "Mechanisms"
        }, 
        {
            "location": "/operators/file_output/#size-based", 
            "text": "With size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property  maxLength  Like any other property this can be set in Java application code or in the property file.", 
            "title": "Size Based"
        }, 
        {
            "location": "/operators/file_output/#time-based", 
            "text": "In time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property  rotationWindows", 
            "title": "Time Based"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context_1", 
            "text": "When an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#fault-tolerance", 
            "text": "There are two issues that should be addressed in order to make the operator fault-tolerant:    The operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.    While writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:     alwaysWriteToTmp : enables/disables writing to a temporary file.  Default : true.   Most of the complexity in the code comes from making this operator fault-tolerant.", 
            "title": "Fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#checkpointed-states-needed-for-fault-tolerance", 
            "text": "endOffsets : contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator  setup(...)  and is also used while loading a stream to find out if the operator has seen a file before.    fileNameToTmpName : contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.    finalizedFiles : contains set of files which were requested to be finalized per window id.    finalizedPart : contains the latest  part  of each file which was requested to be finalized.    The use of  finalizedFiles  and  finalizedPart  are explained in detail under  requestFinalize(...)  method.", 
            "title": "Checkpointed states needed for fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#recovering-files", 
            "text": "When the operator is re-deployed, it checks in its  setup(...)  method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the  endOffsets . When it doesn't the operator truncates the file.  For example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.", 
            "title": "Recovering files"
        }, 
        {
            "location": "/operators/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/file_splitter/#ports", 
            "text": "Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#abstract-methods", 
            "text": "FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/file_splitter/#configuration", 
            "text": "blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/file_splitter/#example-application", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_1", 
            "text": "Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_1", 
            "text": "file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/file_splitter/#example-application_1", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_2", 
            "text": "Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_2", 
            "text": "scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/hdht/", 
            "text": "HDHT\n\n\nSome applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.\n\n\nThe programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are\n\n\n\n\nEmbedded, Hadoop native solution. Does not require install/manage of other services.\n\n\nIntegrates with Apex check-pointing mechanism to provide exactly once guarantee.\n\n\n\n\nThis document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.\n\n\nConcepts\n\n\nWrite Ahead Log\n\n\nEach tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.\n\n\nUncommitted Cache\n\n\nUncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.\n\n\nData Files\n\n\nHDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n- \nTFile\n: Files are written in hadoop \nTfile\n format\n- \nDTFile\n: Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n- \nHFile\n : Files are written in HBase format.\n\n\nMetadata\n\n\nMetadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.\n\n\nPartition (HDHT Bucket)\n\n\nBy default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.\n\n\nInterface\n\n\nHDHT supports two basic operations \nget\n and \nput\n, they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.\n\n\nOperations supported by HDHT are.\n\n\nbyte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);\n\n\n\n\nAll methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.\n\n\n\n\nput\n store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.\n\n\ngetUncommitted\n does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.\n\n\nget\n does a lookup in persisted storage file and return the data. \nNote\n \nget\n does not\n return data from uncommitted cache.\n\n\n\n\nArchitecture\n\n\nPlease refer to \nHDHT Blog\n\nfor the architecture of HDHT.\n\n\nCodec\n\n\nHDHT provides an abstract implementation \nAbstractSinglePortHDHTWriter\n, which uses a user defined codec for serialization and de-serialization.\n\n\npublic interface HDHTCodec\nEVENT\n extends StreamCodec\nEVENT\n\n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}\n\n\n\n\nIt has following methods\n- \ngetKeyBytes\n Return key as a byte array from the event.\n- \ngetValueBytes\n Return value as a byte array from event.\n- \nfromKeyValue\n HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n- \ngetPartition\n This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.\n\n\nConfiguration\n\n\nfileStore\n\n\nThis setting determines the format in which files are written. Default is DTFileImpl.\n\n\nbasePath\n\n\nLocation in HDFS where data files are stored. This is required configuration parameter.\n\n\nProperty File Syntax\n\n\n   \nproperty\n\n     \nname\ndt.operator.{name}.store.basePath\n/name\n\n     \nvalue\n/home/hdhtdatadir\n/value\n\n  \n/property\n\n\n\n\n\nJava API.\n\n\n/* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath(\nHDHTdata\n);\nstore.setFileStore(hdsFile);\n\n\n\n\nmaxFileSize\n\n\nSize of each file. Default value is 134217728134217728 (i.e 128MB).\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.maxFileSize\n/name\n\n   \nvalue\n{value in bytes}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxFileSize(64 * 1024 * 1024);\n\n\n\n\nflushSize\n\n\nHDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.flushSize\n/name\n\n  \nvalue\n{number}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushSize(1000000);\n\n\n\n\nflushIntervalCount\n\n\nThis setting will force data flush even if number of tuples are below \nflushSize\n. Default value is 120 windows.\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.flushIntervalCount\n/name\n\n   \nvalue\n{number of windows}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushIntervalCount(120);\n\n\n\n\nmaxWalFileSize\n\n\nWrite Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.maxWalFileSize\n/name\n\n  \nvalue\n{value in bytes}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxWalFileSize(128 * 1024 * 1024);\n\n\n\n\nExample\n\n\nThis is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.\n\n\nStore Operator\n\n\nHDHT provides following abstract implementations\n\n \nHDHTReader\n - This class implements functionality required for \nget\n, It access HDHT\nin read-only mode.\n\n \nHDHTWriter\n - This class extends functionality of \nHDHTReader\n by adding support for \nput\n,\n  this class also maintains \nuncommitted cache\n, which can be accessed through \ngetUncommitted\n\n  method.\n* \nAbstractSinglePortHDHTWriter\n - This class extends from \nHDHTWriter\n and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.\n\n\nFor this example we will use \nAbstractSinglePortHDHTWriter\n for the store, we need to\nimplement codec which is used by \nAbstractSinglePortHDHTWriter\n for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.\n\n\nImplement a Codec\n\n\n  public static class StringCodec extends KryoSerializableStreamCodec\nString\n implements HDHTCodec\nString\n {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }\n\n\n\n\nThe store operator is implemented as shown below, we will need to provide an implementation of\n\ngetCodec\n, and override \nprocessEvent\n to change default behavior of storing data in HDHT\ndirectly.\n\n\npublic class HDHTWordCounter extends AbstractSinglePortHDHTWriter\nString\n\n{\n  public transient DefaultOutputPort\nPair\nString, Long\n out = new DefaultOutputPort\n();\n  private transient HashMap\nString, AtomicLong\n cache;\n\n  @Override\n  protected HDHTCodec\nString\n getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry\nString, AtomicLong\n entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair\n(word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap\n();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException(\nUnable to flush to HDHT\n);\n    }\n  }\n}\n\n\n\n\nSample Application.\n\n\n@ApplicationAnnotation(name=\nHDHTWordCount\n)\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator(\nReader\n, new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory(\n/home/data\n);\n\n    WordSplitter splitter = dag.addOperator(\nSplitter\n, new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator(\nStore\n, new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator(\nConsole\n, new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\nWALBenchMarkDir\n);\n    store.setFileStore(hdsFile);\n\n    dag.addStream(\nlines\n, gen.output, splitter.input);\n    dag.addStream(\ns1\n, splitter.output, store.input);\n    dag.addStream(\ns2\n, store.out, console.input);\n  }\n}\n\n\n\n\nPerformance tuning\n\n\nEffect of frequent WAL flushes.\n\n\nHDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.\n\n\nApplication Level Cache\n\n\nMaintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.\n\n\nKey design\n\n\nHDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.\n\n\nFor keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is \nS\n bytes and flush is\ntriggered every \nT\n seconds, and HDFS write bandwidth per container is \nB\n bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most \n(T / (S/B))\n files. \n\n\nIf S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.\n\n\nFile Backend\n\n\nPrefer \nDTFile\n backend implementation over \nTFile\n backend implementation if you are going to issue frequent \nget\n operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.\n\n\nLimitations\n\n\n\n\nDynamic Partitioning is not supported yet.\n\n\nWrite to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#hdht", 
            "text": "Some applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.  The programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are   Embedded, Hadoop native solution. Does not require install/manage of other services.  Integrates with Apex check-pointing mechanism to provide exactly once guarantee.   This document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/hdht/#write-ahead-log", 
            "text": "Each tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.", 
            "title": "Write Ahead Log"
        }, 
        {
            "location": "/operators/hdht/#uncommitted-cache", 
            "text": "Uncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.", 
            "title": "Uncommitted Cache"
        }, 
        {
            "location": "/operators/hdht/#data-files", 
            "text": "HDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n-  TFile : Files are written in hadoop  Tfile  format\n-  DTFile : Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n-  HFile  : Files are written in HBase format.", 
            "title": "Data Files"
        }, 
        {
            "location": "/operators/hdht/#metadata", 
            "text": "Metadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.", 
            "title": "Metadata"
        }, 
        {
            "location": "/operators/hdht/#partition-hdht-bucket", 
            "text": "By default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.", 
            "title": "Partition (HDHT Bucket)"
        }, 
        {
            "location": "/operators/hdht/#interface", 
            "text": "HDHT supports two basic operations  get  and  put , they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.  Operations supported by HDHT are.  byte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);  All methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.   put  store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.  getUncommitted  does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.  get  does a lookup in persisted storage file and return the data.  Note   get  does not\n return data from uncommitted cache.", 
            "title": "Interface"
        }, 
        {
            "location": "/operators/hdht/#architecture", 
            "text": "Please refer to  HDHT Blog \nfor the architecture of HDHT.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/hdht/#codec", 
            "text": "HDHT provides an abstract implementation  AbstractSinglePortHDHTWriter , which uses a user defined codec for serialization and de-serialization.  public interface HDHTCodec EVENT  extends StreamCodec EVENT \n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}  It has following methods\n-  getKeyBytes  Return key as a byte array from the event.\n-  getValueBytes  Return value as a byte array from event.\n-  fromKeyValue  HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n-  getPartition  This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.", 
            "title": "Codec"
        }, 
        {
            "location": "/operators/hdht/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/hdht/#filestore", 
            "text": "This setting determines the format in which files are written. Default is DTFileImpl.", 
            "title": "fileStore"
        }, 
        {
            "location": "/operators/hdht/#basepath", 
            "text": "Location in HDFS where data files are stored. This is required configuration parameter.  Property File Syntax      property \n      name dt.operator.{name}.store.basePath /name \n      value /home/hdhtdatadir /value \n   /property   Java API.  /* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath( HDHTdata );\nstore.setFileStore(hdsFile);", 
            "title": "basePath"
        }, 
        {
            "location": "/operators/hdht/#maxfilesize", 
            "text": "Size of each file. Default value is 134217728134217728 (i.e 128MB).  Property File Syntax    property \n    name dt.operator.{name}.maxFileSize /name \n    value {value in bytes} /value \n  /property   Java API.  store.setMaxFileSize(64 * 1024 * 1024);", 
            "title": "maxFileSize"
        }, 
        {
            "location": "/operators/hdht/#flushsize", 
            "text": "HDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.  Property File Syntax  property \n   name dt.operator.{name}.flushSize /name \n   value {number} /value  /property   Java API.  store.setFlushSize(1000000);", 
            "title": "flushSize"
        }, 
        {
            "location": "/operators/hdht/#flushintervalcount", 
            "text": "This setting will force data flush even if number of tuples are below  flushSize . Default value is 120 windows.  Property File Syntax    property \n    name dt.operator.{name}.flushIntervalCount /name \n    value {number of windows} /value \n  /property   Java API.  store.setFlushIntervalCount(120);", 
            "title": "flushIntervalCount"
        }, 
        {
            "location": "/operators/hdht/#maxwalfilesize", 
            "text": "Write Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)  Property File Syntax  property \n   name dt.operator.{name}.maxWalFileSize /name \n   value {value in bytes} /value  /property   Java API.  store.setMaxWalFileSize(128 * 1024 * 1024);", 
            "title": "maxWalFileSize"
        }, 
        {
            "location": "/operators/hdht/#example", 
            "text": "This is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.", 
            "title": "Example"
        }, 
        {
            "location": "/operators/hdht/#store-operator", 
            "text": "HDHT provides following abstract implementations   HDHTReader  - This class implements functionality required for  get , It access HDHT\nin read-only mode.   HDHTWriter  - This class extends functionality of  HDHTReader  by adding support for  put ,\n  this class also maintains  uncommitted cache , which can be accessed through  getUncommitted \n  method.\n*  AbstractSinglePortHDHTWriter  - This class extends from  HDHTWriter  and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.  For this example we will use  AbstractSinglePortHDHTWriter  for the store, we need to\nimplement codec which is used by  AbstractSinglePortHDHTWriter  for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.", 
            "title": "Store Operator"
        }, 
        {
            "location": "/operators/hdht/#implement-a-codec", 
            "text": "public static class StringCodec extends KryoSerializableStreamCodec String  implements HDHTCodec String  {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }  The store operator is implemented as shown below, we will need to provide an implementation of getCodec , and override  processEvent  to change default behavior of storing data in HDHT\ndirectly.  public class HDHTWordCounter extends AbstractSinglePortHDHTWriter String \n{\n  public transient DefaultOutputPort Pair String, Long  out = new DefaultOutputPort ();\n  private transient HashMap String, AtomicLong  cache;\n\n  @Override\n  protected HDHTCodec String  getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry String, AtomicLong  entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair (word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap ();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException( Unable to flush to HDHT );\n    }\n  }\n}  Sample Application.  @ApplicationAnnotation(name= HDHTWordCount )\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator( Reader , new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory( /home/data );\n\n    WordSplitter splitter = dag.addOperator( Splitter , new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator( Store , new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator( Console , new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( WALBenchMarkDir );\n    store.setFileStore(hdsFile);\n\n    dag.addStream( lines , gen.output, splitter.input);\n    dag.addStream( s1 , splitter.output, store.input);\n    dag.addStream( s2 , store.out, console.input);\n  }\n}", 
            "title": "Implement a Codec"
        }, 
        {
            "location": "/operators/hdht/#performance-tuning", 
            "text": "", 
            "title": "Performance tuning"
        }, 
        {
            "location": "/operators/hdht/#effect-of-frequent-wal-flushes", 
            "text": "HDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.", 
            "title": "Effect of frequent WAL flushes."
        }, 
        {
            "location": "/operators/hdht/#application-level-cache", 
            "text": "Maintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.", 
            "title": "Application Level Cache"
        }, 
        {
            "location": "/operators/hdht/#key-design", 
            "text": "HDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.  For keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is  S  bytes and flush is\ntriggered every  T  seconds, and HDFS write bandwidth per container is  B  bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most  (T / (S/B))  files.   If S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.", 
            "title": "Key design"
        }, 
        {
            "location": "/operators/hdht/#file-backend", 
            "text": "Prefer  DTFile  backend implementation over  TFile  backend implementation if you are going to issue frequent  get  operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.", 
            "title": "File Backend"
        }, 
        {
            "location": "/operators/hdht/#limitations", 
            "text": "Dynamic Partitioning is not supported yet.  Write to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "Limitations"
        }, 
        {
            "location": "/operators/kafkaInputOperator/", 
            "text": "Kafka Input Operator\n\n\nIntroduction\n\n\nThis is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is a fault-tolerant and scalable Malhar Operator.\n\n\nWhy is it needed ?\n\n\nKafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.\n\n\nAbstractKafkaInputOperator\n\n\nThis is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.\n\n\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\nmaxTuplesPerWindow\n\n\nControls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE \n\n\n\n\n\n\nidempotentStorageManager\n\n\nThis is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager.\nNoopIdempotentStorageManager\n\n\n\n\n\n\nstrategy\n\n\nOperator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.\n\n\nONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.\n\n\nONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE\n\n\n\n\n\n\nmsgRateUpperBound\n\n\nMaximum messages upper bound. Operator repartitions when the \nmsgProcessedPS\n exceeds this bound. \nmsgProcessedPS\n is the average number of messages processed per second by this operator.\n\n\n\n\n\n\nbyteRateUpperBound\n\n\nMaximum bytes upper bound. Operator repartitions when the \nbytesPS\n exceeds this bound. \nbytesPS\n is the average number of bytes processed per second by this operator.\n\n\n\n\n\n\n\n\noffsetManager\n\n\nThis is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)\n\n\n\n\n\n\nrepartitionInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds\n\n\n\n\n\n\nrepartitionCheckInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds\n\n\n\n\n\n\ninitialPartitionCount\n\n\nWhen the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1\n\n\n\n\n\n\nconsumer\n\n\nThis is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\nvoid emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.\n\n\nKafkaConsumer\n\n\nThis is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.\n\n\nPre-requisites\n\n\nThis operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nzookeeper\n\n\nString\n\n\n\n\nSpecifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere\n\n\nc1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster\n\n\n\n\n\n\ncacheSize\n\n\nint\n\n\n1024\n\n\nMaximum of buffered messages hold in memory.\n\n\n\n\n\n\ntopic\n\n\nString\n\n\ndefault_topic\n\n\nIndicates the name of the topic.\n\n\n\n\n\n\ninitialOffset\n\n\nString\n\n\nlatest\n\n\nIndicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.\n\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\n\n\nvoid commitOffset(): Commit the offsets at checkpoint.\n\n\nMap \nKafkaPartition, Long\n getCurrentOffsets(): Return the current\n    offset status.\n\n\nresetPartitionsAndOffset(Set \nKafkaPartition\n partitionIds,\n    Map \nKafkaPartition, Long\n startOffset): Reset the partitions with\n    partitionIds and offsets with startOffset.\n\n\n\n\nConfiguration Parameters\u00a0for SimpleKafkaConsumer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nbufferSize\n\n\nint\n\n\n1 MB\n\n\nSpecifies the maximum total size of messages for each fetch request.\n\n\n\n\n\n\nmetadataRefreshInterval\n\n\nint\n\n\n30 Seconds\n\n\nInterval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.\n\n\n\n\n\n\nmetadataRefreshRetryLimit\n\n\nint\n\n\n-1\n\n\nSpecifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.\n\n\n\n\n\n\n\n\n\nOffsetManager\n\n\nThis is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\npublic interface OffsetManager\n{\n  public Map\nKafkaPartition, Long\n loadInitialOffsets();\n  public void updateOffsets(Map\nKafkaPartition, Long\n offsetsOfPartitions);\n}\n\n\n\n\nAbstract Methods\n\n\nMap \nKafkaPartition, Long\n loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.\n\n\nupdateOffsets(Map \nKafkaPartition, Long\n offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.\n\n\nPartitioning\n\n\nThe logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.\n\n\nResponse processStats(BatchedOperatorStats stats)\n\n\nThe application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.\n\n\nDefinePartitions\n\n\nBased on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.\n\n\nAbstractSinglePortKafkaInputOperator\n\n\nThis class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.\n\n\nPorts\n\n\noutputPort \nT\n: Tuples extracted from Kafka messages are emitted through\nthis port.\n\n\nAbstract Methods\n\n\nT getTuple(Message msg) : Converts the Kafka message to tuple.\n\n\nConcrete Classes\n\n\n\n\n\n\nKafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.\n\n\n\n\n\n\nKafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.\n\n\n\n\n\n\nApplication Example\n\n\nThis section builds an Apex application using Kafka input operator.\nBelow is the code snippet:\n\n\n@ApplicationAnnotation(name = \nKafkaApp\n)\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator(\nMessageReader\n, new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator(\nOutput\n, new ConsoleOutputOperator());\n\n  dag.addStream(\nMessageData\n, input.outputPort, output.input);\n}\n}\n\n\n\n\nBelow is the configuration for using the earliest offset, \u201ctest\u201d as the topic name and\n\u201clocalhost:2181\u201d as the zookeeper forum:\n\n\nproperty\n\n  \nname\ndt.operator.MessageReader.prop.initialOffset\n/name\n\n  \nvalue\nearliest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.topic\n/name\n\n\nvalue\ntest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.zookeeper\n/nam\n\n\nvalue\nlocalhost:2181\n/value\n\n\n/property\n\n\n\n\n\nPlease note that \nMessageReader\n is the string passed as the first argument to the\n\naddOperator()\n call. The above stanza sets these parameters for this operator\nregardless of the application it resides in; if you want to set them on a\nper-application basis, you can use this instead (where \nKafkaApp\n is the name of\nthe application):\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.initialOffset\n/name\n\n  \nvalue\nearliest\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.topic\n/name\n\n  \nvalue\ntest-topic\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\ndt.application.KafkaApp.operator.MessageReader.prop.zookeeper\n/name\n\n  \nvalue\nnode21:2181\n/value\n\n\n/property\n\n\n\n\n\nUsing KafkaInput Operator in a Kerberised Environment\n\n\nYou can use the KafkaInput operator in a Kerberised environment using one of the following methods: \n\n Operator Properties Method\n\n Application Package Method\n* Command Line Method\n\n\nOperator Properties Method\n\n\nTo use this method, do the following:\n\n\n\n\nConfigure and use the current operator properties by copying the \nkeytab\n and \njaas\n file on all the kafka nodes. \n\n\nAfter the \nclient_jaas\n file and \nkeytab\n are available on all the nodes, add the operator properties as shown below and provide the corresponding environment properties:\n\n\n\n\nproperty\n\n  \nname\ndt.operator.kafkaOutput.prop.properties(security.protocol)\n/name\n\n  \nvalue\nSASL_PLAINTEXT\n/value\n\n\n/property\n\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(sasl.kerberos.service.name)\n/name\n\n    \nvalue\nkafka\n/value\n\n  \n/property\n\n\n   \nproperty\n\n    \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n    \nvalue\n-Djava.security.auth.login.config=/path/to/kafka_client_jaas.con\n/value\n\n  \n/property\n\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.topic\n/name\n\n    \nvalue\nlat_long_lookup\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(bootstrap.servers)\n/name\n\n    \nvalue\n${apex.app-param.server-kafka-input-brokers}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(key.serializer)\n/name\n\n    \nvalue\norg.apache.kafka.common.serialization.StringSerializer\n/value\n\n  \n/property\n\n\n\n\n\nThe \nKafka_client_jaas.con\n file should have content similar to the following config:\n\n\nKafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    storeKey=true\n    keyTab=\n/etc/security/keytabs/kafka_client.keytab\n\n    principal=\nkafka-client-1@EXAMPLE.COM\n;\n};\n\n\n\n\nApplication Package Method\n\n\nTo use this method, do the following to ensure that the \nkeytab\n and \njaas\n files are available with the application package:\n\n\n\n\nInclude the following property in \napppackage.xml\n file:\n\n\n\n\nfileSet\n\n \ndirectory\n${basedir}/\n/directory\n\n \noutputDirectory\n/lib\n/outputDirectory\n\n \nincludes\n\n   \ninclude\nkaf*.*\n/include\n\n \n/includes\n\n\n/fileSet\n\n\n\n\n\n\n\nPut the keytab and kafka_client_jaas file with \n.jar\n extension in the base directory, that is the directory parallel to \npom.xml\n file.\n\n\nUse the following properties to load the \njaas_conf\n file. This will ensure that the \nkafka_client_keytab.jar\n and \nkafka_client_jaas.jar\n are available in the classpath while the application is running.\n\n\n\n\nproperty\n\n  \nname\ndt.operator.kafkaOutput.prop.properties(security.protocol)\n/name\n\n  \nvalue\nSASL_PLAINTEXT\n/value\n\n\n/property\n\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(sasl.kerberos.service.name)\n/name\n\n    \nvalue\nkafka\n/value\n\n  \n/property\n\n\n  \nproperty\n\n    \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n    \nvalue\n-Djava.security.auth.login.config=./kafka_client_jaas.jar\n/value\n\n  \n/property\n\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.topic\n/name\n\n    \nvalue\nlat_long_lookup\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(bootstrap.servers)\n/name\n\n    \nvalue\n${apex.app-param.server-kafka-input-brokers}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.kafkaOutput.prop.properties(key.serializer)\n/name\n\n    \nvalue\norg.apache.kafka.common.serialization.StringSerializer\n/value\n\n  \n/property\n\n\n\nKafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    storeKey=true\n    keyTab=\n./kafka_client_keytab.jar\n\n    principal=\nkafka-client-1@EXAMPLE.COM\n;\n};\n\n\n\n\nCommand Line Method\n\n\nWhile launching through command line, add the \n\nkeytab\n and \njaas\n files in the libjars command line option. These will copy to the classpath.\n\n\nNote:\n This approach will work only through command line.", 
            "title": "Kafka Input"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafka-input-operator", 
            "text": "", 
            "title": "Kafka Input Operator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#introduction", 
            "text": "This is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is a fault-tolerant and scalable Malhar Operator.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#why-is-it-needed", 
            "text": "Kafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.", 
            "title": "Why is it needed ?"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractkafkainputoperator", 
            "text": "This is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.", 
            "title": "AbstractKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters", 
            "text": "Parameter  Description    maxTuplesPerWindow  Controls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE     idempotentStorageManager  This is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager. NoopIdempotentStorageManager    strategy  Operator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.  ONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.  ONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE    msgRateUpperBound  Maximum messages upper bound. Operator repartitions when the  msgProcessedPS  exceeds this bound.  msgProcessedPS  is the average number of messages processed per second by this operator.    byteRateUpperBound  Maximum bytes upper bound. Operator repartitions when the  bytesPS  exceeds this bound.  bytesPS  is the average number of bytes processed per second by this operator.     offsetManager  This is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)    repartitionInterval  Interval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds    repartitionCheckInterval  Interval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds    initialPartitionCount  When the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1    consumer  This is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods", 
            "text": "void emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafkaconsumer", 
            "text": "This is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.", 
            "title": "KafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#pre-requisites", 
            "text": "This operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters_1", 
            "text": "Parameter  Type  Default  Description    zookeeper  String   Specifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where  c1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster    cacheSize  int  1024  Maximum of buffered messages hold in memory.    topic  String  default_topic  Indicates the name of the topic.    initialOffset  String  latest  Indicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_1", 
            "text": "void commitOffset(): Commit the offsets at checkpoint.  Map  KafkaPartition, Long  getCurrentOffsets(): Return the current\n    offset status.  resetPartitionsAndOffset(Set  KafkaPartition  partitionIds,\n    Map  KafkaPartition, Long  startOffset): Reset the partitions with\n    partitionIds and offsets with startOffset.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters-for-simplekafkaconsumer", 
            "text": "Parameter  Type  Default  Description    bufferSize  int  1 MB  Specifies the maximum total size of messages for each fetch request.    metadataRefreshInterval  int  30 Seconds  Interval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.    metadataRefreshRetryLimit  int  -1  Specifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.", 
            "title": "Configuration Parameters\u00a0for SimpleKafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#offsetmanager", 
            "text": "This is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  public interface OffsetManager\n{\n  public Map KafkaPartition, Long  loadInitialOffsets();\n  public void updateOffsets(Map KafkaPartition, Long  offsetsOfPartitions);\n}", 
            "title": "OffsetManager"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_2", 
            "text": "Map  KafkaPartition, Long  loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.  updateOffsets(Map  KafkaPartition, Long  offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#partitioning", 
            "text": "The logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#response-processstatsbatchedoperatorstats-stats", 
            "text": "The application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.", 
            "title": "Response processStats(BatchedOperatorStats stats)"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#definepartitions", 
            "text": "Based on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.", 
            "title": "DefinePartitions"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractsingleportkafkainputoperator", 
            "text": "This class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.", 
            "title": "AbstractSinglePortKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#ports", 
            "text": "outputPort  T : Tuples extracted from Kafka messages are emitted through\nthis port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_3", 
            "text": "T getTuple(Message msg) : Converts the Kafka message to tuple.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#concrete-classes", 
            "text": "KafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.    KafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.", 
            "title": "Concrete Classes"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#application-example", 
            "text": "This section builds an Apex application using Kafka input operator.\nBelow is the code snippet:  @ApplicationAnnotation(name =  KafkaApp )\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator( MessageReader , new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator( Output , new ConsoleOutputOperator());\n\n  dag.addStream( MessageData , input.outputPort, output.input);\n}\n}  Below is the configuration for using the earliest offset, \u201ctest\u201d as the topic name and\n\u201clocalhost:2181\u201d as the zookeeper forum:  property \n   name dt.operator.MessageReader.prop.initialOffset /name \n   value earliest /value  /property  property  name dt.operator.MessageReader.prop.topic /name  value test /value  /property  property  name dt.operator.MessageReader.prop.zookeeper /nam  value localhost:2181 /value  /property   Please note that  MessageReader  is the string passed as the first argument to the addOperator()  call. The above stanza sets these parameters for this operator\nregardless of the application it resides in; if you want to set them on a\nper-application basis, you can use this instead (where  KafkaApp  is the name of\nthe application):  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.initialOffset /name \n   value earliest /value  /property  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.topic /name \n   value test-topic /value  /property  property \n   name dt.application.KafkaApp.operator.MessageReader.prop.zookeeper /name \n   value node21:2181 /value  /property", 
            "title": "Application Example"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#using-kafkainput-operator-in-a-kerberised-environment", 
            "text": "You can use the KafkaInput operator in a Kerberised environment using one of the following methods:   Operator Properties Method  Application Package Method\n* Command Line Method", 
            "title": "Using KafkaInput Operator in a Kerberised Environment"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#operator-properties-method", 
            "text": "To use this method, do the following:   Configure and use the current operator properties by copying the  keytab  and  jaas  file on all the kafka nodes.   After the  client_jaas  file and  keytab  are available on all the nodes, add the operator properties as shown below and provide the corresponding environment properties:   property \n   name dt.operator.kafkaOutput.prop.properties(security.protocol) /name \n   value SASL_PLAINTEXT /value  /property \n\n   property \n     name dt.operator.kafkaOutput.prop.properties(sasl.kerberos.service.name) /name \n     value kafka /value \n   /property \n\n    property \n     name dt.attr.CONTAINER_JVM_OPTIONS /name \n     value -Djava.security.auth.login.config=/path/to/kafka_client_jaas.con /value \n   /property \n\n   property \n     name dt.operator.kafkaOutput.prop.topic /name \n     value lat_long_lookup /value \n   /property \n   property \n     name dt.operator.kafkaOutput.prop.properties(bootstrap.servers) /name \n     value ${apex.app-param.server-kafka-input-brokers} /value \n   /property \n   property \n     name dt.operator.kafkaOutput.prop.properties(key.serializer) /name \n     value org.apache.kafka.common.serialization.StringSerializer /value \n   /property   The  Kafka_client_jaas.con  file should have content similar to the following config:  KafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    storeKey=true\n    keyTab= /etc/security/keytabs/kafka_client.keytab \n    principal= kafka-client-1@EXAMPLE.COM ;\n};", 
            "title": "Operator Properties Method"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#application-package-method", 
            "text": "To use this method, do the following to ensure that the  keytab  and  jaas  files are available with the application package:   Include the following property in  apppackage.xml  file:   fileSet \n  directory ${basedir}/ /directory \n  outputDirectory /lib /outputDirectory \n  includes \n    include kaf*.* /include \n  /includes  /fileSet    Put the keytab and kafka_client_jaas file with  .jar  extension in the base directory, that is the directory parallel to  pom.xml  file.  Use the following properties to load the  jaas_conf  file. This will ensure that the  kafka_client_keytab.jar  and  kafka_client_jaas.jar  are available in the classpath while the application is running.   property \n   name dt.operator.kafkaOutput.prop.properties(security.protocol) /name \n   value SASL_PLAINTEXT /value  /property \n\n   property \n     name dt.operator.kafkaOutput.prop.properties(sasl.kerberos.service.name) /name \n     value kafka /value \n   /property \n\n   property \n     name dt.attr.CONTAINER_JVM_OPTIONS /name \n     value -Djava.security.auth.login.config=./kafka_client_jaas.jar /value \n   /property \n\n   property \n     name dt.operator.kafkaOutput.prop.topic /name \n     value lat_long_lookup /value \n   /property \n   property \n     name dt.operator.kafkaOutput.prop.properties(bootstrap.servers) /name \n     value ${apex.app-param.server-kafka-input-brokers} /value \n   /property \n   property \n     name dt.operator.kafkaOutput.prop.properties(key.serializer) /name \n     value org.apache.kafka.common.serialization.StringSerializer /value \n   /property \n\n\nKafkaClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    storeKey=true\n    keyTab= ./kafka_client_keytab.jar \n    principal= kafka-client-1@EXAMPLE.COM ;\n};", 
            "title": "Application Package Method"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#command-line-method", 
            "text": "While launching through command line, add the  keytab  and  jaas  files in the libjars command line option. These will copy to the classpath.  Note:  This approach will work only through command line.", 
            "title": "Command Line Method"
        }, 
        {
            "location": "/operators/snapshot_server/", 
            "text": "Snapshot Server Operators\n\n\nThis document is intended as a guide for understanding and using the Snapshot Server operators.\n\n\nIntroduction\n\n\nThe Snapshot Server is the operator which houses the most recent\nsnapshot of tabular data. The Snapshot Server receives queries from an\nembedded query operator and sends query results to a result\noperator. The most recent tabular data can be sent in two forms: a\nlist of maps and a list of POJOs. We will now describe how to\nvisualize both forms of tabular data.\n\n\nAppDataSnapshotServerMap\n\n\nThe AppDataSnapshotServerMap operator accepts a list of maps. Each map\nrepresents a row of a table. Each key in the map represents a column\nof the table and each value represents the value of a column for a\nparticular row. The name, and type of each column is specified through\nthe snapshotSchemaJSON property. The following is an example of a\nsnapshotSchema:\n\n\n{\n  \nvalues\n: [\n    {\nname\n: \nword\n, \ntype\n: \nstring\n}, \n    {\nname\n: \ncount\n, \ntype\n: \ninteger\n}\n  ]\n}\n\n\n\n\nHere you can see that the table has two columns word and count of\ntypes string and integer respectively. According to this schema it is\nexpected that the Maps in the provided list of Maps will have values\nfor the keys \u201cword\u201d and \u201ccount\u201d.\n\n\nNow that the Snapshot server is configured we need to connect the\nembedded query operator and the result operator. The code snippet\nbelow shows how to do this:\n\n\nString gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\nURI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n\nAppDataSnapshotServerMap snapshotServer = dag.addOperator(\nSnapshotServer\n, new AppDataSnapshotServerMap());\nsnapshotServer.setSnapshotSchemaJSON(snapshotServerJSON);\n\nPubSubWebSocketAppDataQuery wsQuery = new PubSubWebSocketAppDataQuery();\nwsQuery.setUri(uri);\nwsQuery.setTopic(\u201cquery\u201d);\nsnapshotServer.setEmbeddableQueryInfoProvider(wsQuery);\n\nPubSubWebSocketAppDataResult wsResult = dag.addOperator(\nQueryResult\n, new PubSubWebSocketAppDataResult());\nwsResult.setUri(uri);\nwsResult.setTopic(\u201cresult\u201d);\n\ndag.addStream(\nResult\n, snapshotServer.queryResult, queryResultPort);\n\n\n\n\nNote that the \ndt.attr.GATEWAY_CONNECT_ADDRESS\n property needs to be\nspecified in the dt-site.xml and must contain the gateway\u2019s ip\naddress. This value will be automatically set if the application is\nlaunched through dtGateway.\n\n\nAppDataSnapshotServerPOJO\n\n\nThe AppDataSnapshotServerPOJO operator accepts a list of POJOs. Each\nPOJO represents a row of a table. Each field of the POJO represents\nrepresents the value for a particular column. The\nAppDataSnapshotServerPOJO is configured in the same way as the\nAppDataSnapshotServerMap except that there is an addition property\nthat must be set. This additional property is fieldToGetter\nproperty. The fieldToGetter property is a Map from String to\nString. The key is the name of a column as defined in the SchemaJSON\nstring. The value is the java getter method that is required to\nextract the value of that column. Let\u2019s walk through an example of a\nvalid setting for fieldToGetter. If this schema is configured:\n\n\n{\n  \nvalues\n: [\n    {\nname\n: \nword\n, \ntype\n: \nstring\n}, \n    {\nname\n: \ncount\n, \ntype\n: \ninteger\n}\n  ]\n}\n\n\n\n\nA valid setting for fieldToGetter would be this:\n\n\nfieldToGetter.put(\u201cword\u201d, \u201cgetWord()\u201d);\nfieldToGetter.put(\u201ccount\u201d, \u201cgetCount()\u201d);\n\n\n\n\nExamples\n\n\nPlease look at the following java code for example usages of the\noperators mentioned above:\n\n\n\n\nApplicationAppData.java\n\n\nTwitterTrendingHashtagsApplication.java", 
            "title": "Snapshot Server"
        }, 
        {
            "location": "/operators/snapshot_server/#snapshot-server-operators", 
            "text": "This document is intended as a guide for understanding and using the Snapshot Server operators.", 
            "title": "Snapshot Server Operators"
        }, 
        {
            "location": "/operators/snapshot_server/#introduction", 
            "text": "The Snapshot Server is the operator which houses the most recent\nsnapshot of tabular data. The Snapshot Server receives queries from an\nembedded query operator and sends query results to a result\noperator. The most recent tabular data can be sent in two forms: a\nlist of maps and a list of POJOs. We will now describe how to\nvisualize both forms of tabular data.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operators/snapshot_server/#appdatasnapshotservermap", 
            "text": "The AppDataSnapshotServerMap operator accepts a list of maps. Each map\nrepresents a row of a table. Each key in the map represents a column\nof the table and each value represents the value of a column for a\nparticular row. The name, and type of each column is specified through\nthe snapshotSchemaJSON property. The following is an example of a\nsnapshotSchema:  {\n   values : [\n    { name :  word ,  type :  string }, \n    { name :  count ,  type :  integer }\n  ]\n}  Here you can see that the table has two columns word and count of\ntypes string and integer respectively. According to this schema it is\nexpected that the Maps in the provided list of Maps will have values\nfor the keys \u201cword\u201d and \u201ccount\u201d.  Now that the Snapshot server is configured we need to connect the\nembedded query operator and the result operator. The code snippet\nbelow shows how to do this:  String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\nURI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n\nAppDataSnapshotServerMap snapshotServer = dag.addOperator( SnapshotServer , new AppDataSnapshotServerMap());\nsnapshotServer.setSnapshotSchemaJSON(snapshotServerJSON);\n\nPubSubWebSocketAppDataQuery wsQuery = new PubSubWebSocketAppDataQuery();\nwsQuery.setUri(uri);\nwsQuery.setTopic(\u201cquery\u201d);\nsnapshotServer.setEmbeddableQueryInfoProvider(wsQuery);\n\nPubSubWebSocketAppDataResult wsResult = dag.addOperator( QueryResult , new PubSubWebSocketAppDataResult());\nwsResult.setUri(uri);\nwsResult.setTopic(\u201cresult\u201d);\n\ndag.addStream( Result , snapshotServer.queryResult, queryResultPort);  Note that the  dt.attr.GATEWAY_CONNECT_ADDRESS  property needs to be\nspecified in the dt-site.xml and must contain the gateway\u2019s ip\naddress. This value will be automatically set if the application is\nlaunched through dtGateway.", 
            "title": "AppDataSnapshotServerMap"
        }, 
        {
            "location": "/operators/snapshot_server/#appdatasnapshotserverpojo", 
            "text": "The AppDataSnapshotServerPOJO operator accepts a list of POJOs. Each\nPOJO represents a row of a table. Each field of the POJO represents\nrepresents the value for a particular column. The\nAppDataSnapshotServerPOJO is configured in the same way as the\nAppDataSnapshotServerMap except that there is an addition property\nthat must be set. This additional property is fieldToGetter\nproperty. The fieldToGetter property is a Map from String to\nString. The key is the name of a column as defined in the SchemaJSON\nstring. The value is the java getter method that is required to\nextract the value of that column. Let\u2019s walk through an example of a\nvalid setting for fieldToGetter. If this schema is configured:  {\n   values : [\n    { name :  word ,  type :  string }, \n    { name :  count ,  type :  integer }\n  ]\n}  A valid setting for fieldToGetter would be this:  fieldToGetter.put(\u201cword\u201d, \u201cgetWord()\u201d);\nfieldToGetter.put(\u201ccount\u201d, \u201cgetCount()\u201d);", 
            "title": "AppDataSnapshotServerPOJO"
        }, 
        {
            "location": "/operators/snapshot_server/#examples", 
            "text": "Please look at the following java code for example usages of the\noperators mentioned above:   ApplicationAppData.java  TwitterTrendingHashtagsApplication.java", 
            "title": "Examples"
        }, 
        {
            "location": "/app_data_framework/", 
            "text": "App Data Framework User Guide\n\n\nIntroduction\n\n\nThe App Data Framework provides a way for data in Apex applications to be queried and delivered, so that the end user can easily access the application data and visualize it. \n\n\nIn this document, we will first look at a simple example of how an application developer can quickly add this capability to an application. We will then explore the basic building blocks of the App Data Framework, and the data schemas that Apache Apex and DataTorrent RTS support.\n\n\nThis document assumes that the reader has the basic knowledge of Apache Apex.\n\n\nExamples\n\n\nA Simple Example\n\n\nIn Apache Apex Malhar, the Twitter example demonstrates the usage of the App Data Framework. The application calculates top 10 hashtags mentioned in tweets in the last 5 minutes across a 1% random tweet sampling on a rolling window basis.\n\n\nIn DataTorrent RTS, you can create a dashboard for this Twitter application that has a bar chart widget with the current top hashtags in Twitter constantly updated:\n\n\n\n\nThe topology of the Twitter example looks like this:\n\n\n\n\nThe \nSnapshotServer\n operator (along with a Query Operator embedded in it) and the \nQueryResult\n operator enable the latest Twitter top hashtags to be visualizable in the widget. \n\n\nWe will explain them in the following sections.\n\n\nThe code of this Twitter example with App Data support is available \nhere\n\n\nA More Complicated Example\n\n\nIn DataTorrent RTS, the Sales example demonstrates a more complicated usage of the App Data Framework. The application generates random sales events, which has dimension keys of sales channels, region and products. Then it aggregates tax, sales amount and discount across all the key combinations.\n\n\n\n\nThe topology of the Sales example looks like this:\n\n\n\n\nSimilar to the Twitter example, the \nStore\n operator along with the Query Operator embedded in it, and the \nQueryResult\n operator enable the data to be queried and be delivered to the widget for visualization.\n\n\nThe code of this Sales example is available \nhere\n\n\nArchitecture Overview\n\n\nAt a very high level, this is the architecture diagram for the App Data Framework. We will explain each of the components.\n\n\n\n\nApp Data Source\n\n\nEach App Data Source is a queryable unit, and it is represented by three operators in your application. It is possible to have multiple such App Data Sources in one application.\n\n\n\n\nData Source\n\n\nThe Data Source Operator that runs in an Apex application that processes the queries from the Query Operator and gives back the results. In the Twitter example, it is the \nSnapshot Server\n operator. The Java class of this operator is \ncom.datatorrent.lib.appdata.snapshot.AppDataSnapshotServerMap\n in Apex Malhar. In the Sales example, it is the \nStore\n operator. The Java class of this operator is \ncom.datatorrent.contrib.dimensions.AppDataSingleSchemaDimensionStoreHDHT\n, which is provided by DataTorrent RTS.\n\n\nQuery Operator\n\n\nThe Query Operator runs embedded in the Data Source Operator, listens for incoming queries from the Message Bus and hands it over to the Data Source Operator. Note that since the Query Operator is embedded within the Data Source Operator, it will not be visible in the DAG view in \ndtManage\n. In both the Twitter and the Sales examples, the Java class that implements the Query Operator is \ncom.datatorrent.lib.io.PubSubWebSocketAppDataQuery\n.\n\n\nResult Operator\n\n\nThe Result Operator runs in an Apex application that publishes the results from the Data Source Operator to the Message Bus. In the Twitter example, it is the \nQueryResult\n operator. The Java class of this operator in both the Twitter and Sales examples is \ncom.datatorrent.lib.io.PubSubWebSocketAppDataResult\n.\n\n\nMessage Bus\n\n\nBecause the operators can run in any node in the cluster, a Message Bus with a pub-sub mechanism is used for delivery for both the queries and the results. This mechanism requires that the caller sends the query message to a topic that the Query Operator listens to, and the Result Operator sends the query result message passed from the Data Source Operator to a different topic that the caller listens to. Typically, the caller is a web browser.\n\n\nIn DataTorrent RTS, this mechanism is provided by \ndtGateway\n. It uses WebSocket to achieve this so that the queries can be made and the results can be processed directly by a web browser. \n\n\nReferring back to the architecture diagram, the web browser, the Query Operator and the Result Operator all connect to dtGateway via WebSocket, with a pub-sub protocol on top of it.\n\n\nSchemas\n\n\nEach App Data Source must provide a schema that describes what is a valid data query and what is a valid data response, so that the caller knows how to query and how to interpret the response from the App Data Sources. We will describe two schemas that underlie the Twitter example and the Sales example.\n\n\nUI Widgets\n\n\nIn order to allow the end users to visualize the result data in a web browser, we need web-based UI widgets that take user input and send the queries and display the results according to the App Data Sources\u2019 schemas. In DataTorrent RTS, we have a number of UI widgets built in. We will also describe how a user can write their own custom widgets.\n\n\nTechnical Details\n\n\nApp Data Source\n\n\nAs described in the overview, an App Data Source consists of three operators: Data Source, Query and Result.\n\n\nThe Result Operator must implement this interface in \ncom.datatorrent.common.experimental.AppData\n:\n\n\ninterface ConnectionInfoProvider \n{\n  String getAppDataUrl();\n  String getTopic();\n}\n\n\n\n\nThe Query Operator, which is embedded in the Data Store Operator, must implement this interface in \ncom.datatorrent.common.experimental.AppData\n:\n\n\ninterface EmbeddableQueryInfoProvider\nQUERY_TYPE\n extends Operator, ConnectionInfoProvider, Operator.ActivationListener\nOperatorContext\n\n{\n  DefaultOutputPort\nQUERY_TYPE\n getOutputPort();\n\n  void enableEmbeddedMode();\n}\n\n\n\n\nThe port that is returned by the embedded Query Operator\u2019s \ngetOutputPort()\n method will be connected to the input port of the Data Source Operator that is annotated with the marker annotation \ncom.datatorrent.common.experimental.AppData.QueryPort\n.\n\n\nOn the DAG level, the Result Operator needs to connect to an output port of the Data Source Operator that is annotated with the marker annotation \ncom.datatorrent.common.experimental.AppData.ResultPort\n. \n\n\nWith this setup, the Data Source, along with the query topic and the result topic, will be discoverable by STRAM. STRAM will return the information to the caller via its REST API.\n\n\nBelow is the App Data Source discovery information returned by the \n/ws/v2/applications/{appid}\n REST call from dtGateway for the Twitter example:\n\n\n{\n  ...\n  \nappDataSources\n: [\n    {\n      \nname\n: \nSnapshotServer.queryResult\n,\n      \noperatorName\n: \nSnapshotServer\n,\n      \nportName\n: \nqueryResult\n,\n      \nquery\n: {\n        \noperatorName\n: \nSnapshotServer.query\n,\n        \ntopic\n: \nTwitterHashtagQueryDemo\n,\n        \nurl\n: \npubsub\n\n      },\n      \nresult\n: {\n        \nappendQIDToTopic\n: true,\n        \noperatorName\n: \nQueryResult\n,\n        \u201ctopic\n: \nTwitterHashtagQueryResultDemo\n,\n        \nurl\n: \npubsub\n\n      }\n    }, ... ]\n  ...\n}\n\n\n\n\nThe \nname\n field contains the name of the Data Source, and it has the following form: \ndataSourceOperatorName\n.\nresultPortName\n.\n\n\nThe \noperatorName\n field is the Data Source operator name.\n\n\nThe \nportName\n field is the result port name of the Data Source operator.\n\n\nThe \nquery\n field describes the query mechanism, including the topic. \n\n\nThe \nresult\n field describes the result mechanism, including the topic and whether the query ID will be appended to the topic. Setting \nappendQIDToTopic\n to true increases the granularity of topics so that callers can receive results only for their queries. If the cost of creating such a large number of topics is high, it can be set to false; callers then will need to filter the results suitably.\n\n\nMessage Bus\n\n\ndtGateway has a WebSocket service that provides the pubsub mechanism required by the Message Bus. The URL is \nws://{dtGatewayAddress}/pubsub\n. You can publish data with a topic by sending a WebSocket message like this:\n\n\n{\n  \u201ctype\u201d: \u201cpublish\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}\n\n\n\n\nThe \ntopic\n field is the topic name, and the \ndata\n field is the payload and can be any JSON object.\n\n\nAll client that have subscribed to the topic will receive the data. The format is:\n\n\n{\n  \u201ctype\u201d: \u201cdata\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}\n\n\n\n\nAnd to subscribe to the topic, the client must send this message to dtGateway using WebSocket:\n\n\n{\n  \u201ctype\u201d: \u201csubscribe\u201d, \u201ctopic\u201d: \u201c{topic}\u201d\n}\n\n\n\n\nYou can try it with dtGateway using a web browser with the help of a WebSocket client (e.g. \u201cSimple WebSocket Client\u201d extension in Google Chrome).\n\n\nNote that dtGateway does not keep the history of the messages. \n\n\nInteraction between Data Source and Web Browser\n\n\nThe web browser, the Query Operator and the Result Operator all connect to dtGateway via the Message Bus. The following describes the step-by-step interaction among them on a high level:\n\n\n\n\nThe UI widget in the browser subscribes to the result topic\n\n\nThe UI widget sends a schema query to the query topic.\n\n\nThe Data Source Operator gets the schema query and sends the schema result to the result topic, which the UI widget receives.\n\n\nThe UI widget in the browser subscribes to the result topic\n\n\nThe UI widget sends a data query\n\n\nThe Data Source Operator gets the data query and processes it and sends back the data result to the result topic.\n\n\nThe UI widget receives the data result and renders it.\n\n\n\n\nLet\u2019s look at the actual messages being exchanged for the Twitter example.\n\n\nSnapshot Schema\n\n\nThe Snapshot Schema serves a simple snapshot of tabular data, which is typically constantly being updated and is used in the Twitter example. We will describe the Snapshot schema as we look at the messages.\n\n\nStep 1: Browser sends Schema Response Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.20716154835833223\n\n}\n\n\n\n\nIn preparation for a Schema Query, the browser first subscribes to the result topic (with the query ID appended) to get the Schema Result back. In this case, the browser generates a random string, \u201c0.20716154835833223\u201d for the query ID, and appends the query ID to the result topic that it is subscribing to. \n\n\nStep 2: Browser sends Schema Query\n\n\n{\n  \ntype\n: \npublish\n,\n  \ntopic\n: \nTwitterHashtagQueryDemo\n,\n  \ndata\n: {\n    \nid\n: 0.20716154835833223,\n    \ntype\n: \nschemaQuery\n\n  }\n}\n\n\n\n\nThe browser sends a message with type, \nschemaQuery\n. This is the query that asks for the schema type and schema data. \n\n\nStep 3: Data Source sends Schema Response\n\n\n{\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.20716154835833223\n,\n  \ndata\n: {\n    \nid\n: \n0.20716154835833223\n,\n    \ntype\n: \nschemaResult\n,\n    \ndata\n: [\n      {\n        \nvalues\n: [\n          {\n            \nname\n: \nhashtag\n,\n            \ntype\n: \nstring\n\n          },\n          {\n            \nname\n: \ncount\n,\n            \ntype\n: \ninteger\n\n          }\n        ],\n        \nschemaType\n: \nsnapshot\n,\n        \nschemaVersion\n: \n1.0\n\n      }\n    ]\n  },\n  \ntype\n: \ndata\n\n}\n\n\n\n\nThe Data Source sends back the \nschemaResult\n, which contains the response to \nschemaQuery\n, which contains schema type and schema data. In this case, schemaType is snapshot and schemaVersion is 1.0, and the available fields in the data are \u201chashtag\u201d, which is a string, and \u201ccount\u201d, which is an integer.\n\n\nStep 4: Browser sends Data Result Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.6760250790172551\n\n}\n\n\n\n\nSimilar to the previous subscribe message, the browser subscribes to the result topic to get the response for the \ndataQuery\n that the browser is going to issue.\n\n\nStep 5: Browser sends Data Query Request\n\n\n{\n  \ntype\n: \npublish\n,\n  \ntopic\n: \nTwitterHashtagQueryDemo\n,\n  \ndata\n: {\n    \nid\n: 0.6760250790172551,\n    \ntype\n: \ndataQuery\n,\n    \ndata\n: {\n      \nfields\n: [\n        \nhashtag\n,\n        \ncount\n\n      ]\n    },\n    \ncountdown\n: 30,\n    \nincompleteResultOK\n: true\n  }\n}\n\n\n\n\nThe browser sends the \ndataQuery\n message that asks for actual data.\n\n\nThe \ncountdown\n field expects an optional integer value. It tells the Data Source that the Data Source should return results once for subsequent {countdown} application windows. In this example, the value is 30. That means the Data Source should execute this query 30 times, once for each application window for the next 30 application windows.\n\n\nThe \nincompleteResultOK\n field is an optional boolean value default to be false. If the value is true, the Data Source should return results as soon as they are available even if they are partial results. This is useful when {countdown} is greater than 1 and the Data Source could take a long time to return the complete result set and if it is desirable for the caller to receive the results as soon as possible. If this value is false, Data Source should return the complete result set to the caller\n\n\nWithin the \ndata\n field, the \nfields\n field tells the Data Source what fields to return in the result.\n\n\nStep 6: Data Source sends Data Response\n\n\n{\n  \ntopic\n: \nTwitterHashtagQueryResultDemo.0.6760250790172551\n,\n  \ndata\n: {\n    \nid\n: \n0.6760250790172551\n,\n    \ntype\n: \ndataResult\n,\n    \ndata\n: [\n      {\n        \ncount\n: \n1398\n,\n        \nhashtag\n: \n\uc0ac\uc124\ud1a0\ud1a0\ucd94\ucc9c\uc0ac\uc774\ud2b8\n\n      },\n      {\n        \ncount\n: \n1415\n,\n        \nhashtag\n: \nTopDance\n\n      },\n      {\n        \ncount\n: \n1498\n,\n        \nhashtag\n: \nisola\n\n      },\n      {\n        \ncount\n: \n1521\n,\n        \nhashtag\n: \nRT\u3057\u305f\u4eba\u5168\u54e1\u30d5\u30a9\u30ed\u30fc\u3059\u308b\n\n      },\n      {\n        \ncount\n: \n1728\n,\n        \nhashtag\n: \n4\u670819\u65e5\u306f\u897f\u6728\u91ce\u771f\u59eb\u306e\u8a95\u751f\u65e5\n\n      },\n      {\n        \ncount\n: \n1787\n,\n        \nhashtag\n: \n\ub124\uc784\ub4dc\uc0ac\ub2e4\ub9ac\n\n      },\n      {\n        \ncount\n: \n2079\n,\n        \nhashtag\n: \nALDUBActing101\n\n      },\n      {\n        \ncount\n: \n2280\n,\n        \nhashtag\n: \n\u5730\u9707\n\n      },\n      {\n        \ncount\n: \n2712\n,\n        \nhashtag\n: \n\u897f\u6728\u91ce\u771f\u59eb\u751f\u8a95\u796d2016\n\n      },\n      {\n        \ncount\n: \n2714\n,\n        \nhashtag\n: \n\u0634\u0639\u064a\u0628_\u064a\u0633\u064a\u0621_\u0644\u0644\u0633\u0639\u0648\u062f\u064a\u0647\n\n      }\n    ],\n    \ncountdown\n: \n29\n\n  },\n  \ntype\n: \ndata\n\n}\n\n\n\n\nThe Data Source sends the \ndataResult\n message in response to the \ndataQuery\n. In this case, it contains a list of (hashtag, count) sets in the \ndata\n field.\n\n\nIf the \ncountdown\n field of the \ndataQuery\n is non-zero, there will be multiple such messages that correspond to one \ndataQuery\n message.  The \ncountdown\n field in the \ndataResult\n message decrements every time it sends a result.  When it counts down to zero, the query has expired and the client must send another \ndataQuery\n message to receive further updates.\n\n\nIn this case, because the countdown value is 30 in the \ndataQuery\n, the Data Source will keep sending the latest data set for the next 29 application windows following this initial response.\n\n\nFor more information about the operators behind this Snapshot Schema, please refer to \nthis document\n.\n\n\nDimensions Schema\n\n\nThe Dimensions Schema is an extension of the Snapshot Schema with the notion of time and key dimensions. It is supported in DataTorrent RTS. \n\n\nWe will look at the message exchange in the Sales example.\n\n\nStep 1: Browser sends Schema Response Subscribe\n\n\n{\ntype\n:\nsubscribe\n,\ntopic\n:\nSalesQueryResultDemo.0.676153457723558\n}\n\n\n\n\nThis is similar to the Snapshot example we saw earlier.\n\n\nStep 2: Browser sends Schema Query\n\n\n{\n   \ntype\n:\npublish\n,\n   \ntopic\n:\nSalesQueryDemo\n,\n   \ndata\n:{\n      \nid\n: 0.22710832906886935,\n      \ntype\n:\nschemaQuery\n\n   }\n}\n\n\n\n\nAgain, this is similar to the Snapshot example we saw earlier.\n\n\nStep 3: Data Source sends Schema Response.\n\n\n{\n   \ntopic\n:\nSalesQueryResultDemo.0.22710832906886935\n,\n   \ndata\n:{\n      \nid\n:\n0.22710832906886935\n,\n      \ntype\n:\nschemaResult\n,\n      \ndata\n:[\n         {\n            \nschemaType\n:\ndimensions\n,\n            \nschemaVersion\n:\n1.0\n,\n            \ntime\n:{\n               \nbuckets\n:[\n1m\n,\n1h\n,\n1d\n,\n5m\n],\n               \nslidingAggregateSupported\n:true\n            },\n            \nkeys\n:[\n               {\n                  \nname\n:\nchannel\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nMobile\n, \nOnline\n, \nStore\n]\n               },\n               {\n                  \nname\n:\nregion\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nAtlanta\n, \nBoston\n,\nChicago\n,\nCleveland\n,\nDallas\n,\n                     \nMinneapolis\n,\nNew York\n,\nPhiladelphia\n,\nSan Francisco\n,\nSt. Louis\n]\n               },\n               {\n                  \nname\n:\nproduct\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nLaptops\n,\nPrinters\n,\nRouters\n,\nSmart Phones\n,\nTablets\n]\n               }\n            ],\n            \nvalues\n:[\n               {\n                  \nname\n:\ntax:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nsales:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ndiscount:SUM\n,\n                  \ntype\n:\ndouble\n\n               }\n            ],\n            \ndimensions\n:[\n               {\n                  \ncombination\n:[\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nregion\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nregion\n,\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nchannel\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nregion\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nproduct\n,\n                     \nregion\n,\n                     \nchannel\n\n                  ]\n               }\n            ]\n         }\n      ]\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nCompared to the Snapshot Schema, this is considerably more complex. The response to the query describes the keys and aggregates that are provided by the data source. The basic components of the schema are the following:\n\n\ntimebuckets\n: These are the time buckets over which aggregations are computed. For example, 1d (one day), 1h (one hour), 1m (one minute), 1s (one second).\n\n\nkeys\n: These are the search parameters you need to provide when you do a query. The keys have an enumValues. This is an optional section which contains all possible values of a key. This section may be updated by the back end if new key values are encountered.\n\n\nvalues\n: These are the results which are returned by a query. Notice that the values are of the form (name):(aggregation). The first portion of the value name describes what is being aggregated. The second portion of the value name describes the aggregation being used.\n\n\ncombinations\n: This describes the valid combinations of keys that can be specified when doing a query.\n\n\nStep 4: Browser sends Data Result Subscribe\n\n\n{\n  \ntype\n: \nsubscribe\n,\n  \ntopic\n: \nSalesQueryResultDemo.0.3180227109696716\n\n}\n\n\n\n\nStep 5: Browser sends Data Query Request\n\n\nNow that the valid combinations, keys, values, and timebuckets are known, we can issue a query using them. \n\n\n{\n   \ntype\n:\npublish\n,\n   \ntopic\n:\nSalesQueryDemo\n,\n   \ndata\n:{\n      \nid\n: \n0.3180227109696716\n,\n      \ntype\n:\ndataQuery\n,\n      \ndata\n:{\n         \ntime\n:{\n            \nlatestNumBuckets\n:10,\n            \nbucket\n:\n1m\n\n         },\n         \nincompleteResultOK\n:true,\n         \nkeys\n:{\n\n         },\n         \nfields\n:[\n            \ntime\n,\n            \nchannel\n,\n            \nregion\n,\n            \nproduct\n,\n            \ntax:SUM\n,\n            \nsales:SUM\n,\n            \ndiscount:SUM\n\n         ]\n      },\n      \ncountdown\n:29,\n      \nincompleteResultOK\n:true\n   }\n}\n\n\n\n\ntime\n: This specifies which time buckets to query. In this case the most recent 10 one minute time buckets are queried. The example above illustrates how to query the last N timebuckets, but it is also possible to do history queries by specifying a time section like the following:\n\n\n        \ntime\n:{\n         \nbucket\n:\n1m\n,\n         \nfrom\n:1460765563547,\n         \nto\n:1460767363547\n       }\n\n\n\n\nThis time section specifies a historical query starting at the \u201cfrom\u201d unix timestamp (inclusive) up until the \u201cto\u201d timestamp (inclusive).\n\n\nkeys\n: These are the search parameters. This section can be empty to query global aggregations. If there are specific key combinations you want to search for you can do this:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: \u201cAtlanta\u201d\n}\n\n\n\n\nIt is possible to specify a key with a list like the following:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: [ \u201cAtlanta\u201d, \u201cDallas\u201d ]\n}\n\n\n\n\nThis will query all the data with a \nchannel\n value of \u201cMobile\u201d and a \nregion\n value of \u201cAtlanta\u201d OR \u201cDallas\u201d. It is also possible to specify a key with an empty array like the following:\n\n\n\u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: []\n}\n\n\n\n\nThis will query all the data with a \nchannel\n value of \u201cMobile\u201d and ANY value of \nregion\n.\nNote that this is different from not specifying the \nregion\n value at all, in which case, the aggregated value of all regions will be returned. This is similar to the difference between the SQL queries of \nSELECT SUM(sales) WHERE channel=\u2019Mobile\u2019\n (for the case of not specifying \nregion\n) and \nSELECT SUM(sales), region WHERE channel=\u2019Mobile\u2019 GROUP BY region\n (for the case of \nregion\n being an empty array).\n\n\nfields\n: This specifies what data to include in the query result. If the field \ntime\n is included, then a timestamp is included for each data point. Similarly the keys can also be specified as fields with the results. Lastly aggregated values can also be specified as fields to be returned in results.\n\n\nStep 6: Data Source sends Data Response.\n\n\nAfter a query is issued, results are periodically returned:\n\n\n{\n   \ntopic\n:\nSalesQueryResultDemo.0.3180227109696716\n,\n   \ndata\n: {\n      \nid\n:\n0.3180227109696716\n,\n      \ntype\n:\ndataResult\n,\n      \ndata\n:[\n         {\n            \ntime\n:\n1460771580000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n78363.47\n,\n            \nsales:SUM\n:\n922988.9199999997\n,\n            \ndiscount:SUM\n:\n69135.71999999999\n\n         },\n         {\n            \ntime\n:\n1460771640000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n142011.85\n,\n            \nsales:SUM\n:\n1672677.0099999998\n,\n            \ndiscount:SUM\n:\n125287.46999999999\n\n         },\n         {\n            \ntime\n:\n1460771700000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n139069.53999999998\n,\n            \nsales:SUM\n:\n1638011.5399999998\n,\n            \ndiscount:SUM\n:\n122692.48999999999\n\n         },\n         {\n            \ntime\n:\n1460771760000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n80699.90000000001\n,\n            \nsales:SUM\n:\n950502.3500000003\n,\n            \ndiscount:SUM\n:\n71196.95000000001\n\n         },\n         {\n            \ntime\n:\n1460771820000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n35914.69\n,\n            \nsales:SUM\n:\n423017.9000000001\n,\n            \ndiscount:SUM\n:\n69339.05000000002\n\n         },\n         {\n            \ntime\n:\n1460771880000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n106977.84999999998\n,\n            \nsales:SUM\n:\n1260017.5600000003\n,\n            \ndiscount:SUM\n:\n629939.14\n\n         },\n         {\n            \ntime\n:\n1460771940000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n253243.50000000003\n,\n            \nsales:SUM\n:\n2982796.9300000006\n,\n            \ndiscount:SUM\n:\n1491234.3599999999\n\n         },\n         {\n            \ntime\n:\n1460772000000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n161384.77999999997\n,\n            \nsales:SUM\n:\n1900857.2300000002\n,\n            \ndiscount:SUM\n:\n950325.01\n\n         },\n         {\n            \ntime\n:\n1460772060000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n125283.99000000002\n,\n            \nsales:SUM\n:\n1475625.3600000003\n,\n            \ndiscount:SUM\n:\n737730.77\n\n         },\n         {\n            \ntime\n:\n1460772120000\n,\n            \nregion\n:\nBoston\n,\n            \nproduct\n:\nALL\n,\n            \nchannel\n:\nMobile\n,\n            \ntax:SUM\n:\n58684.59\n,\n            \nsales:SUM\n:\n691200.9299999999\n,\n            \ndiscount:SUM\n:\n345562.2700000001\n\n         }\n      ],\n      \ncountdown\n:\n297\n\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nAs you can see each result represents a time bucket with the corresponding values for the fields. If the ANY value (with an empty list) query is specified for a key you will see multiple results for the same time bucket, but each result will have different values for the keys.\n\n\nDetailed documentation of the dimensions operators can be found here: \nhttp://docs.datatorrent.com/operators/dimensions_computation/\n\n\nAdvanced Topics\n\n\nSchema Keys\n\n\nAlthough in most cases, a Data Source has one fixed schema, it is sometimes necessary for a Data Source to support multiple schemas. This is useful if the schemas are dynamically created depending on the incoming data to the Data Source Operator.\n\n\nSchema Key is the feature we added to the App Data Framework for this purpose. If the Data Source has multiple schemas, the \nschemaResult\n will contain a list of schemas with the corresponding schema keys in the \ncontext\n field:\n\n\n{\n  \u201cid\u201d: \u201c{client_generated_id}\u201d,\n  \u201ctype\u201d: \u201cschemaResult\u201d,\n  \u201cdata\u201d: [ \n    {\n      \u201ccontext\u201d: {\n        \u201cschemaKeys\u201d: {\n          \u201c{key1}\u201d: \u201c{value1}\u201d,\n          \u201c{key2}\u201d: \u201c{value2}\u201d, ...\n         }\n      },\n      {rest of the schema}\n    }, ...\n  ]\n}\n\n\n\n\nAnd if the \nschemaResult\n returns multiple schemas, all \ndataQuery\n messages must include the schema keys:\n\n\n{\n   \u201cid\u201d: \u201c{client_generated_id}\u201d,\n   \u201ctype\u201d: \u201cdataQuery\u201d, \n   \u201cdata\u201d: {\n     \u201ccontext\u201d: {\n       \u201cschemaKeys\u201d { //optional\n    \u201c{keyName1}\u201d: \u201c{keyValue1}\u201d,\n    \u201c{keyName2}\u201d: \u201c{keyValue2}\u201d \n       }\n     }\n     {rest of the query}\n   }\n   \u201ccountdown\u201d: \u201c{countdown}\u201d,\n   \u201cincompleteResultOK\u201d: true/false\n}\n\n\n\n\nThis feature is supported by the DimensionsStore operator.\n\n\nDimensions Schema: Additional Values for Combinations\n\n\nFor the Dimensions Schema supported by the DimensionsStore operator, there can be more complex schema specifications which have results for values only for certain combinations. This is the schema for yet another example about aggregation of advertisement data in the DataTorrent RTS example repository.\n\n\n{\n   \ntopic\n:\nAdsQueryGenericResultDemo.0.08170037856325507\n,\n   \ndata\n:{\n      \nid\n:\n0.08170037856325507\n,\n      \ntype\n:\nschemaResult\n,\n      \ndata\n:[\n         {\n            \nschemaType\n:\ndimensions\n,\n            \nschemaVersion\n:\n1.0\n,\n            \ntime\n:{\n               \nbuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n               \nslidingAggregateSupported\n:true,\n               \nfrom\n:\n1459987200000\n,\n               \nto\n:\n1460768100000\n\n            },\n            \nkeys\n:[\n               {\n                  \nname\n:\npublisher\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n,\ngoogle\n,\nbing\n,\namazon\n\n                  ]\n               },\n               {\n                  \nname\n:\nadvertiser\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n,\nmacys\n,\ntaco bell\n,\n                     \nwalmart\n,\nkohls\n,\nsan diego zoo\n,\npandas\n,\njack in the box\n,\n                     \ntomatina\n,\nron swanson\n\n                  ]\n               },\n               {\n                  \nname\n:\nlocation\n,\n                  \ntype\n:\nstring\n,\n                  \nenumValues\n:[\n                     \nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n,\nAZ\n,\nAR\n,\nCA\n,\nCO\n,\nCT\n,\nDE\n,\nFL\n,\n                     \nGA\n,\nHI\n,\nID\n\n                  ]\n               }\n            ],\n            \nvalues\n:[\n               {\n                  \nname\n:\nimpressions:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nimpressions:SUM\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nimpressions:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nclicks:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nclicks:SUM\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nclicks:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ncost:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\ncost:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\ncost:AVG\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nrevenue:COUNT\n,\n                  \ntype\n:\nlong\n\n               },\n               {\n                  \nname\n:\nrevenue:SUM\n,\n                  \ntype\n:\ndouble\n\n               },\n               {\n                  \nname\n:\nrevenue:AVG\n,\n                  \ntype\n:\ndouble\n\n               }\n            ],\n            \ndimensions\n:[\n               {\n                  \ncombination\n:[\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nadvertiser\n\n                  ],\n                  \nadditionalValues\n:[\n                     {\n                        \nname\n:\nimpressions:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nimpressions:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\ncost:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\ncost:MIN\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MIN\n,\n                        \ntype\n:\ndouble\n\n                     }\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \npublisher\n\n                  ],\n                  \nadditionalValues\n:[\n                     {\n                        \nname\n:\nimpressions:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nimpressions:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MAX\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\nclicks:MIN\n,\n                        \ntype\n:\nlong\n\n                     },\n                     {\n                        \nname\n:\ncost:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\ncost:MIN\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MAX\n,\n                        \ntype\n:\ndouble\n\n                     },\n                     {\n                        \nname\n:\nrevenue:MIN\n,\n                        \ntype\n:\ndouble\n\n                     }\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \nadvertiser\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \npublisher\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nadvertiser\n,\n                     \npublisher\n\n                  ]\n               },\n               {\n                  \ncombination\n:[\n                     \nlocation\n,\n                     \nadvertiser\n,\n                     \npublisher\n\n                  ]\n               }\n            ]\n         }\n      ]\n   },\n   \ntype\n:\ndata\n\n}\n\n\n\n\nHere you can see that a combination can also have \nadditionalValues\n specified. Additional values are values which are only available for a specific combination of keys.\n\n\nThe code for the ads example is available \nhere\n.\n\n\nUI Widgets\n\n\nComing soon", 
            "title": "App Data Framework"
        }, 
        {
            "location": "/app_data_framework/#app-data-framework-user-guide", 
            "text": "", 
            "title": "App Data Framework User Guide"
        }, 
        {
            "location": "/app_data_framework/#introduction", 
            "text": "The App Data Framework provides a way for data in Apex applications to be queried and delivered, so that the end user can easily access the application data and visualize it.   In this document, we will first look at a simple example of how an application developer can quickly add this capability to an application. We will then explore the basic building blocks of the App Data Framework, and the data schemas that Apache Apex and DataTorrent RTS support.  This document assumes that the reader has the basic knowledge of Apache Apex.", 
            "title": "Introduction"
        }, 
        {
            "location": "/app_data_framework/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/app_data_framework/#a-simple-example", 
            "text": "In Apache Apex Malhar, the Twitter example demonstrates the usage of the App Data Framework. The application calculates top 10 hashtags mentioned in tweets in the last 5 minutes across a 1% random tweet sampling on a rolling window basis.  In DataTorrent RTS, you can create a dashboard for this Twitter application that has a bar chart widget with the current top hashtags in Twitter constantly updated:   The topology of the Twitter example looks like this:   The  SnapshotServer  operator (along with a Query Operator embedded in it) and the  QueryResult  operator enable the latest Twitter top hashtags to be visualizable in the widget.   We will explain them in the following sections.  The code of this Twitter example with App Data support is available  here", 
            "title": "A Simple Example"
        }, 
        {
            "location": "/app_data_framework/#a-more-complicated-example", 
            "text": "In DataTorrent RTS, the Sales example demonstrates a more complicated usage of the App Data Framework. The application generates random sales events, which has dimension keys of sales channels, region and products. Then it aggregates tax, sales amount and discount across all the key combinations.   The topology of the Sales example looks like this:   Similar to the Twitter example, the  Store  operator along with the Query Operator embedded in it, and the  QueryResult  operator enable the data to be queried and be delivered to the widget for visualization.  The code of this Sales example is available  here", 
            "title": "A More Complicated Example"
        }, 
        {
            "location": "/app_data_framework/#architecture-overview", 
            "text": "At a very high level, this is the architecture diagram for the App Data Framework. We will explain each of the components.", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/app_data_framework/#app-data-source", 
            "text": "Each App Data Source is a queryable unit, and it is represented by three operators in your application. It is possible to have multiple such App Data Sources in one application.", 
            "title": "App Data Source"
        }, 
        {
            "location": "/app_data_framework/#data-source", 
            "text": "The Data Source Operator that runs in an Apex application that processes the queries from the Query Operator and gives back the results. In the Twitter example, it is the  Snapshot Server  operator. The Java class of this operator is  com.datatorrent.lib.appdata.snapshot.AppDataSnapshotServerMap  in Apex Malhar. In the Sales example, it is the  Store  operator. The Java class of this operator is  com.datatorrent.contrib.dimensions.AppDataSingleSchemaDimensionStoreHDHT , which is provided by DataTorrent RTS.", 
            "title": "Data Source"
        }, 
        {
            "location": "/app_data_framework/#query-operator", 
            "text": "The Query Operator runs embedded in the Data Source Operator, listens for incoming queries from the Message Bus and hands it over to the Data Source Operator. Note that since the Query Operator is embedded within the Data Source Operator, it will not be visible in the DAG view in  dtManage . In both the Twitter and the Sales examples, the Java class that implements the Query Operator is  com.datatorrent.lib.io.PubSubWebSocketAppDataQuery .", 
            "title": "Query Operator"
        }, 
        {
            "location": "/app_data_framework/#result-operator", 
            "text": "The Result Operator runs in an Apex application that publishes the results from the Data Source Operator to the Message Bus. In the Twitter example, it is the  QueryResult  operator. The Java class of this operator in both the Twitter and Sales examples is  com.datatorrent.lib.io.PubSubWebSocketAppDataResult .", 
            "title": "Result Operator"
        }, 
        {
            "location": "/app_data_framework/#message-bus", 
            "text": "Because the operators can run in any node in the cluster, a Message Bus with a pub-sub mechanism is used for delivery for both the queries and the results. This mechanism requires that the caller sends the query message to a topic that the Query Operator listens to, and the Result Operator sends the query result message passed from the Data Source Operator to a different topic that the caller listens to. Typically, the caller is a web browser.  In DataTorrent RTS, this mechanism is provided by  dtGateway . It uses WebSocket to achieve this so that the queries can be made and the results can be processed directly by a web browser.   Referring back to the architecture diagram, the web browser, the Query Operator and the Result Operator all connect to dtGateway via WebSocket, with a pub-sub protocol on top of it.", 
            "title": "Message Bus"
        }, 
        {
            "location": "/app_data_framework/#schemas", 
            "text": "Each App Data Source must provide a schema that describes what is a valid data query and what is a valid data response, so that the caller knows how to query and how to interpret the response from the App Data Sources. We will describe two schemas that underlie the Twitter example and the Sales example.", 
            "title": "Schemas"
        }, 
        {
            "location": "/app_data_framework/#ui-widgets", 
            "text": "In order to allow the end users to visualize the result data in a web browser, we need web-based UI widgets that take user input and send the queries and display the results according to the App Data Sources\u2019 schemas. In DataTorrent RTS, we have a number of UI widgets built in. We will also describe how a user can write their own custom widgets.", 
            "title": "UI Widgets"
        }, 
        {
            "location": "/app_data_framework/#technical-details", 
            "text": "", 
            "title": "Technical Details"
        }, 
        {
            "location": "/app_data_framework/#app-data-source_1", 
            "text": "As described in the overview, an App Data Source consists of three operators: Data Source, Query and Result.  The Result Operator must implement this interface in  com.datatorrent.common.experimental.AppData :  interface ConnectionInfoProvider \n{\n  String getAppDataUrl();\n  String getTopic();\n}  The Query Operator, which is embedded in the Data Store Operator, must implement this interface in  com.datatorrent.common.experimental.AppData :  interface EmbeddableQueryInfoProvider QUERY_TYPE  extends Operator, ConnectionInfoProvider, Operator.ActivationListener OperatorContext \n{\n  DefaultOutputPort QUERY_TYPE  getOutputPort();\n\n  void enableEmbeddedMode();\n}  The port that is returned by the embedded Query Operator\u2019s  getOutputPort()  method will be connected to the input port of the Data Source Operator that is annotated with the marker annotation  com.datatorrent.common.experimental.AppData.QueryPort .  On the DAG level, the Result Operator needs to connect to an output port of the Data Source Operator that is annotated with the marker annotation  com.datatorrent.common.experimental.AppData.ResultPort .   With this setup, the Data Source, along with the query topic and the result topic, will be discoverable by STRAM. STRAM will return the information to the caller via its REST API.  Below is the App Data Source discovery information returned by the  /ws/v2/applications/{appid}  REST call from dtGateway for the Twitter example:  {\n  ...\n   appDataSources : [\n    {\n       name :  SnapshotServer.queryResult ,\n       operatorName :  SnapshotServer ,\n       portName :  queryResult ,\n       query : {\n         operatorName :  SnapshotServer.query ,\n         topic :  TwitterHashtagQueryDemo ,\n         url :  pubsub \n      },\n       result : {\n         appendQIDToTopic : true,\n         operatorName :  QueryResult ,\n        \u201ctopic :  TwitterHashtagQueryResultDemo ,\n         url :  pubsub \n      }\n    }, ... ]\n  ...\n}  The  name  field contains the name of the Data Source, and it has the following form:  dataSourceOperatorName . resultPortName .  The  operatorName  field is the Data Source operator name.  The  portName  field is the result port name of the Data Source operator.  The  query  field describes the query mechanism, including the topic.   The  result  field describes the result mechanism, including the topic and whether the query ID will be appended to the topic. Setting  appendQIDToTopic  to true increases the granularity of topics so that callers can receive results only for their queries. If the cost of creating such a large number of topics is high, it can be set to false; callers then will need to filter the results suitably.", 
            "title": "App Data Source"
        }, 
        {
            "location": "/app_data_framework/#message-bus_1", 
            "text": "dtGateway has a WebSocket service that provides the pubsub mechanism required by the Message Bus. The URL is  ws://{dtGatewayAddress}/pubsub . You can publish data with a topic by sending a WebSocket message like this:  {\n  \u201ctype\u201d: \u201cpublish\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}  The  topic  field is the topic name, and the  data  field is the payload and can be any JSON object.  All client that have subscribed to the topic will receive the data. The format is:  {\n  \u201ctype\u201d: \u201cdata\u201d, \u201ctopic\u201d: \u201c{topic}\u201d, \u201cdata\u201d: {data}\n}  And to subscribe to the topic, the client must send this message to dtGateway using WebSocket:  {\n  \u201ctype\u201d: \u201csubscribe\u201d, \u201ctopic\u201d: \u201c{topic}\u201d\n}  You can try it with dtGateway using a web browser with the help of a WebSocket client (e.g. \u201cSimple WebSocket Client\u201d extension in Google Chrome).  Note that dtGateway does not keep the history of the messages.", 
            "title": "Message Bus"
        }, 
        {
            "location": "/app_data_framework/#interaction-between-data-source-and-web-browser", 
            "text": "The web browser, the Query Operator and the Result Operator all connect to dtGateway via the Message Bus. The following describes the step-by-step interaction among them on a high level:   The UI widget in the browser subscribes to the result topic  The UI widget sends a schema query to the query topic.  The Data Source Operator gets the schema query and sends the schema result to the result topic, which the UI widget receives.  The UI widget in the browser subscribes to the result topic  The UI widget sends a data query  The Data Source Operator gets the data query and processes it and sends back the data result to the result topic.  The UI widget receives the data result and renders it.   Let\u2019s look at the actual messages being exchanged for the Twitter example.", 
            "title": "Interaction between Data Source and Web Browser"
        }, 
        {
            "location": "/app_data_framework/#snapshot-schema", 
            "text": "The Snapshot Schema serves a simple snapshot of tabular data, which is typically constantly being updated and is used in the Twitter example. We will describe the Snapshot schema as we look at the messages.  Step 1: Browser sends Schema Response Subscribe  {\n   type :  subscribe ,\n   topic :  TwitterHashtagQueryResultDemo.0.20716154835833223 \n}  In preparation for a Schema Query, the browser first subscribes to the result topic (with the query ID appended) to get the Schema Result back. In this case, the browser generates a random string, \u201c0.20716154835833223\u201d for the query ID, and appends the query ID to the result topic that it is subscribing to.   Step 2: Browser sends Schema Query  {\n   type :  publish ,\n   topic :  TwitterHashtagQueryDemo ,\n   data : {\n     id : 0.20716154835833223,\n     type :  schemaQuery \n  }\n}  The browser sends a message with type,  schemaQuery . This is the query that asks for the schema type and schema data.   Step 3: Data Source sends Schema Response  {\n   topic :  TwitterHashtagQueryResultDemo.0.20716154835833223 ,\n   data : {\n     id :  0.20716154835833223 ,\n     type :  schemaResult ,\n     data : [\n      {\n         values : [\n          {\n             name :  hashtag ,\n             type :  string \n          },\n          {\n             name :  count ,\n             type :  integer \n          }\n        ],\n         schemaType :  snapshot ,\n         schemaVersion :  1.0 \n      }\n    ]\n  },\n   type :  data \n}  The Data Source sends back the  schemaResult , which contains the response to  schemaQuery , which contains schema type and schema data. In this case, schemaType is snapshot and schemaVersion is 1.0, and the available fields in the data are \u201chashtag\u201d, which is a string, and \u201ccount\u201d, which is an integer.  Step 4: Browser sends Data Result Subscribe  {\n   type :  subscribe ,\n   topic :  TwitterHashtagQueryResultDemo.0.6760250790172551 \n}  Similar to the previous subscribe message, the browser subscribes to the result topic to get the response for the  dataQuery  that the browser is going to issue.  Step 5: Browser sends Data Query Request  {\n   type :  publish ,\n   topic :  TwitterHashtagQueryDemo ,\n   data : {\n     id : 0.6760250790172551,\n     type :  dataQuery ,\n     data : {\n       fields : [\n         hashtag ,\n         count \n      ]\n    },\n     countdown : 30,\n     incompleteResultOK : true\n  }\n}  The browser sends the  dataQuery  message that asks for actual data.  The  countdown  field expects an optional integer value. It tells the Data Source that the Data Source should return results once for subsequent {countdown} application windows. In this example, the value is 30. That means the Data Source should execute this query 30 times, once for each application window for the next 30 application windows.  The  incompleteResultOK  field is an optional boolean value default to be false. If the value is true, the Data Source should return results as soon as they are available even if they are partial results. This is useful when {countdown} is greater than 1 and the Data Source could take a long time to return the complete result set and if it is desirable for the caller to receive the results as soon as possible. If this value is false, Data Source should return the complete result set to the caller  Within the  data  field, the  fields  field tells the Data Source what fields to return in the result.  Step 6: Data Source sends Data Response  {\n   topic :  TwitterHashtagQueryResultDemo.0.6760250790172551 ,\n   data : {\n     id :  0.6760250790172551 ,\n     type :  dataResult ,\n     data : [\n      {\n         count :  1398 ,\n         hashtag :  \uc0ac\uc124\ud1a0\ud1a0\ucd94\ucc9c\uc0ac\uc774\ud2b8 \n      },\n      {\n         count :  1415 ,\n         hashtag :  TopDance \n      },\n      {\n         count :  1498 ,\n         hashtag :  isola \n      },\n      {\n         count :  1521 ,\n         hashtag :  RT\u3057\u305f\u4eba\u5168\u54e1\u30d5\u30a9\u30ed\u30fc\u3059\u308b \n      },\n      {\n         count :  1728 ,\n         hashtag :  4\u670819\u65e5\u306f\u897f\u6728\u91ce\u771f\u59eb\u306e\u8a95\u751f\u65e5 \n      },\n      {\n         count :  1787 ,\n         hashtag :  \ub124\uc784\ub4dc\uc0ac\ub2e4\ub9ac \n      },\n      {\n         count :  2079 ,\n         hashtag :  ALDUBActing101 \n      },\n      {\n         count :  2280 ,\n         hashtag :  \u5730\u9707 \n      },\n      {\n         count :  2712 ,\n         hashtag :  \u897f\u6728\u91ce\u771f\u59eb\u751f\u8a95\u796d2016 \n      },\n      {\n         count :  2714 ,\n         hashtag :  \u0634\u0639\u064a\u0628_\u064a\u0633\u064a\u0621_\u0644\u0644\u0633\u0639\u0648\u062f\u064a\u0647 \n      }\n    ],\n     countdown :  29 \n  },\n   type :  data \n}  The Data Source sends the  dataResult  message in response to the  dataQuery . In this case, it contains a list of (hashtag, count) sets in the  data  field.  If the  countdown  field of the  dataQuery  is non-zero, there will be multiple such messages that correspond to one  dataQuery  message.  The  countdown  field in the  dataResult  message decrements every time it sends a result.  When it counts down to zero, the query has expired and the client must send another  dataQuery  message to receive further updates.  In this case, because the countdown value is 30 in the  dataQuery , the Data Source will keep sending the latest data set for the next 29 application windows following this initial response.  For more information about the operators behind this Snapshot Schema, please refer to  this document .", 
            "title": "Snapshot Schema"
        }, 
        {
            "location": "/app_data_framework/#dimensions-schema", 
            "text": "The Dimensions Schema is an extension of the Snapshot Schema with the notion of time and key dimensions. It is supported in DataTorrent RTS.   We will look at the message exchange in the Sales example.  Step 1: Browser sends Schema Response Subscribe  { type : subscribe , topic : SalesQueryResultDemo.0.676153457723558 }  This is similar to the Snapshot example we saw earlier.  Step 2: Browser sends Schema Query  {\n    type : publish ,\n    topic : SalesQueryDemo ,\n    data :{\n       id : 0.22710832906886935,\n       type : schemaQuery \n   }\n}  Again, this is similar to the Snapshot example we saw earlier.  Step 3: Data Source sends Schema Response.  {\n    topic : SalesQueryResultDemo.0.22710832906886935 ,\n    data :{\n       id : 0.22710832906886935 ,\n       type : schemaResult ,\n       data :[\n         {\n             schemaType : dimensions ,\n             schemaVersion : 1.0 ,\n             time :{\n                buckets :[ 1m , 1h , 1d , 5m ],\n                slidingAggregateSupported :true\n            },\n             keys :[\n               {\n                   name : channel ,\n                   type : string ,\n                   enumValues :[ Mobile ,  Online ,  Store ]\n               },\n               {\n                   name : region ,\n                   type : string ,\n                   enumValues :[ Atlanta ,  Boston , Chicago , Cleveland , Dallas ,\n                      Minneapolis , New York , Philadelphia , San Francisco , St. Louis ]\n               },\n               {\n                   name : product ,\n                   type : string ,\n                   enumValues :[ Laptops , Printers , Routers , Smart Phones , Tablets ]\n               }\n            ],\n             values :[\n               {\n                   name : tax:SUM ,\n                   type : double \n               },\n               {\n                   name : sales:SUM ,\n                   type : double \n               },\n               {\n                   name : discount:SUM ,\n                   type : double \n               }\n            ],\n             dimensions :[\n               {\n                   combination :[\n\n                  ]\n               },\n               {\n                   combination :[\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      region \n                  ]\n               },\n               {\n                   combination :[\n                      product \n                  ]\n               },\n               {\n                   combination :[\n                      region ,\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      channel \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      region \n                  ]\n               },\n               {\n                   combination :[\n                      product ,\n                      region ,\n                      channel \n                  ]\n               }\n            ]\n         }\n      ]\n   },\n    type : data \n}  Compared to the Snapshot Schema, this is considerably more complex. The response to the query describes the keys and aggregates that are provided by the data source. The basic components of the schema are the following:  timebuckets : These are the time buckets over which aggregations are computed. For example, 1d (one day), 1h (one hour), 1m (one minute), 1s (one second).  keys : These are the search parameters you need to provide when you do a query. The keys have an enumValues. This is an optional section which contains all possible values of a key. This section may be updated by the back end if new key values are encountered.  values : These are the results which are returned by a query. Notice that the values are of the form (name):(aggregation). The first portion of the value name describes what is being aggregated. The second portion of the value name describes the aggregation being used.  combinations : This describes the valid combinations of keys that can be specified when doing a query.  Step 4: Browser sends Data Result Subscribe  {\n   type :  subscribe ,\n   topic :  SalesQueryResultDemo.0.3180227109696716 \n}  Step 5: Browser sends Data Query Request  Now that the valid combinations, keys, values, and timebuckets are known, we can issue a query using them.   {\n    type : publish ,\n    topic : SalesQueryDemo ,\n    data :{\n       id :  0.3180227109696716 ,\n       type : dataQuery ,\n       data :{\n          time :{\n             latestNumBuckets :10,\n             bucket : 1m \n         },\n          incompleteResultOK :true,\n          keys :{\n\n         },\n          fields :[\n             time ,\n             channel ,\n             region ,\n             product ,\n             tax:SUM ,\n             sales:SUM ,\n             discount:SUM \n         ]\n      },\n       countdown :29,\n       incompleteResultOK :true\n   }\n}  time : This specifies which time buckets to query. In this case the most recent 10 one minute time buckets are queried. The example above illustrates how to query the last N timebuckets, but it is also possible to do history queries by specifying a time section like the following:           time :{\n          bucket : 1m ,\n          from :1460765563547,\n          to :1460767363547\n       }  This time section specifies a historical query starting at the \u201cfrom\u201d unix timestamp (inclusive) up until the \u201cto\u201d timestamp (inclusive).  keys : These are the search parameters. This section can be empty to query global aggregations. If there are specific key combinations you want to search for you can do this:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: \u201cAtlanta\u201d\n}  It is possible to specify a key with a list like the following:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: [ \u201cAtlanta\u201d, \u201cDallas\u201d ]\n}  This will query all the data with a  channel  value of \u201cMobile\u201d and a  region  value of \u201cAtlanta\u201d OR \u201cDallas\u201d. It is also possible to specify a key with an empty array like the following:  \u201ckey\u201d: {\n  \u201cchannel\u201d: \u201cMobile\u201d\n  \u201cregion\u201d: []\n}  This will query all the data with a  channel  value of \u201cMobile\u201d and ANY value of  region .\nNote that this is different from not specifying the  region  value at all, in which case, the aggregated value of all regions will be returned. This is similar to the difference between the SQL queries of  SELECT SUM(sales) WHERE channel=\u2019Mobile\u2019  (for the case of not specifying  region ) and  SELECT SUM(sales), region WHERE channel=\u2019Mobile\u2019 GROUP BY region  (for the case of  region  being an empty array).  fields : This specifies what data to include in the query result. If the field  time  is included, then a timestamp is included for each data point. Similarly the keys can also be specified as fields with the results. Lastly aggregated values can also be specified as fields to be returned in results.  Step 6: Data Source sends Data Response.  After a query is issued, results are periodically returned:  {\n    topic : SalesQueryResultDemo.0.3180227109696716 ,\n    data : {\n       id : 0.3180227109696716 ,\n       type : dataResult ,\n       data :[\n         {\n             time : 1460771580000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 78363.47 ,\n             sales:SUM : 922988.9199999997 ,\n             discount:SUM : 69135.71999999999 \n         },\n         {\n             time : 1460771640000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 142011.85 ,\n             sales:SUM : 1672677.0099999998 ,\n             discount:SUM : 125287.46999999999 \n         },\n         {\n             time : 1460771700000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 139069.53999999998 ,\n             sales:SUM : 1638011.5399999998 ,\n             discount:SUM : 122692.48999999999 \n         },\n         {\n             time : 1460771760000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 80699.90000000001 ,\n             sales:SUM : 950502.3500000003 ,\n             discount:SUM : 71196.95000000001 \n         },\n         {\n             time : 1460771820000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 35914.69 ,\n             sales:SUM : 423017.9000000001 ,\n             discount:SUM : 69339.05000000002 \n         },\n         {\n             time : 1460771880000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 106977.84999999998 ,\n             sales:SUM : 1260017.5600000003 ,\n             discount:SUM : 629939.14 \n         },\n         {\n             time : 1460771940000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 253243.50000000003 ,\n             sales:SUM : 2982796.9300000006 ,\n             discount:SUM : 1491234.3599999999 \n         },\n         {\n             time : 1460772000000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 161384.77999999997 ,\n             sales:SUM : 1900857.2300000002 ,\n             discount:SUM : 950325.01 \n         },\n         {\n             time : 1460772060000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 125283.99000000002 ,\n             sales:SUM : 1475625.3600000003 ,\n             discount:SUM : 737730.77 \n         },\n         {\n             time : 1460772120000 ,\n             region : Boston ,\n             product : ALL ,\n             channel : Mobile ,\n             tax:SUM : 58684.59 ,\n             sales:SUM : 691200.9299999999 ,\n             discount:SUM : 345562.2700000001 \n         }\n      ],\n       countdown : 297 \n   },\n    type : data \n}  As you can see each result represents a time bucket with the corresponding values for the fields. If the ANY value (with an empty list) query is specified for a key you will see multiple results for the same time bucket, but each result will have different values for the keys.  Detailed documentation of the dimensions operators can be found here:  http://docs.datatorrent.com/operators/dimensions_computation/", 
            "title": "Dimensions Schema"
        }, 
        {
            "location": "/app_data_framework/#advanced-topics", 
            "text": "", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/app_data_framework/#schema-keys", 
            "text": "Although in most cases, a Data Source has one fixed schema, it is sometimes necessary for a Data Source to support multiple schemas. This is useful if the schemas are dynamically created depending on the incoming data to the Data Source Operator.  Schema Key is the feature we added to the App Data Framework for this purpose. If the Data Source has multiple schemas, the  schemaResult  will contain a list of schemas with the corresponding schema keys in the  context  field:  {\n  \u201cid\u201d: \u201c{client_generated_id}\u201d,\n  \u201ctype\u201d: \u201cschemaResult\u201d,\n  \u201cdata\u201d: [ \n    {\n      \u201ccontext\u201d: {\n        \u201cschemaKeys\u201d: {\n          \u201c{key1}\u201d: \u201c{value1}\u201d,\n          \u201c{key2}\u201d: \u201c{value2}\u201d, ...\n         }\n      },\n      {rest of the schema}\n    }, ...\n  ]\n}  And if the  schemaResult  returns multiple schemas, all  dataQuery  messages must include the schema keys:  {\n   \u201cid\u201d: \u201c{client_generated_id}\u201d,\n   \u201ctype\u201d: \u201cdataQuery\u201d, \n   \u201cdata\u201d: {\n     \u201ccontext\u201d: {\n       \u201cschemaKeys\u201d { //optional\n    \u201c{keyName1}\u201d: \u201c{keyValue1}\u201d,\n    \u201c{keyName2}\u201d: \u201c{keyValue2}\u201d \n       }\n     }\n     {rest of the query}\n   }\n   \u201ccountdown\u201d: \u201c{countdown}\u201d,\n   \u201cincompleteResultOK\u201d: true/false\n}  This feature is supported by the DimensionsStore operator.", 
            "title": "Schema Keys"
        }, 
        {
            "location": "/app_data_framework/#dimensions-schema-additional-values-for-combinations", 
            "text": "For the Dimensions Schema supported by the DimensionsStore operator, there can be more complex schema specifications which have results for values only for certain combinations. This is the schema for yet another example about aggregation of advertisement data in the DataTorrent RTS example repository.  {\n    topic : AdsQueryGenericResultDemo.0.08170037856325507 ,\n    data :{\n       id : 0.08170037856325507 ,\n       type : schemaResult ,\n       data :[\n         {\n             schemaType : dimensions ,\n             schemaVersion : 1.0 ,\n             time :{\n                buckets :[ 1m , 1h , 1d ],\n                slidingAggregateSupported :true,\n                from : 1459987200000 ,\n                to : 1460768100000 \n            },\n             keys :[\n               {\n                   name : publisher ,\n                   type : string ,\n                   enumValues :[ twitter , facebook , yahoo , google , bing , amazon \n                  ]\n               },\n               {\n                   name : advertiser ,\n                   type : string ,\n                   enumValues :[ starbucks , safeway , mcdonalds , macys , taco bell ,\n                      walmart , kohls , san diego zoo , pandas , jack in the box ,\n                      tomatina , ron swanson \n                  ]\n               },\n               {\n                   name : location ,\n                   type : string ,\n                   enumValues :[\n                      N , LREC , SKY , AL , AK , AZ , AR , CA , CO , CT , DE , FL ,\n                      GA , HI , ID \n                  ]\n               }\n            ],\n             values :[\n               {\n                   name : impressions:COUNT ,\n                   type : long \n               },\n               {\n                   name : impressions:SUM ,\n                   type : long \n               },\n               {\n                   name : impressions:AVG ,\n                   type : double \n               },\n               {\n                   name : clicks:COUNT ,\n                   type : long \n               },\n               {\n                   name : clicks:SUM ,\n                   type : long \n               },\n               {\n                   name : clicks:AVG ,\n                   type : double \n               },\n               {\n                   name : cost:COUNT ,\n                   type : long \n               },\n               {\n                   name : cost:SUM ,\n                   type : double \n               },\n               {\n                   name : cost:AVG ,\n                   type : double \n               },\n               {\n                   name : revenue:COUNT ,\n                   type : long \n               },\n               {\n                   name : revenue:SUM ,\n                   type : double \n               },\n               {\n                   name : revenue:AVG ,\n                   type : double \n               }\n            ],\n             dimensions :[\n               {\n                   combination :[\n\n                  ]\n               },\n               {\n                   combination :[\n                      location \n                  ]\n               },\n               {\n                   combination :[\n                      advertiser \n                  ],\n                   additionalValues :[\n                     {\n                         name : impressions:MAX ,\n                         type : long \n                     },\n                     {\n                         name : impressions:MIN ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MAX ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MIN ,\n                         type : long \n                     },\n                     {\n                         name : cost:MAX ,\n                         type : double \n                     },\n                     {\n                         name : cost:MIN ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MAX ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MIN ,\n                         type : double \n                     }\n                  ]\n               },\n               {\n                   combination :[\n                      publisher \n                  ],\n                   additionalValues :[\n                     {\n                         name : impressions:MAX ,\n                         type : long \n                     },\n                     {\n                         name : impressions:MIN ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MAX ,\n                         type : long \n                     },\n                     {\n                         name : clicks:MIN ,\n                         type : long \n                     },\n                     {\n                         name : cost:MAX ,\n                         type : double \n                     },\n                     {\n                         name : cost:MIN ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MAX ,\n                         type : double \n                     },\n                     {\n                         name : revenue:MIN ,\n                         type : double \n                     }\n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      advertiser \n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      publisher \n                  ]\n               },\n               {\n                   combination :[\n                      advertiser ,\n                      publisher \n                  ]\n               },\n               {\n                   combination :[\n                      location ,\n                      advertiser ,\n                      publisher \n                  ]\n               }\n            ]\n         }\n      ]\n   },\n    type : data \n}  Here you can see that a combination can also have  additionalValues  specified. Additional values are values which are only available for a specific combination of keys.  The code for the ads example is available  here .", 
            "title": "Dimensions Schema: Additional Values for Combinations"
        }, 
        {
            "location": "/app_data_framework/#ui-widgets_1", 
            "text": "Coming soon", 
            "title": "UI Widgets"
        }, 
        {
            "location": "/dtgateway_api/", 
            "text": "DataTorrent dtGateway API v2 Specification\n\n\nREST API\n\n\nReturn codes\n\n\n\n\n200\n: OK\n\n\n400\n: The request is not in the format that the server expects\n\n\n404\n: The resource is not found\n\n\n500\n: Something is wrong on the server side\n\n\n\n\nREST URI Specification\n\n\nGET /ws/v2/applications/{appid}/metrics\n\n\nFunction: List all available metrics for an application.\n\n\nExample: \n\n\n{\n    \napplicationId\n: \n{applicationId}\n,\n    \nmetrics\n: [\n        {\n            \nname\n: \n{metrcName}\n,\n            \ntype\n: \n{metricType}\n,\n            \nvalues\n: \n{value}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: \n\n\nExample:\n\n\n{\n    \nid\n: \n{appId}\n,\n    \nname\n: \n{name}\n,\n    \nqueue\n: \n{queue}\n,\n    \nstate\n: \n{applicationState}\n,\n    \nfinalStatus\n: \n{finalStatus}\n,\n    \nstartedTime\n: \n{applicationStartedTime}\n,\n    \nfinishedTime\n: \n{applicationFinishedTime}\n,\n    \ndiagnostics\n: \n{diagnostics}\n,\n    \napplicationType\n: \n{applicationType}\n,\n    \ntrackingUrl\n: \n{trackingUrl}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nuser\n: \n{user}\n,\n    \ncurrentAttemptId\n: \n{currentAttemptId}\n,\n    \nrunningContainers\n: \n{runningContainers}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \nallocatedVCores\n: \n{allocatedVCores}\n,\n    \ncanWrite\n: \ntrue/false\n,\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nappDataSources\n: [\n        {\n            \nquery\n:\n            {\n                \noperatorName\n: \n{operatorName-1}\n,\n                \nurl\n: \n{url-1}\n,\n                \ntopic\n: \n{topic-1}\n\n            },\n            \nresult\n:\n            {\n                \noperatorName\n: \n{operatorName-1}\n,\n                \nurl\n: \n{url-1}\n,\n                \ntopic\n: \n{topic-1}\n,\n                \nappendQIDToTopic\n: \ntrue/false\n\n            },\n            \ntype\n: \n{type}\n,\n            \nname\n: \n{dataSourceName}\n\n        },\n        ...\n    ],\n    \nmetrics\n:\n    {\n        \nOperator1\n:\n        {\n            \n{metricName1}\n: \n{value}\n,\n            \n{metricName2}\n: \n{value}\n\n                ...\n        },\n        \nOperator2\n:\n        {\n            \n{metricName1}\n: \n{value}\n,\n            \n{metricName2}\n: \n{value}\n\n                ...\n        },\n        ...\n    },\n    \nattributes\n:\n    {\n        \n{attributeName}\n: \n{attributeValue}\n,\n        ...\n        \n{attributeName-n}\n: \n{attributeValue-n}\n\n    },\n    \nappMasterTrackingUrl\n: \n{appMasterTrackingUrl}\n,\n    \nversion\n: \n{apex version}\n,\n    \nstats\n:\n    {\n        \nlatency\n: \n{latency}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nwindowStartMillis\n: \n{windowStartMillis}\n,\n        \ncriticalPath\n: [\n            \n{criticalPathNumber-1}\n,\n            ...\n            \n{criticalPathNumber-n}\n\n        ],\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n: \n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n: \n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n: \n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nmemoryRequired\n: \n{memoryRequired}\n,\n        \ntotalVCoresAllocated\n: \n{totalVCoresAllocated}\n,\n        \nvcoresRequired\n: \n{vcoresRequired}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n,\n        \nallocatedContainers\n: \n{numberOfAllocatedContainers}\n\n    },\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nappPackageSource\n:\n    {\n        \nuser\n: \n{user}\n,\n        \nname\n: \n{appPackageName}\n,\n        \nversion\n: \n{appPackageVersion}\n,\n        \nappName\n: \n{appName}\n,\n        \nconfigPackage\n:\n        {\n            \nuser\n: \n{user}\n,\n            \nname\n: \n{configPackageName}\n,\n            \nversion\n: \n{configPackageVersion}\n\n        }\n    },\n    \nlaunchDisabled\n: \ntrue/false\n,\n    \nservices\n: [\n        {\n            \nname\n: \n{serviceName}\n,\n            \nstate\n: \n{state}\n,\n            \ntype\n: \ndocker/apex\n,\n            \nrequiredServices\n: [\n{requiredService1}\n, \n{requiredServices}\n, ...],\n            \nproxy\n:\n            {\n                \nname\n: \n{proxyName}\n,\n                \naddress\n: \nproxyAddress\n\n            },\n            \ninstalledTime\n: \n{installTime}\n,\n            \nstartedTime\n: \n{startTime}\n,\n            \nenabled\n: \ntrue/false\n,\n            \nmemoryMB\n: \n{memoryMB}\n,\n            \nsrcUrl\n: \n{srcUrl}\n,\n            \ndocker/apex property name-1\n: \n{value-1}\n,\n            ...\n            \ndocker/apex property name-n\n: \n{value-n}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/failureRootCause\n\n\nFunction: Return the root cause of why an application failed to run.\n\n\nExample:\n\n\n{\n    \ntitle\n: \nFC_ERROROUTPUT\n,\n    \ntype\n: \nmarkdown\n,\n    \ncontent\n: \n{root cause of why the application {appid} failed}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}\ntypes={all,appmaster}]\n\n\nFunction:\n\n\nExample: \n\n\n{\n    \ncontainers\n: [\n        {\n            \nid\n: \n{id}\n,\n            \nhost\n: \n{host}\n,\n            \nstate\n: \n{NEW,ALLOCATED,ACTIVE,KILLED}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: {lastHeartbeat},\n            \nnumOperators\n: {numOperators},\n            \noperators:\n {\n                \nid1\n: \nname1\n,\n                \nid2\n: \nname2\n,\n                \nid3\n: \nname3\n\n            },\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \ngcCollectionTime\n: \n{gcCollectionTime}\n,\n            \ngcCollectionCount\n: \n{gcCollectionCount}\n,\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstartedTime\n: \n{containerStartTime}\n,\n            \nfinishedTime\n: \n{containerFinishedTime}\n,\n            \nrawContainerLogsUrl\n: \n{rawContainerLogsUrl}\n,\n            \ncontainerType\n: \nAPP_MASTER|STREAMING\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction:\n\n\nExample:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nhost\n: \n{host}\n,\n    \nstate\n: \n{NEW,ALLOCATED,ACTIVE,KILLED}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n:\n    {\n        lastHeartbeat\n    },\n    \nnumOperators\n:\n    {\n        numOperators\n    },\n    \noperators:\n\n    {\n        \nid1\n: \nname1\n,\n        \nid2\n: \nname2\n,\n        \nid3\n: \nname3\n\n    },\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \ngcCollectionTime\n: \n{gcCollectionTime}\n,\n    \ngcCollectionCount\n: \n{gcCollectionCount}\n,\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstartedTime\n: \n{containerStartTime}\n,\n    \nfinishedTime\n: \n{containerFinishedTime}\n,\n    \nrawContainerLogsUrl\n: \n{rawContainerLogsUrl}\n,\n    \ncontainerType\n: \nAPP_MASTER|STREAMING\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction:\n\n\nExample: \n\n\n{\n    \nlogs\n: [\n        {\n            \nname\n: \nlogName-1\n,\n            \nlength\n: \n{length}\n,\n            \nrawUrl\n: \n{urlToLog}\n\n        },\n        {\n            \nname\n: \nlogName-2\n,\n            \nlength\n: \n{length}\n,\n            \nrawUrl\n: \n{urlToLog}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}\nlastNBytes={numberOfBytes}]\n\n\nFunction: Return the raw log, or the last N bytes of the log if lastNBytes is given.\n\n\nReturn: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\nExample: \n\n\n{\n    \nlines\n: [\n        {\n            \nbyteOffset\n: \n{byteOffsetFromStartOfTheLog}\n,\n            \nline\n: \n{one line from {logName}}\n\n        },\n        {\n            \nbyteOffset\n: \n{byteOffsetFromStartOfTheLog}\n,\n            \nline\n: \n{the next line from {logName}}}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/logs/{logName}[?grep={regexp}\ndescendingOrder={true/false}\nincludeOffset={true/false}\nlastNBytes={numberOfBytes}]\n\n\nFunction: Return GC events for the application in sorted order.\n\n\nReturn: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\nExample: \n\n\n{\n    \nlines\n: [\n        {\n            \nbyteOffset\n: \n{byteOffset}\n,\n            \nline\n: \n{GC event}\n,\n            \nstartTime\n: \n{eventStartTime}\n,\n            \ntype\n: \nGC\n,\n            \noccupiedHeapMemoryBefore\n: \n{occupiedHeapMemoryBefore}\n,\n            \noccupiedHeapMemoryAfter\n: \n{occupiedHeapMemoryAfter}\n,\n            \nheapCapacity\n: \n{heapCapacity}\n,\n            \nheapReductionPercentage\n: \n{heapReductionPercentage}\n,\n            \nduration\n: \n{duration}\n,\n            \ncontainer\n: \n{containerId}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{operatorName}/logs/{logName}[?grep={regexp}\ndescendingOrder={true/false}\nlastNBytes={numberOfBytes}]\n\n\nFunction: Return GC events for the an logical operator in sorted order. A logical operator maps to one or more physical operator(s). \nEach physical operator belongs to a container and all the GC events from all such containers are collected and returned.\n\n\nExample: \n\n\n{\n    \nlines\n: [\n        {\n            \nbyteOffset\n: \n{byteOffset}\n,\n            \nline\n: \n{GC event}\n,\n            \nstartTime\n: \n{eventStartTime}\n,\n            \ntype\n: \nGC\n,\n            \noccupiedHeapMemoryBefore\n: \n{occupiedHeapMemoryBefore}\n,\n            \noccupiedHeapMemoryAfter\n: \n{occupiedHeapMemoryAfter}\n,\n            \nheapCapacity\n: \n{heapCapacity}\n,\n            \nheapReductionPercentage\n: \n{heapReductionPercentage}\n,\n            \nduration\n: \n{duration}\n,\n            \ncontainer\n: \n{containerId}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/issues\n\n\nFunction: Return list of issues of the applicaiton.\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        ...\n    ]    \n}\n\n\n\n\nGET /ws/v2/appPackages?hasServices={true/false}\nhasUI={true/false}\n\n\nFunction: \n\n\nExample: \n\n\n{\n    \nappPackages\n: [\n        {\n            \nowner\n: \n{owner}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nappPackageName\n: \n{appPackageName}\n,\n            \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nappPackageDisplayName\n: \n{appPackageDisplayName}\n,\n            \nappPackageDescription\n: \n{appPackageDescription}\n,\n            \napplications\n: [\n                {\n                    \nname\n: \n{application-1}\n,\n                    \ndisplayName\n: \n{applicationDisplayName-1}\n,\n                    \ntype\n: \nclass/json\n,\n                    \nlaunchDisabled\n: \ntrue/false\n\n                },\n                ...\n            ],\n            \ncanWrite\n: \ntrue/false\n,\n            \nui\n: {\n                \ndashboards\n: [\n                    {\n                        \nname\n: \n{dashboardName-1}\n,\n                        \nfile\n: \n{dashboardFile-1}\n,\n                        \nappNames\n: [\n                            \n{applicationUsingThisDashborad-1}\n,\n                            \n{applicationUsingThisDashborad-2}\n,\n                            ...\n                        ]\n                    },\n                    ...\n                ]\n            },\n            \nservices\n: {\n                \nservices\n: [\n                    {\n                        // Sample docker service\n                        \nname\n: \n{serviceName}\n,\n                        \ntype\n: \ndocker\n,\n                        \nsrcUrl\n: \n{srcUrl}\n,\n                        \ndocker\n: {\n                            \nrun\n: \n{options and arguments to run docker service}\n\n                        },\n                        \nproxy\n: {\n                            \nname\n: \n{proxyName}\n,\n                            \naddress\n: \n{proxyAddress}\n\n                        },\n                        \nrequiredServices\n: [\n                            \n{requiredServiceName-1}\n,\n                            \n{requiredServiceName-2}\n,\n                            ...\n                        ]\n                    },\n                    {\n                        // Sample Apex service\n                        \nname\n: \n{serviceName}\n,\n                        \ntype\n: \napex\n,\n                        \nsrcUrl\n: \n{srcUrl}\n,\n                        \napex\n: {\n                            \nappName\n: \nApexApplicationName\n,\n                            \nlaunchArgs\n: {\n                                \n{launchArgName}\n: \n{launchArgValue}\n\n                            }\n                        },\n                        \nproxy\n: {\n                            \nname\n: \n{proxyName}\n,\n                            \naddress\n: \n{proxyAddress}\n\n                        },\n                        \nmetadata\n: {\n                            \nQueryIP\n: \n{queryIP}\n,\n                            \nQueryPort\n: \n{queryPort}\n\n                        }\n                    }\n                ],\n                \napplications\n: [\n                    {\n                        \nname\n: \n{applicationUsesService}\n,\n                        \nrequiredServices\n: [\n                            {\n                                \nname\n: \nrequiredServiceName\n,\n                                \n{servivePropertyName-1}\n: \n{serviceProperty}\n,\n                                \n{servivePropertyName-2}\n: \n{serviceProperty}\n,\n                                ...\n                            },\n                            ...\n                        ]\n                    }\n                ]\n            }\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}?includeDescription={true/false}\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns: If \nincludeDescription\n is set to be false or not provided, return meta data for such app package with properties as simple name-value pairs. If \nincludeDescription\n is true, properties will also include description information as well.\n\n\nExample: \n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nappPackageGroupId\n: \n{appPackageGroupId}\n,\n    \ndtEngineVersion\n: \n{dtEngineVersion}\n,\n    \nappPackageDescription\n: \n{appPackageDescription}\n,\n    \nappPackageDisplayName\n: \n{appPackageDisplayName}\n,\n    \nclassPath\n: [\n        \n{classPath}\n\n    ],\n    \napplications\n: [\n        {applicationMetaData-1},\n        {applicationMetaData-2},\n        ...\n    ],\n    \nappJars\n: [\n        \n{appJar-1}\n,\n        \n{appJar-2}\n\n        ...\n    ],\n    \nappJsonFiles\n: [\n        {\nappJsonFile-1\n},\n        {\nappJsonFile-2\n},\n        ...\n    ],\n    \nappPropertiesFiles\n: [],\n    \nrequiredProperties\n: [\n        // when includeDescription=false\n        {\npropertyName-1\n},\n        {\npropertyName-2\n},\n        ...\n\n        // When includeDescription=true\n        {\npropertyName-1\n}: {\n            \nvalue\n: null,\n            \ndescription\n: \n{descriptionOfProperty}\n\n        },\n        ...\n    ],\n    \ndefaultProperties\n: {\n        // when includeDescription=false\n        {\npropertyName-1\n}: {defaultValueOfProperty-1},\n        {\npropertyName-2\n}: {defaultValueOfProperty-2},\n        ...\n\n        // When includeDescription=true\n        {\npropertyName-1\n}: {\n            \nvalue\n: \n{defaultValueOfProperty}\n,\n            \ndescription\n: \n{descriptionOfProperty}\n\n        },\n        ...\n    },\n    \nconfigs\n: [\n        {\nconfig-1\n},\n        {\nconfig-2\n},\n        ...\n    ],\n    \nowner\n: \n{owner}\n,\n    \nmodificationTime\n: \n{modificationTime}\n,\n    \ncanWrite\n: \ntrue/false\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nFunction: \n\n\nExample: \n\n\n{\n    \napplications\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nfile\n: \n{fileName}\n,\n            \ntype\n: \n{type}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \ndag\n: {dag in json format},\n            \nerror\n: \n{error}\n,\n            \nerrorStackTrace\n: \n{errorStackTrace}\n,\n            \nrequiredProperties\n: [\n                {\npropertyName-1\n},\n                {\npropertyName-2\n},\n                ...\n            ],\n            \ndefaultProperties\n: {\n                {\npropertyName-1\n}: {defaultValueOfProperty-1},\n                {\npropertyName-2\n}: {defaultValueOfProperty-2},\n                ...\n            }\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}[?includeDescription={true/false}]\n\n\nFunction: Gets the meta data for that application\n\n\nExample: \n\n\n{\n    \nname\n: \n{name}\n,\n    \nfile\n: \n{fileName}\n,\n    \ntype\n: \n{type}\n,\n    \ndisplayName\n: \n{displayName}\n,\n    \ndag\n:{dag in json format},\n    \nerror\n: \n{error}\n,\n    \nerrorStackTrace\n: \n{errorStackTrace}\n,\n    \nrequiredProperties\n: [\n        // when includeDescription=false\n        {\npropertyName-1\n},\n        {\npropertyName-2\n},\n        ...\n\n        // When includeDescription=true\n        {\npropertyName-1\n}: {\n            \nvalue\n: null,\n            \ndescription\n: \n{descriptionOfProperty}\n\n        },\n        ...\n    ],\n    \ndefaultProperties\n: {\n        // when includeDescription=false\n        {\npropertyName-1\n}: {defaultValueOfProperty-1},\n        {\npropertyName-2\n}: {defaultValueOfProperty-2},\n        ...\n\n        // When includeDescription=true\n        {\npropertyName-1\n}: {\n            \nvalue\n: \n{defaultValueOfProperty}\n,\n            \ndescription\n: \n{descriptionOfProperty}\n\n        },\n        ...\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/classSchemas\n\n\nFunction: This is deprecated.\n\n\nReturn:\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/classSchemas/{classSchemasName}[?version={version}]\n\n\nFunction: This is deprecated.\n\n\nReturn:\n\n\nGET /ws/v2/cluster/config\n\n\nFunction: Get the list of all configurations on cluster.\n\n\nExample: \n\n\n{\n    \n{configName-1}\n: \n{configValue-1}\n,\n    \n{configName-2}\n: \n{configValue-2}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/cluster/queues\n\n\nFunction: Get the list of queues on cluster..\n\n\nExample: \n\n\n    \nqueues\n: [\n        {\n            \ncapacity\n: \n{Capacity of the queue}\n,\n            \ncurrentCapacity\n: \n{Current capacity of the queue}\n,\n            \nmaxCapacity\n: \n{Maximum capacity of the queue}\n,\n            \nname\n: \n{Queue name}\n,\n            \nstate\n: \n{State of the Queue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns the list of all configuration properties defined in the configuration files.\n\n\nExample: \n\n\n   {\n    \n{name}\n:\n    {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n,\n        \nscope\n: \n{scope}\n\n    },\n    ...\n}\n\n\n\n\nGET /ws/v2/config/docker\n\n\nFunction: Get docker configuration status\n\n\nExample: \n\n\n{\n  \nisFound\n: \ntrue/false\n, \n  \u201cversion\u201d: \u201c{version}\u201d,\n  \nisCompatible\n: \ntrue/false\n,\n  \ndt.dockerHost\n: \n{dockerHostAddress}\n\n}\n\n\n\n\nPUT /ws/v2/config/docker\n\n\nFunction: Set docker configuration\n\n\nPayload:\n{\n    \"dt.dockerHost\": \"\"{dockerHostAddress}\"\n}\n\n\nPOST /ws/v2/licenses?filename={license-name}\n\n\nFunction: The request payload of the API is the content of the license.\n\n\nExample: \n\n\nReturns:\n{\n    \ncurrentTime\n: \n{currentTimeMills}\n,\n    \nstartDate\n: \n{startTimeMills}\n,\n    \nexpireTime\n: \n{expireTimeMills}\n,\n    \nmemoryMBAllowed\n: \n{memoryMBAllowed}\n,\n    \nmemoryMBUsed\n: \n{memoryMBUsed}\n,\n    \nissuedTo\n: \n{issuedTo}\n,\n    \nissuedBy\n: \n{issuedBy}\n,\n    \nissuerWebsite\n: \n{issuerWebsite}\n,\n    \nsupportedBy\n: \n{supportedBy}\n,\n    \nsupportURL\n: \n{supportURL}\n,\n    \ncategory\n: \nDT Premium/DT PLUS\n,\n    \nexceedGracePeriod\n: \n{exceedGracePeriod}\n,\n    \nvalid\n: {true/false},\n    \nid\n: \n{licenseId}\n,\n    \nlicenseType\n: \n{licenseType}\n\n}\n\n\n\n\nGET /ws/v2/licenses\n\n\nFunction: This is deprecated. Use the API \nGET /v2/licenses/current\n instead.\n\n\nExample: \n\n\n \nlicenses\n: [\n    {\n        \ncurrentTime\n: \n{currentTimeMills}\n,\n        \nstartDate\n: \n{startTimeMills}\n,\n        \nexpireTime\n: \n{expireTimeMills}\n,\n        \nmemoryMBAllowed\n: \n{memoryMBAllowed}\n,\n        \nmemoryMBUsed\n: \n{memoryMBUsed}\n,\n        \nissuedTo\n: \n{issuedTo}\n,\n        \nissuedBy\n: \n{issuedBy}\n,\n        \nissuerWebsite\n: \n{issuerWebsite}\n,\n        \nsupportedBy\n: \n{supportedBy}\n,\n        \nsupportURL\n: \n{supportURL}\n,\n        \ncategory\n: \nDT Premium/DT PLUS\n,\n        \nexceedGracePeriod\n: \n{exceedGracePeriod}\n,\n        \nvalid\n: {true/false},\n        \nid\n: \n{licenseId}\n,\n        \nlicenseType\n: \n{licenseType}\n\n    }]\n}\n\n\n\n\nGET /ws/v2/licenses/{id}\n\n\nFunction: Get the selected license. The parameter 'id' can be the string \"current\" or the valid current license id.\n\n\nExample:\n\n\n{\n    \ncurrentTime\n: \n{currentTimeMills}\n,\n    \nstartDate\n: \n{startTimeMills}\n,\n    \nexpireTime\n: \n{expireTimeMills}\n,\n    \nmemoryMBAllowed\n: \n{memoryMBAllowed}\n,\n    \nmemoryMBUsed\n: \n{memoryMBUsed}\n,\n    \nissuedTo\n: \n{issuedTo}\n,\n    \nissuedBy\n: \n{issuedBy}\n,\n    \nissuerWebsite\n: \n{issuerWebsite}\n,\n    \nsupportedBy\n: \n{supportedBy}\n,\n    \nsupportURL\n: \n{supportURL}\n,\n    \ncategory\n: \nDT Premium/DT PLUS\n,\n    \nexceedGracePeriod\n: \n{exceedGracePeriod}\n,\n    \nvalid\n: {true/false},\n    \nid\n: \n{licenseId}\n,\n    \nlicenseType\n: \n{licenseType}\n\n}\n\n\n\n\nGET /ws/v2/phoneHome/report[?period={total/previous/current}]\n\n\nFunction: Get a report which is in the same format of the usage report gateway is generating and sending back to DataTorrent. When the query parameter \"period\" is omitted, by default it will return stats from the total period.\n\n\nExample:\n\n\n{\n    \nlicenseId\n: \n{licenseId}\n,\n    \nnumNodesInCluster\n: \n{numberOfNodesInCluster}\n,\n    \nlicense\n:\n    {\n        \ncurrentTime\n: \n{currentTimeMills}\n,\n        \nstartDate\n: \n{startTimeMills}\n,\n        \nexpireTime\n: \n{expireTimeMills}\n,\n        \nmemoryMBAllowed\n: \n{memoryMBAllowed}\n,\n        \nmemoryMBUsed\n: \n{memoryMBUsed}\n,\n        \nissuedBy\n: \n{issuedBy}\n,\n        \nissuerWebsite\n: \n{issuerWebsite}\n,\n        \nsupportedBy\n: \n{supportedBy}\n,\n        \nsupportURL\n: \n{supportURL}\n,\n        \ncategory\n: \n{DT Premium/DT PLUS}\n,\n        \nexceedGracePeriod\n: \n{exceedGracePeriod}\n,\n        \nvalid\n: {true/false},\n        \nexpirationTimeNotificationLevel\n: \n{INFO/WARN/ERROR}\n,\n        \nid\n: \n{licenseId}\n,\n        \nlicenseType\n: \n{licenseType}\n\n    },\n    \nrts.version\n: \n{rtsVersion}\n,\n    \napex.version\n: \n{apexVersion}\n,\n    \njava.vendor\n: \n{javaVendor}\n,\n    \njava.version\n: \n{javaVersion}\n,\n    \nos.arch\n: \n{operatingSystemArchitecture}\n,\n    \nos.name\n: \n{operatingSystemName}\n,\n    \nos.version\n: \n{operatingSystemVersion}\n,\n    \nhadoop.version\n: \n{hadoopVersion}\n,\n    \nmetrics\n:\n    {\n        \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n        \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n        \ncpuPercentage\n: \n{cpuPercentage}\n,\n        \ngatewayUpTimeMills\n: \n{gatewayUpTimeMills}\n,\n        \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n        \nperiod\n: \n{total/previous/current}\n,\n        \nstartTime\n: \n{startTimeOfThePeriod}\n,\n        \nappsRunningMax\n: \n{appsRunningMaxInThePeriod}\n,\n        \nageOfAppsAvg\n: \n{ageOfAppsAvgInThePeriod}\n,\n        \nappsRunningAvg\n: \n{appsRunningAvgInThePeriod}\n,\n        \nmemMBMax\n: \n{memMBMaxInThePeriod}\n,\n        \nmemMBMin\n: \n{memMBMinInThePeriod}\n,\n        \nnumAppsFinished\n: \n{numAppsFinishedInThePeriod}\n,\n        \nnumOfContainersAvg\n: \n{numOfContainersAvgInThePeriod}\n,\n        \nendTime\n: \n{endTimeOfThePeriod}\n,\n        \nappsRunningMin\n: \n{appsRunningMinInThePeriod}\n,\n        \nnumOfOperatorsAvg\n: \n{numOfOperatorsAvgInThePeriod}\n\n    }\n}\n\n\n\n\nGET /ws/v2/services\n\n\nFunction:  list of all the currently installed services and their statuses.\n\n\nExample:\n\n\n\n{\n    \nservices\n: [\n    {\n        \nname\n: \n{serviceName}\n,\n        \nstate\n: \n{serviceState}\n,\n        \nstartedTime\n: {serviceStartTime},\n        \ninstalledTime\n: {serviceInstallTime},\n        \nenabled\n: {true / false},\n        \ntype\n: \n{docker/apex}\n,\n        \ndependentApps\n: [\n        {\n            appId: \n{appId}\n,\n            appName: \n{appName}\n,\n            state: \n{appState}\n,\n            user: \n{user}\n\n        }],\n        \nmemoryMB\n: {memoryMB},\n        \nrequiredServices\n: [\n            \n{requiredService-1}\n,\n            ...\n        ],\n        \nsrcUrl\n: \n{srcUrl}\n,\n        \ndocker\n:\n        {\n            \nrun\n: \n{options and arguments to run docker service}\n,\n            \nexec\n: \n{optional exec command to run after the container is started}\n\n        },\n        \nproxy\n:\n        {\n            \nname\n: \n{proxyName}\n,\n            \naddress\n: \n{proxyAddress}\n,\n        \nfollowRedirect\n: {true/false}\n        },\n        \ncontainerId\n:{containerId}\n    },\n    {\n        \nname\n: \n{serviceName}\n,\n        \ntype\n: \napex\n,\n        \nsrcUrl\n: \n{srcUrl}\n,\n        \napex\n:\n        {\n            \nappName\n: \nApexApplicationName\n,\n            \nlaunchArgs\n:\n            {\n                \n{launchArgName}\n: \n{launchArgValue}\n\n            }\n        },\n        \nproxy\n:\n        {\n            \nname\n: \n{proxyName}\n,\n            \naddress\n: \n{proxyAddress}\n,\n        \nfollowRedirect\n: {true/false}\n        },\n        \nmetadata\n:\n        {\n            \nvar-name\n: \n{value}\n\n        }\n    }]\n}\n\n\n\n\nGET /ws/v2/services/{name}\n\n\nFunction: Get details for a specific service\n\n\nExample: \n\n\n    {\n        \nname\n: \n{serviceName}\n,\n        \nstate\n: \n{serviceState}\n,\n        \nstartedTime\n: {serviceStartTime},\n        \ninstalledTime\n: {serviceInstallTime},\n        \nenabled\n: {true / false},\n        \ntype\n: \n{docker/apex}\n,\n        \ndependentApps\n: [\n        {\n            appId: \n{appId}\n,\n            appName: \n{appName}\n,\n            state: \n{appState}\n,\n            user: \n{user}\n\n        }],\n        \nmemoryMB\n: {memoryMB},\n        \nrequiredServices\n: [\n            \n{requiredService-1}\n,\n            ...\n        ],\n        \nsrcUrl\n: \n{srcUrl}\n,\n        \ndocker\n:\n        {\n            \nrun\n: \n{options and arguments to run docker service}\n,\n            \nexec\n: \n{optional exec command to run after the container is started}\n\n        },\n        \nproxy\n:\n        {\n            \nname\n: \n{proxyName}\n,\n            \naddress\n: \n{proxyAddress}\n,\n        \nfollowRedirect\n: {true/false}\n        },\n        \ncontainerId\n:{containerId}\n    }\n\n\n\n\nPUT /ws/v2/services/{name}\n\n\nFunction: Install a new service with specified JSON params\n\n\nPayload:\n\n\n{\n    \nname\n: \n{serviceName}\n,\n    \nenabled\n:\n    {\n        true / false\n    },\n    \ntype\n: \n{docker/apex}\n,\n    \nrequiredServices\n: [\n        \n{requiredService-1}\n,\n        ...\n    ],\n    \nsrcUrl\n: \n{srcUrl}\n,\n    \ndocker\n:\n    {\n        \nrun\n: \n{options and arguments to run docker service}\n\n    },\n    \nproxy\n:\n    {\n        \nname\n: \n{proxyName}\n,\n        \naddress\n: \n{proxyAddress}\n\n    }\n}\n\n\n\n\nDELETE /ws/v2/services/{name}\n\n\nFunction: Delete specified service\n\n\nPOST /ws/v2/services/{name}\n\n\nFunction: Update specified service\n\n\nPayload:\n\n\n{\n    \nname\n: \n{serviceName}\n,\n    \nenabled\n:\n    {\n        true / false\n    },\n    \ntype\n: \n{docker/apex}\n,\n    \nrequiredServices\n: [\n        \n{requiredService-1}\n,\n        ...\n    ],\n    \nsrcUrl\n: \n{srcUrl}\n,\n    \ndocker\n:\n    {\n        \nrun\n: \n{options and arguments to run docker service}\n\n    },\n    \nproxy\n:\n    {\n        \nname\n: \n{proxyName}\n,\n        \naddress\n: \n{proxyAddress}\n\n    }\n}\n\n\n\n\nPOST /ws/v2/services/{name}/start\n\n\nFunction: Start the specified service.\n\n\nPOST /ws/v2/services/{name}/stop\n\n\nFunction: Stop the specified service\n\n\nPOST /ws/v2/services/install[?async={true/false}]\n\n\nFunction: Installs multiple services based on JSON params array.  Services are launched after install. hasAppDataSources is an optional flag, if set to true will cause service to respond immediately with 200 or error without waiting for download, installation and launch to complete\n\n\nPayload:\n\n\n[\n    {\n        \nname\n: \n{serviceName}\n,\n        \ntype\n: \ndocker\n,\n        \nsrcUrl\n: \n{dockerAddress}\n,\n        \ndocker\n:\n        {\n            \nrun\n: \n{run options}\n,\n            \nexec\n: \necho optional command and args\n\n        },\n        \nproxy\n:\n        {\n            \nname\n: \n{proxyName}\n,\n            \naddress\n: \n{proxyAddress}\n\n        },\n        \nrequiredServices\n: [\n{requiredService-1}\n],\n        \nenabled\n: true //enabled by default\n    }\n]\n\n\n\n\nGET /ws/v2/about\n\n\nExample:\n\n\n\n    \nversion\n: \n{Apex version}\n,\n    \nbuildDate\n: \n{Apex build date and time}\n,\n    \nbuildRevision\n: \n{Apex revision}\n,\n    \nbuildVersion\n: \n{Apex build version}\n,\n    \nbuildUser\n: \n{Apex build user}\n,\n    // above 5 fields are deprecated and will be removed in 4.0\n    \napexVersion\n: \n{Apex version}\n,\n    \napexBuildDate\n: \n{Apex build date and time}\n,\n    \napexBuildRevision\n: \n{Apex revision}\n,\n    \napexBuildVersion\n: \n{Apex build version}\n,\n    \napexBuildUser\n: \n{Apex build user}\n,\n    \nhadoopVersion\n: \n{Hadoop version}\n,\n    \nhadoopBuildDate\n: \n{Hadoop build date and time}\n,\n    \nhadoopBuildRevision\n: \n{Hadoop build revision}\n,\n    \nhadoopBuildVersion\n: \n{Hadoop build version}\n,\n    \nhadoopBuildUser\n: \n{Hadoop build user}\n,\n    \nrtsVersion\n: \n{RTS version}\n,\n    \nrtsBuildDate\n: \n{RTS build date and time}\n,\n    \nrtsBuildRevision\n: \n{RTS revision}\n,\n    \nrtsBuildVersion\n: \n{RTS build version}\n,\n    \nrtsBuildUser\n: \n{RTS build user}\n,\n    \nos.arch\n: \n{operating system architecture}\n,\n    \nos.name\n: \n{operating system name}\n,\n    \nos.version\n: \n{operating system version}\n,\n    \njavaVendor\n: \n{java vendor}\n,\n    \njavaVersion\n: \n{java version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \nhadoopLocation\n: \n{Hadoop location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhadoopIsSecurityEnabled\n: {true/false},\n    \nhaEnabled\n: {true/false},\n    \ntimeZone\n: \n{time zone}\n,\n    \nhostname\n: \n{hostname}\n\n}\n\n\n\n\n\nGET /ws/v2/about\n\n\nFunction:\n\n\nReturn:\n\n\n{\n    \nbuildVersion\n: \n{Apex build version}\n,\n    \nbuildDate\n: \n{Apex build date and time}\n,\n    \nbuildRevision\n: \n{Apex revision}\n,\n    \nbuildUser\n: \n{Apex build user}\n,\n    \nversion\n: \n{Apex version}\n,\n    \nrtsBuildVersion\n: \n{RTS build version}\n,\n    \nrtsBuildDate\n: \n{RTS build date and time}\n,\n    \nrtsBuildRevision\n: \n{RTS revision}\n,\n    \nrtsBuildUser\n: \n{RTS build user}\n,\n    \nrtsVersion\n: \n{RTS version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \njavaVersion\n: \n{java_version}\n,\n    \nhadoopLocation\n: \n{hadoop_location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhostname\n: \n{hostname}\n,\n    \nhadoopIsSecurityEnabled\n: \n{true/false}\n\n}\n\n\n\n\nGET /ws/v2/cluster/metrics\n\n\nFunction: List metrics that are relevant to the entire cluster\n\n\nReturn:\n\n\n{\n    \naverageAge\n: \n{average running application age in milliseconds}\n,\n    \ncpuPercentage\n: \n{cpuPercentage}\n,\n    \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n    \nmaxMemoryAllocatedMB\n: \n{maxMemoryAllocatedMB}\n,\n    \nnumAppsFailed\n: \n{numAppsFailed}\n,\n    \nnumAppsFinished\n: \n{numAppsFinished}\n,\n    \nnumAppsKilled\n: \n{numAppsKilled}\n,\n    \nnumAppsPending\n: \n{numAppsPending}\n,\n    \nnumAppsRunning\n: \n{numAppsRunning}\n,\n    \nnumAppsSubmitted\n: \n{numAppsSubmitted}\n,\n    \nnumContainers\n: \n{numContainers}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications[?states={STATE_FILTER}\nname={NAME_FILTER}\nuser={USER_FILTER]\n\n\nFunction: List IDs of all streaming applications\n\n\nReturn:\n\n\n{\n    \napps\n: [\n        {\n            \ndiagnostics\n: \n{diagnostics}\n,\n            \nelapsedTime\n: \n{elapsedTime}\n,\n            \nfinalStatus\n: \n{finalStatus}\n,\n            \nfinishedTime\n: \n{finishedTime}\n,\n            \nid\n: \n{appId}\n,\n            \nname\n: \n{name}\n,\n            \nqueue\n: \n{queue}\n,\n            \nstartedTime\n: \n{startedTime}\n,\n            \nstate\n: \n{state}\n,\n            \ntrackingUrl\n: \n{trackingUrl}\n,\n            \nuser\n: \n{user}\n\n        },  \n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: Get the information for the specified application\n\n\nReturn:\n\n\n{\n    \nid\n: \n{appid}\n,\n    \nname\n: \n{name}\n,\n    \nstate\n: \n{state}\n,\n    \ntrackingUrl\n: \n{tracking url}\n,\n    \nfinalStatus\n: {finalStatus},\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nstartedTime\n: \n{startTime}\n,\n    \nuser\n: \n{user}\n,\n    \nversion\n: \n{stram version}\n,\n    \nremainingLicensedMB\n: \n{remainingLicensedMB}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nattributes\n: {\n           \n{attributeName}\n: \n{attributeValue}\n, \n           \n{attributeName-n}\n: \n{attributeValue-n}\n, \n    },\n    \nstats\n: {\n        \nallocatedContainers\n: \n{allocatedContainer}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nlatency\n: \n{overall latency}\n,\n        \ncriticalPath\n: \n{list of operator id that represents the critical path}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n:\n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n:\n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n:\n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan\n\n\nFunction: Return the physical plan for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nisUnifier\n: true/false\n        },\n         \u2026\n     ],\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators\n\n\nFunction: Return list of operators for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncounters\n: {\n                \n{counterName}\n: \n{counterValue}\n, \n                ...\n             },\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nunifierClass\n: \n{unifierClass}\n\n        },\n         \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/streams\n\n\nFunction: Return physical streams\n\n\nReturn:\n\n\n{\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}\n\n\nFunction: Return information of the given operator for the given application\n\n\nReturn:\n\n\n{\n    \nclassName\n: \n{className}\n,\n    \ncontainer\n: \n{containerId}\n,\n    \ncounters\n: {\n      \n{counterName}: \n{counterValue}\n, ...            \n    }\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \ncurrentWindowId\n: \n{currentWindowId}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \nports\n: [\n       {\n          \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n          \nname\n: \n{name}\n,\n          \ntotalTuples\n: \n{totalTuples}\n,\n          \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n          \ntype\n: \ninput/output\n\n       }, ...\n    ],\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,\n    \nname\n: \n{name}\n,\n    \nrecordingStartTime\n: \n{recordingStartTime}\n,\n    \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n    \nstatus\n: \n{status}\n,\n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory\n\n\nFunction: Return container deploy history of this operator\nSince: 1.0.6\n\n\nReturn:\n\n\n{\n   \ncontainers\n: [  \n        {  \n            \ncontainer\n: \n{containerId}\n,   \n            \nstartTime\n: \n{startTime}\n  \n        }, ...  \n    ],   \n    \nname\n: \n{operatorName}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports\n\n\nFunction: Get the information of all ports of the given operator of the\ngiven application\n\n\nReturn:\n\n\n{  \n    \nports\n: [\n        {  \n            \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,  \n            \ntotalTuples\n: \n{totalTuples}\n,   \n            \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n            \ntype\n: \noutput\n  \n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}\n\n\nFunction: Get the information of a specified port\n\n\nReturn:\n\n\n{  \n    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n    \nname\n: \n{name}\n,   \n    \ntotalTuples\n: \n{totalTuples}\n,   \n    \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n    \ntype\n: \n{type}\n  \n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}\nq={searchTerm}\npackagePrefixes={comma-separated-package-prefixes}]\n\n\nFunction: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        { \nname\n:\n{className}\n },\n       \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}\n\n\nFunction: Get the description of the given operator class\n\n\nReturn:\n\n\n{\n    \ninputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n          ...\n    ],\n    \noutputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n        \u2026\n    ],\n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/shutdown\n\n\nFunction: Shut down the application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/kill\n\n\nFunction: Kill the given application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start\n\n\nFunction: Start recording on operator\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop\n\n\nFunction: Stop recording on operator\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start\n\n\nFunction: Start recording on port\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop\n\n\nFunction: Stop recording on port\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]\n\n\nFunction: Return the list of containers for this application\n\n\nReturn:\n\n\n{\n    \ncontainers\n: [\n        {\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \nnumOperators\n: \n{numOperators}\n,\n            \noperators:\n {\n                \nid1\n: \nname1\n,\n                \nid2\n: \nname2\n,\n                \nid3\n: \nname3\n\n            },\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstate\n: \n{state}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction: Return the information of the specified container\n\n\nReturn:\n\n\n{\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \noperators:\n {\n        \nid1\n: \nname1\n,\n        \nid2\n: \nname2\n,\n        \nid3\n: \nname3\n\n    },\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstate\n: \n{state}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction: Return the container log list\n\n\nReturn:\n\n\n{\n    \nlogs\n: [\n        {\n            \nlength\n: \n{log length}\n,\n            \nname\n: \n{logName}\n\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/stackTrace\n\n\nSince: 3.4.0 \n\n\nFunction: Return the container stack trace\n\n\nReturn:\n\n\n{\n    \nthreads\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nstate\n: \n{state}\n,\n            \nid\n: \n{id}\n,\n            \nstackTraceElements\n: [\n                \n{line1}\n,\n                \n{line2}\n, ...\n            ]\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}\nlastNbytes={N}]\n\n\nFunction: Return the log with provided options\n\n\nReturn: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json).\nThe options (start, end) and (lastNbytes) are mutually exclusive.\n\n\n{\n    \nlines\n: [\n        { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \n{line}\n }, { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \nRandomNumber : {Line}\n } \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill\n\n\nFunction: Kill this container\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Return the logical plan of this application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n      {\n        \nname\n: \n{name}\n,\n        \nattributes\n: {attributeMap},\n        \nclass\n: \n{class}\n,\n        \nports\n: {\n           [\n            {\n                \nname\n: \n{name}\n,\n                \nattributes\n: {attributeMap},\n                \ntype\n: \ninput/output\n\n            }, ...\n           ]\n         },\n         \nproperties\n: {\n            \nclass\n: \n{class}\n\n         }\n      }, ...\n    ],\n    \nstreams\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/attributes\n\n\nFunction: Return the application attributes\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators\n\n\nFunction: Return the list of info of the logical operator\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n\n\nFunction: Return the info of the logical operator\n\n\nReturn:\n\n\n{\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Return the properties of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Set the properties of the logical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Return the properties of the physical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Set the properties of the physical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes\n\n\nFunction: Get the attributes of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes\n\n\nFunction:  Get the attributes of the port\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Change logical plan of this application\nPayload:\n\n\n{\n    \nrequests\n: [\n        {\n            \nrequestType\n: \nAddStreamSinkRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nCreateOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \noperatorFQCN\n: \n{operatorFQCN}\n,\n        },\n        {\n            \nrequestType\n: \nCreateStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsourceOperatorName\n: \n{sourceOperatorName}\n,\n            \nsourceOperatorPortName\n: \n{sourceOperatorPortName}\n\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nRemoveOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n        },\n        {\n            \nrequestType\n: \nRemoveStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n        },\n        {\n            \nrequestType\n: \nSetOperatorPropertyRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \npropertyName\n: \n{propertyName}\n,\n            \npropertyValue\n: \n{propertyValue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta\n\n\nFunction: Return the meta information about the statistics stored for\nthis operator\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \noperatorName\n: \n{operatorName}\n,\n    \noperatorIds\n: [ {opid}, \u2026 ],\n    \nstartTime\n: \n{startTime}\n,\n    \nendTime\n: \n{endTime}\n,\n    \ncount\n: \n{count}\n,\n    \nended\n: \n{boolean}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the statistics stored for this logical operator\n\n\n{\n    \noperatorStats\n: [\n        {\n            \noperatorId\n: \n{operatorId}\n,\n            \ntimestamp\n: \n{timestamp}\n,\n            \nstats\n: {\n                \ncontainer\n: \ncontainerId\n,\n                \nhost\n: \nhost\n,\n                \ntotalTuplesProcessed\n, \n{totalTuplesProcessed}\n,\n                \ntotalTuplesEmitted\n, \n{totalTuplesEmitted}\n,\n                \ntuplesProcessedPSMA\n, \n{tuplesProcessedPSMA}\n,\n                \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n                \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n                \nlatencyMA\n: \n{latencyMA}\n,\n                \nports\n: [ {\n                    \nname\n: \n{name}\n,\n                    \ntype\n:\n{input/output}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n, \n{tuplesPSMA}\n,\n                    \nbufferServerBytesPSMA\n, \n{bufferServerBytesPSMA}\n\n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta\n\n\nFunction: Return the meta information about the container statistics\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \ncontainers\n: {\n        \n{containerId}\n: {\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nhost\n: \n{host}\n,\n            \nmemoryMBAllocated\n, \n{memoryMBAllocated}\n\n        },\n        \u2026\n    },\n    \nstartTime\n: \n{startTime}\n\n    \nendTime\n: \n{endTime}\n\n    \ncount\n: \n{count}\n\n    \nended\n: {boolean}\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the container statistics stored for this application\n\n\n{\n    \ncontainerStats\n: [\n        {\n            \ncontainerId\n: \n{containerId}\n\n            \ntimestamp\n: \n{timestamp}\n\n            \nstats\n: {\n                \nnumOperators\n: \n{numOperators}\n,\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/recordings\n\n\nFunction: Get the list of all recordings for this application\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [{\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings\n\n\nFunction: Get the list of recordings on this operator\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [ {\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Get the information about the recording\n\n\nReturn:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nstartTime\n: \n{startTime}\n,\n    \nappId\n: \n{appId}\n,\n    \noperatorId\n: \n{operatorId}\n,\n    \ncontainerId\n: \n{containerId}\n,\n    \ntotalTuples\n: \n{totalTuples}\n,\n    \nports\n: [ {\n       \nname\n: \n{portName}\n,\n       \nstreamName\n: \n{streamName}\n,\n       \ntype\n: \n{type}\n,\n       \nid\n: \n{index}\n,\n       \ntupleCount\n: \n{tupleCount}\n\n     } \u2026 ],\n    \nended\n: {boolean},\n    \nwindowIdRanges\n: [ {\n       \nlow\n: \n{lowId}\n,\n       \nhigh\n: \n{highId}\n\n     } \u2026 ],\n    \nproperties\n: {\n       \nname\n: \nvalue\n, ...\n     }\n}\n\n\n\n\nDELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Deletes the specified recording\n\n\nSince: 1.0.4\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples\n\n\nQuery Parameters:\n\n\noffset\nstartWindow\nlimit\nports\nexecuteEmptyWindow\n\n\n\nFunction: Get the tuples\n\n\nReturn:\n\n\n{\n    \nstartOffset\n: \n{startOffset}\n,\n    \ntuples\n: [ {\n        \nwindowId\n: \n{windowId}\n,\n        \ntuples\n: [ {\n            \nportId\n: \n{portId}\n,\n            \ndata\n: \n{tupleData}\n\n        }, \u2026 ]\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/events?from={fromTime}\nto={toTime}\noffset={offset}\nlimit={limit}\n\n\nFunction: Get the events\n\n\nReturn:\n\n\n{\n    \nevents\n: [ {\n           \nid\n: \n{id}\n,\n        \ntimestamp\n: \n{timestamp}\n,\n        \ntype\n: \n{type}\n,\n        \ndata\n: {\n            \nname\n: \nvalue\n, \u2026\n        }\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/user\n\n\nFunction: Get the user profile information, list of roles and list of\npermissions given the user\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/settings\n\n\nFunction: Get the current user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}\n\n\nFunction: Get the specified user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Get the specified user's setting key\n\n\nReturn:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nPUT /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Set the specified user's setting key\nPayload:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nGET /ws/v2/auth/roles\n\n\nFunction: Get the list of roles the system has\n\n\nReturn:\n\n\n{\n    \nroles\n: [\n       {\n         \nname\n: \n{role1}\n,\n         \npermissions\n: [ \n{permission1}\n, \u2026 ]\n       }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/auth/roles/{role}\n\n\nFunction: Get the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/auth/roles/{role}\n\n\nFunction: create or edit the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/auth/restoreDefaultRoles\n\n\nFunction: Restores default roles\n\n\nDELETE /ws/v2/auth/roles/{role}\n\n\nFunction: delete the given role\n\n\nGET /ws/v2/auth/permissions\n\n\nFunction: Get the list of possible permissions\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ {\n       \nname\n: \n{permissionName}\n,\n       \nadminOnly\n: true/false\n    }, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/applications/{appid}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPOST /ws/v2/licenses\n\n\nFunction: Add a license to the registry\n\n\nPayload: The license file content\n\n\nReturn:\n\n\n{\n  \nid\n: \n{licenseId}\n,\n  \nexpireTime\n: {unixTimeMillis},\n  \nexpirationTimeNotificationPeriod\n: {timeMillis},\n  \nnodesAllowed\n: {nodesAllowed},\n  \nmemoryMBAllowed\n: {memoryMBAllowed},\n  \nexceedGracePeriod\n: {timeMillis},\n  \ncontextType\n: \n{contextType}\n,\n  \ntype\n: \n{type}\n,\n  \nfeatures\n: [ \n{feature1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/licenses/current\n\n\nFunction: Get info on the current license\n\n\n{\n      \nid\n: \n{licenseId}\n,\n      \ncurrentTime\n: {unixTimeMillis},\n      \nexpireTime\n: {unixTimeMillis},\n      \nnodesAllowed\n: {nodesAllowed},\n      \nnodesUsed\n: {nodesUsed},\n      \nmemoryMBAllowed\n: {memoryMBAllowed},\n      \nmemoryMBUsed\n: {memoryMBUsed},\n      \nexceedGracePeriod\n: {timeMillis}, // memory exceed grace period\n      \nexceedRemainingTime\n: {timeMillis},  // (optional)\n      \nviolation\n: \nmemory\n, // returns violation type (optional)\n      \ncontextType\n: \n{community|standard|enterprise}\n,\n      \ntype\n: \n{evaluation|non_production|production}\n\n      \nfeatures\n: [ \n{feature1}\n, \u2026 ], // for community, empty array\n      \ncurrent\n: true/false,\n      \nexpirationTimeNotificationLevel\n: \n{INFO|WARN|ERROR}\n, // (optional)\n      \nvalid\n: true/false // true, if the license is valid\n}\n\n\n\n\nGET /ws/v2/config/installMode\n\n\nFunction: returns the install mode\n\n\n{\n  \ninstallMode\n: \n{evaluation|community|app}\n,\n  \nappPackageName\n: \n{optionalAppPackageName}\n,\n  \nappPackageVersion\n: \n{optionalAppPackageVersion}\n\n}\n\n\n\n\nGET /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction: returns the download type\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nPUT /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction:\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nFeature List:  \n\n\n\n\nSYSTEM_APPS\n\n\nSYSTEM_ALERTS\n\n\nAPP_DATA_DASHBOARDS\n\n\nRUNTIME_DAG_CHANGE\n\n\nRUNTIME_PROPERTY_CHANGE\n\n\nAPP_CONTAINER_LOGS\n\n\nLOGGING_LEVELS\n\n\nAPP_DATA_TRACKER\n\n\nJAAS_LDAP_AUTH\n\n\nAPP_BUILDER\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns list of properties from dt-site.xml.\n\n\nReturn:\n\n\n{\n    \n{name}\n: {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n    }\n\n}\n\n\n\n\nGET /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Returns single property from dt-site.xml, specify by name\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nPOST /ws/v2/config/properties\n\n\nFunction: Overwrites all specified properties in dt-site.xml\n\n\nPayload:\n\n\n{\n    \nproperties\n: [\n        {\n            \nname\n: \n{name}\n\n            \nvalue\n: \n{PROPERTY_VALUE}\n,\n            \nlocal\n: true/false,\n                    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nPUT /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Overwrites or creates new property in dt-site.xml\nPayload:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \nlocal\n: true/false,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nDELETE /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Deletes a property from dt-site.xml. \n\n\nGET /ws/v2/config/hadoopExecutable\n\n\nFunction: Returns the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nPUT /ws/v2/config/hadoopExecutable\n\n\nFunction: Sets the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nGET /ws/v2/config/issues\n\n\nFunction: Returns list of potential issues with environment\n\n\nReturn:\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        {...},\n        {...}\n    ]    \n}\n\n\n\n\nGET /ws/v2/config/ipAddresses\n\n\nFunction: Returns list of ip addresses the gateway can listen to\n\n\nReturn:\n\n\n{\n    \nipAddresses\n: [\n      \n1.2.3.4\n, ...\n    ]    \n}\n\n\n\n\nPOST /ws/v2/config/restart\n\n\nFunction: Restarts the gateway\n\n\nPayload: none\n\n\nGET /proxy/rm/v1/\u2026\n\n\nPOST /proxy/rm/v1/\u2026\n\n\nFunction: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.\n\n\nGET /proxy/stram/v2/...\n\n\nPOST /proxy/stram/v2/\u2026\n\n\nPUT /proxy/stram/v2/\u2026\n\n\nDELETE /proxy/stram/v2/\u2026\n\n\nFunction: Proxy calls to Stram Web Services.\n\n\nPOST /ws/v2/applications/{appid}/loggers\n\n\nFunction: Set the logger levels of packages/classes.\n\n\nPayload:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers\n\n\nFunction: Gets the logger levels of packages/classes.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"\n\n\nFunction: searches for all classes that match the pattern.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nname\n : \n{fully qualified class name}\n,\n            \nlevel\n: \n{logger level}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/restart[?queue={queue}]\n\n\nSince: 3.4.0\nFunction: Restart the terminated application. Payload is optional.\nPayload:\n\n\n{\n  \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n  \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of appPackages the user can view in the system\n\n\n{\n    \nappPackages\n: [\n        {\n                 \nappPackageName\n: \n{appPackageName}\n,\n                 \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nowner\n: \n{owner}\n,\n        }, ...\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages?merge={replace|fail|ours|theirs}\n\n\nSince: 1.0.4\n\n\nFunction: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package\n\n\nPayload: the raw zip file\n\n\nReturn: The information of the app package\n\n\nGET /ws/v2/appPackages/{owner}/{name}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of versions of appPackages with the given name in the system owned by the specified user\n\n\n{\n    \nversions\n: [\n        \n1.0-SNAPSHOT\n\n    ]\n}\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the appPackage\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download\n\n\nSince: 1.0.4\n\n\nFunction: Downloads the appPackage zip file\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns:\n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nmodificationTime\n:  \n{modificationTime}\n,\n    \nowner\n: \n{owner}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs\n\n\nSince: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:\n\n\n{\n    \nconfigs\n: [\n        \nmy-app-conf1.xml\n\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the properties XML of the specified config\n\n\nReturns:\n\n\nconfiguration\n\n        \nproperty\n\n                \nname\n...\n/name\n\n                \nvalue\n...\n/value\n\n        \n/property\n\n        \u2026\n\n/configuration\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Creates or replaces the specified config with the property parameters specified payload\n\n\nPayload: configuration in XML\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the specified config\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of applications in the appPackage\n\n\nReturns:\n\n\n{\n    \napplications\n: [\n        {\n            \ndag\n: {dag in json format},\n            \nfile\n: \n{fileName}\n,\n            \nname\n: \n{name}\n,\n            \ntype\n: \n{type}\n,\n            \nerror\n: \n{error}\n,\n            \nfileContent\n: {originalFileContentForJSONTypeApp}\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta data for that application\n\n\nReturns:\n\n\n{\n    \nfile\n: \n{fileName}\n,\n    \nname\n: \n{name}\n,\n    \ntype\n: \n{json/class/properties}\n,\n    \nerror\n: \n{error}\n\n    \ndag\n: {\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n         }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n    },\n    \nfileContent\n: {originalFileContentForJSONTypeApp}\n}\n\n\n\n\nPOST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge\n\n\nFunction: Merge the configuration, json apps, and resources files from the app package specified by user/name/version from the payload to the specified app package in the url, without overwriting any existing file in the specified app package. If replaceExisting is true, the files in the app, conf and resources directory of the app package will be replaced by the ones in the app package specified in the payload. Otherwise, they will not be replaced. The fields user, name and replaceExisting in the payload are optional. If user and name are not specified, they are default to be the same as in the URI path. replaceExisting's default is false.\n\n\nPayload:\n\n\n{\n \nuser\n: \n{user}\n,\n \nname\n: \n{name}\n,\n \nversion\n: \n{versionToMergeFrom}\n,\n \nreplaceExisting\n: \n{true/false}\n\n}\n\n\n\n\nPOST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}\noriginalAppId={originalAppId}\nqueue={queueName}]\n\n\nSince: 1.0.4\n\n\nFunction: Launches the application with the given configuration specified in the POST payload\n\n\nPayload:\n\n\n{\n    \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}\n\n\nSince: 1.0.4\n\n\nFunction: Get the properties of the operator given the classname in the jar\n\n\n{  \n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]\n\n\nFunction: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app package\n\n\nPayload:\n\n\n{\n        \ndisplayName\n: \n{displayName}\n,\n        \ndescription\n: \n{description}\n,\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n          }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n}\n\n\n\n\nReturn:\n\n\n{\n        \nerror\n: \n{error}\n\n}\n\n\n\n\nAvailable port attributes to set: \n\n\n\n\nAUTO_RECORD\n\n\nIS_OUTPUT_UNIFIED\n\n\nPARTITION_PARALLEL\n\n\nQUEUE_CAPACITY\n\n\nSPIN_MILLIS\n\n\nSTREAM_CODEC\n\n\nUNIFIER_LIMIT\n\n\n\n\nAvailable locality options to set: \n\n\n\n\nTHREAD_LOCAL\n\n\nCONTAINER_LOCAL\n\n\nNODE_LOCAL\n\n\nRACK_LOCAL\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}\n\n\nSince: 1.0.5\n\n\nFunction: Deletes non-jar based application in the app package\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators\n\n\nSince: 1.0.5\n\n\nFunction: Get the classes of operators from specified app package.\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        {\n            \nname\n:\n{fullyQualifiedClassName}\n, \n            \ntitle\n: \n{title}\n,\n            \nshortDesc\n: \n{description}\n,\n            \nlongDesc\n: \n{description}\n,\n            \ncategory\n: \n{categoryName}\n,\n            \ndoclink\n: \n{doc url}\n,\n            \ntags\n: [ \n{tag}\n, \n{tag}\n, \u2026 ],\n            \ninputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ]\n            \noutputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ],\n            \nproperties\n: [  \n                {\n                    \nname\n:\n{propertyName}\n,\n                    \ncanGet\n: {canGet},\n                    \ncanSet\n: {canSet},\n                    \ntype\n:\n{type}\n,\n                    \ndescription\n:\n{description}\n,\n                    \nproperties\n: ...\n                }, \u2026\n            ],\n            \ndefaultValue\n: {\n                \n{propertyName}\n: [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/import\n\n\nFunction: List the importable app packages on Gateway's local file\nsystem\n\n\nReturn:\n\n\n{\n    \nappPackages: [\n        {\n            \nfile\n: \n{file}\n,\n            \nname\n: \n{name}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \nversion\n: \n{version}\n,\n            \ndescription\n: \n{description}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages/import\n\n\nFunction: Import app package from Gateway's local file system\n\n\nPayload:\n\n\n{\n        \nfiles\n: [\n{file}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Creates or replaces the specified system alert. The condition has access to an object in its scope called \n_topic\n. An example alert might take the form of the following:\n\n\n_topic[\"applications.application_1400294100000_0001\"].allocatedContainers \n 5\n\n\n\nPayload:\n\n\n{\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n        \ntimeThresholdMillis\n:\n{time}\n\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Deletes the specified system alert\n\n\nGET /ws/v2/systemAlerts/alerts?inAlert={true/false}\n\n\nFunction: Gets the created alerts\n\n\nReturn:\n\n\n{\n    \nalerts\n: [{\n        \nname\n: \n{alertName}\n,\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n        \ntimeThresholdMillis\n:\n{time}\n,\n        \nalertStatus\n: {\n            \nisInAlert\n:{true/false}\n            \ninTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }\n    }, \u2026  ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Gets the specified system alert\n\n\nReturn:\n\n\n{\n    \nname\n: \n{alertName}\n,\n    \ncondition\n:\n{condition in javascript}\n,        \n    \nemail\n:\n{email}\n,\n    \ndescription\n: \n{description}\n,\n    \ntimeThresholdMillis\n:\n{time}\n,\n    \nalertStatus\n: {\n        \nisInAlert\n:{true/false}\n        \ninTime\n: \n{time}\n,\n        \nmessage\n: \n{message}\n,\n        \nemailSent\n: {true/false}\n    }\n}\n\n\n\n\nGET /ws/v2/systemAlerts/history\n\n\nFunction: Gets the history of alerts\n\n\nReturn:\n\n\n{\n    \nhistory\n: [\n        {\n            \nname\n:\n{alertName}\n,\n            \ninTime\n:\n{time}\n,\n            \noutTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\nFunction: Gets the topic data that is used for evaluating alert\ncondition\n\n\nReturn:\n\n\n{\n     \n{topicName}\n: {json object data}, ...\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Creates or replaces the specified system alert template.\n\n\nPayload:\n\n\n{\n    \nisSystemTemplate\n: true,\n    \ndescription\n: \n{description}\n,\n    \nparameters\n: [\n        {\n          \nvariable\n: \n{replacement variable in Javascript block}\n,\n          \nlabel\n: \n{input label}\n,\n          \ntype\n: \n{number/text}\n,\n          \nplaceholder\n: \n{input placeholder}\n,\n          \ntooltip\n: \n{input tooltip}\n,\n          \nrequired\n: {true/false},\n          \ndefault\n: \n{default value}\n,     // optional\n          \nvalues\n: {                       // optional\n            \n{key}\n: \n{value}\n,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n    \nscript\n: \n{Javascript block}\n\n}\n\n\n\n\nExample:\n\n\n{\n    \ntemplates\n: [\n        {\n            \nisSystemTemplate\n: true,\n            \ndescription\n: \nAn alert template example.\n,\n            \nparameters\n: [\n                {\n                  \nvariable\n: \ncomparison\n,\n                  \nlabel\n: \nComparison\n,\n                  \ntype\n: \ntext\n,\n                  \nplaceholder\n: \nSelect a comparison\n,\n                  \ntooltip\n: \nChoose the comparison to use.\n,\n                  \nrequired\n: true,\n                  \ndefault\n: \n,\n                  \nvalues\n: {\n                    \n: \nless than\n,\n                    \n===\n: \nequals to\n,\n                    \n: \ngreater than\n\n                  }\n                },\n                {\n                  \nvariable\n: \ncount\n,\n                  \nlabel\n: \nNumber of Killed Containers\n,\n                  \ntype\n: \nnumber\n,\n                  \nplaceholder\n: \nEnter a valid number\n,\n                  \ntooltip\n: \nEnter the number.\n,\n                  \nrequired\n: false\n                }\n            ],\n            \nscript\n: \n/* Alert when number of killed containers is {{comparison}} {{count}} */\n\n                _topic['cluster.metrics'].numContainers {{comparison.key}} ({{count}} !== null ? {{count}} : 0);\n\n        }\n    ]\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Deletes the specified system alert template.\n\n\nGET /ws/v2/systemAlerts/templates/system\n\n\nFunction: Gets the created system alert templates\n\n\nReturn:\n\n\n{\n    \ntemplates\n: [\n        {\n            \nisSystemTemplate\n: true,\n            \ndescription\n: \n{description}\n,\n            \nparameters\n: [\n                {\n                  \nvariable\n: \n{replacement variable in Javascript block}\n,\n                  \nlabel\n: \n{input label}\n,\n                  \ntype\n: \n{number/text}\n,\n                  \nplaceholder\n: \n{input placeholder}\n,\n                  \ntooltip\n: \n{input tooltip}\n,\n                  \nrequired\n: {true/false},\n                  \ndefault\n: \n{default value}\n,     // optional\n                  \nvalues\n: {                       // optional\n                    \n{key}\n: \n{value}\n,\n                    \u2026\n                  }\n                },\n                \u2026\n            ],\n            \nscript\n: \n{Javascript block}\n\n        },\n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/templates/system/{name}\n\n\nFunction: Gets the specified system alert template\n\n\nReturn:\n\n\n{\n    \nisSystemTemplate\n: true,\n    \ndescription\n: \n{description}\n,\n    \nparameters\n: [\n        {\n          \nvariable\n: \n{replacement variable in Javascript block}\n,\n          \nlabel\n: \n{input label}\n,\n          \ntype\n: \n{number/text}\n,\n          \nplaceholder\n: \n{input placeholder}\n,\n          \ntooltip\n: \n{input tooltip}\n,\n          \nrequired\n: {true/false},\n          \ndefault\n: \n{default value}\n,     // optional\n          \nvalues\n: {                       // optional\n            \n{key}\n: \n{value}\n,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n    \nscript\n: \n{Javascript block}\n\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/validate/script\n\n\nFunction: Validates Java script.\n\n\nPayload:\n\n\n{\n    \nscript\n: {\nscript\n}\n}\n\n\n\n\nGET /ws/v2/auth/users/{user}\n\n\nFunction: Gets the info of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPOST /ws/v2/auth/users/{user}\n\n\nFunction: Changes password and/or roles of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \noldPassword\n: \n{oldPassword}\n,\n    \nnewPassword\n: \n{newPassword}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPUT /ws/v2/auth/users/{user}\n\n\nFunction: Creates new user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nDELETE /ws/v2/auth/users/{user}\n\n\nFunction: Deletes the specified user\n\n\nGET /ws/v2/auth/users\n\n\nFunction: Gets the list of users\n\n\nReturn:\n\n\n{\n    \nusers\n: [ {\n       \nuserName\n: \n{username1}\n,\n       \nroles\n: [ \n{role1}\n, \u2026 ],\n       \npermissions\n: [ \n{permission1}\n, \u2026 ]\n    }\n}\n\n\n\n\nPOST /ws/v2/login\n\n\nFunction: Login\nPayload:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n\n}\n\n\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/logout\n\n\nFunction: Log out the current user\n\n\nReturn:\n\n\n{\n}\n\n\n\n\nPUT /ws/v2/config/auth\n\n\nFunction: Configure authentication. \nThe request specifies the type of authentication to setup such as password, kerberos, ldap etc and the configuration \nparameters for the authentication. The web service sets up the appropriate configuration files for the authentication \nas described in authentication section of \ndtgateway_security\n document. Gateway needs to be \nrestarted for the new authentication to take effect. This can be done by making the gateway restart web service request.\n\n\npayload:\n\n\n{\n    \ntype\n: \n{authenticationType}\n,\n    \nconfiguration\n:{ }\n}\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe web request \nGET /ws/v2/config/auth\n returns payload body that was sent in \nPUT\n request as the response verbatim. \n\n\nPassword\n\n\nFunction: Configure Password authentication \n\n\n{\n    \ntype\n: \npassword\n,\n    \nconfiguration\n:{ }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe web request \nGET /ws/v2/config/auth\n returns payload body that was sent in \nPUT\n request as the response verbatim. \n\n\n{\n    \ntype\n: \npassword\n,\n    \nconfiguration\n:{ }\n}\n\n\n\n\n\nKerberos with no group mapping\n\n\nFunction: Configure Kerberos authentication with no group mapping\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nTwo of the properties are mandatory, they are \"kerberosPrincipal\" \n \"kerberosKeytab\". A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should be \nspecified as \"false\".\n\n\n{\n    \ntype\n:\nkerberos\n,\n       \nconfiguration\n:{  \n         \ngroupSupport\n:\nfalse\n,\n         \nkerberosPrincipal\n:\n{kerberosPrincipal}\n,\n         \nkerberosKeytab\n:\n{Keytab}\n,\n         \ntokenValidity\n:\n{tokenValidity}\n,\n         \ncookieDomain\n:\n{cookieDomain}\n,\n         \ncookiePath\n:\n{cookiePath}\n,      \n         \nsignatureSecret\n:\n{signatureSecret}\n\n         }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nKerberos with group mapping\n\n\nFunction: Configure Kerberos authentication with group mapping \nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nTwo of these properties are mandatory, they are \"kerberosPrincipal\" \n \"kerberosKeytab\". A \"groupSupport\" property \nspecifies whether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should \nbe specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that \ncontains the mapping from kerberos groups to roles.\n\n\n\n{  \n   \ntype\n: \nkerberos\n,\n   \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nkerberosPrincipal\n: \n{kerberosPrincipal}\n,\n        \nkerberosKeytab\n: \n{Keytab}\n,\n        \ntokenValidity\n: \n{Validity}\n,\n        \ncookieDomain\n : \n{cookieDomain}\n,\n        \ncookiePath\n: \n{cookiePath}\n\n        \nsignatureSecret\n: \n{signatureSecret}\n\n        }\n    \ngroupMapping\n: [\n        { \n             \ngroup\n: \nusers\n,\n             \nroles\n: [\ndevelopers\n, \nadmins\n, \nqa\n] \n        },\n        {  \n             \ngroup\n: \nops\n,\n             \nroles\n: [\noperators\n]\n        }\n     ]  \n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nLDAP\n\n\nConfigure LDAP authentication. There are different configurations possible based on how the LDAP server is configured.\n\n\nAnonymous search allowed and no group mapping needed\n\n\nFunction: Configure LDAP authentication with anonymous search available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows LDAP groups\nto be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \ntype\n: \nldap\n,\n   \nconfiguration\n: {  \n       \ngroupSupport\n: \nfalse\n,\n       \nServer\n: \n{Server}\n,\n       \nPort\n: {port}\n +\n       \nuserBaseDn\n: \n{usserBaseDn}\n,\n       \nuserIdAttribute\n: \n{userIdAttribute}\n\n   }\n}\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed and no group mapping needed\n\n\nFunction: Configure LDAP authentication when anonymous search is not available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".\n\n\n\n{  \n    \ntype\n: \nldap\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \nfalse\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n, \n        \nuserObjectClass\n: \n{userObjectClass}\n\n    }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed but group mapping needed\n\n\nFunction: Configure LDAP authentication when anonymous search is not available on LDAP server but group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from LDAP groups to roles.\n\n\n{ \n    \ntype\n: \nldap\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n, \n        \nroleBaseDn\n: \n{roleBaseDn}\n,\n        \nuserRdnAttribute\n:\n{userRdnAttribute}\n, \n        \nroleNameAttribute\n: \n{roleNameAttribute}\n, \n        \nroleObjectClass\n: \n{roleObjectClass}\n, \n        \nuserObjectClass\n: \n{userObjectClass}\n\n    },\n    \ngroupMapping\n: [ \n        { \n            \ngroup\n: \nusers\n,\n            \nroles\n:[\ndevelopers\n] \n        },\n        { \n            \ngroup\n: \nops\n,\n            \nroles\n: [\noperators\n]\n        }\n    ]  \n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nActive Directory\n\n\nConfigure Active Directory authentication. There are different configurations possible based on how the Active Directory \nserver is configured.\n\n\nAnonymous search allowed and no group mapping needed\n\n\nFunction: Configure Active Directory authentication with anonymous search available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows \nActive Directory groups to be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \n   \ntype\n: \nad\n,\n   \nconfiguration\n: {  \n       \ngroupSupport \n: \nfalse\n,\n       \nServer\n: \n{server}\n,\n       \nPort\n: {port},\n       \nuserSearchFilter\n: \n{userSearchFilter}\n,\n       \nuserBaseDn\n: \n{userBaseDn}\n,\n       \nuserIdAttribute\n: \n{userIdAttribute}\n,\n       \nuserDomain\n : \n{userDomain}\n    \n   }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed and no group mapping needed\n\n\nFunction: Configure Active Directory authentication when anonymous search is not available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".\n\n\n\n{  \n   \ntype\n: \nad\n,\n   \nconfiguration\n: {  \n   \ngroupSupport\n: \nfalse\n,\n        \nServer\n: \n{server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n,\n        \nuserObjectClass\n: \n{userObjectClass}\n\n    }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nAnonymous search not allowed but group mapping needed\n\n\nFunction: Configure Active Directory authentication when anonymous search is not available on Active Directory server but group mapping is needed\nThe configuration comprises different properties as described in the \ndtgateway_security\n document. \nThe \"server\", \"userBaseDn\", \"bindDn\" \n \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from Active Directory groups to roles.\n\n\n{ \n    \ntype\n: \nad\n,\n    \nconfiguration\n: {  \n        \ngroupSupport\n: \ntrue\n,\n        \nServer\n: \n{Server}\n,\n        \nPort\n: {port},\n        \nuserBaseDn\n: \n{userBaseDn}\n,\n        \nuserIdAttribute\n: \n{userIdAttribute}\n,\n        \nbindDn\n: \n{bindDn}\n,\n        \nbindPassword\n: \n{bindPassword}\n,\n        \nroleBaseDn\n: \n{roleBaseDn}\n,\n        \nuserRdnAttribute\n: \n{userRdnAttribute}\n,\n        \nroleNameAttribute\n: \nroleNameAttribute\n,\n        \nroleObjectClass\n: \nroleObjectClass\n,\n        \nuserObjectClass\n: \n{userObjectClass}\n,\n     },\n     \ngroupMapping\n: [ ]  \n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPAM\n\n\nConfigure PAM or Pluggable Authentication Mechanism. PAM is the de-facto authentication available on Linux systems.\n\n\nPAM with no group mapping\n\n\nFunction: Configure PAM authentication with no group mapping\nThe configuration comprises of different properties as described in the \ndtgateway_security\n document. \nIn PAM, service name is mandatory \n there is no configuration. A \"groupSupport\" property specifies whether group mapping \nshould be enabled. Group mapping allows PAM groups to be mapped to roles. It should be specified as \"false\".\n\n\n\n{  \n   \ntype\n:\npam\n,\n   \nconfiguration\n:{  \n      \ngroupSupport\n:\nfalse\n,\n      \nserviceName\n:\n{serviceName}\n\n   }\n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPAM with group mapping\n\n\nFunction: Configure PAM authentication with group mapping \nThe configuration comprises of different properties as described in the \ndtgateway_security\n document.\nIn PAM, service name is mandatory \n there is no configuration. Group mapping allows PAM groups to be mapped to roles. It \nshould be specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be \nspecified that contains the mapping from PAM groups to roles.\n\n\n\n{  \n   \ntype\n:\npam\n,\n   \nconfiguration\n:{  \n      \ngroupSupport\n:\ntrue\n,\n      \nserviceName\n:\n{serviceName}\n\n   },\n\n   \ngroupMapping\n: [ \n       { \n          \ngroup\n: \nusers\n,\n          \nroles\n:[\ndevelopers\n] \n       },\n       { \n          \ngroup\n: \nops\n,\n          \nroles\n: [\noperators\n]\n       }\n   ]  \n};\n\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPUT /ws/v2/config/groupMapping\n\n\nFunction: Specify group to role mapping\nSet or update mapping from groups of configured authentication mechanism to RTS roles.\n\n\n\n{\n   \ngroupMapping\n : [\n        {\n           \ngroup\n : \nusers\n,\n           \nrole\n : [\ndevelopers\n, \nadmins\n, \nqa\n, \ninterns\n]\n        },\n        {\n           \ngroup\n: \nops\n,\n           \nrole\n : [\noperators\n]\n        }\n   ]\n};\n\n\n\n\nReturns:\n\n\n{\n}\n\n\n\n\nThe GET response will be same as the json sent in the request for \nPUT\n .\n\n\nPublisher-Subscriber WebSocket Protocol\n\n\ndtGateway provides a light-weight pubsub websocket service.\nThe URL of dtGateway's pubsub websocket service is: \nws://{dtGateway-host-port}/pubsub\n.\nFor example: \nws://localhost:9090/pubsub\n\n\nInput\n\n\nPublishing\n\n\n{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nSubscribing\n\n\n{\"type\":\"subscribe\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing\n\n\n{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing from the number of subscribers of a topic\n\n\n{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nOutput\n\n\nNormal Published Data\n\n\n{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nNumber of Subscribers:\n\n\n{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}\n\n\n\nAuto publish topics\n\n\ndata that gets published every one second:\n\n\n\n\napplications\n - list of streaming applications running in the cluster\n\n\napplications.[appid]\n - information about a particular application\n\n\napplications.[appid].containers\n - information about containers of a particular application\n\n\napplications.[appid].physicalOperators\n - information about operators of a particular application\n\n\napplications.[appid].logicalOperators\n - information about logical operators of a particular application\n\n\napplications.[appid].events\n - events from the AM of a particularapplication\n\n\n\n\ndata that gets published every five seconds:\n\n\n\n\ncluster.metrics\n - metrics of the cluster", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#datatorrent-dtgateway-api-v2-specification", 
            "text": "", 
            "title": "DataTorrent dtGateway API v2 Specification"
        }, 
        {
            "location": "/dtgateway_api/#rest-api", 
            "text": "", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#return-codes", 
            "text": "200 : OK  400 : The request is not in the format that the server expects  404 : The resource is not found  500 : Something is wrong on the server side", 
            "title": "Return codes"
        }, 
        {
            "location": "/dtgateway_api/#rest-uri-specification", 
            "text": "", 
            "title": "REST URI Specification"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidmetrics", 
            "text": "Function: List all available metrics for an application.  Example:   {\n     applicationId :  {applicationId} ,\n     metrics : [\n        {\n             name :  {metrcName} ,\n             type :  {metricType} ,\n             values :  {value} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/metrics"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappid", 
            "text": "Function:   Example:  {\n     id :  {appId} ,\n     name :  {name} ,\n     queue :  {queue} ,\n     state :  {applicationState} ,\n     finalStatus :  {finalStatus} ,\n     startedTime :  {applicationStartedTime} ,\n     finishedTime :  {applicationFinishedTime} ,\n     diagnostics :  {diagnostics} ,\n     applicationType :  {applicationType} ,\n     trackingUrl :  {trackingUrl} ,\n     elapsedTime :  {elapsedTime} ,\n     user :  {user} ,\n     currentAttemptId :  {currentAttemptId} ,\n     runningContainers :  {runningContainers} ,\n     allocatedMB :  {allocatedMB} ,\n     allocatedVCores :  {allocatedVCores} ,\n     canWrite :  true/false ,\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     gatewayConnected :  true/false ,\n     appDataSources : [\n        {\n             query :\n            {\n                 operatorName :  {operatorName-1} ,\n                 url :  {url-1} ,\n                 topic :  {topic-1} \n            },\n             result :\n            {\n                 operatorName :  {operatorName-1} ,\n                 url :  {url-1} ,\n                 topic :  {topic-1} ,\n                 appendQIDToTopic :  true/false \n            },\n             type :  {type} ,\n             name :  {dataSourceName} \n        },\n        ...\n    ],\n     metrics :\n    {\n         Operator1 :\n        {\n             {metricName1} :  {value} ,\n             {metricName2} :  {value} \n                ...\n        },\n         Operator2 :\n        {\n             {metricName1} :  {value} ,\n             {metricName2} :  {value} \n                ...\n        },\n        ...\n    },\n     attributes :\n    {\n         {attributeName} :  {attributeValue} ,\n        ...\n         {attributeName-n} :  {attributeValue-n} \n    },\n     appMasterTrackingUrl :  {appMasterTrackingUrl} ,\n     version :  {apex version} ,\n     stats :\n    {\n         latency :  {latency} ,\n         plannedContainers :  {plannedContainers} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         windowStartMillis :  {windowStartMillis} ,\n         criticalPath : [\n             {criticalPathNumber-1} ,\n            ...\n             {criticalPathNumber-n} \n        ],\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed :  {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA :  {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted :  {sum of operators:totalTuplesEmitted} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         memoryRequired :  {memoryRequired} ,\n         totalVCoresAllocated :  {totalVCoresAllocated} ,\n         vcoresRequired :  {vcoresRequired} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} ,\n         allocatedContainers :  {numberOfAllocatedContainers} \n    },\n     connectedToThisGateway :  true/false ,\n     appPackageSource :\n    {\n         user :  {user} ,\n         name :  {appPackageName} ,\n         version :  {appPackageVersion} ,\n         appName :  {appName} ,\n         configPackage :\n        {\n             user :  {user} ,\n             name :  {configPackageName} ,\n             version :  {configPackageVersion} \n        }\n    },\n     launchDisabled :  true/false ,\n     services : [\n        {\n             name :  {serviceName} ,\n             state :  {state} ,\n             type :  docker/apex ,\n             requiredServices : [ {requiredService1} ,  {requiredServices} , ...],\n             proxy :\n            {\n                 name :  {proxyName} ,\n                 address :  proxyAddress \n            },\n             installedTime :  {installTime} ,\n             startedTime :  {startTime} ,\n             enabled :  true/false ,\n             memoryMB :  {memoryMB} ,\n             srcUrl :  {srcUrl} ,\n             docker/apex property name-1 :  {value-1} ,\n            ...\n             docker/apex property name-n :  {value-n} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidfailurerootcause", 
            "text": "Function: Return the root cause of why an application failed to run.  Example:  {\n     title :  FC_ERROROUTPUT ,\n     type :  markdown ,\n     content :  {root cause of why the application {appid} failed} \n}", 
            "title": "GET /ws/v2/applications/{appid}/failureRootCause"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatesnewallocatedactivekilledtypesallappmaster", 
            "text": "Function:  Example:   {\n     containers : [\n        {\n             id :  {id} ,\n             host :  {host} ,\n             state :  {NEW,ALLOCATED,ACTIVE,KILLED} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat : {lastHeartbeat},\n             numOperators : {numOperators},\n             operators:  {\n                 id1 :  name1 ,\n                 id2 :  name2 ,\n                 id3 :  name3 \n            },\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             gcCollectionTime :  {gcCollectionTime} ,\n             gcCollectionCount :  {gcCollectionCount} ,\n             containerLogsUrl :  {containerLogsUrl} ,\n             startedTime :  {containerStartTime} ,\n             finishedTime :  {containerFinishedTime} ,\n             rawContainerLogsUrl :  {rawContainerLogsUrl} ,\n             containerType :  APP_MASTER|STREAMING \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}&amp;types={all,appmaster}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontainerid", 
            "text": "Function:  Example:  {\n     id :  {id} ,\n     host :  {host} ,\n     state :  {NEW,ALLOCATED,ACTIVE,KILLED} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :\n    {\n        lastHeartbeat\n    },\n     numOperators :\n    {\n        numOperators\n    },\n     operators: \n    {\n         id1 :  name1 ,\n         id2 :  name2 ,\n         id3 :  name3 \n    },\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     gcCollectionTime :  {gcCollectionTime} ,\n     gcCollectionCount :  {gcCollectionCount} ,\n     containerLogsUrl :  {containerLogsUrl} ,\n     startedTime :  {containerStartTime} ,\n     finishedTime :  {containerFinishedTime} ,\n     rawContainerLogsUrl :  {rawContainerLogsUrl} ,\n     containerType :  APP_MASTER|STREAMING \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogs", 
            "text": "Function:  Example:   {\n     logs : [\n        {\n             name :  logName-1 ,\n             length :  {length} ,\n             rawUrl :  {urlToLog} \n        },\n        {\n             name :  logName-2 ,\n             length :  {length} ,\n             rawUrl :  {urlToLog} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogslognamestartstartposendendposgrepregexpincludeoffsettruefalselastnbytesnumberofbytes", 
            "text": "Function: Return the raw log, or the last N bytes of the log if lastNBytes is given.  Return: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json):  Example:   {\n     lines : [\n        {\n             byteOffset :  {byteOffsetFromStartOfTheLog} ,\n             line :  {one line from {logName}} \n        },\n        {\n             byteOffset :  {byteOffsetFromStartOfTheLog} ,\n             line :  {the next line from {logName}}} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}&amp;end={endPos}&amp;grep={regexp}&amp;includeOffset={true/false}&amp;lastNBytes={numberOfBytes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerslogslognamegrepregexpdescendingordertruefalseincludeoffsettruefalselastnbytesnumberofbytes", 
            "text": "Function: Return GC events for the application in sorted order.  Return: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json):  Example:   {\n     lines : [\n        {\n             byteOffset :  {byteOffset} ,\n             line :  {GC event} ,\n             startTime :  {eventStartTime} ,\n             type :  GC ,\n             occupiedHeapMemoryBefore :  {occupiedHeapMemoryBefore} ,\n             occupiedHeapMemoryAfter :  {occupiedHeapMemoryAfter} ,\n             heapCapacity :  {heapCapacity} ,\n             heapReductionPercentage :  {heapReductionPercentage} ,\n             duration :  {duration} ,\n             container :  {containerId} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/logs/{logName}[?grep={regexp}&amp;descendingOrder={true/false}&amp;includeOffset={true/false}&amp;lastNBytes={numberOfBytes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsoperatornamelogslognamegrepregexpdescendingordertruefalselastnbytesnumberofbytes", 
            "text": "Function: Return GC events for the an logical operator in sorted order. A logical operator maps to one or more physical operator(s). \nEach physical operator belongs to a container and all the GC events from all such containers are collected and returned.  Example:   {\n     lines : [\n        {\n             byteOffset :  {byteOffset} ,\n             line :  {GC event} ,\n             startTime :  {eventStartTime} ,\n             type :  GC ,\n             occupiedHeapMemoryBefore :  {occupiedHeapMemoryBefore} ,\n             occupiedHeapMemoryAfter :  {occupiedHeapMemoryAfter} ,\n             heapCapacity :  {heapCapacity} ,\n             heapReductionPercentage :  {heapReductionPercentage} ,\n             duration :  {duration} ,\n             container :  {containerId} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{operatorName}/logs/{logName}[?grep={regexp}&amp;descendingOrder={true/false}&amp;lastNBytes={numberOfBytes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidissues", 
            "text": "Function: Return list of issues of the applicaiton.  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        ...\n    ]    \n}", 
            "title": "GET /ws/v2/applications/{appid}/issues"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackageshasservicestruefalsehasuitruefalse", 
            "text": "Function:   Example:   {\n     appPackages : [\n        {\n             owner :  {owner} ,\n             modificationTime :  {modificationTime} ,\n             appPackageName :  {appPackageName} ,\n             appPackageVersion :  {appPackageVersion} ,\n             appPackageDisplayName :  {appPackageDisplayName} ,\n             appPackageDescription :  {appPackageDescription} ,\n             applications : [\n                {\n                     name :  {application-1} ,\n                     displayName :  {applicationDisplayName-1} ,\n                     type :  class/json ,\n                     launchDisabled :  true/false \n                },\n                ...\n            ],\n             canWrite :  true/false ,\n             ui : {\n                 dashboards : [\n                    {\n                         name :  {dashboardName-1} ,\n                         file :  {dashboardFile-1} ,\n                         appNames : [\n                             {applicationUsingThisDashborad-1} ,\n                             {applicationUsingThisDashborad-2} ,\n                            ...\n                        ]\n                    },\n                    ...\n                ]\n            },\n             services : {\n                 services : [\n                    {\n                        // Sample docker service\n                         name :  {serviceName} ,\n                         type :  docker ,\n                         srcUrl :  {srcUrl} ,\n                         docker : {\n                             run :  {options and arguments to run docker service} \n                        },\n                         proxy : {\n                             name :  {proxyName} ,\n                             address :  {proxyAddress} \n                        },\n                         requiredServices : [\n                             {requiredServiceName-1} ,\n                             {requiredServiceName-2} ,\n                            ...\n                        ]\n                    },\n                    {\n                        // Sample Apex service\n                         name :  {serviceName} ,\n                         type :  apex ,\n                         srcUrl :  {srcUrl} ,\n                         apex : {\n                             appName :  ApexApplicationName ,\n                             launchArgs : {\n                                 {launchArgName} :  {launchArgValue} \n                            }\n                        },\n                         proxy : {\n                             name :  {proxyName} ,\n                             address :  {proxyAddress} \n                        },\n                         metadata : {\n                             QueryIP :  {queryIP} ,\n                             QueryPort :  {queryPort} \n                        }\n                    }\n                ],\n                 applications : [\n                    {\n                         name :  {applicationUsesService} ,\n                         requiredServices : [\n                            {\n                                 name :  requiredServiceName ,\n                                 {servivePropertyName-1} :  {serviceProperty} ,\n                                 {servivePropertyName-2} :  {serviceProperty} ,\n                                ...\n                            },\n                            ...\n                        ]\n                    }\n                ]\n            }\n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/appPackages?hasServices={true/false}&amp;hasUI={true/false}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionincludedescriptiontruefalse", 
            "text": "Function: Gets the meta information of the app package  Returns: If  includeDescription  is set to be false or not provided, return meta data for such app package with properties as simple name-value pairs. If  includeDescription  is true, properties will also include description information as well.  Example:   {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     appPackageGroupId :  {appPackageGroupId} ,\n     dtEngineVersion :  {dtEngineVersion} ,\n     appPackageDescription :  {appPackageDescription} ,\n     appPackageDisplayName :  {appPackageDisplayName} ,\n     classPath : [\n         {classPath} \n    ],\n     applications : [\n        {applicationMetaData-1},\n        {applicationMetaData-2},\n        ...\n    ],\n     appJars : [\n         {appJar-1} ,\n         {appJar-2} \n        ...\n    ],\n     appJsonFiles : [\n        { appJsonFile-1 },\n        { appJsonFile-2 },\n        ...\n    ],\n     appPropertiesFiles : [],\n     requiredProperties : [\n        // when includeDescription=false\n        { propertyName-1 },\n        { propertyName-2 },\n        ...\n\n        // When includeDescription=true\n        { propertyName-1 }: {\n             value : null,\n             description :  {descriptionOfProperty} \n        },\n        ...\n    ],\n     defaultProperties : {\n        // when includeDescription=false\n        { propertyName-1 }: {defaultValueOfProperty-1},\n        { propertyName-2 }: {defaultValueOfProperty-2},\n        ...\n\n        // When includeDescription=true\n        { propertyName-1 }: {\n             value :  {defaultValueOfProperty} ,\n             description :  {descriptionOfProperty} \n        },\n        ...\n    },\n     configs : [\n        { config-1 },\n        { config-2 },\n        ...\n    ],\n     owner :  {owner} ,\n     modificationTime :  {modificationTime} ,\n     canWrite :  true/false \n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}?includeDescription={true/false}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplications", 
            "text": "Function:   Example:   {\n     applications : [\n        {\n             name :  {name} ,\n             file :  {fileName} ,\n             type :  {type} ,\n             displayName :  {displayName} ,\n             dag : {dag in json format},\n             error :  {error} ,\n             errorStackTrace :  {errorStackTrace} ,\n             requiredProperties : [\n                { propertyName-1 },\n                { propertyName-2 },\n                ...\n            ],\n             defaultProperties : {\n                { propertyName-1 }: {defaultValueOfProperty-1},\n                { propertyName-2 }: {defaultValueOfProperty-2},\n                ...\n            }\n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplicationsappnameincludedescriptiontruefalse", 
            "text": "Function: Gets the meta data for that application  Example:   {\n     name :  {name} ,\n     file :  {fileName} ,\n     type :  {type} ,\n     displayName :  {displayName} ,\n     dag :{dag in json format},\n     error :  {error} ,\n     errorStackTrace :  {errorStackTrace} ,\n     requiredProperties : [\n        // when includeDescription=false\n        { propertyName-1 },\n        { propertyName-2 },\n        ...\n\n        // When includeDescription=true\n        { propertyName-1 }: {\n             value : null,\n             description :  {descriptionOfProperty} \n        },\n        ...\n    ],\n     defaultProperties : {\n        // when includeDescription=false\n        { propertyName-1 }: {defaultValueOfProperty-1},\n        { propertyName-2 }: {defaultValueOfProperty-2},\n        ...\n\n        // When includeDescription=true\n        { propertyName-1 }: {\n             value :  {defaultValueOfProperty} ,\n             description :  {descriptionOfProperty} \n        },\n        ...\n    }\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}[?includeDescription={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionclassschemas", 
            "text": "Function: This is deprecated.  Return:", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/classSchemas"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionclassschemasclassschemasnameversionversion", 
            "text": "Function: This is deprecated.  Return:", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/classSchemas/{classSchemasName}[?version={version}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2clusterconfig", 
            "text": "Function: Get the list of all configurations on cluster.  Example:   {\n     {configName-1} :  {configValue-1} ,\n     {configName-2} :  {configValue-2} ,\n    ...\n}", 
            "title": "GET /ws/v2/cluster/config"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2clusterqueues", 
            "text": "Function: Get the list of queues on cluster..  Example:        queues : [\n        {\n             capacity :  {Capacity of the queue} ,\n             currentCapacity :  {Current capacity of the queue} ,\n             maxCapacity :  {Maximum capacity of the queue} ,\n             name :  {Queue name} ,\n             state :  {State of the Queue} \n        },\n        ...\n    ]\n}", 
            "title": "GET /ws/v2/cluster/queues"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configproperties", 
            "text": "Function: Returns the list of all configuration properties defined in the configuration files.  Example:      {\n     {name} :\n    {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} ,\n         scope :  {scope} \n    },\n    ...\n}", 
            "title": "GET /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configdocker", 
            "text": "Function: Get docker configuration status  Example:   {\n   isFound :  true/false , \n  \u201cversion\u201d: \u201c{version}\u201d,\n   isCompatible :  true/false ,\n   dt.dockerHost :  {dockerHostAddress} \n}", 
            "title": "GET /ws/v2/config/docker"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configdocker", 
            "text": "Function: Set docker configuration  Payload:\n{\n    \"dt.dockerHost\": \"\"{dockerHostAddress}\"\n}", 
            "title": "PUT /ws/v2/config/docker"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2licensesfilenamelicense-name", 
            "text": "Function: The request payload of the API is the content of the license.  Example:   Returns:\n{\n     currentTime :  {currentTimeMills} ,\n     startDate :  {startTimeMills} ,\n     expireTime :  {expireTimeMills} ,\n     memoryMBAllowed :  {memoryMBAllowed} ,\n     memoryMBUsed :  {memoryMBUsed} ,\n     issuedTo :  {issuedTo} ,\n     issuedBy :  {issuedBy} ,\n     issuerWebsite :  {issuerWebsite} ,\n     supportedBy :  {supportedBy} ,\n     supportURL :  {supportURL} ,\n     category :  DT Premium/DT PLUS ,\n     exceedGracePeriod :  {exceedGracePeriod} ,\n     valid : {true/false},\n     id :  {licenseId} ,\n     licenseType :  {licenseType} \n}", 
            "title": "POST /ws/v2/licenses?filename={license-name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2licenses", 
            "text": "Function: This is deprecated. Use the API  GET /v2/licenses/current  instead.  Example:     licenses : [\n    {\n         currentTime :  {currentTimeMills} ,\n         startDate :  {startTimeMills} ,\n         expireTime :  {expireTimeMills} ,\n         memoryMBAllowed :  {memoryMBAllowed} ,\n         memoryMBUsed :  {memoryMBUsed} ,\n         issuedTo :  {issuedTo} ,\n         issuedBy :  {issuedBy} ,\n         issuerWebsite :  {issuerWebsite} ,\n         supportedBy :  {supportedBy} ,\n         supportURL :  {supportURL} ,\n         category :  DT Premium/DT PLUS ,\n         exceedGracePeriod :  {exceedGracePeriod} ,\n         valid : {true/false},\n         id :  {licenseId} ,\n         licenseType :  {licenseType} \n    }]\n}", 
            "title": "GET /ws/v2/licenses"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2licensesid", 
            "text": "Function: Get the selected license. The parameter 'id' can be the string \"current\" or the valid current license id.  Example:  {\n     currentTime :  {currentTimeMills} ,\n     startDate :  {startTimeMills} ,\n     expireTime :  {expireTimeMills} ,\n     memoryMBAllowed :  {memoryMBAllowed} ,\n     memoryMBUsed :  {memoryMBUsed} ,\n     issuedTo :  {issuedTo} ,\n     issuedBy :  {issuedBy} ,\n     issuerWebsite :  {issuerWebsite} ,\n     supportedBy :  {supportedBy} ,\n     supportURL :  {supportURL} ,\n     category :  DT Premium/DT PLUS ,\n     exceedGracePeriod :  {exceedGracePeriod} ,\n     valid : {true/false},\n     id :  {licenseId} ,\n     licenseType :  {licenseType} \n}", 
            "title": "GET /ws/v2/licenses/{id}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2phonehomereportperiodtotalpreviouscurrent", 
            "text": "Function: Get a report which is in the same format of the usage report gateway is generating and sending back to DataTorrent. When the query parameter \"period\" is omitted, by default it will return stats from the total period.  Example:  {\n     licenseId :  {licenseId} ,\n     numNodesInCluster :  {numberOfNodesInCluster} ,\n     license :\n    {\n         currentTime :  {currentTimeMills} ,\n         startDate :  {startTimeMills} ,\n         expireTime :  {expireTimeMills} ,\n         memoryMBAllowed :  {memoryMBAllowed} ,\n         memoryMBUsed :  {memoryMBUsed} ,\n         issuedBy :  {issuedBy} ,\n         issuerWebsite :  {issuerWebsite} ,\n         supportedBy :  {supportedBy} ,\n         supportURL :  {supportURL} ,\n         category :  {DT Premium/DT PLUS} ,\n         exceedGracePeriod :  {exceedGracePeriod} ,\n         valid : {true/false},\n         expirationTimeNotificationLevel :  {INFO/WARN/ERROR} ,\n         id :  {licenseId} ,\n         licenseType :  {licenseType} \n    },\n     rts.version :  {rtsVersion} ,\n     apex.version :  {apexVersion} ,\n     java.vendor :  {javaVendor} ,\n     java.version :  {javaVersion} ,\n     os.arch :  {operatingSystemArchitecture} ,\n     os.name :  {operatingSystemName} ,\n     os.version :  {operatingSystemVersion} ,\n     hadoop.version :  {hadoopVersion} ,\n     metrics :\n    {\n         tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n         tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n         cpuPercentage :  {cpuPercentage} ,\n         gatewayUpTimeMills :  {gatewayUpTimeMills} ,\n         currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n         period :  {total/previous/current} ,\n         startTime :  {startTimeOfThePeriod} ,\n         appsRunningMax :  {appsRunningMaxInThePeriod} ,\n         ageOfAppsAvg :  {ageOfAppsAvgInThePeriod} ,\n         appsRunningAvg :  {appsRunningAvgInThePeriod} ,\n         memMBMax :  {memMBMaxInThePeriod} ,\n         memMBMin :  {memMBMinInThePeriod} ,\n         numAppsFinished :  {numAppsFinishedInThePeriod} ,\n         numOfContainersAvg :  {numOfContainersAvgInThePeriod} ,\n         endTime :  {endTimeOfThePeriod} ,\n         appsRunningMin :  {appsRunningMinInThePeriod} ,\n         numOfOperatorsAvg :  {numOfOperatorsAvgInThePeriod} \n    }\n}", 
            "title": "GET /ws/v2/phoneHome/report[?period={total/previous/current}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2services", 
            "text": "Function:  list of all the currently installed services and their statuses.  Example:  \n{\n     services : [\n    {\n         name :  {serviceName} ,\n         state :  {serviceState} ,\n         startedTime : {serviceStartTime},\n         installedTime : {serviceInstallTime},\n         enabled : {true / false},\n         type :  {docker/apex} ,\n         dependentApps : [\n        {\n            appId:  {appId} ,\n            appName:  {appName} ,\n            state:  {appState} ,\n            user:  {user} \n        }],\n         memoryMB : {memoryMB},\n         requiredServices : [\n             {requiredService-1} ,\n            ...\n        ],\n         srcUrl :  {srcUrl} ,\n         docker :\n        {\n             run :  {options and arguments to run docker service} ,\n             exec :  {optional exec command to run after the container is started} \n        },\n         proxy :\n        {\n             name :  {proxyName} ,\n             address :  {proxyAddress} ,\n         followRedirect : {true/false}\n        },\n         containerId :{containerId}\n    },\n    {\n         name :  {serviceName} ,\n         type :  apex ,\n         srcUrl :  {srcUrl} ,\n         apex :\n        {\n             appName :  ApexApplicationName ,\n             launchArgs :\n            {\n                 {launchArgName} :  {launchArgValue} \n            }\n        },\n         proxy :\n        {\n             name :  {proxyName} ,\n             address :  {proxyAddress} ,\n         followRedirect : {true/false}\n        },\n         metadata :\n        {\n             var-name :  {value} \n        }\n    }]\n}", 
            "title": "GET /ws/v2/services"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2servicesname", 
            "text": "Function: Get details for a specific service  Example:       {\n         name :  {serviceName} ,\n         state :  {serviceState} ,\n         startedTime : {serviceStartTime},\n         installedTime : {serviceInstallTime},\n         enabled : {true / false},\n         type :  {docker/apex} ,\n         dependentApps : [\n        {\n            appId:  {appId} ,\n            appName:  {appName} ,\n            state:  {appState} ,\n            user:  {user} \n        }],\n         memoryMB : {memoryMB},\n         requiredServices : [\n             {requiredService-1} ,\n            ...\n        ],\n         srcUrl :  {srcUrl} ,\n         docker :\n        {\n             run :  {options and arguments to run docker service} ,\n             exec :  {optional exec command to run after the container is started} \n        },\n         proxy :\n        {\n             name :  {proxyName} ,\n             address :  {proxyAddress} ,\n         followRedirect : {true/false}\n        },\n         containerId :{containerId}\n    }", 
            "title": "GET /ws/v2/services/{name}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2servicesname", 
            "text": "Function: Install a new service with specified JSON params  Payload:  {\n     name :  {serviceName} ,\n     enabled :\n    {\n        true / false\n    },\n     type :  {docker/apex} ,\n     requiredServices : [\n         {requiredService-1} ,\n        ...\n    ],\n     srcUrl :  {srcUrl} ,\n     docker :\n    {\n         run :  {options and arguments to run docker service} \n    },\n     proxy :\n    {\n         name :  {proxyName} ,\n         address :  {proxyAddress} \n    }\n}", 
            "title": "PUT /ws/v2/services/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2servicesname", 
            "text": "Function: Delete specified service", 
            "title": "DELETE /ws/v2/services/{name}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2servicesname", 
            "text": "Function: Update specified service  Payload:  {\n     name :  {serviceName} ,\n     enabled :\n    {\n        true / false\n    },\n     type :  {docker/apex} ,\n     requiredServices : [\n         {requiredService-1} ,\n        ...\n    ],\n     srcUrl :  {srcUrl} ,\n     docker :\n    {\n         run :  {options and arguments to run docker service} \n    },\n     proxy :\n    {\n         name :  {proxyName} ,\n         address :  {proxyAddress} \n    }\n}", 
            "title": "POST /ws/v2/services/{name}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2servicesnamestart", 
            "text": "Function: Start the specified service.", 
            "title": "POST /ws/v2/services/{name}/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2servicesnamestop", 
            "text": "Function: Stop the specified service", 
            "title": "POST /ws/v2/services/{name}/stop"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2servicesinstallasynctruefalse", 
            "text": "Function: Installs multiple services based on JSON params array.  Services are launched after install. hasAppDataSources is an optional flag, if set to true will cause service to respond immediately with 200 or error without waiting for download, installation and launch to complete  Payload:  [\n    {\n         name :  {serviceName} ,\n         type :  docker ,\n         srcUrl :  {dockerAddress} ,\n         docker :\n        {\n             run :  {run options} ,\n             exec :  echo optional command and args \n        },\n         proxy :\n        {\n             name :  {proxyName} ,\n             address :  {proxyAddress} \n        },\n         requiredServices : [ {requiredService-1} ],\n         enabled : true //enabled by default\n    }\n]", 
            "title": "POST /ws/v2/services/install[?async={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2about", 
            "text": "Example:  \n     version :  {Apex version} ,\n     buildDate :  {Apex build date and time} ,\n     buildRevision :  {Apex revision} ,\n     buildVersion :  {Apex build version} ,\n     buildUser :  {Apex build user} ,\n    // above 5 fields are deprecated and will be removed in 4.0\n     apexVersion :  {Apex version} ,\n     apexBuildDate :  {Apex build date and time} ,\n     apexBuildRevision :  {Apex revision} ,\n     apexBuildVersion :  {Apex build version} ,\n     apexBuildUser :  {Apex build user} ,\n     hadoopVersion :  {Hadoop version} ,\n     hadoopBuildDate :  {Hadoop build date and time} ,\n     hadoopBuildRevision :  {Hadoop build revision} ,\n     hadoopBuildVersion :  {Hadoop build version} ,\n     hadoopBuildUser :  {Hadoop build user} ,\n     rtsVersion :  {RTS version} ,\n     rtsBuildDate :  {RTS build date and time} ,\n     rtsBuildRevision :  {RTS revision} ,\n     rtsBuildVersion :  {RTS build version} ,\n     rtsBuildUser :  {RTS build user} ,\n     os.arch :  {operating system architecture} ,\n     os.name :  {operating system name} ,\n     os.version :  {operating system version} ,\n     javaVendor :  {java vendor} ,\n     javaVersion :  {java version} ,\n     gatewayUser :  {user} ,\n     hadoopLocation :  {Hadoop location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hadoopIsSecurityEnabled : {true/false},\n     haEnabled : {true/false},\n     timeZone :  {time zone} ,\n     hostname :  {hostname} \n}", 
            "title": "GET /ws/v2/about"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2about_1", 
            "text": "Function:  Return:  {\n     buildVersion :  {Apex build version} ,\n     buildDate :  {Apex build date and time} ,\n     buildRevision :  {Apex revision} ,\n     buildUser :  {Apex build user} ,\n     version :  {Apex version} ,\n     rtsBuildVersion :  {RTS build version} ,\n     rtsBuildDate :  {RTS build date and time} ,\n     rtsBuildRevision :  {RTS revision} ,\n     rtsBuildUser :  {RTS build user} ,\n     rtsVersion :  {RTS version} ,\n     gatewayUser :  {user} ,\n     javaVersion :  {java_version} ,\n     hadoopLocation :  {hadoop_location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hostname :  {hostname} ,\n     hadoopIsSecurityEnabled :  {true/false} \n}", 
            "title": "GET /ws/v2/about"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2clustermetrics", 
            "text": "Function: List metrics that are relevant to the entire cluster  Return:  {\n     averageAge :  {average running application age in milliseconds} ,\n     cpuPercentage :  {cpuPercentage} ,\n     currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n     maxMemoryAllocatedMB :  {maxMemoryAllocatedMB} ,\n     numAppsFailed :  {numAppsFailed} ,\n     numAppsFinished :  {numAppsFinished} ,\n     numAppsKilled :  {numAppsKilled} ,\n     numAppsPending :  {numAppsPending} ,\n     numAppsRunning :  {numAppsRunning} ,\n     numAppsSubmitted :  {numAppsSubmitted} ,\n     numContainers :  {numContainers} ,\n     numOperators :  {numOperators} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/cluster/metrics"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsstatesstate_filternamename_filteruseruser_filter", 
            "text": "Function: List IDs of all streaming applications  Return:  {\n     apps : [\n        {\n             diagnostics :  {diagnostics} ,\n             elapsedTime :  {elapsedTime} ,\n             finalStatus :  {finalStatus} ,\n             finishedTime :  {finishedTime} ,\n             id :  {appId} ,\n             name :  {name} ,\n             queue :  {queue} ,\n             startedTime :  {startedTime} ,\n             state :  {state} ,\n             trackingUrl :  {trackingUrl} ,\n             user :  {user} \n        },  \n        \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications[?states={STATE_FILTER}&amp;name={NAME_FILTER}&amp;user={USER_FILTER]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappid_1", 
            "text": "Function: Get the information for the specified application  Return:  {\n     id :  {appid} ,\n     name :  {name} ,\n     state :  {state} ,\n     trackingUrl :  {tracking url} ,\n     finalStatus : {finalStatus},\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     elapsedTime :  {elapsedTime} ,\n     startedTime :  {startTime} ,\n     user :  {user} ,\n     version :  {stram version} ,\n     remainingLicensedMB :  {remainingLicensedMB} ,\n     allocatedMB :  {allocatedMB} ,\n     gatewayConnected :  true/false ,\n     connectedToThisGateway :  true/false ,\n     attributes : {\n            {attributeName} :  {attributeValue} , \n            {attributeName-n} :  {attributeValue-n} , \n    },\n     stats : {\n         allocatedContainers :  {allocatedContainer} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         latency :  {overall latency} ,\n         criticalPath :  {list of operator id that represents the critical path} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         plannedContainers :  {plannedContainers} ,\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed : {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA : {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted : {sum of operators:totalTuplesEmitted} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} \n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplan", 
            "text": "Function: Return the physical plan for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             isUnifier : true/false\n        },\n         \u2026\n     ],\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperators", 
            "text": "Function: Return list of operators for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             counters : {\n                 {counterName} :  {counterValue} , \n                ...\n             },\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             unifierClass :  {unifierClass} \n        },\n         \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanstreams", 
            "text": "Function: Return physical streams  Return:  {\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/streams"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopid", 
            "text": "Function: Return information of the given operator for the given application  Return:  {\n     className :  {className} ,\n     container :  {containerId} ,\n     counters : {\n       {counterName}:  {counterValue} , ...            \n    }\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     currentWindowId :  {currentWindowId} ,\n     failureCount :  {failureCount} ,\n     host :  {host} ,\n     id :  {id} ,\n     ports : [\n       {\n           bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n           name :  {name} ,\n           totalTuples :  {totalTuples} ,\n           tuplesPSMA :  {tuplesPSMA} ,\n           type :  input/output \n       }, ...\n    ],\n     lastHeartbeat :  {lastHeartbeat} ,\n     latencyMA :  {latencyMA} ,\n     name :  {name} ,\n     recordingStartTime :  {recordingStartTime} ,\n     recoveryWindowId :  {recoveryWindowId} ,\n     status :  {status} ,\n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopiddeployhistory", 
            "text": "Function: Return container deploy history of this operator\nSince: 1.0.6  Return:  {\n    containers : [  \n        {  \n             container :  {containerId} ,   \n             startTime :  {startTime}   \n        }, ...  \n    ],   \n     name :  {operatorName} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidports", 
            "text": "Function: Get the information of all ports of the given operator of the\ngiven application  Return:  {  \n     ports : [\n        {  \n             bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,  \n             totalTuples :  {totalTuples} ,   \n             tuplesPSMA :  {tuplesPSMA} ,   \n             type :  output   \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidportsportname", 
            "text": "Function: Get the information of a specified port  Return:  {  \n     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n     name :  {name} ,   \n     totalTuples :  {totalTuples} ,   \n     tuplesPSMA :  {tuplesPSMA} ,   \n     type :  {type}   \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesparentparentqsearchtermpackageprefixescomma-separated-package-prefixes", 
            "text": "Function: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent  Return:  {  \n     operatorClasses : [  \n        {  name : {className}  },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}&amp;q={searchTerm}&amp;packagePrefixes={comma-separated-package-prefixes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesoperatorclass", 
            "text": "Function: Get the description of the given operator class  Return:  {\n     inputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n          ...\n    ],\n     outputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n        \u2026\n    ],\n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidshutdown", 
            "text": "Function: Shut down the application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/shutdown"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidkill", 
            "text": "Function: Kill the given application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/kill"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstart", 
            "text": "Function: Start recording on operator  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstop", 
            "text": "Function: Stop recording on operator  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstart", 
            "text": "Function: Start recording on port  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstop", 
            "text": "Function: Stop recording on port  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatesnewallocatedactivekilled", 
            "text": "Function: Return the list of containers for this application  Return:  {\n     containers : [\n        {\n             host :  {host} ,\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat :  {lastHeartbeat} ,\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             numOperators :  {numOperators} ,\n             operators:  {\n                 id1 :  name1 ,\n                 id2 :  name2 ,\n                 id3 :  name3 \n            },\n             containerLogsUrl :  {containerLogsUrl} ,\n             state :  {state} \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontainerid_1", 
            "text": "Function: Return the information of the specified container  Return:  {\n     host :  {host} ,\n     id :  {id} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :  {lastHeartbeat} ,\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     numOperators :  {numOperators} ,\n     operators:  {\n         id1 :  name1 ,\n         id2 :  name2 ,\n         id3 :  name3 \n    },\n     containerLogsUrl :  {containerLogsUrl} ,\n     state :  {state} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogs_1", 
            "text": "Function: Return the container log list  Return:  {\n     logs : [\n        {\n             length :  {log length} ,\n             name :  {logName} \n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridstacktrace", 
            "text": "Since: 3.4.0   Function: Return the container stack trace  Return:  {\n     threads : [\n        {\n             name :  {name} ,\n             state :  {state} ,\n             id :  {id} ,\n             stackTraceElements : [\n                 {line1} ,\n                 {line2} , ...\n            ]\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/stackTrace"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogslognamestartstartposendendposgrepregexpincludeoffsettruefalselastnbytesn", 
            "text": "Function: Return the log with provided options  Return: if includeOffset=false or not provided, return raw log content (Content-Type: text/plain). Otherwise (Content-Type: application/json).\nThe options (start, end) and (lastNbytes) are mutually exclusive.  {\n     lines : [\n        {  byteOffset : {byteOffset} ,  line :  {line}  }, {  byteOffset : {byteOffset} ,  line :  RandomNumber : {Line}  } \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}&amp;end={endPos}&amp;grep={regexp}&amp;includeOffset={true/false}&amp;lastNbytes={N}]"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplancontainerscontaineridkill", 
            "text": "Function: Kill this container  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplan", 
            "text": "Function: Return the logical plan of this application  Return:  {\n     operators : [\n      {\n         name :  {name} ,\n         attributes : {attributeMap},\n         class :  {class} ,\n         ports : {\n           [\n            {\n                 name :  {name} ,\n                 attributes : {attributeMap},\n                 type :  input/output \n            }, ...\n           ]\n         },\n          properties : {\n             class :  {class} \n         }\n      }, ...\n    ],\n     streams : [\n        {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanattributes", 
            "text": "Function: Return the application attributes  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperators", 
            "text": "Function: Return the list of info of the logical operator  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopname", 
            "text": "Function: Return the info of the logical operator  Return:  {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Return the properties of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Set the properties of the logical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Return the properties of the physical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Set the properties of the physical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameattributes", 
            "text": "Function: Get the attributes of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameportsportnameattributes", 
            "text": "Function:  Get the attributes of the port  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplan", 
            "text": "Function: Change logical plan of this application\nPayload:  {\n     requests : [\n        {\n             requestType :  AddStreamSinkRequest ,\n             streamName :  {streamName} ,\n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  CreateOperatorRequest ,\n             operatorName :  {operatorName} ,\n             operatorFQCN :  {operatorFQCN} ,\n        },\n        {\n             requestType :  CreateStreamRequest ,\n             streamName :  {streamName} ,\n             sourceOperatorName :  {sourceOperatorName} ,\n             sourceOperatorPortName :  {sourceOperatorPortName} \n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  RemoveOperatorRequest ,\n             operatorName :  {operatorName} ,\n        },\n        {\n             requestType :  RemoveStreamRequest ,\n             streamName :  {streamName} ,\n        },\n        {\n             requestType :  SetOperatorPropertyRequest ,\n             operatorName :  {operatorName} ,\n             propertyName :  {propertyName} ,\n             propertyValue :  {propertyValue} \n        },\n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsmeta", 
            "text": "Function: Return the meta information about the statistics stored for\nthis operator  Return:  {\n     appId :  {appId} ,\n     operatorName :  {operatorName} ,\n     operatorIds : [ {opid}, \u2026 ],\n     startTime :  {startTime} ,\n     endTime :  {endTime} ,\n     count :  {count} ,\n     ended :  {boolean} \n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the statistics stored for this logical operator  {\n     operatorStats : [\n        {\n             operatorId :  {operatorId} ,\n             timestamp :  {timestamp} ,\n             stats : {\n                 container :  containerId ,\n                 host :  host ,\n                 totalTuplesProcessed ,  {totalTuplesProcessed} ,\n                 totalTuplesEmitted ,  {totalTuplesEmitted} ,\n                 tuplesProcessedPSMA ,  {tuplesProcessedPSMA} ,\n                 tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n                 cpuPercentageMA :  {cpuPercentageMA} ,\n                 latencyMA :  {latencyMA} ,\n                 ports : [ {\n                     name :  {name} ,\n                     type : {input/output} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA ,  {tuplesPSMA} ,\n                     bufferServerBytesPSMA ,  {bufferServerBytesPSMA} \n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsmeta", 
            "text": "Function: Return the meta information about the container statistics  {\n     appId :  {appId} ,\n     containers : {\n         {containerId} : {\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             host :  {host} ,\n             memoryMBAllocated ,  {memoryMBAllocated} \n        },\n        \u2026\n    },\n     startTime :  {startTime} \n     endTime :  {endTime} \n     count :  {count} \n     ended : {boolean}\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the container statistics stored for this application  {\n     containerStats : [\n        {\n             containerId :  {containerId} \n             timestamp :  {timestamp} \n             stats : {\n                 numOperators :  {numOperators} ,\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidrecordings", 
            "text": "Function: Get the list of all recordings for this application  Return:  {\n     recordings : [{\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordings", 
            "text": "Function: Get the list of recordings on this operator  Return:  {\n     recordings : [ {\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Get the information about the recording  Return:  {\n     id :  {id} ,\n     startTime :  {startTime} ,\n     appId :  {appId} ,\n     operatorId :  {operatorId} ,\n     containerId :  {containerId} ,\n     totalTuples :  {totalTuples} ,\n     ports : [ {\n        name :  {portName} ,\n        streamName :  {streamName} ,\n        type :  {type} ,\n        id :  {index} ,\n        tupleCount :  {tupleCount} \n     } \u2026 ],\n     ended : {boolean},\n     windowIdRanges : [ {\n        low :  {lowId} ,\n        high :  {highId} \n     } \u2026 ],\n     properties : {\n        name :  value , ...\n     }\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Deletes the specified recording  Since: 1.0.4", 
            "title": "DELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsidtuples", 
            "text": "Query Parameters:  offset\nstartWindow\nlimit\nports\nexecuteEmptyWindow  Function: Get the tuples  Return:  {\n     startOffset :  {startOffset} ,\n     tuples : [ {\n         windowId :  {windowId} ,\n         tuples : [ {\n             portId :  {portId} ,\n             data :  {tupleData} \n        }, \u2026 ]\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappideventsfromfromtimetototimeoffsetoffsetlimitlimit", 
            "text": "Function: Get the events  Return:  {\n     events : [ {\n            id :  {id} ,\n         timestamp :  {timestamp} ,\n         type :  {type} ,\n         data : {\n             name :  value , \u2026\n        }\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/events?from={fromTime}&amp;to={toTime}&amp;offset={offset}&amp;limit={limit}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profileuser", 
            "text": "Function: Get the user profile information, list of roles and list of\npermissions given the user  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/profile/user"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettings", 
            "text": "Function: Get the current user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuser", 
            "text": "Function: Get the specified user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuserkey", 
            "text": "Function: Get the specified user's setting key  Return:  {\n     value : {value}\n}", 
            "title": "GET /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2profilesettingsuserkey", 
            "text": "Function: Set the specified user's setting key\nPayload:  {\n     value : {value}\n}", 
            "title": "PUT /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authroles", 
            "text": "Function: Get the list of roles the system has  Return:  {\n     roles : [\n       {\n          name :  {role1} ,\n          permissions : [  {permission1} , \u2026 ]\n       }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/auth/roles"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authrolesrole", 
            "text": "Function: Get the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authrolesrole", 
            "text": "Function: create or edit the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "PUT /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authrestoredefaultroles", 
            "text": "Function: Restores default roles", 
            "title": "POST /ws/v2/auth/restoreDefaultRoles"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authrolesrole", 
            "text": "Function: delete the given role", 
            "title": "DELETE /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authpermissions", 
            "text": "Function: Get the list of possible permissions  Return:  {\n     permissions : [ {\n        name :  {permissionName} ,\n        adminOnly : true/false\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2applicationsappidpermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidpermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownernamepermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownernamepermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2licenses", 
            "text": "Function: Add a license to the registry  Payload: The license file content  Return:  {\n   id :  {licenseId} ,\n   expireTime : {unixTimeMillis},\n   expirationTimeNotificationPeriod : {timeMillis},\n   nodesAllowed : {nodesAllowed},\n   memoryMBAllowed : {memoryMBAllowed},\n   exceedGracePeriod : {timeMillis},\n   contextType :  {contextType} ,\n   type :  {type} ,\n   features : [  {feature1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/licenses"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2licensescurrent", 
            "text": "Function: Get info on the current license  {\n       id :  {licenseId} ,\n       currentTime : {unixTimeMillis},\n       expireTime : {unixTimeMillis},\n       nodesAllowed : {nodesAllowed},\n       nodesUsed : {nodesUsed},\n       memoryMBAllowed : {memoryMBAllowed},\n       memoryMBUsed : {memoryMBUsed},\n       exceedGracePeriod : {timeMillis}, // memory exceed grace period\n       exceedRemainingTime : {timeMillis},  // (optional)\n       violation :  memory , // returns violation type (optional)\n       contextType :  {community|standard|enterprise} ,\n       type :  {evaluation|non_production|production} \n       features : [  {feature1} , \u2026 ], // for community, empty array\n       current : true/false,\n       expirationTimeNotificationLevel :  {INFO|WARN|ERROR} , // (optional)\n       valid : true/false // true, if the license is valid\n}", 
            "title": "GET /ws/v2/licenses/current"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configinstallmode", 
            "text": "Function: returns the install mode  {\n   installMode :  {evaluation|community|app} ,\n   appPackageName :  {optionalAppPackageName} ,\n   appPackageVersion :  {optionalAppPackageVersion} \n}", 
            "title": "GET /ws/v2/config/installMode"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function: returns the download type  {\n   value :  true/false \n}", 
            "title": "GET /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function:  {\n   value :  true/false \n}  Feature List:     SYSTEM_APPS  SYSTEM_ALERTS  APP_DATA_DASHBOARDS  RUNTIME_DAG_CHANGE  RUNTIME_PROPERTY_CHANGE  APP_CONTAINER_LOGS  LOGGING_LEVELS  APP_DATA_TRACKER  JAAS_LDAP_AUTH  APP_BUILDER", 
            "title": "PUT /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configproperties_1", 
            "text": "Function: Returns list of properties from dt-site.xml.  Return:  {\n     {name} : {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} \n    }\n\n}", 
            "title": "GET /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesproperty_name", 
            "text": "Function: Returns single property from dt-site.xml, specify by name  Return:  {\n     value :  {PROPERTY_VALUE} ,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "GET /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configproperties", 
            "text": "Function: Overwrites all specified properties in dt-site.xml  Payload:  {\n     properties : [\n        {\n             name :  {name} \n             value :  {PROPERTY_VALUE} ,\n             local : true/false,\n                     description :  {PROPERTY_DESCRIPTION} \n        }, \u2026\n    ]\n}", 
            "title": "POST /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesproperty_name", 
            "text": "Function: Overwrites or creates new property in dt-site.xml\nPayload:  {\n     value :  {PROPERTY_VALUE} ,\n     local : true/false,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "PUT /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2configpropertiesproperty_name", 
            "text": "Function: Deletes a property from dt-site.xml.", 
            "title": "DELETE /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2confighadoopexecutable", 
            "text": "Function: Returns the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "GET /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2confighadoopexecutable", 
            "text": "Function: Sets the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "PUT /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configissues", 
            "text": "Function: Returns list of potential issues with environment  Return:  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        {...},\n        {...}\n    ]    \n}", 
            "title": "GET /ws/v2/config/issues"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configipaddresses", 
            "text": "Function: Returns list of ip addresses the gateway can listen to  Return:  {\n     ipAddresses : [\n       1.2.3.4 , ...\n    ]    \n}", 
            "title": "GET /ws/v2/config/ipAddresses"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configrestart", 
            "text": "Function: Restarts the gateway  Payload: none", 
            "title": "POST /ws/v2/config/restart"
        }, 
        {
            "location": "/dtgateway_api/#get-proxyrmv1", 
            "text": "", 
            "title": "GET /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-proxyrmv1", 
            "text": "Function: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.", 
            "title": "POST /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#get-proxystramv2", 
            "text": "", 
            "title": "GET /proxy/stram/v2/..."
        }, 
        {
            "location": "/dtgateway_api/#post-proxystramv2", 
            "text": "", 
            "title": "POST /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#put-proxystramv2", 
            "text": "", 
            "title": "PUT /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#delete-proxystramv2", 
            "text": "Function: Proxy calls to Stram Web Services.", 
            "title": "DELETE /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidloggers", 
            "text": "Function: Set the logger levels of packages/classes.  Payload:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggers", 
            "text": "Function: Gets the logger levels of packages/classes.  Return:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggerssearchpatternpattern", 
            "text": "Function: searches for all classes that match the pattern.  Return:  {\n     loggers  : [\n        {\n             name  :  {fully qualified class name} ,\n             level :  {logger level} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\""
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidrestartqueuequeue", 
            "text": "Since: 3.4.0\nFunction: Restart the terminated application. Payload is optional.\nPayload:  {\n   {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n   appId :  {appId} \n}", 
            "title": "POST /ws/v2/applications/{appid}/restart[?queue={queue}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackages", 
            "text": "Since: 1.0.4  Function: Gets the list of appPackages the user can view in the system  {\n     appPackages : [\n        {\n                  appPackageName :  {appPackageName} ,\n                  appPackageVersion :  {appPackageVersion} ,\n             modificationTime :  {modificationTime} ,\n             owner :  {owner} ,\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/appPackages"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesmergereplacefailourstheirs", 
            "text": "Since: 1.0.4  Function: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package  Payload: the raw zip file  Return: The information of the app package", 
            "title": "POST /ws/v2/appPackages?merge={replace|fail|ours|theirs}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownername", 
            "text": "Since: 1.0.4  Function: Gets the list of versions of appPackages with the given name in the system owned by the specified user  {\n     versions : [\n         1.0-SNAPSHOT \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Deletes the appPackage", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversiondownload", 
            "text": "Since: 1.0.4  Function: Downloads the appPackage zip file", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Gets the meta information of the app package  Returns:  {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     modificationTime :   {modificationTime} ,\n     owner :  {owner} ,\n    ...\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigs", 
            "text": "Since: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:  {\n     configs : [\n         my-app-conf1.xml \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Gets the properties XML of the specified config  Returns:  configuration \n         property \n                 name ... /name \n                 value ... /value \n         /property \n        \u2026 /configuration", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Creates or replaces the specified config with the property parameters specified payload  Payload: configuration in XML", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Deletes the specified config", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplications_1", 
            "text": "Since: 1.0.4  Function: Gets the list of applications in the appPackage  Returns:  {\n     applications : [\n        {\n             dag : {dag in json format},\n             file :  {fileName} ,\n             name :  {name} ,\n             type :  {type} ,\n             error :  {error} ,\n             fileContent : {originalFileContentForJSONTypeApp}\n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplicationsappname", 
            "text": "Since: 1.0.4  Function: Gets the meta data for that application  Returns:  {\n     file :  {fileName} ,\n     name :  {name} ,\n     type :  {json/class/properties} ,\n     error :  {error} \n     dag : {\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n         }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n    },\n     fileContent : {originalFileContentForJSONTypeApp}\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesuserapppackagenameapppackageversionmerge", 
            "text": "Function: Merge the configuration, json apps, and resources files from the app package specified by user/name/version from the payload to the specified app package in the url, without overwriting any existing file in the specified app package. If replaceExisting is true, the files in the app, conf and resources directory of the app package will be replaced by the ones in the app package specified in the payload. Otherwise, they will not be replaced. The fields user, name and replaceExisting in the payload are optional. If user and name are not specified, they are default to be the same as in the URI path. replaceExisting's default is false.  Payload:  {\n  user :  {user} ,\n  name :  {name} ,\n  version :  {versionToMergeFrom} ,\n  replaceExisting :  {true/false} \n}", 
            "title": "POST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesownerpackagenamepackageversionapplicationsappnamelaunchconfigconfignameoriginalappidoriginalappidqueuequeuename", 
            "text": "Since: 1.0.4  Function: Launches the application with the given configuration specified in the POST payload  Payload:  {\n     {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n     appId :  {appId} \n}", 
            "title": "POST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}&amp;originalAppId={originalAppId}&amp;queue={queueName}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperatorsclassname", 
            "text": "Since: 1.0.4  Function: Get the properties of the operator given the classname in the jar  {  \n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationnameerrorifexiststruefalse", 
            "text": "Function: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app package  Payload:  {\n         displayName :  {displayName} ,\n         description :  {description} ,\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n          }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n}  Return:  {\n         error :  {error} \n}  Available port attributes to set:    AUTO_RECORD  IS_OUTPUT_UNIFIED  PARTITION_PARALLEL  QUEUE_CAPACITY  SPIN_MILLIS  STREAM_CODEC  UNIFIER_LIMIT   Available locality options to set:    THREAD_LOCAL  CONTAINER_LOCAL  NODE_LOCAL  RACK_LOCAL", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationname", 
            "text": "Since: 1.0.5  Function: Deletes non-jar based application in the app package", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperators", 
            "text": "Since: 1.0.5  Function: Get the classes of operators from specified app package.  Return:  {  \n     operatorClasses : [  \n        {\n             name : {fullyQualifiedClassName} , \n             title :  {title} ,\n             shortDesc :  {description} ,\n             longDesc :  {description} ,\n             category :  {categoryName} ,\n             doclink :  {doc url} ,\n             tags : [  {tag} ,  {tag} , \u2026 ],\n             inputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ]\n             outputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ],\n             properties : [  \n                {\n                     name : {propertyName} ,\n                     canGet : {canGet},\n                     canSet : {canSet},\n                     type : {type} ,\n                     description : {description} ,\n                     properties : ...\n                }, \u2026\n            ],\n             defaultValue : {\n                 {propertyName} : [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesimport", 
            "text": "Function: List the importable app packages on Gateway's local file\nsystem  Return:  {\n     appPackages: [\n        {\n             file :  {file} ,\n             name :  {name} ,\n             displayName :  {displayName} ,\n             version :  {version} ,\n             description :  {description} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesimport", 
            "text": "Function: Import app package from Gateway's local file system  Payload:  {\n         files : [ {file} , \u2026 ]\n}", 
            "title": "POST /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertsalertsname", 
            "text": "Function: Creates or replaces the specified system alert. The condition has access to an object in its scope called  _topic . An example alert might take the form of the following:  _topic[\"applications.application_1400294100000_0001\"].allocatedContainers   5  Payload:  {\n         condition : {condition in javascript} ,\n         email : {email} ,\n     description :  {description} ,\n         timeThresholdMillis : {time} \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2systemalertsalertsname", 
            "text": "Function: Deletes the specified system alert", 
            "title": "DELETE /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsinalerttruefalse", 
            "text": "Function: Gets the created alerts  Return:  {\n     alerts : [{\n         name :  {alertName} ,\n         condition : {condition in javascript} ,\n         email : {email} ,\n     description :  {description} ,\n         timeThresholdMillis : {time} ,\n         alertStatus : {\n             isInAlert :{true/false}\n             inTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }\n    }, \u2026  ]\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts?inAlert={true/false}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsname", 
            "text": "Function: Gets the specified system alert  Return:  {\n     name :  {alertName} ,\n     condition : {condition in javascript} ,        \n     email : {email} ,\n     description :  {description} ,\n     timeThresholdMillis : {time} ,\n     alertStatus : {\n         isInAlert :{true/false}\n         inTime :  {time} ,\n         message :  {message} ,\n         emailSent : {true/false}\n    }\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertshistory", 
            "text": "Function: Gets the history of alerts  Return:  {\n     history : [\n        {\n             name : {alertName} ,\n             inTime : {time} ,\n             outTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/systemAlerts/history"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstopicdata", 
            "text": "Function: Gets the topic data that is used for evaluating alert\ncondition  Return:  {\n      {topicName} : {json object data}, ...\n}", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertstemplatessystemname", 
            "text": "Function: Creates or replaces the specified system alert template.  Payload:  {\n     isSystemTemplate : true,\n     description :  {description} ,\n     parameters : [\n        {\n           variable :  {replacement variable in Javascript block} ,\n           label :  {input label} ,\n           type :  {number/text} ,\n           placeholder :  {input placeholder} ,\n           tooltip :  {input tooltip} ,\n           required : {true/false},\n           default :  {default value} ,     // optional\n           values : {                       // optional\n             {key} :  {value} ,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n     script :  {Javascript block} \n}  Example:  {\n     templates : [\n        {\n             isSystemTemplate : true,\n             description :  An alert template example. ,\n             parameters : [\n                {\n                   variable :  comparison ,\n                   label :  Comparison ,\n                   type :  text ,\n                   placeholder :  Select a comparison ,\n                   tooltip :  Choose the comparison to use. ,\n                   required : true,\n                   default :  ,\n                   values : {\n                     :  less than ,\n                     === :  equals to ,\n                     :  greater than \n                  }\n                },\n                {\n                   variable :  count ,\n                   label :  Number of Killed Containers ,\n                   type :  number ,\n                   placeholder :  Enter a valid number ,\n                   tooltip :  Enter the number. ,\n                   required : false\n                }\n            ],\n             script :  /* Alert when number of killed containers is {{comparison}} {{count}} */\n\n                _topic['cluster.metrics'].numContainers {{comparison.key}} ({{count}} !== null ? {{count}} : 0); \n        }\n    ]\n}", 
            "title": "PUT /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2systemalertstemplatessystemname", 
            "text": "Function: Deletes the specified system alert template.", 
            "title": "DELETE /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstemplatessystem", 
            "text": "Function: Gets the created system alert templates  Return:  {\n     templates : [\n        {\n             isSystemTemplate : true,\n             description :  {description} ,\n             parameters : [\n                {\n                   variable :  {replacement variable in Javascript block} ,\n                   label :  {input label} ,\n                   type :  {number/text} ,\n                   placeholder :  {input placeholder} ,\n                   tooltip :  {input tooltip} ,\n                   required : {true/false},\n                   default :  {default value} ,     // optional\n                   values : {                       // optional\n                     {key} :  {value} ,\n                    \u2026\n                  }\n                },\n                \u2026\n            ],\n             script :  {Javascript block} \n        },\n        \u2026\n    ]\n}", 
            "title": "GET /ws/v2/systemAlerts/templates/system"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstemplatessystemname", 
            "text": "Function: Gets the specified system alert template  Return:  {\n     isSystemTemplate : true,\n     description :  {description} ,\n     parameters : [\n        {\n           variable :  {replacement variable in Javascript block} ,\n           label :  {input label} ,\n           type :  {number/text} ,\n           placeholder :  {input placeholder} ,\n           tooltip :  {input tooltip} ,\n           required : {true/false},\n           default :  {default value} ,     // optional\n           values : {                       // optional\n             {key} :  {value} ,\n            \u2026\n          }\n        },\n        \u2026\n    ],\n     script :  {Javascript block} \n}", 
            "title": "GET /ws/v2/systemAlerts/templates/system/{name}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertsvalidatescript", 
            "text": "Function: Validates Java script.  Payload:  {\n     script : { script }\n}", 
            "title": "PUT /ws/v2/systemAlerts/validate/script"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusersuser", 
            "text": "Function: Gets the info of the given user  Return:  {\n     userName :  {userName} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "GET /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authusersuser", 
            "text": "Function: Changes password and/or roles of the given user  Return:  {\n     userName :  {userName} ,\n     oldPassword :  {oldPassword} ,\n     newPassword :  {newPassword} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "POST /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authusersuser", 
            "text": "Function: Creates new user  Return:  {\n     userName :  {userName} ,\n     password :  {password} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "PUT /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authusersuser", 
            "text": "Function: Deletes the specified user", 
            "title": "DELETE /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusers", 
            "text": "Function: Gets the list of users  Return:  {\n     users : [ {\n        userName :  {username1} ,\n        roles : [  {role1} , \u2026 ],\n        permissions : [  {permission1} , \u2026 ]\n    }\n}", 
            "title": "GET /ws/v2/auth/users"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2login", 
            "text": "Function: Login\nPayload:  {\n     userName :  {userName} ,\n     password :  {password} \n}  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/login"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2logout", 
            "text": "Function: Log out the current user  Return:  {\n}", 
            "title": "POST /ws/v2/logout"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configauth", 
            "text": "Function: Configure authentication. \nThe request specifies the type of authentication to setup such as password, kerberos, ldap etc and the configuration \nparameters for the authentication. The web service sets up the appropriate configuration files for the authentication \nas described in authentication section of  dtgateway_security  document. Gateway needs to be \nrestarted for the new authentication to take effect. This can be done by making the gateway restart web service request.  payload:  {\n     type :  {authenticationType} ,\n     configuration :{ }\n}  Returns:  {\n}  The web request  GET /ws/v2/config/auth  returns payload body that was sent in  PUT  request as the response verbatim.", 
            "title": "PUT /ws/v2/config/auth"
        }, 
        {
            "location": "/dtgateway_api/#password", 
            "text": "Function: Configure Password authentication   {\n     type :  password ,\n     configuration :{ }\n};  Returns:  {\n}  The web request  GET /ws/v2/config/auth  returns payload body that was sent in  PUT  request as the response verbatim.   {\n     type :  password ,\n     configuration :{ }\n}", 
            "title": "Password"
        }, 
        {
            "location": "/dtgateway_api/#kerberos-with-no-group-mapping", 
            "text": "Function: Configure Kerberos authentication with no group mapping\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nTwo of the properties are mandatory, they are \"kerberosPrincipal\"   \"kerberosKeytab\". A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should be \nspecified as \"false\".  {\n     type : kerberos ,\n        configuration :{  \n          groupSupport : false ,\n          kerberosPrincipal : {kerberosPrincipal} ,\n          kerberosKeytab : {Keytab} ,\n          tokenValidity : {tokenValidity} ,\n          cookieDomain : {cookieDomain} ,\n          cookiePath : {cookiePath} ,      \n          signatureSecret : {signatureSecret} \n         }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Kerberos with no group mapping"
        }, 
        {
            "location": "/dtgateway_api/#kerberos-with-group-mapping", 
            "text": "Function: Configure Kerberos authentication with group mapping \nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nTwo of these properties are mandatory, they are \"kerberosPrincipal\"   \"kerberosKeytab\". A \"groupSupport\" property \nspecifies whether group mapping should be enabled. Group mapping allows Kerberos groups to be mapped to roles. It should \nbe specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that \ncontains the mapping from kerberos groups to roles.  \n{  \n    type :  kerberos ,\n    configuration : {  \n         groupSupport :  true ,\n         kerberosPrincipal :  {kerberosPrincipal} ,\n         kerberosKeytab :  {Keytab} ,\n         tokenValidity :  {Validity} ,\n         cookieDomain  :  {cookieDomain} ,\n         cookiePath :  {cookiePath} \n         signatureSecret :  {signatureSecret} \n        }\n     groupMapping : [\n        { \n              group :  users ,\n              roles : [ developers ,  admins ,  qa ] \n        },\n        {  \n              group :  ops ,\n              roles : [ operators ]\n        }\n     ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Kerberos with group mapping"
        }, 
        {
            "location": "/dtgateway_api/#ldap", 
            "text": "Configure LDAP authentication. There are different configurations possible based on how the LDAP server is configured.", 
            "title": "LDAP"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-allowed-and-no-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication with anonymous search available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows LDAP groups\nto be mapped to roles. It should be specified as \"false\".  \n{   type :  ldap ,\n    configuration : {  \n        groupSupport :  false ,\n        Server :  {Server} ,\n        Port : {port}  +\n        userBaseDn :  {usserBaseDn} ,\n        userIdAttribute :  {userIdAttribute} \n   }\n}  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-and-no-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication when anonymous search is not available on LDAP server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".  \n{  \n     type :  ldap ,\n     configuration : {  \n         groupSupport :  false ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} , \n         userObjectClass :  {userObjectClass} \n    }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-but-group-mapping-needed", 
            "text": "Function: Configure LDAP authentication when anonymous search is not available on LDAP server but group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from LDAP groups to roles.  { \n     type :  ldap ,\n     configuration : {  \n         groupSupport :  true ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} , \n         roleBaseDn :  {roleBaseDn} ,\n         userRdnAttribute : {userRdnAttribute} , \n         roleNameAttribute :  {roleNameAttribute} , \n         roleObjectClass :  {roleObjectClass} , \n         userObjectClass :  {userObjectClass} \n    },\n     groupMapping : [ \n        { \n             group :  users ,\n             roles :[ developers ] \n        },\n        { \n             group :  ops ,\n             roles : [ operators ]\n        }\n    ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed but group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#active-directory", 
            "text": "Configure Active Directory authentication. There are different configurations possible based on how the Active Directory \nserver is configured.", 
            "title": "Active Directory"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-allowed-and-no-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication with anonymous search available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\" property is mandatory. Also, at least one of userBaseDn, authIdentity or userSearchFilter properties must be \nspecified. A \"groupSupport\" property specifies whether group mapping should be enabled. Group mapping allows \nActive Directory groups to be mapped to roles. It should be specified as \"false\".  \n{  \n    type :  ad ,\n    configuration : {  \n        groupSupport  :  false ,\n        Server :  {server} ,\n        Port : {port},\n        userSearchFilter :  {userSearchFilter} ,\n        userBaseDn :  {userBaseDn} ,\n        userIdAttribute :  {userIdAttribute} ,\n        userDomain  :  {userDomain}     \n   }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-and-no-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication when anonymous search is not available on Active Directory server and no group mapping is needed\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"false\".  \n{  \n    type :  ad ,\n    configuration : {  \n    groupSupport :  false ,\n         Server :  {server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} ,\n         userObjectClass :  {userObjectClass} \n    }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed and no group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#anonymous-search-not-allowed-but-group-mapping-needed_1", 
            "text": "Function: Configure Active Directory authentication when anonymous search is not available on Active Directory server but group mapping is needed\nThe configuration comprises different properties as described in the  dtgateway_security  document. \nThe \"server\", \"userBaseDn\", \"bindDn\"   \"bindPassword\" properties are mandatory. A \"groupSupport\" property specifies \nwhether group mapping should be enabled. Group mapping allows LDAP groups to be mapped to roles. It should be specified \nas \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be specified that contains \nthe mapping from Active Directory groups to roles.  { \n     type :  ad ,\n     configuration : {  \n         groupSupport :  true ,\n         Server :  {Server} ,\n         Port : {port},\n         userBaseDn :  {userBaseDn} ,\n         userIdAttribute :  {userIdAttribute} ,\n         bindDn :  {bindDn} ,\n         bindPassword :  {bindPassword} ,\n         roleBaseDn :  {roleBaseDn} ,\n         userRdnAttribute :  {userRdnAttribute} ,\n         roleNameAttribute :  roleNameAttribute ,\n         roleObjectClass :  roleObjectClass ,\n         userObjectClass :  {userObjectClass} ,\n     },\n      groupMapping : [ ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "Anonymous search not allowed but group mapping needed"
        }, 
        {
            "location": "/dtgateway_api/#pam", 
            "text": "Configure PAM or Pluggable Authentication Mechanism. PAM is the de-facto authentication available on Linux systems.", 
            "title": "PAM"
        }, 
        {
            "location": "/dtgateway_api/#pam-with-no-group-mapping", 
            "text": "Function: Configure PAM authentication with no group mapping\nThe configuration comprises of different properties as described in the  dtgateway_security  document. \nIn PAM, service name is mandatory   there is no configuration. A \"groupSupport\" property specifies whether group mapping \nshould be enabled. Group mapping allows PAM groups to be mapped to roles. It should be specified as \"false\".  \n{  \n    type : pam ,\n    configuration :{  \n       groupSupport : false ,\n       serviceName : {serviceName} \n   }\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PAM with no group mapping"
        }, 
        {
            "location": "/dtgateway_api/#pam-with-group-mapping", 
            "text": "Function: Configure PAM authentication with group mapping \nThe configuration comprises of different properties as described in the  dtgateway_security  document.\nIn PAM, service name is mandatory   there is no configuration. Group mapping allows PAM groups to be mapped to roles. It \nshould be specified as \"true\". When group mapping is enabled an additional \"groupMapping\" configuration should be \nspecified that contains the mapping from PAM groups to roles.  \n{  \n    type : pam ,\n    configuration :{  \n       groupSupport : true ,\n       serviceName : {serviceName} \n   },\n\n    groupMapping : [ \n       { \n           group :  users ,\n           roles :[ developers ] \n       },\n       { \n           group :  ops ,\n           roles : [ operators ]\n       }\n   ]  \n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PAM with group mapping"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configgroupmapping", 
            "text": "Function: Specify group to role mapping\nSet or update mapping from groups of configured authentication mechanism to RTS roles.  \n{\n    groupMapping  : [\n        {\n            group  :  users ,\n            role  : [ developers ,  admins ,  qa ,  interns ]\n        },\n        {\n            group :  ops ,\n            role  : [ operators ]\n        }\n   ]\n};  Returns:  {\n}  The GET response will be same as the json sent in the request for  PUT  .", 
            "title": "PUT /ws/v2/config/groupMapping"
        }, 
        {
            "location": "/dtgateway_api/#publisher-subscriber-websocket-protocol", 
            "text": "dtGateway provides a light-weight pubsub websocket service.\nThe URL of dtGateway's pubsub websocket service is:  ws://{dtGateway-host-port}/pubsub .\nFor example:  ws://localhost:9090/pubsub", 
            "title": "Publisher-Subscriber WebSocket Protocol"
        }, 
        {
            "location": "/dtgateway_api/#input", 
            "text": "", 
            "title": "Input"
        }, 
        {
            "location": "/dtgateway_api/#publishing", 
            "text": "{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Publishing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing", 
            "text": "{\"type\":\"subscribe\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing"
        }, 
        {
            "location": "/dtgateway_api/#unsubscribing", 
            "text": "{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}", 
            "title": "Unsubscribing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing-to-the-number-of-subscribers-of-a-topic", 
            "text": "{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing to the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#unsubscribing-from-the-number-of-subscribers-of-a-topic", 
            "text": "{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Unsubscribing from the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#output", 
            "text": "", 
            "title": "Output"
        }, 
        {
            "location": "/dtgateway_api/#normal-published-data", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Normal Published Data"
        }, 
        {
            "location": "/dtgateway_api/#number-of-subscribers", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}", 
            "title": "Number of Subscribers:"
        }, 
        {
            "location": "/dtgateway_api/#auto-publish-topics", 
            "text": "data that gets published every one second:   applications  - list of streaming applications running in the cluster  applications.[appid]  - information about a particular application  applications.[appid].containers  - information about containers of a particular application  applications.[appid].physicalOperators  - information about operators of a particular application  applications.[appid].logicalOperators  - information about logical operators of a particular application  applications.[appid].events  - events from the AM of a particularapplication   data that gets published every five seconds:   cluster.metrics  - metrics of the cluster", 
            "title": "Auto publish topics"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/", 
            "text": "How to import and launch an app-template\n\n\nThis document has step-by-step guide on how to import and launch app-template.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n\n\n\n\n\nPage listing the verticals for different use-cases is displayed. Navigating to the desired vertical shows available suites of applications for that vertical.\n\n\n\n\n\n\nNavigating to the desired suite shows available applications in the suite.\n\n\n\n\n\n\n\n\nClick on \nimport\n link for the desired application.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n   Detailed information about the application package like version, last modified time, and short description is available on this page. Applications in this application package will be displayed in the table. Click on the \nlaunch\n button for the desired application. In the confirmation modal, click the \nConfigure\n button.\n\n\n\n\n\n\nLaunch page for the application configuration is displayed.  \n\n    Fill in the required properties for the application configuration. For more details, please refer to properties section in the documentation for the respective app.\n\n\n\n\n\n\nAdvanced properties for the application have preset default values. Thus, changing these values is optional. But, if required you can change these properties for fine tuning the application.\n\n   Under optional properties section, click on down arrow button and then click on \ndefault properties\n.\n\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button on top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nphysical\n tab shows the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nTo look at the details of specific container and to monitor logs, stout, stderr etc. container details page can be reached from the container link in the physical tab.", 
            "title": "Import and Launch App-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/#how-to-import-and-launch-an-app-template", 
            "text": "This document has step-by-step guide on how to import and launch app-template.", 
            "title": "How to import and launch an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/import-launch/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n       Page listing the verticals for different use-cases is displayed. Navigating to the desired vertical shows available suites of applications for that vertical.    Navigating to the desired suite shows available applications in the suite.     Click on  import  link for the desired application.    Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n    \n   Detailed information about the application package like version, last modified time, and short description is available on this page. Applications in this application package will be displayed in the table. Click on the  launch  button for the desired application. In the confirmation modal, click the  Configure  button.    Launch page for the application configuration is displayed.   \n    Fill in the required properties for the application configuration. For more details, please refer to properties section in the documentation for the respective app.    Advanced properties for the application have preset default values. Thus, changing these values is optional. But, if required you can change these properties for fine tuning the application. \n   Under optional properties section, click on down arrow button and then click on  default properties .    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button on top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       physical  tab shows the status of physical instances of the operator, containers etc.\n       To look at the details of specific container and to monitor logs, stout, stderr etc. container details page can be reached from the container link in the physical tab.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/", 
            "text": "How to customize an app-template\n\n\nThis document describes how to customize an app-template.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0.5 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.6.0)+\n\n\n\n\n\n\n\n\nUse following command to clone the repository:\n\n\ngit clone git@github.com:DataTorrent/moodI.git\n\n\n\n\n\n\nChange directory to containing app-template you wish to customize:\n\n\ncd moodI/app-templates/kafka-to-hdfs-filter-transform/\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n    \n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.\n\n\n\n\n\n\nNote on customization\n\n\n\n\n\n\nApplication.java\n defines the Application class which defines the application pipeline. Application is represented as directed acyclic graph (DAG). Vertices of the graph represents operators or computational units. Edges represents data flow or streams.\n\n\n\n\n\n\nApplication\n class implements \nStreamingApplication\n interface by defining implementation for \npopulateDAG\n method .\n\n\n\n\n\n\npopulateDAG\n methods receives an argument with an instance of DAG object. App developers can add operators, streams to this dag object to define the pipeline based on the need.\n\n\n\n\n\n\nSecond argument to \npopulateDAG\n method is an instance of Configuration object.\nAll the properties specified in \nproperties.xml\n will be available through this configuration object.\n\n\n\n\n\n\nMost of the commonly used functionality, connectors are available in \nmoodI\n, \nmalhar\n operator library. If required functionality is not available in the operator library; one can implement own operators for custom computations.\n\n\n\n\n\n\nAdd the operators to the DAG using \ndag.addOperator()\n API and connect the operators with upstream, downstream operators using \ndag.addStream()\n API.\n\n\nFor example, suppose one needs to modify \nkafka-to-hdfs-filter-transform\n such that count of tuples discarded by the filter operator should be displayed on the console. This can be achived by adding following code to \npopulateDAG\n method in \nApplication.java\n:\n\n\n\n\n\n\n\nCounter counter = dag.addOperator(\ncounter\n, Counter.class);\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, ConsoleOutputOperator.class);\ndag.addStream(\nFilteredOut\n, filterOperator.falsePort, counter.input);\ndag.addStream(\nFilteredOutToConsole\n, counter.output, console.input);\n\n\n\n\nFollow the \ncustomization steps\n for applying your changes.", 
            "title": "Customizing an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#how-to-customize-an-app-template", 
            "text": "This document describes how to customize an app-template.", 
            "title": "How to customize an app-template"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0.5 +  git  : 1.7 +  Hadoop  (Apache-2.6.0)+     Use following command to clone the repository:  git clone git@github.com:DataTorrent/moodI.git    Change directory to containing app-template you wish to customize:  cd moodI/app-templates/kafka-to-hdfs-filter-transform/    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.\n        Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/0.10.0/common/customize/#note-on-customization", 
            "text": "Application.java  defines the Application class which defines the application pipeline. Application is represented as directed acyclic graph (DAG). Vertices of the graph represents operators or computational units. Edges represents data flow or streams.    Application  class implements  StreamingApplication  interface by defining implementation for  populateDAG  method .    populateDAG  methods receives an argument with an instance of DAG object. App developers can add operators, streams to this dag object to define the pipeline based on the need.    Second argument to  populateDAG  method is an instance of Configuration object.\nAll the properties specified in  properties.xml  will be available through this configuration object.    Most of the commonly used functionality, connectors are available in  moodI ,  malhar  operator library. If required functionality is not available in the operator library; one can implement own operators for custom computations.    Add the operators to the DAG using  dag.addOperator()  API and connect the operators with upstream, downstream operators using  dag.addStream()  API.  For example, suppose one needs to modify  kafka-to-hdfs-filter-transform  such that count of tuples discarded by the filter operator should be displayed on the console. This can be achived by adding following code to  populateDAG  method in  Application.java :    \nCounter counter = dag.addOperator( counter , Counter.class);\nConsoleOutputOperator console = dag.addOperator( console , ConsoleOutputOperator.class);\ndag.addStream( FilteredOut , filterOperator.falsePort, counter.input);\ndag.addStream( FilteredOutToConsole , counter.output, console.input);  Follow the  customization steps  for applying your changes.", 
            "title": "Note on customization"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/", 
            "text": "Database to Database Sync Application\n\n\nSummary\n\n\nThis application demonstrates continuous archival of big data from database tables.\nApplication connects to the source database table over JDBC connection and continuously polls for new data. This database rows from source table are archieved in the destination database in scalable, fault-tolerent fashion. Application also allows to do custom transformations on the rows before they are copied to the destination.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nJdbc Input Database Url\n\n\nString\n\n\njdbc:postgresql://node1 .corp1.com:5432/testdb\n\n\nConnection URL for the source database.\n\n\n\n\n\n\nJdbc Input Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for source database\n\n\n\n\n\n\nJdbc Input Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for source database\n\n\n\n\n\n\nJdbc Input Table Name\n\n\nString\n\n\ntest_event_input_table\n\n\nSource table name\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:postgresql://dest1 .corp1.com:5432/testdb\n\n\nConnection URL for the destination database.\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for destination database\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for destination database\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\ntest_event_output_table\n\n\nDestination table name\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBatch Size For Jdbc Input\n\n\n300\n\n\nint\n\n\n1000\n\n\nNo of rows to fetch in single query for input.\n\n\n\n\n\n\nColumns Expressions\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nString\n\n\ncolumn1_name, column2_name, column3_name\n\n\nComma separated list of columns to select from the source table.\n\n\n\n\n\n\nJdbc Input Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\norg.postgresql .Driver\n\n\nFQCN for jdbc driver class for source database\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\norg.postgresql .Driver\n\n\nFQCN for jdbc driver class for destination database\n\n\n\n\n\n\nNumber Of Partition Required\n\n\n4\n\n\nint\n\n\n2\n\n\nNumber of partitions for reading data from JDBC source database. There will one additional partition used for polling the databse.\n\n\n\n\n\n\nTuple Class Name For Jdbc Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input database row\n\n\n\n\n\n\nTuple Class Name For Jdbc Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input database row.\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nUnique Key For Range Queries\n\n\nACCOUNT_NO\n\n\nString\n\n\nid\n\n\nColumn to be used for partitioning.\n\n\n\n\n\n\nWhere Condition\n\n\n\n\nString\n\n\n\n\nWhere condition used for filtering rows from source table. Empty string indicates select all rows.\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\nApplication is configured for pre-defined schema mentioned \nhere\n. For using this application for custom schema objects; changes will be needed in \nApplication.java\n. One can define field info mapping for the input fields in \naddInputFieldInfos()\n. Simillarly,\nfield info mapping for the output fields can be defined in  \naddOutputFieldInfos()\n.\n\n\n\n\n\n\nPojoEvent\n is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath. \nConfiguration package\n can be used for this purpose.\n\n\n\n\n\n\nThis application has been tested with PostgreSQL database. But, this can be used on other databases like MySQL, Oracle etc provided that JDBC client jar for the same is available in the classpath. Please specify \nJdbc Input Database Driver\n, \nJdbc Output Database Driver\n properties to point to respective driver class for the database you are connecting to.", 
            "title": "Database-to-database-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#database-to-database-sync-application", 
            "text": "", 
            "title": "Database to Database Sync Application"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#summary", 
            "text": "This application demonstrates continuous archival of big data from database tables.\nApplication connects to the source database table over JDBC connection and continuously polls for new data. This database rows from source table are archieved in the destination database in scalable, fault-tolerent fashion. Application also allows to do custom transformations on the rows before they are copied to the destination.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Jdbc Input Database Url  String  jdbc:postgresql://node1 .corp1.com:5432/testdb  Connection URL for the source database.    Jdbc Input Store Password  String  postgres  Password for source database    Jdbc Input Store Username  String  postgres  Username for source database    Jdbc Input Table Name  String  test_event_input_table  Source table name    Jdbc Output Database Url  String  jdbc:postgresql://dest1 .corp1.com:5432/testdb  Connection URL for the destination database.    Jdbc Output Store Password  String  postgres  Password for destination database    Jdbc Output Store Username  String  postgres  Username for destination database    Jdbc Output Table Name  String  test_event_output_table  Destination table name", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Batch Size For Jdbc Input  300  int  1000  No of rows to fetch in single query for input.    Columns Expressions  ACCOUNT_NO, NAME, AMOUNT  String  column1_name, column2_name, column3_name  Comma separated list of columns to select from the source table.    Jdbc Input Database Driver  org.postgresql .Driver  String  org.postgresql .Driver  FQCN for jdbc driver class for source database    Jdbc Output Database Driver  org.postgresql .Driver  String  org.postgresql .Driver  FQCN for jdbc driver class for destination database    Number Of Partition Required  4  int  2  Number of partitions for reading data from JDBC source database. There will one additional partition used for polling the databse.    Tuple Class Name For Jdbc Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input database row    Tuple Class Name For Jdbc Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input database row.    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing output for transform operator (if included in the DAG. Not included by default).    Unique Key For Range Queries  ACCOUNT_NO  String  id  Column to be used for partitioning.    Where Condition   String   Where condition used for filtering rows from source table. Empty string indicates select all rows.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/database-to-database-sync/#notes", 
            "text": "Application is configured for pre-defined schema mentioned  here . For using this application for custom schema objects; changes will be needed in  Application.java . One can define field info mapping for the input fields in  addInputFieldInfos() . Simillarly,\nfield info mapping for the output fields can be defined in   addOutputFieldInfos() .    PojoEvent  is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath.  Configuration package  can be used for this purpose.    This application has been tested with PostgreSQL database. But, this can be used on other databases like MySQL, Oracle etc provided that JDBC client jar for the same is available in the classpath. Please specify  Jdbc Input Database Driver ,  Jdbc Output Database Driver  properties to point to respective driver class for the database you are connecting to.", 
            "title": "Notes"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/", 
            "text": "HDFS line copy Application\n\n\nSummary\n\n\nThis application demonstrates data preparation pipeline which reads lines from files on source HDFS. It performs customized filtering, transformations on the line and writes them into destination HDFS.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\nOutput File Name\n\n\nString\n\n\noutput.txt\n\n\nName of the output file. This name will be appended with suffix for each part.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{      \"separator\": \"\n\",      \"quoteChar\": \"\\\"\",      \"lineDelimiter\":\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }\n\n\nString\n\n\nJSON string defining schema for CSV formatter\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{      \"separator\": \"\n\",      \"quoteChar\": \"\\\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }\n\n\nString\n\n\nJSON string defining schema for CSV parser\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.\n\n\n\n\n\n\nNumber Of Readers For Partitioning\n\n\n2\n\n\nint\n\n\nBlocks reader operator would be partioned into these many partitions.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be emitted by CSV Parser\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be consumed by CSV formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be consumed by Transform operator\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent.apps.PojoEvent\n\n\nString\n\n\nFQCN for the tuple object to be emitted by Transform operator\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\nApplication is pre-configured for pre-defined schema mentioned \nhere\n. For using this application for custom schema objects; changes will be needed in the configuration.\n\n\n\n\n\n\nPojoEvent\n is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath. \nConfiguration package\n can be used for this purpose.", 
            "title": "HDFS-line-copy"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#hdfs-line-copy-application", 
            "text": "", 
            "title": "HDFS line copy Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#summary", 
            "text": "This application demonstrates data preparation pipeline which reads lines from files on source HDFS. It performs customized filtering, transformations on the line and writes them into destination HDFS.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.    Output File Name  String  output.txt  Name of the output file. This name will be appended with suffix for each part.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Csv Formatter Schema  {      \"separator\": \" \",      \"quoteChar\": \"\\\"\",      \"lineDelimiter\":\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }  String  JSON string defining schema for CSV formatter    Csv Parser Schema  {      \"separator\": \" \",      \"quoteChar\": \"\\\"\",      \"fields\": [      {      \"name\": \"accountNumber\",      \"type\": \"Integer\"      },      {      \"name\": \"name\",      \"type\": \"String\"      },      {      \"name\": \"amount\",      \"type\": \"Integer\"      }      ]      }  String  JSON string defining schema for CSV parser    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.    Number Of Readers For Partitioning  2  int  Blocks reader operator would be partioned into these many partitions.    Tuple Class Name For Csv Parser Output  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be emitted by CSV Parser    Tuple Class Name For Formatter Input  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be consumed by CSV formatter    Tuple Class Name For Transform Input  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be consumed by Transform operator    Tuple Class Name For Transform Output  com.datatorrent.apps.PojoEvent  String  FQCN for the tuple object to be emitted by Transform operator", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-line-copy/#notes", 
            "text": "Application is pre-configured for pre-defined schema mentioned  here . For using this application for custom schema objects; changes will be needed in the configuration.    PojoEvent  is the POJO (plain old java objects used for representing record present in the database row). One can define custom class to represent custom schema and include it in the classpath.  Configuration package  can be used for this purpose.", 
            "title": "Notes"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/", 
            "text": "HDFS part file copy Application\n\n\nSummary\n\n\nThis application demonstrates continuous big data archival from HDFS.\nIt ingests files as blocks for backup on remote HDFS cluster.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nMinimum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMinimum no of partitions for Block Reader operator.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.", 
            "title": "HDFS-part-file-copy"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#hdfs-part-file-copy-application", 
            "text": "", 
            "title": "HDFS part file copy Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#summary", 
            "text": "This application demonstrates continuous big data archival from HDFS.\nIt ingests files as blocks for backup on remote HDFS cluster.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-part-file-copy/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Maximum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Minimum Readers For Dynamic Partitioning  1  int  Minimum no of partitions for Block Reader operator.    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/", 
            "text": "HDFS to HDFS filter transform Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data preparation while reading data from a source Hadoop cluster. This data is considered to be in delimited format, which is further filtered, transformed based on configurable properties. Finally, this prepared data is written back in a desired format to the destination Hadoop cluster. This could be easily utilized and extended by any developer to create a fast, fault tolerant and scalable Big Data Application to serve business with rich data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/appuser/output\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBlock Size For Hdfs Splitter\n\n\n1048576 (1MB)\n\n\nlong\n\n\nNo of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nMinimum Readers For Dynamic Partitioning\n\n\n1\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.", 
            "title": "HDFS-to-HDFS-filter-transform"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#hdfs-to-hdfs-filter-transform-application", 
            "text": "", 
            "title": "HDFS to HDFS filter transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#summary", 
            "text": "This application template demonstrates continuous big data preparation while reading data from a source Hadoop cluster. This data is considered to be in delimited format, which is further filtered, transformed based on configurable properties. Finally, this prepared data is written back in a desired format to the destination Hadoop cluster. This could be easily utilized and extended by any developer to create a fast, fault tolerant and scalable Big Data Application to serve business with rich data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Input Directory Or File Path  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path  String  /user/appuser/output  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/hdfs-to-hdfs-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Block Size For Hdfs Splitter  1048576 (1MB)  long  No of bytes record reader operator would consider at a time for splitting records. Record reader might add latencies for higher block sizes. Suggested value is 1-10 MB    Maximum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Minimum Readers For Dynamic Partitioning  1  int  Maximum no of partitions for Block Reader operator.    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/", 
            "text": "Kafka to Cassandra Filter Transform Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data preparation while reading data messages from a configured Kafka topic. This data is considered to be in JSON format, which is further filtered, transformed based on configurable properties. Finally, this prepared message is written to a Cassandra  .\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nCassandra Node Name\n\n\nString\n\n\nlocalhost\n\n\nHost name for cassandra server\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKey Space On Cassandra\n\n\nString\n\n\ntestdb\n\n\nKeyspace to be used for output\n\n\n\n\n\n\nTable Name\n\n\nString\n\n\ntest_input\n\n\nName of the table on cassandra store\n\n\n\n\n\n\nJson Parser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.", 
            "title": "Kafka-to-Cassandra-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#kafka-to-cassandra-filter-transform-application", 
            "text": "", 
            "title": "Kafka to Cassandra Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#summary", 
            "text": "This application template demonstrates continuous big data preparation while reading data messages from a configured Kafka topic. This data is considered to be in JSON format, which is further filtered, transformed based on configurable properties. Finally, this prepared message is written to a Cassandra  .", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Cassandra Node Name  String  localhost  Host name for cassandra server    Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Key Space On Cassandra  String  testdb  Keyspace to be used for output    Table Name  String  test_input  Name of the table on cassandra store    Json Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-cassandra-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/", 
            "text": "Kafka to Database Sync Application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kafka topic. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nCsv Parser Schema\n\n\nString\n\n\n{  \"separator\": \"\n\",  \"quoteChar\": \"\\\"\",   \"fields\": [{\"name\": \"accountNumber\",\"type\": \"Integer\"} ,{\"name\": \"name\",\"type\": \"String\"},{\"name\": \"amount\",\"type\": \"Integer\"}]}\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:postgresql://dest1 .corp1.com:5432/testdb\n\n\nConnection URL for the destination database.\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npostgres\n\n\nPassword for destination database\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\npostgres\n\n\nUsername for destination database\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\ntest_event_output_table\n\n\nDestination table name\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\norg.postgresql .Driver\n\n\nString\n\n\n\n\nFQCN for jdbc driver class for destination database\n\n\n\n\n\n\nTransform Expression Info\n\n\n{\"accountNumber\":\"{$.accountNumber}\", \"name\":\"{$.name}.toUpperCase()\", \"amount\":\"{$.amount}\"}\n\n\nString\n\n\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\n{\"accountNumber\":\"INTEGER\", \"name\":\"STRING\", \"amount\":\"INTEGER\"}\n\n\nString\n\n\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.", 
            "title": "Kafka-to-Database-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#kafka-to-database-sync-application", 
            "text": "", 
            "title": "Kafka to Database Sync Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kafka topic. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Csv Parser Schema  String  {  \"separator\": \" \",  \"quoteChar\": \"\\\"\",   \"fields\": [{\"name\": \"accountNumber\",\"type\": \"Integer\"} ,{\"name\": \"name\",\"type\": \"String\"},{\"name\": \"amount\",\"type\": \"Integer\"}]}  JSON representing schema to be used by CSV parser    Jdbc Output Database Url  String  jdbc:postgresql://dest1 .corp1.com:5432/testdb  Connection URL for the destination database.    Jdbc Output Store Password  String  postgres  Password for destination database    Jdbc Output Store Username  String  postgres  Username for destination database    Jdbc Output Table Name  String  test_event_output_table  Destination table name    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-database-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.    Jdbc Output Database Driver  org.postgresql .Driver  String   FQCN for jdbc driver class for destination database    Transform Expression Info  {\"accountNumber\":\"{$.accountNumber}\", \"name\":\"{$.name}.toUpperCase()\", \"amount\":\"{$.amount}\"}  String   JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  {\"accountNumber\":\"INTEGER\", \"name\":\"STRING\", \"amount\":\"INTEGER\"}  String   JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/", 
            "text": "Kafka to HDFS Filter Transform Application\n\n\nSummary\n\n\nThis application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic names on Kakfa\n\n\n\n\n\n\nOutput Directory Path\n\n\nString\n\n\n/user/dtuser/output/dir1\nhdfs://node1.corp1.com/user/dtuser/output\n\n\nHDFS path (absolute or relative)\n\n\n\n\n\n\nParser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.", 
            "title": "Kafka-to-HDFS-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#kafka-to-hdfs-filter-transform-application", 
            "text": "", 
            "title": "Kafka to HDFS Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#summary", 
            "text": "This application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and HDFS as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Topic Name  String  transactions  Topic names on Kakfa    Output Directory Path  String  /user/dtuser/output/dir1 hdfs://node1.corp1.com/user/dtuser/output  HDFS path (absolute or relative)    Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-hdfs-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/", 
            "text": "Kafka to Kafka Filter Transform Application\n\n\nSummary\n\n\nThis application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and Kafka as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFilter Condition For Tuples\n\n\nString\n\n\n({$}.getPremium() \n= 50000)\n\n\nQuasi java expression\n\n\n\n\n\n\nKafka Output Topic Name\n\n\nString\n\n\noutput_transactions\n\n\nOutput topic name on Kakfa\n\n\n\n\n\n\nJson Parser Field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}\n\n\nJSON map with key indicating input field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\nKafka Broker List\n\n\nString\n\n\nlocalhost:9092\nnode1.corp1.com:9092, node2.corp1.com:9092\n\n\nComma seperated list of kafka brokers\n\n\n\n\n\n\nKafka Input Topic Name\n\n\nString\n\n\ntransactions\n\n\nTopic name on Kakfa\n\n\n\n\n\n\nKafka Producer Properties\n\n\nString\n\n\nserializer.class =kafka.serializer. StringEncoder ,producer.type=async ,metadata.broker.list=localhost:9092\n\n\nProducer properties for Kafka output\n\n\n\n\n\n\nTransform Expression Info\n\n\nString\n\n\n{\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}\n\n\nJSON map with key indicating output field. Value indicating expression to be used for calculating its value\n\n\n\n\n\n\nTransform Output field Info\n\n\nString\n\n\n{\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}\n\n\nJSON map with key indicating output field. Value indicating data \ntype\n for the field.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nInitial Offset Of Topic For Kafka Consumer\n\n\nLATEST\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\nWhether to read from beginning or read from current offset.\n\n\n\n\n\n\nNumber Of Partitions For Kafka Consumer\n\n\ncom.datatorrent.common .partitioner.StatelessPartitioner:1\n\n\nString\n\n\ncom.datatorrent.common .partitioner.StatelessPartitioner:16\n\n\nParallel instances for Kafka operator", 
            "title": "Kafka-to-Kafka-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#kafka-to-kafka-filter-transform-application", 
            "text": "", 
            "title": "Kafka to Kafka Filter Transform Application"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#summary", 
            "text": "This application demonstrates continuous ingestion of streaming data source into the big data lake. Application uses Kafka as a streaming source and Kafka as big data lake destination. Depending on the context, data in kafka could be coming from logs of different events, sensor data, call details records, transactions etc.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Filter Condition For Tuples  String  ({$}.getPremium()  = 50000)  Quasi java expression    Kafka Output Topic Name  String  output_transactions  Output topic name on Kakfa    Json Parser Field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\",  \"premium\":\"LONG\"}  JSON map with key indicating input field. Value indicating data  type  for the field.    Kafka Broker List  String  localhost:9092 node1.corp1.com:9092, node2.corp1.com:9092  Comma seperated list of kafka brokers    Kafka Input Topic Name  String  transactions  Topic name on Kakfa    Kafka Producer Properties  String  serializer.class =kafka.serializer. StringEncoder ,producer.type=async ,metadata.broker.list=localhost:9092  Producer properties for Kafka output    Transform Expression Info  String  {\"customerName\":\"{$} .getCustomerName() .toUpperCase()\"}  JSON map with key indicating output field. Value indicating expression to be used for calculating its value    Transform Output field Info  String  {\"policyNumber\":\"LONG\", \"customerName\":\"STRING\", \"premium\":\"LONG\"}  JSON map with key indicating output field. Value indicating data  type  for the field.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kafka-to-kafka-filter-transform/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Initial Offset Of Topic For Kafka Consumer  LATEST  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST  Whether to read from beginning or read from current offset.    Number Of Partitions For Kafka Consumer  com.datatorrent.common .partitioner.StatelessPartitioner:1  String  com.datatorrent.common .partitioner.StatelessPartitioner:16  Parallel instances for Kafka operator", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/", 
            "text": "Kinesis to Redshift application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nKinesis Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nKinesis Endpoint\n\n\nString\n\n\nkinesis.us-east-1.amazonaws.com\n\n\nAWS service end-point for Kinesis\n\n\n\n\n\n\nKinesis Secret Key\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret Key for accessing Kinesis.\n\n\n\n\n\n\nKinesis Stream Name\n\n\nString\n\n\nevents\n\n\nStream name to read data from Kinesis\n\n\n\n\n\n\nRedshift Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for redshift\n\n\n\n\n\n\nRedshift Secret Key\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret Key for accessing Redshift.\n\n\n\n\n\n\nRegion Of Input File\n\n\nString\n\n\nap-southeast-1\n\n\nRegion for the intermediate S3 storage.\n\n\n\n\n\n\nJdbc Output Database Driver\n\n\nString\n\n\ncom.amazon.redshift .jdbc4.Driver\n\n\nFQCN for Redshift JDBC output database driver.\n\n\n\n\n\n\nJdbc Output Database Url\n\n\nString\n\n\njdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev\n\n\nConnection URL for redshift\n\n\n\n\n\n\nJdbc Output Store Password\n\n\nString\n\n\npassword\n\n\nPassword for redshift store instance.\n\n\n\n\n\n\nJdbc Output Store Username\n\n\nString\n\n\nusername\n\n\nUsername for redshift store instance.\n\n\n\n\n\n\nJdbc Output Table Name\n\n\nString\n\n\nevents\n\n\ntable to be used from redshift store instance.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nEmr Cluster Id\n\n\nEMR-CLUSTERID\n\n\nString\n\n\n\n\nCluster id for EMR.\n\n\n\n\n\n\nRedshift Delimiter\n\n\n\n\nString\n\n\n\n\nDelimiter to be used for redshift output\n\n\n\n\n\n\nRedshift Reader Mode\n\n\nREAD_FROM_S3\n\n\nString\n\n\nREAD_FROM_S3\nREAD_FROM_EMR\n\n\nChoose preferred intermediate store. S3 or HDFS.\n\n\n\n\n\n\nS3Bucket Name\n\n\nS3_BUCKET_NAME\n\n\nString\n\n\n\n\nBucket name for intermediate storage\n\n\n\n\n\n\nS3Directory Name\n\n\nS3_DIRECTORY_NAME\n\n\nString\n\n\n\n\nDirectory name for intermediate storage\n\n\n\n\n\n\nBatch Size For Jdbc Output\n\n\n500\n\n\nString\n\n\n\n\nNumber rows to push into redshift in single query\n\n\n\n\n\n\nMax Length Of Rolling File\n\n\n1048576 (1 MB)\n\n\nString\n\n\n\n\nMaximums size in bytes for files on intermediate storage.\n\n\n\n\n\n\nTuple Class Name For Jdbc Output\n\n\ncom.datatorrent .apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent .apps.PojoEvent\n\n\nFQCN for tuples written on redshift", 
            "title": "Kinesis-to-Redshift"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#kinesis-to-redshift-application", 
            "text": "", 
            "title": "Kinesis to Redshift application"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Kinesis Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Kinesis Endpoint  String  kinesis.us-east-1.amazonaws.com  AWS service end-point for Kinesis    Kinesis Secret Key  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret Key for accessing Kinesis.    Kinesis Stream Name  String  events  Stream name to read data from Kinesis    Redshift Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for redshift    Redshift Secret Key  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret Key for accessing Redshift.    Region Of Input File  String  ap-southeast-1  Region for the intermediate S3 storage.    Jdbc Output Database Driver  String  com.amazon.redshift .jdbc4.Driver  FQCN for Redshift JDBC output database driver.    Jdbc Output Database Url  String  jdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev  Connection URL for redshift    Jdbc Output Store Password  String  password  Password for redshift store instance.    Jdbc Output Store Username  String  username  Username for redshift store instance.    Jdbc Output Table Name  String  events  table to be used from redshift store instance.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-redshift/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Emr Cluster Id  EMR-CLUSTERID  String   Cluster id for EMR.    Redshift Delimiter   String   Delimiter to be used for redshift output    Redshift Reader Mode  READ_FROM_S3  String  READ_FROM_S3 READ_FROM_EMR  Choose preferred intermediate store. S3 or HDFS.    S3Bucket Name  S3_BUCKET_NAME  String   Bucket name for intermediate storage    S3Directory Name  S3_DIRECTORY_NAME  String   Directory name for intermediate storage    Batch Size For Jdbc Output  500  String   Number rows to push into redshift in single query    Max Length Of Rolling File  1048576 (1 MB)  String   Maximums size in bytes for files on intermediate storage.    Tuple Class Name For Jdbc Output  com.datatorrent .apps.PojoEvent  String  com.datatorrent .apps.PojoEvent  FQCN for tuples written on redshift", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/", 
            "text": "Kinesis to S3 application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 output\n\n\n\n\n\n\nS3Output Directory Path\n\n\nString\n\n\nkinesis_to_s3\n\n\nOutput directory for AWS S3\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3 output\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nString\n\n\nkinesis.us-east-1.amazonaws.com\n\n\nAWS service end-point for Kinesis\n\n\n\n\n\n\nKinesis Access Key\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for kinesis\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 output.\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing Kinesis.\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\nString\n\n\nevents\n\n\nStream name to read data from Kinesis\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSize Of S3File In Bytes\n\n\n134217728 (128 MB)\n\n\nlong\n\n\nOutput files on S3 will be rolled over to new files after this size is reached.\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{\"separator\":\"\n\" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{\"separator\":\"\n\",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\nJSON representing schema for objects given to CSV formatter.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input row\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input row for formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Kinesis-to-S3"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#kinesis-to-s3-application", 
            "text": "", 
            "title": "Kinesis to S3 application"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading data messages from a configured Kinesis stream. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      S3Output Bucket Name  String  com.example.app.s3  Bucket name for AWS S3 output    S3Output Directory Path  String  kinesis_to_s3  Output directory for AWS S3    Access Key For Kinesis Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Access Key For S3Output  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3 output    End Point For Kinesis Input  String  kinesis.us-east-1.amazonaws.com  AWS service end-point for Kinesis    Kinesis Access Key  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for kinesis    Secret Access Key For S3Output  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 output.    Secret Key For Kinesis Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing Kinesis.    Stream Name For Kinesis Input  String  events  Stream name to read data from Kinesis", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/kinesis-to-s3/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Size Of S3File In Bytes  134217728 (128 MB)  long  Output files on S3 will be rolled over to new files after this size is reached.    Csv Parser Schema  {\"separator\":\" \" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}  String  JSON representing schema to be used by CSV parser    Csv Formatter Schema  {\"separator\":\" \",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}  String  JSON representing schema for objects given to CSV formatter.    Tuple Class Name For Csv Parser Output  com.datatorrent. apps.PojoEvent  String  POJO class representing input row    Tuple Class Name For Formatter Input  com.datatorrent. apps.PojoEvent  String  POJO class representing input row for formatter    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  POJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/", 
            "text": "S3 to HDFS application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while reading blocks of data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAccess Key For S3Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3\n\n\n\n\n\n\nBucket Name For S3Input\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 input\n\n\n\n\n\n\nInput Directory Or File Path On S3\n\n\nString\n\n\n/path/to/input\n\n\nDirectory path within S3 bucket\n\n\n\n\n\n\nOutput Directory Path On HDFS\n\n\nString\n\n\n/user/dtuser /output/dir1\nhdfs://node1.corp1.com /user/dtuser/output\n\n\nHDFS path (absolute or relative)\n\n\n\n\n\n\nSecret Key For S3Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 input.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n4\n\n\nint\n\n\nMaximum allowed partitions for Block Reader. Allocating more partitions is for scaling the application by allocating more resources to handle larger dataset\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n16\n\n\nint\n\n\nLimit to control number of blocks emitted to downstream operators.", 
            "title": "S3-to-HDFS-sync"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#s3-to-hdfs-application", 
            "text": "", 
            "title": "S3 to HDFS application"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while reading blocks of data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync or Retention Application to serve business with continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Access Key For S3Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3    Bucket Name For S3Input  String  com.example.app.s3  Bucket name for AWS S3 input    Input Directory Or File Path On S3  String  /path/to/input  Directory path within S3 bucket    Output Directory Path On HDFS  String  /user/dtuser /output/dir1 hdfs://node1.corp1.com /user/dtuser/output  HDFS path (absolute or relative)    Secret Key For S3Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 input.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-hdfs-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Maximum Readers For Dynamic Partitioning  4  int  Maximum allowed partitions for Block Reader. Allocating more partitions is for scaling the application by allocating more resources to handle larger dataset    Number Of Blocks Per Window  16  int  Limit to control number of blocks emitted to downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/", 
            "text": "S3 to Redshift application\n\n\nSummary\n\n\nThis application template demonstrates continuous big data sync from a source to destination while scanning data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync, Preparation or Retention Application to serve business with rich continuous data.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAccess Key For S3Input\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3\n\n\n\n\n\n\nBucket Name For S3Input\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 input\n\n\n\n\n\n\nFiles For Scanning\n\n\nString\n\n\n/path/to/input\n\n\nDirectory path within S3 bucket\n\n\n\n\n\n\nJdbc Driver Of Redshift\n\n\nString\n\n\ncom.amazon.redshift .jdbc4.Driver\n\n\nFQCN for Redshift JDBC output database driver.\n\n\n\n\n\n\nJdbc Url Of Redshift\n\n\nString\n\n\njdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev\n\n\nConnection URL for redshift\n\n\n\n\n\n\nRegion For S3Or EMR\n\n\nString\n\n\nap-southeast-1\n\n\nRegion for S3 used for intermediate storage\n\n\n\n\n\n\nSecret Key For S3Input\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 input.\n\n\n\n\n\n\nTable Name For Redshift\n\n\nString\n\n\nevents\n\n\ntable to be used from redshift store instance.\n\n\n\n\n\n\nUser Name To Connect Redshift\n\n\nString\n\n\nusername\n\n\nUsername for redshift store instance.\n\n\n\n\n\n\nUser Password To Connect Redshift\n\n\nString\n\n\npassword\n\n\nPassword for redshift store instance.\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nRedshift Delimiter\n\n\n\n\nString\n\n\n,\n\n\nDelimiter to be used for redshift output\n\n\n\n\n\n\nRedshift Reader Mode\n\n\nREAD_FROM_S3\n\n\nString\n\n\nREAD_FROM_S3\nREAD_FROM_EMR\n\n\nChoose preferred intermediate store. S3 or HDFS.\n\n\n\n\n\n\nBlock Reader Size For S3Input\n\n\n1048576 (1 MB)\n\n\nlong\n\n\n\n\nSize of data chunk to be read in single request from S3.\n\n\n\n\n\n\nBucket Name For Redshift In S3\n\n\ndummyBucket\n\n\nString\n\n\n\n\nBucket name for intermediate storage\n\n\n\n\n\n\nCluster Id For Redshift\n\n\nEMR-CLUSTERID\n\n\nString\n\n\n\n\nCluster id for EMR.\n\n\n\n\n\n\nS3Directory Name\n\n\nS3_DIRECTORY_NAME\n\n\nString\n\n\n\n\nDirectory name for intermediate storage\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{\"separator\":\"\n\" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\n\n\nJSON representing schema to be used by CSV parser\n\n\n\n\n\n\nCsv Formatter Schema\n\n\n{\"separator\":\"\n\",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}\n\n\nString\n\n\n\n\nJSON representing schema for objects given to CSV formatter.\n\n\n\n\n\n\nMax Length Of Rolling File\n\n\n1048576 (1 MB)\n\n\nString\n\n\n\n\nMaximums size in bytes for files on intermediate storage.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\n\n\nLimit to control number of blocks emitted to downstream operators.\n\n\n\n\n\n\nNumber Of Readers For Partitioning\n\n\n2\n\n\nint\n\n\n\n\nLimit to control number of blocks emitted to downstream operators.\n\n\n\n\n\n\nTuple Class Name For Csv Parser Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input row\n\n\n\n\n\n\nTuple Class Name For Formatter Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input row for formatter\n\n\n\n\n\n\nTuple Class Name For Transform Input\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing input for transform operator (if included in the DAG. Not included by default).\n\n\n\n\n\n\nTuple Class Name For Transform Output\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nString\n\n\ncom.datatorrent. apps.PojoEvent\n\n\nPOJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "S3-to-HDFS-Filter-Transform"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#s3-to-redshift-application", 
            "text": "", 
            "title": "S3 to Redshift application"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#summary", 
            "text": "This application template demonstrates continuous big data sync from a source to destination while scanning data from a configured S3 bucket. This could be easily utilized and extended by any developer to create a fast,  fault tolerant and scalable Big Data Sync, Preparation or Retention Application to serve business with rich continuous data.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Access Key For S3Input  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3    Bucket Name For S3Input  String  com.example.app.s3  Bucket name for AWS S3 input    Files For Scanning  String  /path/to/input  Directory path within S3 bucket    Jdbc Driver Of Redshift  String  com.amazon.redshift .jdbc4.Driver  FQCN for Redshift JDBC output database driver.    Jdbc Url Of Redshift  String  jdbc:redshift://examplecluster .example123id.us-east-1 .redshift.amazonaws.com:5439/dev  Connection URL for redshift    Region For S3Or EMR  String  ap-southeast-1  Region for S3 used for intermediate storage    Secret Key For S3Input  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 input.    Table Name For Redshift  String  events  table to be used from redshift store instance.    User Name To Connect Redshift  String  username  Username for redshift store instance.    User Password To Connect Redshift  String  password  Password for redshift store instance.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/0.10.0/s3-to-redshift/#advanced-properties-optional", 
            "text": "Property  Default  Type  Example  Notes      Redshift Delimiter   String  ,  Delimiter to be used for redshift output    Redshift Reader Mode  READ_FROM_S3  String  READ_FROM_S3 READ_FROM_EMR  Choose preferred intermediate store. S3 or HDFS.    Block Reader Size For S3Input  1048576 (1 MB)  long   Size of data chunk to be read in single request from S3.    Bucket Name For Redshift In S3  dummyBucket  String   Bucket name for intermediate storage    Cluster Id For Redshift  EMR-CLUSTERID  String   Cluster id for EMR.    S3Directory Name  S3_DIRECTORY_NAME  String   Directory name for intermediate storage    Csv Parser Schema  {\"separator\":\" \" ,\"quoteChar\":\"\\\"\", \"fields\": [{\"name\":\"accountNumber\" ,\"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\": \"amount\", \"type\":\"Integer\"}]}  String   JSON representing schema to be used by CSV parser    Csv Formatter Schema  {\"separator\":\" \",\"quoteChar\": \"\\\"\",\"fields\":[{\"name\":\"accountNumber\", \"type\":\"Integer\"}, {\"name\":\"name\", \"type\":\"String\"}, {\"name\":\"amount\", \"type\":\"Integer\"}]}  String   JSON representing schema for objects given to CSV formatter.    Max Length Of Rolling File  1048576 (1 MB)  String   Maximums size in bytes for files on intermediate storage.    Number Of Blocks Per Window  1  int   Limit to control number of blocks emitted to downstream operators.    Number Of Readers For Partitioning  2  int   Limit to control number of blocks emitted to downstream operators.    Tuple Class Name For Csv Parser Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input row    Tuple Class Name For Formatter Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input row for formatter    Tuple Class Name For Transform Input  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing input for transform operator (if included in the DAG. Not included by default).    Tuple Class Name For Transform Output  com.datatorrent. apps.PojoEvent  String  com.datatorrent. apps.PojoEvent  POJO class representing output for transform operator (if included in the DAG. Not included by default).", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/", 
            "text": "Database dump to HDFS Sync application\n\n\nSummary\n\n\nIngest records from a PostgreSQL Database table to hadoop HDFS. This application reads messages from configured PostgreSQL table and writes each record as a comma-separated line in HDFS files.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/database-to-hdfs\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n. Page listing the applications available on AppFactory is displayed.\n\n\nSearch for Database to see all applications related to Database.\n\n\nClick on import button for \nDatabase dump to HDFS Sync App.\n\nNotification is displayed on the top right corner after application package is successfully imported.\n\n\n\n\n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click the launch button for \nDatabase-to-HDFS-Sync\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nDatabase-to-HDFS-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process all rows from the table \ntest_event_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \ndatabase-node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n, and we wish to write the output to the HDFS file\n\n/user/appuser/output/output.txt\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nJdbc Input Database Url\n\n\njdbc:postgresql://database-node.com:5432/testdb\n\n\n\n\n\n\nJdbc Input Store Password\n\n\npostgres\n\n\n\n\n\n\nJdbc Input Store Username\n\n\npostgres\n\n\n\n\n\n\nJdbc Input Table Name\n\n\ntest_event_table\n\n\n\n\n\n\nOutput Directory Path On HDFS\n\n\n/user/appuser/output\n\n\n\n\n\n\nOutput File Name On HDFS\n\n\noutput.txt\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://database-node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.tableName\n\n\nTable name for input records\n\n\nString\n\n\ntest_event_table\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.whereCondition\n\n\nWhere clause condition (if any) for input records. Keep blank to fetch all records.\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.partitionCount\n\n\nNumber of JDBC input partitions for parallel reading.\n\n\n4\n\n\n1\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.batchSize\n\n\nBatch size to read data from JDBC.\n\n\n300\n\n\n300\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.key\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO\n\n\nACCOUNT_NO\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.columnsExpression\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\n\n\n\n\ndt.operator.JdbcPoller.port.outputPort.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcPoller.prop.pollInterval\n\n\nPoll interval for scanning new records in milisec\n\n\n1000\n\n\n1000\n\n\n\n\n\n\ndt.operator.formatter.prop.schema\n\n\nSchema for CSV formatter\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter. port.in.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/database-to-hdfs\n:\n\n\ncd examples/tutorials/database-to-hdfs\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Database dump to HDFS App"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#database-dump-to-hdfs-sync-application", 
            "text": "", 
            "title": "Database dump to HDFS Sync application"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#summary", 
            "text": "Ingest records from a PostgreSQL Database table to hadoop HDFS. This application reads messages from configured PostgreSQL table and writes each record as a comma-separated line in HDFS files.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/database-to-hdfs .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    . Page listing the applications available on AppFactory is displayed.  Search for Database to see all applications related to Database.  Click on import button for  Database dump to HDFS Sync App. \nNotification is displayed on the top right corner after application package is successfully imported.      Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click the launch button for  Database-to-HDFS-Sync  application. In the confirmation modal, click the Configure button.    The  Database-to-HDFS-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process all rows from the table  test_event_table  in a\nPostgreSQL database named  testdb  accessible at  database-node.com  port  5432  with credentials\nusername= postgres , password= postgres , and we wish to write the output to the HDFS file /user/appuser/output/output.txt . Properties should be set as follows:     Name  Value      Jdbc Input Database Url  jdbc:postgresql://database-node.com:5432/testdb    Jdbc Input Store Password  postgres    Jdbc Input Store Username  postgres    Jdbc Input Table Name  test_event_table    Output Directory Path On HDFS  /user/appuser/output    Output File Name On HDFS  output.txt     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n    \n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.JdbcPoller.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcPoller.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://database-node.com:5432/testdb    dt.operator.JdbcPoller.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcPoller.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcPoller.prop.tableName  Table name for input records  String  test_event_table    dt.operator.JdbcPoller.prop.whereCondition  Where clause condition (if any) for input records. Keep blank to fetch all records.  String     dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.JdbcPoller.prop.partitionCount  Number of JDBC input partitions for parallel reading.  4  1    dt.operator.JdbcPoller.prop.batchSize  Batch size to read data from JDBC.  300  300    dt.operator.JdbcPoller.prop.key  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO  ACCOUNT_NO    dt.operator.JdbcPoller.prop.columnsExpression  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO, NAME, AMOUNT  ACCOUNT_NO, NAME, AMOUNT    dt.operator.JdbcPoller.port.outputPort.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcPoller.prop.pollInterval  Poll interval for scanning new records in milisec  1000  1000    dt.operator.formatter.prop.schema  Schema for CSV formatter  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter. port.in.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/database-to-hdfs/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/database-to-hdfs :  cd examples/tutorials/database-to-hdfs    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/", 
            "text": "Database to Database Sync application\n\n\nSummary\n\n\nIngest records from a source PostgreSQL Database table to destination PostgreSQL database table. This application reads messages from configured PostgreSQL table and writes each record to output PostgreSQL table.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/database-to-database-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n   Page listing the applications available on AppFactory is displayed.\n\n\nSearch for Database to see all applications related to Database.\n\n\n\n\nClick on import button for \nDatabase to Database Sync App\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n   \n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nDatabase-to-Database-Sync\n application.\n\n\n\n\n\n\nThe \nDatabase-to-Database-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process all rows from the table \ntest_event_input_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \nsource-database-node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n, and we wish to write the output records to other\nPostgreSQL database table \ntest_event_output_table\n accessible at \ndestination-database-node.com\n port \n5432\n with same creditials listed above. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nJdbc Input Database Url\n\n\njdbc:postgresql://source-database-node.com:5432/testdb\n\n\n\n\n\n\nJdbc Input Store Password\n\n\npostgres\n\n\n\n\n\n\nJdbc Input Store Username\n\n\npostgres\n\n\n\n\n\n\nJdbc Input Table Name\n\n\ntest_event_input_table\n\n\n\n\n\n\nJdbc Output Database Url\n\n\njdbc:postgresql://destination-database-node:5432/testdb\n\n\n\n\n\n\nJdbc Output Store Password\n\n\npostgres\n\n\n\n\n\n\nJdbc Output Store Username\n\n\npostgres\n\n\n\n\n\n\nJdbc Output Table Name\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the dialog to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n   \n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\nMeta-data\n table is required for Jdbc Output operator for transactional data and application consistency.\n\n\n\n\n\n\n\n\nTable Name\n\n\nColumn Names\n\n\n\n\n\n\n\n\n\n\ndt_meta\n\n\ndt_app_id (VARCHAR)\ndt_operator_id (INT) \ndt_window (BIGINT)\n\n\n\n\n\n\n\n\nQuery for \nMeta-data\n table creation:\n\n\nCREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://node1.company.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.tableName\n\n\nTable name for input records\n\n\nString\n\n\ntest_event_input_table\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.whereCondition\n\n\nWhere clause condition (if any) for input records. Keep blank to fetch all records.\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://node2.company.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tableName\n\n\nTable name for output records\n\n\nString\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.partitionCount\n\n\nNumber of JDBC input partitions for parallel reading.\n\n\n4\n\n\n1\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.batchSize\n\n\nBatch size to read data from JDBC.\n\n\n300\n\n\n300\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.key\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO\n\n\nACCOUNT_NO\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.columnsExpression\n\n\nKey column for the table. This will be used for partitoning of rows.\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\nACCOUNT_NO, NAME, AMOUNT\n\n\n\n\n\n\ndt.operator.JdbcInput.port.outputPort.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcInput.prop.pollInterval\n\n\nPoll interval for scanning new records in milisec\n\n\n1000\n\n\n1000\n\n\n\n\n\n\ndt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/database-to-database-sync\n:\n\n\ncd examples/tutorials/database-to-database-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Database to Database Sync App"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#database-to-database-sync-application", 
            "text": "", 
            "title": "Database to Database Sync application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#summary", 
            "text": "Ingest records from a source PostgreSQL Database table to destination PostgreSQL database table. This application reads messages from configured PostgreSQL table and writes each record to output PostgreSQL table.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/database-to-database-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    \n   Page listing the applications available on AppFactory is displayed.  Search for Database to see all applications related to Database.   Click on import button for  Database to Database Sync App \nNotification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.\n     Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Database-to-Database-Sync  application.    The  Database-to-Database-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process all rows from the table  test_event_input_table  in a\nPostgreSQL database named  testdb  accessible at  source-database-node.com  port  5432  with credentials\nusername= postgres , password= postgres , and we wish to write the output records to other\nPostgreSQL database table  test_event_output_table  accessible at  destination-database-node.com  port  5432  with same creditials listed above. Properties should be set as follows:     Name  Value      Jdbc Input Database Url  jdbc:postgresql://source-database-node.com:5432/testdb    Jdbc Input Store Password  postgres    Jdbc Input Store Username  postgres    Jdbc Input Table Name  test_event_input_table    Jdbc Output Database Url  jdbc:postgresql://destination-database-node:5432/testdb    Jdbc Output Store Password  postgres    Jdbc Output Store Username  postgres    Jdbc Output Table Name  test_event_output_table     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the dialog to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#prerequistes", 
            "text": "Meta-data  table is required for Jdbc Output operator for transactional data and application consistency.     Table Name  Column Names      dt_meta  dt_app_id (VARCHAR) dt_operator_id (INT)  dt_window (BIGINT)     Query for  Meta-data  table creation:  CREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.JdbcInput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcInput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://node1.company.com:5432/testdb    dt.operator.JdbcInput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcInput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcInput.prop.tableName  Table name for input records  String  test_event_input_table    dt.operator.JdbcInput.prop.whereCondition  Where clause condition (if any) for input records. Keep blank to fetch all records.  String     dt.operator.JdbcOuput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://node2.company.com:5432/testdb    dt.operator.JdbcOuput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.tableName  Table name for output records  String  test_event_output_table", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.JdbcInput.prop.partitionCount  Number of JDBC input partitions for parallel reading.  4  1    dt.operator.JdbcInput.prop.batchSize  Batch size to read data from JDBC.  300  300    dt.operator.JdbcInput.prop.key  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO  ACCOUNT_NO    dt.operator.JdbcInput.prop.columnsExpression  Key column for the table. This will be used for partitoning of rows.  ACCOUNT_NO, NAME, AMOUNT  ACCOUNT_NO, NAME, AMOUNT    dt.operator.JdbcInput.port.outputPort.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcInput.prop.pollInterval  Poll interval for scanning new records in milisec  1000  1000    dt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/database-to-database-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/database-to-database-sync :  cd examples/tutorials/database-to-database-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-sync/", 
            "text": "HDFS Sync App\n\n\nSummary\n\n\nIngest and backup hadoop HDFS data from one cluster to another in a fault tolerant way for use cases such as disaster recovery. This application copies files from the configured source path to the destination file path. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n  \n\n  Page listing the applications available on AppFactory is displayed.\n\n\nSearch for HDFS to see all applications related to HDFS.\n\n\n\n\nClick on import button for \nHDFS Sync App\n. Notification is displayed on the top right corner after application package is imported successfully\n\n\n\n\n\n\n\n\nSearch for HDFS to see all applications related to HDFS.\n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS-Sync-App\n\napplication. In the confirmation modal, click the Configure button. The \nHDFS-Sync-App\n application configuration is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to copy all files in \n/user/appuser/input\n from \nremote-cluster\n to \n/user/appuser/archive\n on the host cluster (on which app is running). Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nhdfs://remote-cluster/user/appuser/input\n\n\n\n\n\n\nOutput Directory Path\n\n\n/user/appuser/archive\n\n\n\n\n\n\n\n\nThis application is tuned for better performance if reading data from remote cluster to host cluster.\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the application.\n   A notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (these properties are all strings and\nare HDFS paths: the first is the destination and the second the source).\n\n\n\n\n\n\n\n\nProperty\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\n/user/appuser/output/dir1\nhdfs://node1.corp1.com/user/appuser/output\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.files\n\n\n/user/appuser/input/dir1\n/user/appuser/input/dir2/file1.log\nhdfs://node1.corp1.com/user/appuser/input\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for \ncluster-\nmemory- \nconf.xml\n\n\nDefault for  \nsandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.HDFSInputModule.prop.blocksThreshold\n\n\nRate at which block metadata is emitted per second\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-sync\n:\n\n\ncd examples/tutorials/hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. This application is for copying files from source to destination. Thus, \nApplication.java\n does not involve any processing operator in between.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#hdfs-sync-app", 
            "text": "", 
            "title": "HDFS Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#summary", 
            "text": "Ingest and backup hadoop HDFS data from one cluster to another in a fault tolerant way for use cases such as disaster recovery. This application copies files from the configured source path to the destination file path. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n   \n  Page listing the applications available on AppFactory is displayed.  Search for HDFS to see all applications related to HDFS.   Click on import button for  HDFS Sync App . Notification is displayed on the top right corner after application package is imported successfully     Search for HDFS to see all applications related to HDFS.    Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  HDFS-Sync-App \napplication. In the confirmation modal, click the Configure button. The  HDFS-Sync-App  application configuration is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to copy all files in  /user/appuser/input  from  remote-cluster  to  /user/appuser/archive  on the host cluster (on which app is running). Properties should be set as follows:     name  value      Input Directory Or File Path  hdfs://remote-cluster/user/appuser/input    Output Directory Path  /user/appuser/archive     This application is tuned for better performance if reading data from remote cluster to host cluster.\nDetails about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the application.\n   A notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status. The  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties (these properties are all strings and\nare HDFS paths: the first is the destination and the second the source).     Property  Example      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  /user/appuser/output/dir1 hdfs://node1.corp1.com/user/appuser/output    dt.operator.HDFSInputModule.prop.files  /user/appuser/input/dir1 /user/appuser/input/dir2/file1.log hdfs://node1.corp1.com/user/appuser/input", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster- memory-  conf.xml  Default for   sandbox- memory  -conf.xml      dt.operator.HDFSInputModule.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.HDFSInputModule.prop.blocksThreshold  Rate at which block metadata is emitted per second  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-sync :  cd examples/tutorials/hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. This application is for copying files from source to destination. Thus,  Application.java  does not involve any processing operator in between.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/", 
            "text": "HDFS to HDFS line copy application\n\n\nSummary\n\n\nIngest and backup hadoop HDFS data as lines from one cluster to another in a fault tolerant way. This application reads lines from the configured source path and writes them to the destination file path. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-line-copy\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n   Page listing the applications available on AppFactory is displayed. \n\n\nSearch for HDFS to see all applications related to HDFS.\n\n\nClick on import button for \nHDFS to HDFS Line Copy App.\n Notification is displayed on the top right corner after application package is successfully imported.\n\n\n\n\n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS line copy\n\napplication. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nHDFS-line-copy\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\nFor example, suppose we wish to process lines from all the files in \n/user/appuser/input\n from \nsource-cluster\n and send the output to \noutput.txt\n in \n/user/appuser/output\n in the \ndestination-cluster\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nInput Directory Or File Path\n\n\nhdfs://source-cluster/user/appuser/input\n\n\n\n\n\n\nOutput Directory Path\n\n\nhdfs://destination-cluster/user/appuser/output\n\n\n\n\n\n\nOutput File Name\n\n\noutput.txt\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner to launch the application.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nPage with listing of all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (all properties are strings).\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.files\n\n\nHDFS path for input file or directory\n\n\n/user/appuser/input/directory1\n/user/appuser/input/file2.log\nhdfs://node1.corp1.com/user/user1/input\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nName of the output file. This name will be appended with suffix for each part.\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nHDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster\n-memory\n-conf.xml\n\n\nDefault for\nsandbox\n-memory\n-conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.minReaders\n\n\nMinimum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n1\n\n\n1\n\n\n\n\n\n\ndt.operator.recordReader.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-to-kafka-sync\n:\n\n\ncd examples/tutorials/hdfs-line-copy\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with the \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS to HDFS Line Copy App"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#hdfs-to-hdfs-line-copy-application", 
            "text": "", 
            "title": "HDFS to HDFS line copy application"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#summary", 
            "text": "Ingest and backup hadoop HDFS data as lines from one cluster to another in a fault tolerant way. This application reads lines from the configured source path and writes them to the destination file path. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-line-copy .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#mandatory-properties", 
            "text": "End user must specify the values for these properties (all properties are strings).     Property  Description  Example      dt.operator.recordReader.prop.files  HDFS path for input file or directory  /user/appuser/input/directory1 /user/appuser/input/file2.log hdfs://node1.corp1.com/user/user1/input    dt.operator.fileOutput.prop.outputFileName  Name of the output file. This name will be appended with suffix for each part.  output.txt    dt.operator.fileOutput.prop.filePath  HDFS path for the output directory. Generally, this refers to path on the hadoop cluster on which app is running.  /user/appuser/output", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster -memory -conf.xml  Default for sandbox -memory -conf.xml      dt.operator.recordReader.prop.minReaders  Minimum number of BlockReader partitions for parallel reading.  int  1  1    dt.operator.recordReader.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-line-copy/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-to-kafka-sync :  cd examples/tutorials/hdfs-line-copy    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with the  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/", 
            "text": "HDFS to Kafka Sync App\n\n\nSummary\n\n\nThis application reads lines from configured HDFS path and writes each line as a message in configured Apache Kafka topic.\nThis document illustrates step by step guide to launch, configure, customize\nthis application.The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/hdfs-to-kafka-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n   Page listing the applications available on AppFactory is displayed.\n\n\nSearch for Kafka to see all applications related to Kafka.\n\n\n\n\nClick on import button for \nHDFS to Kafka Sync App\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nHDFS to Kafka Sync\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nHDFS-to-Kafka-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process lines from all files in \n/user/appuser/input\n from \nsource-cluster\n and send the output to kafka on \nkafka-server-node\n with topic \ntest\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nKafka Topic Name\n\n\ntest\n\n\n\n\n\n\nInput Directory Or File Path On HDFS\n\n\n/user/appuser/input\n\n\n\n\n\n\nKafka Producer Properties\n\n\nserializer.class=kafka.serializer.DefaultEncoder, producer.type=async, metadata.broker.list=kafka-server-node:9092\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right right corner to launch the application.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   The \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaOutput. prop.producerProperties\n\n\nProperties for Kafka producer\n\n\nComma separated String\n\n\nserializer.class=kafka.serializer.DefaultEncoder, producer.type=async,\nmetadata.broker.list=kafka-server-node:9092\n\n\n\n\n\n\ndt.operator.kafkaOutput .prop.topic\n\n\nKafka topic for output records\n\n\nString\n\n\ntest\n\n\n\n\n\n\ndt.operator.recordReader\nprop.files\n\n\nHDFS path for input file or directory\n\n\nString\n\n\n/user/appuser/input/directory1\n/user/appuser/input/file2.log\nhdfs://node1.corp1.com/user/appuser/input\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox edition\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nCluster default\n\n\nSandbox default\n\n\n\n\n\n\n\n\n\n\ndt.operator.recordReader.prop.minReaders\n\n\nMinimum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n1\n\n\n1\n\n\n\n\n\n\ndt.operator.recordReader.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.kafkaOutput.attr.PARTITIONER\n\n\nPartitoning for Kafka output operator\n\n\nString\n\n\nSee\n (1)\n\n\nSee\n (2)\n\n\n\n\n\n\n\n\n\n\nCluster default\n: \ncom.datatorrent.common.partitioner.StatelessPartitioner:16\n\n\nSandbox default\n: \ncom.datatorrent.common.partitioner.StatelessPartitioner:1\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/hdfs-to-kafka-sync\n:\n\n\ncd examples/tutorials/hdfs-to-kafka-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n    \nmvn clean package\n\n\nThis will generate the application package with the \n.apa\n extension inside the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "HDFS to Kafka Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#hdfs-to-kafka-sync-app", 
            "text": "", 
            "title": "HDFS to Kafka Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#summary", 
            "text": "This application reads lines from configured HDFS path and writes each line as a message in configured Apache Kafka topic.\nThis document illustrates step by step guide to launch, configure, customize\nthis application.The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/hdfs-to-kafka-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    \n   Page listing the applications available on AppFactory is displayed.  Search for Kafka to see all applications related to Kafka.   Click on import button for  HDFS to Kafka Sync App Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  HDFS to Kafka Sync  application. In the confirmation modal, click the Configure button.    The  HDFS-to-Kafka-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process lines from all files in  /user/appuser/input  from  source-cluster  and send the output to kafka on  kafka-server-node  with topic  test . Properties should be set as follows:     name  value      Kafka Topic Name  test    Input Directory Or File Path On HDFS  /user/appuser/input    Kafka Producer Properties  serializer.class=kafka.serializer.DefaultEncoder, producer.type=async, metadata.broker.list=kafka-server-node:9092     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right right corner to launch the application.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n   The  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.kafkaOutput. prop.producerProperties  Properties for Kafka producer  Comma separated String  serializer.class=kafka.serializer.DefaultEncoder, producer.type=async, metadata.broker.list=kafka-server-node:9092    dt.operator.kafkaOutput .prop.topic  Kafka topic for output records  String  test    dt.operator.recordReader prop.files  HDFS path for input file or directory  String  /user/appuser/input/directory1 /user/appuser/input/file2.log hdfs://node1.corp1.com/user/appuser/input", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox edition  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Cluster default  Sandbox default      dt.operator.recordReader.prop.minReaders  Minimum number of BlockReader partitions for parallel reading.  int  1  1    dt.operator.recordReader.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.kafkaOutput.attr.PARTITIONER  Partitoning for Kafka output operator  String  See  (1)  See  (2)      Cluster default :  com.datatorrent.common.partitioner.StatelessPartitioner:16  Sandbox default :  com.datatorrent.common.partitioner.StatelessPartitioner:1   You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/hdfs-to-kafka-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/hdfs-to-kafka-sync :  cd examples/tutorials/hdfs-to-kafka-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:\n     mvn clean package  This will generate the application package with the  .apa  extension inside the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/", 
            "text": "HDFS to S3 Sync Application\n\n\nSummary\n\n\nThis application demonstrates continuous big data sync from a source HDFS to destination S3. It read files from source HDFS and upload it into Amazon S3 using multipart upload feature.\n\n\nQuick links\n\n\n\n\n\n\nHow to import and launch an app-template\n\n\n\n\n\n\nHow to customize an app-template\n\n\n\n\n\n\nREADME for the application\n\n\n\n\n\n\nSource code\n\n\n\n\n\n\nPlease send feedback or feature requests to :\n    \nfeedback@datatorrent.com\n\n\n\n\n\n\nJoin our user discussion group at :\n    \ndt-users@googlegroups.com\n\n\n\n\n\n\nRequired Properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nType\n\n\nExample\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAws Credentials Access Key Id\n\n\nString\n\n\nACCESS_XXX_KEY_XX_ID\n\n\nAWS credentials access key id for S3\n\n\n\n\n\n\nAws Credentials Secret Access Key\n\n\nString\n\n\n8your+own0+AWS0 secret1230+8key8goes0here\n\n\nAWS Secret access key for accessing S3 output.\n\n\n\n\n\n\nInput Directory Or File Path On HDFS\n\n\nString\n\n\n/user/appuser/input /directory1\n/user/appuser /input/file2.log\nhdfs://node1.corp1 .com/user/user1 /input\n\n\nHDFS path for input file or directory\n\n\n\n\n\n\nOutput Directory Path On S3Storage\n\n\nString\n\n\nhdfs_to_s3\n\n\nOutput directory for AWS S3\n\n\n\n\n\n\nS3Storage Bucket Name\n\n\nString\n\n\ncom.example.app.s3\n\n\nBucket name for AWS S3 output\n\n\n\n\n\n\n\n\nAdvanced Properties (optional)\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nType\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nMaximum Readers For Dynamic Partitioning\n\n\n16\n\n\nint\n\n\nMaximum no of partitions for Block Reader operator.\n\n\n\n\n\n\nMinimum Readers For Dynamic Partitioning\n\n\n2\n\n\nint\n\n\nMinimum no of partitions for Block Reader operator.\n\n\n\n\n\n\nNumber Of Blocks Per Window\n\n\n1\n\n\nint\n\n\nFile splitter will emit these many blocks per window for downstream operators.", 
            "title": "HDFS to S3 Sync App"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/#hdfs-to-s3-sync-application", 
            "text": "", 
            "title": "HDFS to S3 Sync Application"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/#summary", 
            "text": "This application demonstrates continuous big data sync from a source HDFS to destination S3. It read files from source HDFS and upload it into Amazon S3 using multipart upload feature.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/#quick-links", 
            "text": "How to import and launch an app-template    How to customize an app-template    README for the application    Source code    Please send feedback or feature requests to :\n     feedback@datatorrent.com    Join our user discussion group at :\n     dt-users@googlegroups.com", 
            "title": "Quick links"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/#required-properties", 
            "text": "End user must specify the values for these properties.     Property  Type  Example  Notes      Aws Credentials Access Key Id  String  ACCESS_XXX_KEY_XX_ID  AWS credentials access key id for S3    Aws Credentials Secret Access Key  String  8your+own0+AWS0 secret1230+8key8goes0here  AWS Secret access key for accessing S3 output.    Input Directory Or File Path On HDFS  String  /user/appuser/input /directory1 /user/appuser /input/file2.log hdfs://node1.corp1 .com/user/user1 /input  HDFS path for input file or directory    Output Directory Path On S3Storage  String  hdfs_to_s3  Output directory for AWS S3    S3Storage Bucket Name  String  com.example.app.s3  Bucket name for AWS S3 output", 
            "title": "Required Properties"
        }, 
        {
            "location": "/app-templates/hdfs-to-s3-sync/#advanced-properties-optional", 
            "text": "Property  Default  Type  Notes      Maximum Readers For Dynamic Partitioning  16  int  Maximum no of partitions for Block Reader operator.    Minimum Readers For Dynamic Partitioning  2  int  Minimum no of partitions for Block Reader operator.    Number Of Blocks Per Window  1  int  File splitter will emit these many blocks per window for downstream operators.", 
            "title": "Advanced Properties (optional)"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/", 
            "text": "Kafka To Database Sync application\n\n\nSummary\n\n\nIngest string messages seperated by '|' from configured kafka topic and writes each message as a record in DataBase. This application uses PoJoEvent as an example schema, this can be customized to use custom schema based on specific needs.\n\n\nThe source code is available at:\n\nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-database-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\nPage listing the applications available on AppFactory is displayed.\n\n\nSearch for Database to see all applications related to Database.\n\n\n\n\nClick on import button for \nKafka to Database Sync App\n\\Notification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-Database-Sync\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nKafka-to-Database-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish string messages seperated with '|' to the table \ntest_event_table\n in a\nPostgreSQL database named \ntestdb\n accessible at \ntarget-database.node.com\n port \n5432\n with credentials\nusername=\npostgres\n, password=\npostgres\n. Properties should be set as follows:\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nCsv Parser Schema\n\n\n{ \"separator\": \"|\", \"quoteChar\": \"\\\"\", \"fields\": [ { \"name\": \"accountNumber\", \"type\": \"Integer\" }, { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ] }\n\n\n\n\n\n\nJdbc Output Database Url\n\n\njdbc:postgresql://target-database.node.com:5432/testdb\n\n\n\n\n\n\nJdbc Output Store Password\n\n\npostgres\n\n\n\n\n\n\nJdbc Output Store Username\n\n\npostgres\n\n\n\n\n\n\nJdbc Output Table Name\n\n\ntest_event_output_table\n\n\n\n\n\n\nKafka Broker List\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\nKafka Topic Name\n\n\ntransactions\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n   \n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n   \n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\nClick on the \nphysical\n tab to look at the status of physical instances of operators, containers etc.\n   \n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\n\n\nKafka configured with version 0.9.\n\n\nMeta-data\n table is required for Jdbc Output operator for transactional data and application consistency.\n\n\n\n\n\n\n\n\n\n\nTable Name\n\n\nColumn Names\n\n\n\n\n\n\n\n\n\n\ndt_meta\n\n\ndt_app_id (VARCHAR)\ndt_operator_id (INT) \ndt_window (BIGINT)\n\n\n\n\n\n\n\n\nQuery for \nMeta-data\n table creation:\n\n\nCREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nList of Brokers for kafka input\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nKafka topics for input\n\n\nString\n\n\ntransactions\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nKafka input offset\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseDriver\n\n\nJDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.\n\n\nString\n\n\norg.postgresql.Driver\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.databaseUrl\n\n\nJDBC connection URL\n\n\nString\n\n\njdbc:postgresql://target-database.node.com:5432/testdb\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.password\n\n\nPassword for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.store.userName\n\n\nUsername for Database credentials\n\n\nString\n\n\npostgres\n\n\n\n\n\n\ndt.operator.JdbcOuput.prop.tableName\n\n\nTable name for output records\n\n\nString\n\n\ntest_event_output_table\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault for\n cluster\n-memory\n- conf.xml\n\n\nDefault for\n  sandbox\n-memory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.csvParser.port.out.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) emitted by Kafka input\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.csvParser.prop.schema\n\n\nSchema for CSV parser\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/kafka-to-database-sync\n:\n\n\ncd examples/tutorials/kafka-to-database-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the \nApplication.java\n for this project.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package file with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n    \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "Kafka to Database Sync App"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#kafka-to-database-sync-application", 
            "text": "", 
            "title": "Kafka To Database Sync application"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#summary", 
            "text": "Ingest string messages seperated by '|' from configured kafka topic and writes each message as a record in DataBase. This application uses PoJoEvent as an example schema, this can be customized to use custom schema based on specific needs.  The source code is available at: https://github.com/DataTorrent/app-templates/tree/master/kafka-to-database-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    \nPage listing the applications available on AppFactory is displayed.  Search for Database to see all applications related to Database.   Click on import button for  Kafka to Database Sync App \\Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-Database-Sync  application. In the confirmation modal, click the Configure button.    The  Kafka-to-Database-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish string messages seperated with '|' to the table  test_event_table  in a\nPostgreSQL database named  testdb  accessible at  target-database.node.com  port  5432  with credentials\nusername= postgres , password= postgres . Properties should be set as follows:     Name  Value      Csv Parser Schema  { \"separator\": \"|\", \"quoteChar\": \"\\\"\", \"fields\": [ { \"name\": \"accountNumber\", \"type\": \"Integer\" }, { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ] }    Jdbc Output Database Url  jdbc:postgresql://target-database.node.com:5432/testdb    Jdbc Output Store Password  postgres    Jdbc Output Store Username  postgres    Jdbc Output Table Name  test_event_output_table    Kafka Broker List  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    Kafka Topic Name  transactions     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the\napplication.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and to find\nits logs.\n       Click on the  Monitor  tab from the top navigation bar.\n       A page listing all running applications is displayed. Search for the current instance based on name or application id or any other relevant field. Click on the application name or id to navigate to the details page.\n      Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, StrAM events, operator status based on logical operators, stream status, and a chart with key metrics.\n     Click on the  physical  tab to look at the status of physical instances of operators, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#prerequistes", 
            "text": "Kafka configured with version 0.9.  Meta-data  table is required for Jdbc Output operator for transactional data and application consistency.      Table Name  Column Names      dt_meta  dt_app_id (VARCHAR) dt_operator_id (INT)  dt_window (BIGINT)     Query for  Meta-data  table creation:  CREATE TABLE dt_meta (dt_app_id varchar(100) NOT NULL,\ndt_operator_id int NOT NULL, dt_window bigint NOT NULL,\nCONSTRAINT dt_app_id UNIQUE (dt_app_id,dt_operator_id,dt_window));", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.kafkaInput.prop.clusters  List of Brokers for kafka input  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.topics  Kafka topics for input  String  transactions    dt.operator.kafkaInput.prop.initialOffset  Kafka input offset  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.JdbcOuput.prop.store.databaseDriver  JDBC driver class. This has to be on CLASSPATH. PostgreSQL driver is added as a dependency.  String  org.postgresql.Driver    dt.operator.JdbcOuput.prop.store.databaseUrl  JDBC connection URL  String  jdbc:postgresql://target-database.node.com:5432/testdb    dt.operator.JdbcOuput.prop.store.password  Password for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.store.userName  Username for Database credentials  String  postgres    dt.operator.JdbcOuput.prop.tableName  Table name for output records  String  test_event_output_table", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml  (the first 2 are integers and the rest are strings).\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Default for  cluster -memory - conf.xml  Default for   sandbox -memory  -conf.xml      dt.operator.csvParser.port.out.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by JDBC input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.JdbcOutput.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) emitted by Kafka input  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.csvParser.prop.schema  Schema for CSV parser  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-database-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/kafka-to-database-sync :  cd examples/tutorials/kafka-to-database-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the  Application.java  for this project.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package file with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n        Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/", 
            "text": "Kafka to HDFS Filter Application\n\n\nSummary\n\n\nIngest filtered messages from kafka to hadoop HDFS for continuous ingestion to hadoop.\nThe source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-filter\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n   Page listing the applications available on AppFactory is displayed.\n\n\nSearch for Kafka to see all applications related to Kafka.\n\n\n\n\nClick on import button for \nKafka to HDFS Filter App\n.\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-HDFS-Filter\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nKafka-to-HDFS-Filter\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process all messages seperated by '|' from topic \ntransactions\n at the kafka server running on \nkafka-source.node.com\n with port \n9092\n, filters the messages based on the filter criteria \n({$}.getAmount() \n= 20000)\n and write them to \noutput.txt\n under \n/user/appuser/output\n on HDFS.\nProperties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nOutput Directory Path\n\n\n/user/appuser/output\n\n\n\n\n\n\nOutput File Name\n\n\noutput.txt\n\n\n\n\n\n\nKafka Broker List\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\nKafka Topic Name\n\n\ntransactions\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the application.\nA notification will be displayed at the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nPrerequistes\n\n\nKafka configured with version 0.9.\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.filter.prop.condition\n\n\nFilter condition\n\n\nCondition\n\n\n({$}.getAmount() \n= 20000)\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nComma separated list of kafka-brokers\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nInitial offset to read from Kafka\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nTopics to read from Kafka\n\n\nString\n\n\nevent_data\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\nThe messages or records emitted are specified by the value of the \nTUPLE_CLASS\n attribute in the configuration file namely \nPojoEvent\n in this case.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster-\nmemory\n- conf.xml\n\n\nDefault for\n sandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.maxLength\n\n\nMaximum length for output file after which file is rotated\n\n\nlong\n\n\nLong.MAX_VALUE\n\n\nLong.MAX_VALUE\n\n\n\n\n\n\ndt.operator.csvParser.prop.schema\n\n\nSchema for CSV Parser\n\n\nSchema\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter.prop.schema\n\n\nSchema for CSV formatter\n\n\nSchema\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n{\"separator\": \"\n\",\n\"quoteChar\": \"\\\"\",\n\"lineDelimiter\": \"\", \"fields\": [\n{\n\"name\": \"accountNumber\", \n\"type\": \"Integer\"\n},\n {\n\"name\": \"name\",\n\"type\": \"String\"\n},\n{\n\"name\": \"amount\",\n\"type\": \"Integer\"\n}\n]}\n\n\n\n\n\n\ndt.operator.formatter. port.in.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter\n\n\nPOJO\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\ndt.operator.filter.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) input to Filter\n\n\nPOJO\n\n\ncom.datatorrent.apps.PojoEvent\n\n\ncom.datatorrent.apps.PojoEvent\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kafka-to-hdfs-filter':\n\n\ncd examples/tutorials/kafka-to-hdfs-filter\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kafka to HDFS Filter App"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#kafka-to-hdfs-filter-application", 
            "text": "", 
            "title": "Kafka to HDFS Filter Application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#summary", 
            "text": "Ingest filtered messages from kafka to hadoop HDFS for continuous ingestion to hadoop.\nThe source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-filter  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    \n   Page listing the applications available on AppFactory is displayed.  Search for Kafka to see all applications related to Kafka.   Click on import button for  Kafka to HDFS Filter App .\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-HDFS-Filter  application. In the confirmation modal, click the Configure button.    The  Kafka-to-HDFS-Filter  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process all messages seperated by '|' from topic  transactions  at the kafka server running on  kafka-source.node.com  with port  9092 , filters the messages based on the filter criteria  ({$}.getAmount()  = 20000)  and write them to  output.txt  under  /user/appuser/output  on HDFS.\nProperties should be set as follows:     name  value      Output Directory Path  /user/appuser/output    Output File Name  output.txt    Kafka Broker List  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    Kafka Topic Name  transactions     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the application.\nA notification will be displayed at the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#prerequistes", 
            "text": "Kafka configured with version 0.9.", 
            "title": "Prerequistes"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt    dt.operator.filter.prop.condition  Filter condition  Condition  ({$}.getAmount()  = 20000)    dt.operator.kafkaInput.prop.clusters  Comma separated list of kafka-brokers  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.initialOffset  Initial offset to read from Kafka  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.kafkaInput.prop.topics  Topics to read from Kafka  String  event_data", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .\nThe messages or records emitted are specified by the value of the  TUPLE_CLASS  attribute in the configuration file namely  PojoEvent  in this case.     Property  Description  Type  Default for  cluster- memory - conf.xml  Default for  sandbox- memory  -conf.xml      dt.operator.fileOutput.prop.maxLength  Maximum length for output file after which file is rotated  long  Long.MAX_VALUE  Long.MAX_VALUE    dt.operator.csvParser.prop.schema  Schema for CSV Parser  Schema  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter.prop.schema  Schema for CSV formatter  Schema  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}  {\"separator\": \" \", \"quoteChar\": \"\\\"\", \"lineDelimiter\": \"\", \"fields\": [ { \"name\": \"accountNumber\",  \"type\": \"Integer\" },  { \"name\": \"name\", \"type\": \"String\" }, { \"name\": \"amount\", \"type\": \"Integer\" } ]}    dt.operator.formatter. port.in.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to CSV formatter  POJO  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent    dt.operator.filter.port.input.attr.TUPLE_CLASS  Fully qualified class name for the tuple class POJO(Plain old java objects) input to Filter  POJO  com.datatorrent.apps.PojoEvent  com.datatorrent.apps.PojoEvent     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-filter/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kafka-to-hdfs-filter':  cd examples/tutorials/kafka-to-hdfs-filter    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/", 
            "text": "Kafka to HDFS Sync Application\n\n\nSummary\n\n\nIngest messages from kafka to hadoop HDFS for continuous ingestion to hadoop. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppFactory tab from the top navigation bar. \n   \n \n   Page listing the applications available on AppFactory is displayed.\n\n\n\n\n\n\nSearch for Kafka to view all the applications related to Kafka.\n\n\n\n\n\n\nClick on import button for \nKafka to HDFS Sync App\n. \nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nKafka-to-HDFS-Sync\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nKafka-to-HDFS-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process all messages from topic \ntransactions\n at the kafka server running on localhost port 9092\nand write them to \noutput.txt\n under \n/user/appuser/output\n on HDFS. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nKafka Broker List\n\n\nkafka-server-node:9092\n\n\n\n\n\n\nKafka Topic Name\n\n\ntest\n\n\n\n\n\n\nOutput Directory Path\n\n\n/user/appuser/input\n\n\n\n\n\n\nOutput File Name\n\n\noutput.txt\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.filePath\n\n\nOutput path for HDFS\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\ndt.operator.fileOutput.prop.outputFileName\n\n\nOutput file name\n\n\nString\n\n\noutput.txt\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.clusters\n\n\nComma separated list of kafka-brokers\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.initialOffset\n\n\nInitial offset to read from Kafka\n\n\nString\n\n\nEARLIEST\nLATEST\nAPPLICATION_OR_EARLIEST\nAPPLICATION_OR_LATEST\n\n\n\n\n\n\ndt.operator.kafkaInput.prop.topics\n\n\nTopics to read from Kafka\n\n\nString\n\n\nevent_data\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\n cluster-\nmemory\n- conf.xml\n\n\nDefault for\n sandbox-\nmemory\n -conf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.fileOutput.prop.maxLength\n\n\nMaximum length for output file after which file is rotated\n\n\nlong\n\n\nLong.MAX_VALUE\n\n\nLong.MAX_VALUE\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the \nspecify custom property\n step within \nsteps to launch application\n.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kafka-to-hdfs-sync':\n\n\ncd examples/tutorials/kafka-to-hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kafka to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#kafka-to-hdfs-sync-application", 
            "text": "", 
            "title": "Kafka to HDFS Sync Application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#summary", 
            "text": "Ingest messages from kafka to hadoop HDFS for continuous ingestion to hadoop. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kafka-to-hdfs-sync.  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar. \n     \n   Page listing the applications available on AppFactory is displayed.    Search for Kafka to view all the applications related to Kafka.    Click on import button for  Kafka to HDFS Sync App . \nNotification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  Kafka-to-HDFS-Sync  application. In the confirmation modal, click the Configure button.    The  Kafka-to-HDFS-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process all messages from topic  transactions  at the kafka server running on localhost port 9092\nand write them to  output.txt  under  /user/appuser/output  on HDFS. Properties should be set as follows:     name  value      Kafka Broker List  kafka-server-node:9092    Kafka Topic Name  test    Output Directory Path  /user/appuser/input    Output File Name  output.txt     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      dt.operator.fileOutput.prop.filePath  Output path for HDFS  String  /user/appuser/output    dt.operator.fileOutput.prop.outputFileName  Output file name  String  output.txt    dt.operator.kafkaInput.prop.clusters  Comma separated list of kafka-brokers  String  node1.company.com:9098, node2.company.com:9098, node3.company.com:9098    dt.operator.kafkaInput.prop.initialOffset  Initial offset to read from Kafka  String  EARLIEST LATEST APPLICATION_OR_EARLIEST APPLICATION_OR_LATEST    dt.operator.kafkaInput.prop.topics  Topics to read from Kafka  String  event_data", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for  cluster- memory - conf.xml  Default for  sandbox- memory  -conf.xml      dt.operator.fileOutput.prop.maxLength  Maximum length for output file after which file is rotated  long  Long.MAX_VALUE  Long.MAX_VALUE     You can override default values for advanced properties by specifying custom values for these properties in the  specify custom property  step within  steps to launch application .", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/kafka-to-hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kafka-to-hdfs-sync':  cd examples/tutorials/kafka-to-hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/", 
            "text": "Kinesis to S3 Application\n\n\nSummary\n\n\nIngest messages from kinesis and write to S3 bucket.\nThe source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/kinesis-to-s3.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n   \n\n   The Page listing the applications available on AppFactory is displayed.\n\n\nSearch for Kinesis to see all applications related to Kinesis.\n\n\nClick on import button for \nKinesis to S3 App\n.\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n   \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nkinesis-to-S3\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nKinesis-to-S3\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to process all messages from Kinesis stream \ntransactions\n \n  and write them to \noutput.txt\n under \n/user/appuser/output\n on S3. Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\ntransactions\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nKINESIS_ACCESS_KEY\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nKINESIS_SECRET_KEY\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nKINESIS_END_POINT\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nS3_ACCESS_KEY\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nS3_SECRET_KEY\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nS3_BUCKET_NAME\n\n\n\n\n\n\nS3Output Directory Path\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n   \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n   \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status.\n   \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n   \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n   \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStream Name For Kinesis Input\n\n\nName of the stream from where the records to be fetched\n\n\nString\n\n\ntransactions\n\n\n\n\n\n\nAccess Key For Kinesis Input\n\n\nIndicates the accessKey which have read access to the Kinesis stream\n\n\nString\n\n\nKINESIS_ACCESS_KEY\n\n\n\n\n\n\nSecret Key For Kinesis Input\n\n\nIndicates the secret AccessKey which have read access to the Kinesis stream\n\n\nString\n\n\nKINESIS_SECRET_KEY\n\n\n\n\n\n\nEnd Point For Kinesis Input\n\n\nIndicates the endpoint of the kinesis stream\n\n\nString\n\n\nKINESIS_END_POINT\n\n\n\n\n\n\nAccess Key For S3Output\n\n\nAWS access key which have write access to the S3 bucket\n\n\nString\n\n\nS3_ACCESS_KEY\n\n\n\n\n\n\nSecret Access Key For S3Output\n\n\nAWS Secret key which have write access to the S3 bucket\n\n\nString\n\n\nS3_SECRET_KEY\n\n\n\n\n\n\nS3Output Bucket Name\n\n\nName of S3 bucket\n\n\nSting\n\n\nS3_BUCKET_NAME\n\n\n\n\n\n\nS3Output Directory Path\n\n\nOutput path for S3 files\n\n\nString\n\n\n/user/appuser/output\n\n\n\n\n\n\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variables\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to 'examples/tutorials/kinesis-to-s3':\n\n\ncd examples/tutorials/kinesis-to-s3\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages.\nClick on the \nLaunch\n button for the uploaded application package.  \n\nFollow the \nsteps\n for launching an application.", 
            "title": "Kinesis to S3 App"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#kinesis-to-s3-application", 
            "text": "", 
            "title": "Kinesis to S3 Application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#summary", 
            "text": "Ingest messages from kinesis and write to S3 bucket.\nThe source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/kinesis-to-s3.  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n    \n   The Page listing the applications available on AppFactory is displayed.  Search for Kinesis to see all applications related to Kinesis.  Click on import button for  Kinesis to S3 App .   Notification is displayed on the top right corner after application package is successfully\n   imported.\n       Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  kinesis-to-S3  application. In the confirmation modal, click the Configure button.    The  Kinesis-to-S3  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to process all messages from Kinesis stream  transactions  \n  and write them to  output.txt  under  /user/appuser/output  on S3. Properties should be set as follows:     name  value      Stream Name For Kinesis Input  transactions    Access Key For Kinesis Input  KINESIS_ACCESS_KEY    Secret Key For Kinesis Input  KINESIS_SECRET_KEY    End Point For Kinesis Input  KINESIS_END_POINT    Access Key For S3Output  S3_ACCESS_KEY    Secret Access Key For S3Output  S3_SECRET_KEY    S3Output Bucket Name  S3_BUCKET_NAME    S3Output Directory Path  /user/appuser/output     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after the application is launched successfully and includes the Application ID which can be used to monitor this instance and find its logs.\n       Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n       Application instance details page shows key metrics for monitoring the application status.\n    logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n       Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#mandatory-properties", 
            "text": "End user must specify the values for these properties.     Property  Description  Type  Example      Stream Name For Kinesis Input  Name of the stream from where the records to be fetched  String  transactions    Access Key For Kinesis Input  Indicates the accessKey which have read access to the Kinesis stream  String  KINESIS_ACCESS_KEY    Secret Key For Kinesis Input  Indicates the secret AccessKey which have read access to the Kinesis stream  String  KINESIS_SECRET_KEY    End Point For Kinesis Input  Indicates the endpoint of the kinesis stream  String  KINESIS_END_POINT    Access Key For S3Output  AWS access key which have write access to the S3 bucket  String  S3_ACCESS_KEY    Secret Access Key For S3Output  AWS Secret key which have write access to the S3 bucket  String  S3_SECRET_KEY    S3Output Bucket Name  Name of S3 bucket  Sting  S3_BUCKET_NAME    S3Output Directory Path  Output path for S3 files  String  /user/appuser/output", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/kinesis-to-s3/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variables   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to 'examples/tutorials/kinesis-to-s3':  cd examples/tutorials/kinesis-to-s3    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. Some tips are given as commented blocks in the Application.java for this project    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages.\nClick on the  Launch  button for the uploaded application package.   \nFollow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/", 
            "text": "S3 to HDFS Sync App\n\n\nSummary\n\n\nIngest and backup Amazon S3 data to hadoop HDFS for data download from Amazon to hadoop. This application copies files from the configured S3 location to the destination path in HDFS. The source code is available at: \nhttps://github.com/DataTorrent/app-templates/tree/master/s3-to-hdfs-sync\n.\n\n\nPlease send feedback or feature requests to: \nfeedback@datatorrent.com\n\n\nThis document has a step-by-step guide to configure, customize, and launch this application.\n\n\nSteps to launch application\n\n\n\n\n\n\nClick on the AppFactory tab from the top navigation bar.\n    \n\n\n\n\n\n\nPage listing the applications available on AppFactory is displayed. Search for S3 to see all applications related to S3.\n    \n\n    Click on import button for \nS3 to HDFS Sync App\n.\n\n\n\n\n\n\nNotification is displayed on the top right corner after application package is successfully\n   imported.\n    \n\n\n\n\n\n\nClick on the link in the notification which navigates to the page for this application package.\n\n\n\n\nDetailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for \nS3-to-HDFS-Sync\n application. In the confirmation modal, click the Configure button.\n\n\n\n\n\n\nThe \nS3-to-HDFS-Sync\n application configuration page is displayed. The Required Properties section must be completed before the application can be launched.\n\n\n\n\n\nFor example, suppose we wish to copy from all files in \ndirectory1\n from \ncom.example.s3test\n using \nACCESS_KEY_ID:SECRET_KEY\n combination to \n/user/appuser/output\n on the host cluster (on which app is running). Properties should be set as follows:\n\n\n\n\n\n\n\n\nname\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nAccess Key For S3Input\n\n\nACCESS_KEY_ID\n\n\n\n\n\n\nBucket Name For S3Input\n\n\nBUCKET_NAME\n\n\n\n\n\n\nInput Directory Or File Path On S3\n\n\n/directory1\n\n\n\n\n\n\nOutput Directory Path On HDFS\n\n\n/user/appuser/output/directory1\n\n\n\n\n\n\nSecret Key For S3Input\n\n\nSECRET_KEY\n\n\n\n\n\n\n\n\nDetails about configuration options are available in \nConfiguration options\n section.\n\n\n\n\n\n\nWhen you are finished inputting application configuration properties, click on the \nsave\n button on the top right corner of the page to save the configuration.\n\n\n\n\n\n\nClick on the \nlaunch\n button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n    \n\n\n\n\n\n\nClick on the \nMonitor\n tab from the top navigation bar.\n\n\n\n\n\n\nA page listing all running applications is displayed. Search for the current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n    \n\n\n\n\n\n\nApplication instance details page shows key metrics for monitoring the application status. \nlogical\n tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n    \n\n\n\n\n\n\nClick on the \nphysical\n tab to look at the status of physical instances of the operator, containers etc.\n    \n\n\n\n\n\n\nConfiguration options\n\n\nMandatory properties\n\n\nEnd user must specify the values for these properties (these properties are all strings).\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.HDFSFileCopyModule.prop.outputDirectoryPath\n\n\nHDFS path for destination directory\n\n\n/user/appuser/\n output/directory1\nhdfs://node1.company1.com\n /user/appuser/output\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.files\n\n\nAccess URL for S3 source\n\n\ns3n://ACCESS_KEY_ID:SECRET_KEY\n@BUCKET_NAME/DIRECTORY\n\n\n\n\n\n\n\n\nAdvanced properties\n\n\nThere are pre-saved configurations based on the application environment. Recommended settings for \ndatatorrent sandbox\n are in \nsandbox-memory-conf.xml\n and for a cluster environment in \ncluster-memory-conf.xml\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nDefault for\ncluster-\nmemory-\n conf.xml\n\n\nDefault for\n  sandbox\n-memory -\nconf.xml\n\n\n\n\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.maxReaders\n\n\nMaximum number of BlockReader partitions for parallel reading.\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\ndt.operator.S3InputModule.prop.blocksThreshold\n\n\nRate at which block metadata is emitted per second\n\n\nint\n\n\n16\n\n\n1\n\n\n\n\n\n\n\n\nYou can override default values for advanced properties by specifying custom values for these properties in the step \nspecify custom property\n step mentioned in \nsteps\n to launch an application.\n\n\nSteps to customize the application\n\n\n\n\n\n\nMake sure you have following utilities installed on your machine and available on \nPATH\n in environment variable:\n\n\n\n\nJava\n : 1.7.x\n\n\nmaven\n : 3.0 +\n\n\ngit\n : 1.7 +\n\n\nHadoop\n (Apache-2.2)+\n\n\n\n\n\n\n\n\nUse following command to clone the examples repository:\n\n\ngit clone git@github.com:DataTorrent/app-templates.git\n\n\n\n\n\n\nChange directory to \nexamples/tutorials/s3-to-hdfs-sync\n:\n\n\ncd examples/tutorials/s3-to-hdfs-sync\n\n\n\n\n\n\nImport this maven project in your favorite IDE (e.g. eclipse).\n\n\n\n\n\n\nChange the source code as per your requirements. This application is for copying files from source to destination. Thus, \nApplication.java\n does not involve any processing operator in between.\n\n\n\n\n\n\nMake respective changes in the test case and \nproperties.xml\n based on your environment.\n\n\n\n\n\n\nCompile this project using maven:\n\n\nmvn clean package\n\n\nThis will generate the application package with \n.apa\n extension in the \ntarget\n directory.\n\n\n\n\n\n\nGo to DataTorrent UI Management console on web browser. Click on the \nDevelop\n tab from the top navigation bar.\n\n\n\n\n\n\nClick on \nApplication Packages\n from the list.\n\n\n\n\n\n\nClick on \nupload package\n button and upload the generated \n.apa\n file.\n   \n\n\n\n\n\n\nApplication package page is shown with the listing of all packages. Click on the \nLaunch\n button for the uploaded application package. Follow the \nsteps\n for launching an application.", 
            "title": "S3 to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#s3-to-hdfs-sync-app", 
            "text": "", 
            "title": "S3 to HDFS Sync App"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#summary", 
            "text": "Ingest and backup Amazon S3 data to hadoop HDFS for data download from Amazon to hadoop. This application copies files from the configured S3 location to the destination path in HDFS. The source code is available at:  https://github.com/DataTorrent/app-templates/tree/master/s3-to-hdfs-sync .  Please send feedback or feature requests to:  feedback@datatorrent.com  This document has a step-by-step guide to configure, customize, and launch this application.", 
            "title": "Summary"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#steps-to-launch-application", 
            "text": "Click on the AppFactory tab from the top navigation bar.\n        Page listing the applications available on AppFactory is displayed. Search for S3 to see all applications related to S3.\n     \n    Click on import button for  S3 to HDFS Sync App .    Notification is displayed on the top right corner after application package is successfully\n   imported.\n        Click on the link in the notification which navigates to the page for this application package.   Detailed information about the application package like version, last modified time, and short description is available on this page. Click on launch button for  S3-to-HDFS-Sync  application. In the confirmation modal, click the Configure button.    The  S3-to-HDFS-Sync  application configuration page is displayed. The Required Properties section must be completed before the application can be launched.   \nFor example, suppose we wish to copy from all files in  directory1  from  com.example.s3test  using  ACCESS_KEY_ID:SECRET_KEY  combination to  /user/appuser/output  on the host cluster (on which app is running). Properties should be set as follows:     name  value      Access Key For S3Input  ACCESS_KEY_ID    Bucket Name For S3Input  BUCKET_NAME    Input Directory Or File Path On S3  /directory1    Output Directory Path On HDFS  /user/appuser/output/directory1    Secret Key For S3Input  SECRET_KEY     Details about configuration options are available in  Configuration options  section.    When you are finished inputting application configuration properties, click on the  save  button on the top right corner of the page to save the configuration.    Click on the  launch  button at the top right corner of the page to launch the application.\nA notification will be displayed on the top right corner after application is launched successfully and includes the Application ID which can be used to monitor this instance and to find its logs.\n        Click on the  Monitor  tab from the top navigation bar.    A page listing all running applications is displayed. Search for the current application based on name or application id or any other relevant field. Click on the application name or id to navigate to application instance details page.\n        Application instance details page shows key metrics for monitoring the application status.  logical  tab shows application DAG, Stram events, operator status based on logical operators, stream status, and a chart with key metrics.\n        Click on the  physical  tab to look at the status of physical instances of the operator, containers etc.", 
            "title": "Steps to launch application"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#configuration-options", 
            "text": "", 
            "title": "Configuration options"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#mandatory-properties", 
            "text": "End user must specify the values for these properties (these properties are all strings).     Property  Description  Example      dt.operator.HDFSFileCopyModule.prop.outputDirectoryPath  HDFS path for destination directory  /user/appuser/  output/directory1 hdfs://node1.company1.com  /user/appuser/output    dt.operator.S3InputModule.prop.files  Access URL for S3 source  s3n://ACCESS_KEY_ID:SECRET_KEY @BUCKET_NAME/DIRECTORY", 
            "title": "Mandatory properties"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#advanced-properties", 
            "text": "There are pre-saved configurations based on the application environment. Recommended settings for  datatorrent sandbox  are in  sandbox-memory-conf.xml  and for a cluster environment in  cluster-memory-conf.xml .     Property  Description  Type  Default for cluster- memory-  conf.xml  Default for   sandbox -memory - conf.xml      dt.operator.S3InputModule.prop.maxReaders  Maximum number of BlockReader partitions for parallel reading.  int  16  1    dt.operator.S3InputModule.prop.blocksThreshold  Rate at which block metadata is emitted per second  int  16  1     You can override default values for advanced properties by specifying custom values for these properties in the step  specify custom property  step mentioned in  steps  to launch an application.", 
            "title": "Advanced properties"
        }, 
        {
            "location": "/app-templates/s3-to-hdfs-sync/#steps-to-customize-the-application", 
            "text": "Make sure you have following utilities installed on your machine and available on  PATH  in environment variable:   Java  : 1.7.x  maven  : 3.0 +  git  : 1.7 +  Hadoop  (Apache-2.2)+     Use following command to clone the examples repository:  git clone git@github.com:DataTorrent/app-templates.git    Change directory to  examples/tutorials/s3-to-hdfs-sync :  cd examples/tutorials/s3-to-hdfs-sync    Import this maven project in your favorite IDE (e.g. eclipse).    Change the source code as per your requirements. This application is for copying files from source to destination. Thus,  Application.java  does not involve any processing operator in between.    Make respective changes in the test case and  properties.xml  based on your environment.    Compile this project using maven:  mvn clean package  This will generate the application package with  .apa  extension in the  target  directory.    Go to DataTorrent UI Management console on web browser. Click on the  Develop  tab from the top navigation bar.    Click on  Application Packages  from the list.    Click on  upload package  button and upload the generated  .apa  file.\n       Application package page is shown with the listing of all packages. Click on the  Launch  button for the uploaded application package. Follow the  steps  for launching an application.", 
            "title": "Steps to customize the application"
        }, 
        {
            "location": "/omni_channel_fraud_app/", 
            "text": "Omni Channel Fraud Prevention Application User Guide\n\n\nOverview\n\n\nOmni Channel Fraud Prevention application is a pre-built application which is used to identify frauds in financial transactions. This application is designed to ingest, transform, analyze, act, and visualize data as soon as it is generated thereby preventing fraud transactions in real time that is before it happens.\n\n\nIt is built over DataTorrent RTS platform and can be run on commodity hardware. The platform gives real time insights while providing fault tolerant and scalable way of processing. The fraud identification can be further customized by writing rules as per your business needs. Support is provided for editing rules using CEP Workbench.\n\n\nThis application is integrated with Online Analytical Service (OAS) for OLAP historical and real-time trend analysis. Real-time analytics are provided for continuous KPI visualizations along with integrated alert capabilities for user-defined thresholds.\n\n\nThe application dashboards are based on Apache SuperSet with enhanced ability to visualize and analyze trends.\n\n\nThe following services come preinstalled with the application. You can run these services for analytics, visualizing the analytic outcomes, and for creating customized rules.\n\n\n\n\nOnline Analytics Service\n\n\nOAS Dashboards\n\n\nCEP Workbench\n\n\n\n\nYou can also integrate the application backplane to share the fraud alerts with other fraud-based applications thereby reducing the chance of fraud.\n\n\nIn addition to this, you can evaluate what-if scenarios on the incoming data which can be stored and visualized over different rules and business logic.\nOmni Channel Fraud Prevention application is available only with \nDT Premium license\n.\n\n\nQuick Launch Omni-Channel Fraud Prevention Application\n\n\nFor details about quickly launching the application, refer to \nQuick Start Guide - Omni-Channel Fraud Prevention Application\n.  \n\n\nWorkflow\n\n\nThe following image presents the workflow of the application.\n\n\n\nThe following table lists the various processing phases of the incoming data tuples/event in \nOmni Channel Fraud Prevention\n application:\n\n\n\n\n\n\n\n\nPhase\n\n\nProcess\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1.\n\n\nTransaction Ingestion\n\n\nData from all your input sources is pushed onto a message bus such as Apache kafka. The application leverages platform capabilities to ingest data from Kafka message bus to rest of its pipeline. Data tuples are filtered and transformed. The following operators are involved during Transaction ingestion: Kafka Input Operator, Filter Operator and Transformation Operator\n\n\n\n\n\n\n2\n\n\nTransaction Enrichment\n\n\nData is enriched with the lookup information such as user profile data, card data, geo data, store profile data etc.The following operators are involved in the transaction enrichment phase: Enricher, Geo Data, Customer Data, Card Data, and Product Data.  By default, these are the enrichment operators, you can modify, add, or remove these operators by launching the application with a different application configuration. Refer to \nEnriching Data\n\n\n\n\n\n\n3\n\n\nFraud Rule Execution\n\n\nFraud detection rules can be applied to the enriched data using the CEP Workbench. The following operator are involved in the Fraud Rule Execution phase: Fraud Rule Executor\n\n\n\n\n\n\n4\n\n\nAnalytics\n\n\nOAS and Metrics\n\n\n\n\n\n\n5\n\n\nFraud Triggers and Alerts\n\n\nTriggers are activated based on the output of the upstream processing. The processed transactions are written onto HDFS to run any historical analysis in future. Also, transactions are published on message bus such as kafka to take real time actions. The application is also integrated with alerting systems that send real time alerts to applicable users.  Email alerts are sent by default. The following operator is involved in the Fraud Triggers and Alerts phase: Alert Mail, Data Writer, and Kafka Output operator\n\n\n\n\n\n\n\n\nOperators / Modules\n\n\nThe following operators/modules are included for the Fraud Prevention application.\n\n\n\n\n\n\n\n\nOperator / Module\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nTransaction Receiver\n\n\nThis Kafka input operator receives the transactions.s. It forwards these transactions to downstream operator.\n\n\n\n\n\n\nTransaction Parser\n\n\nThis JSON parser operator parses the incoming transaction messages  and converts them into plain java objects hereafter referred as tuple for further processing.\n\n\n\n\n\n\nTransactionTransformer\n\n\nTransforms tuple fields as per given configuration e.g. fetch deviceIp from mobile details or web details based on transaction source\n\n\n\n\n\n\nTransactionValidator\n\n\nValidates tuples based on configuration e.g. all transactions with transaction amount more than 100$\n\n\n\n\n\n\nUser Profile Enricher\n\n\nThis operator gets the relevant JAVA applicable user details corresponding to a unique ID and enriches the tuple. Using this operator is optional in an FPA application. Refer \nUser Data Enricher\n\n\n\n\n\n\nGeo Data Enricher\n\n\nThe application identifies the geolocation of the transaction by performing a lookup of the transaction IP against the external database like Maxmind database.  Using this operator is optional in an FPA application. Refer \nGeo Data Enricher\n\n\n\n\n\n\nCardDataEnricher\n\n\nThis operator gets relevant card details and enriches tuple. Using this operator is optional in an FPA application. Refer \nCard Data Enricher\n\n\n\n\n\n\nProductDataEnricher\n\n\nThis operator gets relevant product details and enriches tuple.Using this operator is optional in an FPA application. Refer \nProduct Data Enricher\n\n\n\n\n\n\nInput Module\n\n\nInput module consists of two operators: \nKafka Input Operator\n- This operator emits the byte array from Kafka topic whose properties are set on the module. These properties must be the same ones set on the corresponding output module whose messages are subscribed. You can also indicate which offset to start reading messages from kafka. \nAvro Deserializer\n - This operator does the deserialization of the schema set on the module. The schema must be the same one set on corresponding output module whose messages are subscribed.The deserialized class should be in classpath. This will be achieved by querying schema repository to include the jar of the class. Omni-Channel Fraud prevention application. Refer \nApplication Backplane\n\n\n\n\n\n\nAccountDataEnricher\n\n\nThis operator gets relevant user account security information from Account takeover prevention application using application backplane.\n\n\n\n\n\n\nFraud Rules Executor\n\n\nThis operator is the Drools Operator. It applies the pre-defined rules to the incoming tuples and takes a suitable action depending on the outcome of the rules applied to a tuple.\n\n\n\n\n\n\nHDFS Output Operator\n\n\nThis output operator writes messages coming from the Rules Executor to the specified HDFS file path. Using this operator is optional in an FPA application.\n\n\n\n\n\n\nAOO Operator\n\n\nThis operator writes messages to a Kafka topic that are consumed by Online Analytics Service (OAS).\n\n\n\n\n\n\n\n\nSetting the Application\n\n\nBefore you run the \nOmni Channel Fraud Prevention\n application, you must ensure to fulfill the prerequisites and to configure the enrichment.\n\n\nPrerequisites\n\n\nThe following should be installed on the computer before setting up the application:\n\n\n\n\n\n\n\n\nProduct\n\n\nVersion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nApache Hadoop\n\n\n2.6.0 and Above\n\n\nApache Hadoop is an open-source software framework that is used for distributed storage and processing of dataset of big data using the MapReduce programming model.\n\n\n\n\n\n\nDataTorrent RTS\n\n\n3.10\n\n\nDataTorrent RTS, which is built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion, and distribution features.\n\n\n\n\n\n\nApache Kafka\n\n\n0.9\n\n\nApache Kafka is an open-source stream processing platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.\n\n\n\n\n\n\nMaxmind\n\n\n\n\nIf you are using Geo data enrichment, ensure to copy the maxmind city database on HDFS and configure the operator with the location. For more details, refer to \nConfiguring the Properties\n section.\n\n\n\n\n\n\n\n\nEnriching Data\n\n\nMissing fields from your incoming records can be enriched with the lookup data. Enricher operators can be used to enrich the incoming data. You can add custom enricher operators or edit the existing default operators from the \nenrichments.json\n file on the HDFS. The path of the \nenrichments.json\n file must be set before you launch the application. This can be done by adding the \ndt.fraudprevention.enrichments.configFilePath\n property to the application configuration and setting its value to the \nenrichments.json\n file path.\n\n\nBy default, the Fraud Prevention application has the following enrichers:\n\n\n\n\nUser Data Enricher\n\n\nGeo Data Enricher\n\n\nCard Data Enricher\n\n\nProduct Data Enricher\n\n\n\n\nConfiguring the Properties\n\n\nBefore launching the Fraud Prevention application, you can set the application configurations as per your requirement. The properties of the following items can be added to an application configuration during launch.\n\n\n\n\nApplication Properties\n\n\nKafka Input\n\n\nParser\n\n\nTransformer\n\n\nValidator\n\n\nEnrichment Operators\n\n\nOutput Module\n\n\nFraud Rules Executor\n\n\nAOO Operator\n\n\nData Writer\n\n\nAlert Mail\n\n\n\n\nApplication Properties\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.fraudprevention.enableOutputOperators\n\n\nShould write output to Kafka and HDFS\n\n\nboolean\n\n\nFALSE\n\n\n\n\n\n\n\n\nKafka\n\n\nThe \nOmni Channel Fraud Prevention\n application reads input data from kafka and sends output result to kafka. Therefore, you must ensure that your kafka setup is up and running.\n\n\nConfigure your kafka setup details in application properties file.  The following properties must be set:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\napex.app-param.kafkaBrokerList\n\n\nComma separated list of kafka-brokers.\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\nYes\n\n\n\n\n\n\napex.app-param.TransactionReceiverTopic\n\n\nTopics to read from Kafka.\n\n\nString\n\n\ntransactions\n\n\nYes\n\n\n\n\n\n\napex.app-param.FraudTransactionsOutputTopic\n\n\nTopics to write fraud transactions to Kafka.\n\n\nString\n\n\nfraud-transactions\n\n\nYes\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.initialOffset\n\n\nInitial offset to read from Kafka.\n\n\nString\n\n\n- EARLIEST, LATEST, APPLICATION_OR_EARLIEST, APPLICATION_OR_LATEST\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndt.operator.FraudResultPublisher.prop.properties(key.serializer)\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\ndt.operator.FraudResultPublisher.prop.properties(value.serializer)\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\n\n\nParser\n\n\nParser parses json input from kafka and generates plain java object for further processing.\n\n\nConfigure the java class which of plain java object to be generated by parser. The following properties must be set for parser:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionParser.port.out.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO (Plain old java objects) output to generate by parser\n\n\nString\n\n\ncom.datatorrent.cep.schema.Transaction\n\n\n\n\n\n\n\n\nTransformer\n\n\nThe Transformer operator transforms the input fields as per requirements. The following properties must be set for transformer:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionTransformer.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO (Plain old java objects) input by transformer.\n\n\nString\n\n\ncom.datatorrent.cep.schema.Transaction\n\n\n\n\n\n\ndt.operator.TransactionTransformer.port.output.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO(Plain old java objects) output by transformer.\n\n\nString\n\n\ncom.datatorrent.cep.schema.Transaction\n\n\n\n\n\n\ndt.operator.TransactionTransformer.expressionMap(deviceIP)\n\n\nTransform expression. \nNote\n : This property should be left blank, when you use custom schema.\n\n\nCondition Expression\n\n\n{$.mobileDetails} != null? {$.mobileDetails.deviceIP} : {$.webDetails} != null? {$.webDetails.deviceIP} : \n\n\n\n\n\n\ndt.operator.TransactionTransformer.expressionMap(processingStartTime)\n\n\nTransform expression. \nNote\n : This property should be left blank, when you use custom schema.\n\n\nCondition Expression\n\n\n{System.currentTimeMillis()}\n\n\n\n\n\n\n\n\n Validator / Filter\n\n\nValidator operator filters records as per the given condition. Following properties need to be set for validator:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionValidator.port.input.attr.TUPLE_CLASS\n\n\nFully qualified class name for the tuple class POJO (Plain old java objects) input by validator\n\n\nString\n\n\ncom.datatorrent.cep.schema.Transaction\n\n\n\n\n\n\ndt.operator.TransactionValidator.prop.condition\n\n\nFilter condition\n\n\nCondition Expression\n\n\n({$}.getTransAmount() \n= 1000)\n\n\n\n\n\n\n\n\nEnriching Operators\n\n\nFollowing are the enrichment operators for Fraud Prevention application.\n\n\nUser Data Enricher \n\n\nUser details are important to evaluate transactions for potential frauds.\n\n\nFor example, customer home address, spending patterns, risk score, and so on can be used to decide if a specific transaction is fraud. Jdbc store or json file is supported for lookup of customer data.\n\n\nIf you do not have a database, you can use file input source. Put your json input file on HDFS and configure it accordingly.\n\n\nYou can add the properties that are shown in the following json format into the \nenrichment.json\n file.\n\n\n {\n    \nname\n: \nUserProfileEnricher\n,\n    \nstoreType\n : \njson_file\n,\n    \nstoreFields\n : [\n      { \nname\n : \ncardNumber\n, \ntype\n : \nlong\n },\n      { \nname\n: \ncustomerType\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustomerAvgSpending\n, \ntype\n : \ndouble\n },\n      { \nname\n : \ncustomerRiskScore\n, \ntype\n : \ndouble\n },\n      { \nname\n : \ncustGender\n, \ntype\n : \nstring\n },\n      { \nname\n: \ncustMaritalStatus\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustIncomeLevel\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustStreet1\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustStreet2\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustCity\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustState\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustCountry\n, \ntype\n : \nstring\n },\n      { \nname\n : \ncustPoBox\n, \ntype\n : \nlong\n },\n      { \nname\n : \ncustPostalCode\n, \ntype\n : \nlong\n },\n      { \nname\n : \ncustPostalCodeType\n, \ntype\n : \nstring\n },\n      { \nname\n : \nlat\n, \ntype\n : \ndouble\n },\n      { \nname\n : \nlon\n, \ntype\n : \ndouble\n },\n      { \nname\n : \nuserId\n, \ntype\n : \nstring\n }\n    ],\n    \ninputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \noutputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \nreuseObject\n : true,\n    \nproperties\n: {\n      \nfile\n : \nlookupdata/customers.json\n,\n      \nrefreshInterval\n : 5000\n    },\n    \nlookupFields\n : {\n      \ncardNumber\n : \ncardNumber\n\n    },\n    \nincludeFields\n : {\n      \ncustomer.cardNumber\n : \ncardNumber\n,\n      \ncustomer.customerType\n : \ncustomerType\n,\n      \ncustomer.customerAvgSpending\n : \ncustomerAvgSpending\n,\n      \ncustomer.customerRiskScore\n : \ncustomerRiskScore\n,\n      \ncustomer.custStreet1\n : \ncustStreet1\n,\n      \ncustomer.custStreet2\n : \ncustStreet2\n,\n      \ncustomer.custCity\n : \ncustCity\n,\n      \ncustomer.custState\n : \ncustState\n,\n      \ncustomer.custCountry\n : \ncustCountry\n,\n      \ncustomer.custPoBox\n : \ncustPoBox\n,\n      \ncustomer.custPostalCode\n : \ncustPostalCode\n,\n      \ncustomer.custPostalCodeType\n : \ncustPostalCodeType\n,\n      \ncustomer.userId\n : \nuserId\n\n    }\n  }\n\n\n\n\nNote:\n Ensure that the database connection driver jar, if any, is in classpath. The driver jar can be added in classpath using config package.\n\n\nGeo Data Enricher\n\n\nGeoData Enrichment operator refers maxmind database to fetch geo information provided ip address of transaction location. Copy the maxmind city database (GeoLite2 City) to HDFS.\n\n\nNote:\n Extract GeoLite2-City.mmdb file to load on HDFS, don\nt copy the zip file directly.\n\n\nYou can add the properties that are shown in the following json format into the \nenrichment.json\n file.\n\n\n{\n    \nname\n: \nGeoDataEnricher\n,\n    \npassThroughOnError\n : true,\n    \nstoreType\n: \ngeo_mmdb\n,\n    \nstoreFields\n: [\n      { \nname\n : \nIP\n, \ntype\n : \nstring\n },\n      { \nname\n: \nCITY\n, \ntype\n: \nstring\n },\n      { \nname\n : \nSUBDIVISION_ISO\n, \ntype\n : \nstring\n },\n      { \nname\n: \nZIPCODE\n, \ntype\n: \nstring\n },\n      { \nname\n: \nCOUNTRY_ISO\n, \ntype\n: \nstring\n },\n      { \nname\n : \nLATITUDE\n, \ntype\n : \ndouble\n },\n      { \nname\n : \nLONGITUDE\n, \ntype\n : \ndouble\n }\n    ],\n    \ninputType\n: \ncom.datatorrent.cep.schema.Transaction\n,\n    \noutputType\n: \ncom.datatorrent.cep.schema.Transaction\n,\n    \nreuseObject\n: true,\n    \nproperties\n: {\n      \ndbpath\n: \ncity.mmdb\n,\n      \nrefreshInterval\n: 5000\n    },\n    \nlookupFields\n: {\n      \nIP\n: \ndeviceIP\n\n    },\n    \nincludeFields\n: {\n      \ngeoIp.city\n : \nCITY\n,\n      \ngeoIp.state\n : \nSUBDIVISION_ISO\n,\n      \ngeoIp.zipcode\n : \nZIPCODE\n,\n      \ngeoIp.country\n : \nCOUNTRY_ISO\n,\n      \ngeoIp.latitude\n : \nLATITUDE\n,\n      \ngeoIp.longitude\n : \nLONGITUDE\n\n    }\n  }\n\n\n\n\nCard Data Enricher\n\n\nLookup the card data in your database. If you do not have a database you can use file input source. Please put your json input file on HDFS and configure operator accordingly.\n\n\nYou can add the properties that are shown in the following json format into the \nenrichment.json\n file.\n\n\n{\n    \nname\n : \nCardDataEnricher\n,\n    \nstoreType\n : \njson_file\n,\n    \nstoreFields\n : [\n      { \nname\n: \ncardType\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardNumber\n, \ntype\n: \nlong\n },\n      { \nname\n: \ncardCVV\n, \ntype\n: \ninteger\n },\n      { \nname\n: \ncardExpDate\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardName\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardDisplayNumber\n, \ntype\n: \nlong\n },\n      { \nname\n: \ncardNetwork\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardCompany\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardCompanyCountry\n, \ntype\n: \nstring\n },\n      { \nname\n: \ncardLimit\n, \ntype\n: \ndouble\n },\n      { \nname\n: \ncardBalance\n, \ntype\n: \ndouble\n }\n    ],\n    \ninputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \noutputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \nreuseObject\n : true,\n    \nproperties\n: {\n      \nfile\n : \nlookupdata/cards.json\n,\n      \nrefreshInterval\n : 5000\n    },\n    \nlookupFields\n : {\n      \ncardNumber\n : \ncardNumber\n\n    },\n    \nincludeFields\n : {\n      \npaymentCard.cardType\n : \ncardType\n,\n      \npaymentCard.cardNumber\n : \ncardNumber\n,\n      \npaymentCard.cardExpDate\n : \ncardExpDate\n,\n      \npaymentCard.cardName\n : \ncardName\n,\n      \npaymentCard.cardDisplayNumber\n : \ncardDisplayNumber\n,\n      \npaymentCard.cardCompany\n : \ncardCompany\n,\n      \npaymentCard.cardCompanyCountry\n : \ncardCompanyCountry\n,\n      \npaymentCard.cardLimit\n : \ncardLimit\n,\n      \npaymentCard.cardBalance\n : \ncardBalance\n\n    }\n  }\n\n\n\n\nNote\n You must ensure that the database connection driver jar, if any, is in classpath. The driver jar can be added in classpath using config package.\n\n\nProduct Data Enricher\n\n\nLookup the product details in your json records that are stored in HDFS files. You can add the properties that are shown in the following json format into the \nenrichment.json\n file.\n\n\n{\n    \nname\n : \nProductDataEnricher\n,\n    \ntype\n : \nlist\n,\n    \nstoreType\n : \njson_file\n,\n    \nstoreFields\n : [\n      { \nname\n : \nproductId\n, \ntype\n : \nstring\n },\n      { \nname\n : \nproductName\n, \ntype\n : \nstring\n },\n      { \nname\n : \nproductDescription\n, \ntype\n : \nstring\n }\n    ],\n    \ninputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \noutputType\n : \ncom.datatorrent.cep.schema.Transaction\n,\n    \nreuseObject\n : true,\n    \nproperties\n: {\n      \nfile\n : \nlookupdata/products.json\n,\n      \nrefreshInterval\n : 5000\n    },\n    \nlistConf\n: {\n      \nelementType\n : \ncom.datatorrent.cep.schema.Product\n,\n      \nkeyListGetter\n : \nproductIdList\n,\n      \noutputContainerGetter\n : \nproducts\n\n    },\n    \nlookupFields\n : {\n      \nproductId\n : \n{$}\n\n    },\n    \nincludeFields\n : {\n      \nproductId\n : \nproductId\n,\n      \nproductName\n : \nproductName\n,\n      \nproductDescription\n : \nproductDescription\n\n    }\n  }\n\n\n\n\nOutput Module\n\n\nThe following properties must be set for the output module. Also refer \nApplication Backplane\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.fraudprevention.inputModuleSchemaName\n\n\nSet schema for the data.\n\n\nstring\n\n\nUserActivity\n\n\n\n\n\n\ntopic\n\n\nSet the kafka topic name\n\n\nstring\n\n\nATO_analyseddata\n\n\n\n\n\n\n\n\nFraud Rules Executor\n\n\nThe Fraud Rules Executor that is the Drools operator provides a method to load rules from:\n\n\n\n\nHDFS\n\n\nCEP Workbench\n\n\n\n\nIf rules are loaded from files on HDFS, you must configure the following property:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\n\n\n\n\n\n\n\n\nrulesDir\n\n\nPath to HDFS from where to load the rules. If this path is set to null, then the operator loads the rules from the classpath.\n\n\nstring\n\n\n\n\n\n\n\n\nTo load rules from CEP Workbench, you must specify following properties and then specify the following properties in the application configuration. Refer to  \nCEP Workbench\n  \n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nkieSessionName\n\n\nIf rules are to be loaded from application classpath, then specify the name of the session to use. This is created using CEP Workbench\n\n\nstring\n\n\nUserActivity-rules-session\n\n\n\n\n\n\nkiebaseName\n\n\nIf rules are to be loaded from application classpath, then specify the name of the kie base (rule) to use. This is created using CEP Workbench\n\n\nstring\n\n\nato-rules\n\n\n\n\n\n\n\n\nNote:\n If rules are to be loaded from application classpath, the knowledge jar (KJAR) should be in the classpath. Refer to \nApplication Configurations\n.\n\n\nAOO Operator\n\n\nThis operator writes messages to a Kafka topic that are consumed by the OAS (Online Analytics Service). The following properties should be set:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nschema\n\n\nSchema / metadata of the data to be sent to OAS.By default we package \nanalyticsschema.json\n schema to change schema copy your schema file to hdfs and configure, \ndt.fraudprevention.analytics.resourceFileName\n with your schema file path.\n\n\nstring\n\n\nanalyticsschema.json\n\n\n\n\n\n\nserializerClass\n\n\nProvides information about serializing incoming messages in the form of JAVA objects to send to Kafka\n\n\nstring\n\n\ncom.datatorrent.cep.common.ToStringAnalyticsPojoSerializer\n\n\n\n\n\n\ndisablePartialWindowCheck\n\n\nSet whether to disable partition window check or not.   \nNote\n : By disabling the partition window check duplicate data can be sent to Kafka thereby overriding exactly once guarantees.\n\n\nboolean\n\n\nTrue\n\n\n\n\n\n\n\n\nData Writer\n\n\nProcessed transactions and Fraud transactions are written on HDFS files. Specify the file location and the file name. The following properties must be set for data writer:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.FraudResultFileWriter.prop.outputFileName\n\n\nFraud data file Name\n\n\nString\n\n\nfraudOutput.txt\n\n\n\n\n\n\ndt.operator.FraudResultFileWriter.prop.filePath\n\n\nFraud data folder path\n\n\nString\n\n\n/tmp\n\n\n\n\n\n\ndt.operator.TransactionDataWriter.prop.outputFileName\n\n\nTransaction data file Name\n\n\nString\n\n\nresultOutput.txt\n\n\n\n\n\n\ndt.operator.TransactionDataWriter.prop.filePath\n\n\nTransaction data folder path\n\n\nString\n\n\n/tmp\n\n\n\n\n\n\n\n\nAlert Mail\n\n\nWhen a fraud is identified in the system, notifications can be send over mail.\n\n\nNote:\n This feature is applicable only in demo mode as of now. To activate, set demo value to true in input tuple.\n\n\nThe following properties must be set for alert mail:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.from\n\n\nEmail Sender\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.subject\n\n\nEmail Subject Line\n\n\nString\n\n\nFraud Detected\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.content\n\n\nEmail Body Text\n\n\nString\n\n\nFraud Detected CEP \nlist of transactionIds\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.smtpHost\n\n\nSMTP Host\n\n\nString\n\n\nsmtp.gmail.com\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.smtpPort\n\n\nSMTP Port\n\n\nInteger\n\n\n587\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.recipients(to)\n\n\nRecipients List\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.smtpUserName\n\n\nSMTP UserName\n\n\nString\n\n\n\n\n\n\n\n\ndt.operator.FraudResultMailer.prop.smtpPasword\n\n\nSMTP Password\n\n\nString\n\n\n***\n\n\n\n\n\n\n\n\nScaling the Application\n\n\nTo handle higher data loads, you can add more partitions of the processing units i.e. operators.\n\n\nUpdate following properties as per your input load. The following properties must be set for scaling the application:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionReceiver.initialPartitionCount\n\n\nPartition count of Kafka data receiver\n\n\nInteger\n\n\n1\n\n\n\n\n\n\ndt.operator.FraudRulesExecutor.attr.PARTITIONER\n\n\nPartition count of Rule execution opeartor\n\n\nInteger\n\n\n1\n\n\n\n\n\n\ndt.operator.RuleAndTransactionsWriter.attr.PARTITIONER\n\n\nPartition count of file writer operators\n\n\nInteger\n\n\n1\n\n\n\n\n\n\ndt.operator.TransactionDataWriter.partitionedFileNameformat\n\n\nFile Name Format for transaction writer partition\n\n\nString\n\n\n%s-%04d\n\n\n\n\n\n\ndt.operator.FraudResultFileWriter.partitionedFileNameformat\n\n\nFile Name Format for fraud writer partition\n\n\nString\n\n\n%s-%04d\n\n\n\n\n\n\n\n\nRunning the Application\n\n\nThe Fraud Prevention application can be launched from the DataTorrent RTS interface.\n\n\nTo run the application, do the following:\n\n\n\n\nGo to the \nDevelop\n page and upload the application package.\n\n\nSpecify the configuration as per the \nApplication Configuration\n. Refer to \nApplication Configurations\n\n\nLaunch the application. Refer to \nLaunching Applications and Configurations\n\nDuring the launch process, you can name the configuration and save it for future references. After you launch, you can track the details of the processed data in the application from the \nMonitor\n tab.\n\n\n\n\nGenerating Sample Input\n\n\nFor a test run, you may want to generate sample data. To generate sample data, run the \nOmniChannelFraudPreventionDataGenerator\n application from \ndt-cep-omni-fraud-prevention-app-1.4.0.apa\n. Specify kafka server details and topic name which must be the same as configured for Transaction Receiver.\n\n\nRunning Services\n\n\nFraud Prevention application can rely on the following services as dependencies, and all the required services will be automatically installed and launched as needed when the application starts. More details refer Services.\n\n\nFor example when you launch the  \nOmni Channel Fraud Prevention\n application for the first time, the following services are automatically installed and launched along with it:\n\n\n\n\nOnline Analytics Service\n\n\nOAS Dashboards\n\n\nCEP Workbench\n\n\n\n\nStoring and Replaying Data\n\n\nA service is available for Fraud Prevention application to store and replay events. You can record and replay data from a point in time to evaluate the effectiveness of builds, models, rules, and scenarios before they are put into production. Refer \nStore and Replay\n\n\nSyncing with the Application Backplane\n\n\nApplication Backplane enables multiple applications to be integrated to share insights and actions. Fraud Prevention application is combined with Account Takeover Prevention application in real-time. Both these applications remain independent and yet benefit from a network-effect. For more details refer to \nApplication Backplane\n\n\nDashboards in Omni Channel Fraud Prevention Application\n\n\nThe Omni-Channel Fraud Prevention application includes the following dashboards.\n\n\n\n\nReal-time Fraud Prevention Analysis\n\n\nReal-time Fraud Prevention Operations\n\n\n\n\nReal-time Fraud Prevention Analysis\n\n\n\n\nThe following analytic metrics are available within \nReal-time Fraud Prevention Analysis\n dashboard:\n\n\n\n\nFraud Prevention Analysis\n\n\nFraud Breakdown by Channel\n\n\nPercentage Fraud Among Caught\n\n\nFraud in the USA\n\n\nFraud Instances by Device\n\n\nFraud Rule Matches\n\n\nFraud Transactions Amount vs All Transactions Amount\n\n\nFraud transactions broken down by cardType x cardNetwork\n\n\nFraud Transactions Count vs All Transactions Count\n\n\nInstances of Fraud Prevented in last minute\n\n\nTop Cities in Number of Mobile Fraud Transactions\n\n\nTop Cities in Number of POS Fraud Transactions\n\n\nTop Cities in Number of Web Fraud Transactions\n\n\nTotal Fraud Amount Prevented in the last minute\n\n\n\n\nReal-time Fraud Prevention Operations\n\n\n\n\nThe following operational metrics are available within \nReal-time Fraud Prevention Operations\n dashboard:\n\n\n\n\nTop rules\n\n\nTransaction thoroughput\n\n\nTransactions by device type\n\n\nApplication Latency\n\n\nRule Latency\n\n\nRule Executor Free Memory\n\n\nTotal Number of Process Failures\n\n\n\n\nYou can use the OAS dashboards service that is specific to FPA to visualize various metrics over the dashboards. \nOAS Dashboards\n\n\nFor importing and viewing the dashboards, refer to \ndtDashboard\n\n\nTroubleshooting\n\n\n\n\nMy App crashes immediately with Error stating:\n \nFailed to construct kafka consumer\n\n\n\n\nSolution\n: Ensure that your kafka instance is up and running. Also check if your kafka server is accessible from your cluster hosts. Also check right if the kafka server configuration is provided to the operator.\n\n\n\n\nMy Enrichment operator crashes.\n\n\n\n\nSolution:\n Check if you have enrichment data in place. Also verify if enrichment file/data store is accessible from node. Also check  if the correct configuration is provided to the operator.\n\n\n\n\nLatency of my application increases\n\n\n\n\nSolution:\n From DataTorrent RTS UI, check the operator which is a bottleneck. Increase operator memory by referring to the instructions on \ndatatorrent.com\n. Also check Scaling section to add more resources for processing.\n\n\n\n\nI do not see as many number of processed tuple at end operators as I receive from kafka / My tuples are getting filtered out.\n\n\n\n\nSolution:\n Check the following:\n  - Check if the filter condition on the \nValidator\n operator is filtering out most of tuples. You can check that by looking at count of received tuples and processed tuples by the operator.\n  - Check if enrichment is failing due to missing data. In case lookup data is missing enrichment operator can fail. By default, tuples are not passed on to the next operator if enrichment is failed. Based on your requirements either validate your lookup data or configure operators to pass on tuples in case of enrichment failures.", 
            "title": "Omni Channel Fraud Prevention Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#omni-channel-fraud-prevention-application-user-guide", 
            "text": "", 
            "title": "Omni Channel Fraud Prevention Application User Guide"
        }, 
        {
            "location": "/omni_channel_fraud_app/#overview", 
            "text": "Omni Channel Fraud Prevention application is a pre-built application which is used to identify frauds in financial transactions. This application is designed to ingest, transform, analyze, act, and visualize data as soon as it is generated thereby preventing fraud transactions in real time that is before it happens.  It is built over DataTorrent RTS platform and can be run on commodity hardware. The platform gives real time insights while providing fault tolerant and scalable way of processing. The fraud identification can be further customized by writing rules as per your business needs. Support is provided for editing rules using CEP Workbench.  This application is integrated with Online Analytical Service (OAS) for OLAP historical and real-time trend analysis. Real-time analytics are provided for continuous KPI visualizations along with integrated alert capabilities for user-defined thresholds.  The application dashboards are based on Apache SuperSet with enhanced ability to visualize and analyze trends.  The following services come preinstalled with the application. You can run these services for analytics, visualizing the analytic outcomes, and for creating customized rules.   Online Analytics Service  OAS Dashboards  CEP Workbench   You can also integrate the application backplane to share the fraud alerts with other fraud-based applications thereby reducing the chance of fraud.  In addition to this, you can evaluate what-if scenarios on the incoming data which can be stored and visualized over different rules and business logic.\nOmni Channel Fraud Prevention application is available only with  DT Premium license .", 
            "title": "Overview"
        }, 
        {
            "location": "/omni_channel_fraud_app/#quick-launch-omni-channel-fraud-prevention-application", 
            "text": "For details about quickly launching the application, refer to  Quick Start Guide - Omni-Channel Fraud Prevention Application .", 
            "title": "Quick Launch Omni-Channel Fraud Prevention Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#workflow", 
            "text": "The following image presents the workflow of the application.  The following table lists the various processing phases of the incoming data tuples/event in  Omni Channel Fraud Prevention  application:     Phase  Process  Description      1.  Transaction Ingestion  Data from all your input sources is pushed onto a message bus such as Apache kafka. The application leverages platform capabilities to ingest data from Kafka message bus to rest of its pipeline. Data tuples are filtered and transformed. The following operators are involved during Transaction ingestion: Kafka Input Operator, Filter Operator and Transformation Operator    2  Transaction Enrichment  Data is enriched with the lookup information such as user profile data, card data, geo data, store profile data etc.The following operators are involved in the transaction enrichment phase: Enricher, Geo Data, Customer Data, Card Data, and Product Data.  By default, these are the enrichment operators, you can modify, add, or remove these operators by launching the application with a different application configuration. Refer to  Enriching Data    3  Fraud Rule Execution  Fraud detection rules can be applied to the enriched data using the CEP Workbench. The following operator are involved in the Fraud Rule Execution phase: Fraud Rule Executor    4  Analytics  OAS and Metrics    5  Fraud Triggers and Alerts  Triggers are activated based on the output of the upstream processing. The processed transactions are written onto HDFS to run any historical analysis in future. Also, transactions are published on message bus such as kafka to take real time actions. The application is also integrated with alerting systems that send real time alerts to applicable users.  Email alerts are sent by default. The following operator is involved in the Fraud Triggers and Alerts phase: Alert Mail, Data Writer, and Kafka Output operator", 
            "title": "Workflow"
        }, 
        {
            "location": "/omni_channel_fraud_app/#operators-modules", 
            "text": "The following operators/modules are included for the Fraud Prevention application.     Operator / Module  Description      Transaction Receiver  This Kafka input operator receives the transactions.s. It forwards these transactions to downstream operator.    Transaction Parser  This JSON parser operator parses the incoming transaction messages  and converts them into plain java objects hereafter referred as tuple for further processing.    TransactionTransformer  Transforms tuple fields as per given configuration e.g. fetch deviceIp from mobile details or web details based on transaction source    TransactionValidator  Validates tuples based on configuration e.g. all transactions with transaction amount more than 100$    User Profile Enricher  This operator gets the relevant JAVA applicable user details corresponding to a unique ID and enriches the tuple. Using this operator is optional in an FPA application. Refer  User Data Enricher    Geo Data Enricher  The application identifies the geolocation of the transaction by performing a lookup of the transaction IP against the external database like Maxmind database.  Using this operator is optional in an FPA application. Refer  Geo Data Enricher    CardDataEnricher  This operator gets relevant card details and enriches tuple. Using this operator is optional in an FPA application. Refer  Card Data Enricher    ProductDataEnricher  This operator gets relevant product details and enriches tuple.Using this operator is optional in an FPA application. Refer  Product Data Enricher    Input Module  Input module consists of two operators:  Kafka Input Operator - This operator emits the byte array from Kafka topic whose properties are set on the module. These properties must be the same ones set on the corresponding output module whose messages are subscribed. You can also indicate which offset to start reading messages from kafka.  Avro Deserializer  - This operator does the deserialization of the schema set on the module. The schema must be the same one set on corresponding output module whose messages are subscribed.The deserialized class should be in classpath. This will be achieved by querying schema repository to include the jar of the class. Omni-Channel Fraud prevention application. Refer  Application Backplane    AccountDataEnricher  This operator gets relevant user account security information from Account takeover prevention application using application backplane.    Fraud Rules Executor  This operator is the Drools Operator. It applies the pre-defined rules to the incoming tuples and takes a suitable action depending on the outcome of the rules applied to a tuple.    HDFS Output Operator  This output operator writes messages coming from the Rules Executor to the specified HDFS file path. Using this operator is optional in an FPA application.    AOO Operator  This operator writes messages to a Kafka topic that are consumed by Online Analytics Service (OAS).", 
            "title": "Operators / Modules"
        }, 
        {
            "location": "/omni_channel_fraud_app/#setting-the-application", 
            "text": "Before you run the  Omni Channel Fraud Prevention  application, you must ensure to fulfill the prerequisites and to configure the enrichment.", 
            "title": "Setting the Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#prerequisites", 
            "text": "The following should be installed on the computer before setting up the application:     Product  Version  Description      Apache Hadoop  2.6.0 and Above  Apache Hadoop is an open-source software framework that is used for distributed storage and processing of dataset of big data using the MapReduce programming model.    DataTorrent RTS  3.10  DataTorrent RTS, which is built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion, and distribution features.    Apache Kafka  0.9  Apache Kafka is an open-source stream processing platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.    Maxmind   If you are using Geo data enrichment, ensure to copy the maxmind city database on HDFS and configure the operator with the location. For more details, refer to  Configuring the Properties  section.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/omni_channel_fraud_app/#user-data-enricher", 
            "text": "User details are important to evaluate transactions for potential frauds.  For example, customer home address, spending patterns, risk score, and so on can be used to decide if a specific transaction is fraud. Jdbc store or json file is supported for lookup of customer data.  If you do not have a database, you can use file input source. Put your json input file on HDFS and configure it accordingly.  You can add the properties that are shown in the following json format into the  enrichment.json  file.   {\n     name :  UserProfileEnricher ,\n     storeType  :  json_file ,\n     storeFields  : [\n      {  name  :  cardNumber ,  type  :  long  },\n      {  name :  customerType ,  type  :  string  },\n      {  name  :  customerAvgSpending ,  type  :  double  },\n      {  name  :  customerRiskScore ,  type  :  double  },\n      {  name  :  custGender ,  type  :  string  },\n      {  name :  custMaritalStatus ,  type  :  string  },\n      {  name  :  custIncomeLevel ,  type  :  string  },\n      {  name  :  custStreet1 ,  type  :  string  },\n      {  name  :  custStreet2 ,  type  :  string  },\n      {  name  :  custCity ,  type  :  string  },\n      {  name  :  custState ,  type  :  string  },\n      {  name  :  custCountry ,  type  :  string  },\n      {  name  :  custPoBox ,  type  :  long  },\n      {  name  :  custPostalCode ,  type  :  long  },\n      {  name  :  custPostalCodeType ,  type  :  string  },\n      {  name  :  lat ,  type  :  double  },\n      {  name  :  lon ,  type  :  double  },\n      {  name  :  userId ,  type  :  string  }\n    ],\n     inputType  :  com.datatorrent.cep.schema.Transaction ,\n     outputType  :  com.datatorrent.cep.schema.Transaction ,\n     reuseObject  : true,\n     properties : {\n       file  :  lookupdata/customers.json ,\n       refreshInterval  : 5000\n    },\n     lookupFields  : {\n       cardNumber  :  cardNumber \n    },\n     includeFields  : {\n       customer.cardNumber  :  cardNumber ,\n       customer.customerType  :  customerType ,\n       customer.customerAvgSpending  :  customerAvgSpending ,\n       customer.customerRiskScore  :  customerRiskScore ,\n       customer.custStreet1  :  custStreet1 ,\n       customer.custStreet2  :  custStreet2 ,\n       customer.custCity  :  custCity ,\n       customer.custState  :  custState ,\n       customer.custCountry  :  custCountry ,\n       customer.custPoBox  :  custPoBox ,\n       customer.custPostalCode  :  custPostalCode ,\n       customer.custPostalCodeType  :  custPostalCodeType ,\n       customer.userId  :  userId \n    }\n  }  Note:  Ensure that the database connection driver jar, if any, is in classpath. The driver jar can be added in classpath using config package.", 
            "title": "User Data Enricher "
        }, 
        {
            "location": "/omni_channel_fraud_app/#scaling-the-application", 
            "text": "To handle higher data loads, you can add more partitions of the processing units i.e. operators.  Update following properties as per your input load. The following properties must be set for scaling the application:     Property  Description  Type  Example      dt.operator.TransactionReceiver.initialPartitionCount  Partition count of Kafka data receiver  Integer  1    dt.operator.FraudRulesExecutor.attr.PARTITIONER  Partition count of Rule execution opeartor  Integer  1    dt.operator.RuleAndTransactionsWriter.attr.PARTITIONER  Partition count of file writer operators  Integer  1    dt.operator.TransactionDataWriter.partitionedFileNameformat  File Name Format for transaction writer partition  String  %s-%04d    dt.operator.FraudResultFileWriter.partitionedFileNameformat  File Name Format for fraud writer partition  String  %s-%04d", 
            "title": "Scaling the Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#running-the-application", 
            "text": "The Fraud Prevention application can be launched from the DataTorrent RTS interface.  To run the application, do the following:   Go to the  Develop  page and upload the application package.  Specify the configuration as per the  Application Configuration . Refer to  Application Configurations  Launch the application. Refer to  Launching Applications and Configurations \nDuring the launch process, you can name the configuration and save it for future references. After you launch, you can track the details of the processed data in the application from the  Monitor  tab.", 
            "title": "Running the Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#generating-sample-input", 
            "text": "For a test run, you may want to generate sample data. To generate sample data, run the  OmniChannelFraudPreventionDataGenerator  application from  dt-cep-omni-fraud-prevention-app-1.4.0.apa . Specify kafka server details and topic name which must be the same as configured for Transaction Receiver.", 
            "title": "Generating Sample Input"
        }, 
        {
            "location": "/omni_channel_fraud_app/#running-services", 
            "text": "Fraud Prevention application can rely on the following services as dependencies, and all the required services will be automatically installed and launched as needed when the application starts. More details refer Services.  For example when you launch the   Omni Channel Fraud Prevention  application for the first time, the following services are automatically installed and launched along with it:   Online Analytics Service  OAS Dashboards  CEP Workbench", 
            "title": "Running Services"
        }, 
        {
            "location": "/omni_channel_fraud_app/#storing-and-replaying-data", 
            "text": "A service is available for Fraud Prevention application to store and replay events. You can record and replay data from a point in time to evaluate the effectiveness of builds, models, rules, and scenarios before they are put into production. Refer  Store and Replay", 
            "title": "Storing and Replaying Data"
        }, 
        {
            "location": "/omni_channel_fraud_app/#syncing-with-the-application-backplane", 
            "text": "Application Backplane enables multiple applications to be integrated to share insights and actions. Fraud Prevention application is combined with Account Takeover Prevention application in real-time. Both these applications remain independent and yet benefit from a network-effect. For more details refer to  Application Backplane", 
            "title": "Syncing with the Application Backplane"
        }, 
        {
            "location": "/omni_channel_fraud_app/#dashboards-in-omni-channel-fraud-prevention-application", 
            "text": "The Omni-Channel Fraud Prevention application includes the following dashboards.   Real-time Fraud Prevention Analysis  Real-time Fraud Prevention Operations", 
            "title": "Dashboards in Omni Channel Fraud Prevention Application"
        }, 
        {
            "location": "/omni_channel_fraud_app/#troubleshooting", 
            "text": "My App crashes immediately with Error stating:   Failed to construct kafka consumer   Solution : Ensure that your kafka instance is up and running. Also check if your kafka server is accessible from your cluster hosts. Also check right if the kafka server configuration is provided to the operator.   My Enrichment operator crashes.   Solution:  Check if you have enrichment data in place. Also verify if enrichment file/data store is accessible from node. Also check  if the correct configuration is provided to the operator.   Latency of my application increases   Solution:  From DataTorrent RTS UI, check the operator which is a bottleneck. Increase operator memory by referring to the instructions on  datatorrent.com . Also check Scaling section to add more resources for processing.   I do not see as many number of processed tuple at end operators as I receive from kafka / My tuples are getting filtered out.   Solution:  Check the following:\n  - Check if the filter condition on the  Validator  operator is filtering out most of tuples. You can check that by looking at count of received tuples and processed tuples by the operator.\n  - Check if enrichment is failing due to missing data. In case lookup data is missing enrichment operator can fail. By default, tuples are not passed on to the next operator if enrichment is failed. Based on your requirements either validate your lookup data or configure operators to pass on tuples in case of enrichment failures.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Accounttakeover/", 
            "text": "Account Takeover (ATO) application is a pre-built application that can be used to detect and prevent attempts of account takeover in various industries such as finance, telecom, and subscription-based offerings. ATO application is designed to ingest, transform, analyze incoming data, and provide account takeover alerts in real-time before such an activity occurs. It also provides visualizations of incoming as well as processed data.\n\n\nUsing this application, you can process, enrich, analyze and act in real-time upon multiple streams of account event information which can prevent account-take over and fraud.\n\n\nATO application is built over DataTorrent RTS platform and can be run on commodity hardware. The platform provides real time insights and a fault tolerant and scalable method for processing data. The application can be further customized by writing rules as per your business needs and implementing custom data types.\n\n\nIn addition, ATO has the capability to store and replay the incoming data from Kafka input operators and then replay the stored data with a different set of rules to visualize the outcome.  You can also integrate the application backplane, to share the fraud outcomes of ATO with other fraud detection-based applications and thereby reducing the chance of fraud. \nThe following services come preinstalled with an ATO application. You can run these services for analytics, visualizing the analytic outcomes, and for creating customized rules.\n\n\n\n\nOnline Analytics Service\n\n\nOAS Dashboards\n\n\nCEP Workbench\n\n\n\n\nATO application is available with \nDT Premium license\n.\n\n\nQuick Launch Account Takeover Application\n\n\nFor details about quickly launching the application, refer to \nQuick Start Guide - Account Takeover Prevention Application\n.  \n\n\nWorkflow\n\n\nThe following image depicts the workflow in the ATO application:\n\n\n\nSetting the Application\n\n\nBefore you run the  \nATO\n  application, you must ensure to fulfill the prerequisites and to configure the operators in the DAG.\n\n\nPrerequisites\n\n\nThe following should be installed on the cluster before setting up the application:\n\n\n\n\n\n\n\n\nProduct\n\n\nVersion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nApache Hadoop\n\n\n2.6.0 and Above\n\n\nApache Hadoop is an open-source software framework that is used for distributed storage and processing of dataset of big data using the MapReduce programming model.\n\n\n\n\n\n\nDataTorrent RTS\n\n\n3.10.0\n\n\nDataTorrent RTS, which is built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion, and distribution features.\n\n\n\n\n\n\nApache Kafka\n\n\n0.9\n\n\nApache Kafka is an open-source stream processing platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.\n\n\n\n\n\n\n\n\nOperators / Modules\n\n\nThe following operators/modules are included for the ATO application.\n\n\n\n\n\n\n\n\nOperator / Module\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUser Activity Receiver\n\n\nThis Kafka input operator receives current user activity from Syslogs or any other logs. It forwards these logs to downstream operator.\n\n\n\n\n\n\nUser Activity Parser\n\n\nThis JSON parser operator parses the incoming messages and converts them into plain java objects hereafter referred as tuple for further processing.\n\n\n\n\n\n\nUser Profile Enricher\n\n\nThis operator gets the relevant JAVA applicable user details corresponding to a unique ID and enriches the tuple. User details can be extracted from JDBC database store or json file on HDFS.  You can configure the operator based on the enrichment data source you choose.Using this operator is optional in an ATO application.\n\n\n\n\n\n\nGeo Data Enricher\n\n\nThe application identifies the geolocation of the transaction by performing a lookup of the transaction IP against the external database like Maxmind database.  Using this operator is optional in an ATO application.\n\n\n\n\n\n\nRules Executor\n\n\nThis operator is the Drools Operator. It applies the pre-defined rules to the incoming tuples and takes a suitable action depending on the outcome of the rules applied to a tuple.\n\n\n\n\n\n\nOutput Module\n\n\nOutput module consists of two operators:\nAvro Serializer\n serializes the output of the Rules Executor (Drools operator) to send to Kafka Output operator. \nKafka Output Operator\n sends these events to the specified Kafka topic for consumption by other applications. This publishes the information which can be consumed by Omni-Channel Fraud prevention application.\n\n\n\n\n\n\nHDFS Output Operator\n\n\nThis output operator writes messages coming from the Rules Executor to the specified HDFS file path. Using this operator is optional in an ATO application.\n\n\n\n\n\n\nOAS Operator\n\n\nThis operator writes messages to a Kafka topic that are consumed by Online Analytics Service (OAS).\n\n\n\n\n\n\n\n\nConfiguring Rules\n\n\nThe application package contains sample rules. However, you can add rules based on your business requirements. These can be configured in Drools supported formats such as . \ndrl\n , \nxls etc\n. Refer  \nAuthoring Rule Assets\n in Drools documentation.\n\n\nFor the Rules Executor, you can configure the rules either from the CEP Workbench or from HDFS.\n\n\nCEP Work Bench\n\n\nTo configure rules from CEP Workbench,  you must configure the kieBaseName and kieSessionName properties to the application configuration where you are implementing the rules that you have created using CEP Workbench. Refer to \nCEP Workbench\n\n\nHDFS\n\n\nTo configure rules from HDFS, do the following:\n\n\n\n\nCreate the rules file in one of the format that is supported by Drools and save the \noutput rule\n file onto your local machine.\n\n\nCopy this rule file into the HDFS folder.\n\n\nIn the Droolsoperator, configure the folder path in the following operator property, to point to HDFS folder containing rules.\n\n\n\n\n\n\n\n\n\n\nProperty Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrulesDir\n\n\nThe path to HDFS from where you can load the rules. If this path is set to null, then operator loads the rules from the classpath.\n\n\n\n\n\n\n\n\n\n\nRestart the application after updating the rules.\n\n\n\n\nNote:\n When the folder path is not provided to the Drools operator, the packaged rules are uploaded by default.\n\n\nConfiguring Properties\n\n\nThe properties for the following items must be set for running ATO application:\n\n\n\n\nKafka\n\n\nParser\n\n\nUser Profile Enricher\n\n\nGeo Data Enricher\n\n\nRules Executor\n\n\nAvro Serializer\n\n\nHDFS Output Operator\n\n\nAOO Operator\n\n\n\n\nKafka\n\n\nUser Activity Receiver\n operator and \nKafka\n Output\n operator are the respective entry and exit points of the application. These operators read from the Kafka topics and write to the Kafka topics. Therefore, you must ensure that the kafka setup in the system is up and running.\n\n\nConfigure the kafka setup details in the application properties file.  The following required properties must be configured:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nkafkaBrokerList\n\n\nComma separated list of kafka-brokers\n\n\nString\n\n\nnode1.company.com:9098, node2.company.com:9098, node3.company.com:9098\n\n\nYes\n\n\n\n\n\n\nUserActivityReceiverTopic\n\n\nTopics to read from Kafka\n\n\nString\n\n\ntransactions\n\n\nYes\n\n\n\n\n\n\nProcessedTransactionsOutputTopic\n\n\nTopics to write processed transactions to kafka\n\n\nString\n\n\nprocessed-transactions\n\n\nYes\n\n\n\n\n\n\ninitialOffset\n\n\nInitial offset to read from Kafka\n\n\nString\n\n\nEARLIESTLATESTAPPLICATION_OR_EARLIESTAPPLICATION_OR_LATEST\n\n\n\n\n\n\n\n\nkey.serializer\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\nvalue.serializer\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\ndt.operator.TransactionDataPublisher.prop.properties(key.serializer)\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\ndt.operator.TransactionDataPublisher.prop.properties(value.serializer)\n\n\nSerializer class\n\n\nString\n\n\norg.apache.kafka.common.serialization.StringSerializer\n\n\n\n\n\n\n\n\narchivePath\n\n\nPath of archive directory where you can store data for replaying with different rules. [Refer \nStore and Replay\n\n\nString\n\n\n\n\n\n\n\n\n\n\narchiveType\n\n\nArchive information\n\n\nEnum\n\n\nARCHIVE_TYPE_KAFKA\n\n\n\n\n\n\n\n\nenableArchive\n\n\nto enable / disable archiving for replaying data.\n\n\nBoolean\n\n\n\n\n\n\n\n\n\n\nenableReplay\n\n\nValue to enable or disable replay. enableReplay is mutually exclusive with enableArchive, both can be false.\n\n\nBoolean\n\n\n\n\n\n\n\n\n\n\nwhenReplaySetT0\n\n\nWhen enable Replay is true, this can be set. Set the start time from when to replay. Format is \nyyyy-MM-dd\nT\nHH:mm:ss\n\n\nString\n\n\n2017-09-17T01:01:01\n\n\n\n\n\n\n\n\nwhenReplaySetT1\n\n\nWhen enable Replay is true, this can be set. Set the start time to replay. Format is \nyyyy-MM-dd\nT\nHH:mm:ss\n\n\nString\n\n\n2017-09-17T03:01:01\n\n\n\n\n\n\n\n\n\n\nParser\n\n\nParser parses JSON input from kafka and generates plain JAVA object for further processing.\n\n\nConfigure the JAVA class of the plain JAVA object to be generated by parser. The following properties must be set for the parser:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nTUPLE_CLASS schema attribute\n\n\npojo class name of object to be generated by parser\n\n\nString\n\n\ncom.datatorrent.ato.schema.UserActivity\n\n\n\n\n\n\n\n\nUser Profile Enricher\n\n\nMissing fields from your incoming records can be enriched by referring to your lookup data in enrichment phase. By default, the configuration for enrichment is stored in \nenrichments.json\n that is bundled in the application package. You can also write your own configuration in a file and store that file on HDFS. You can configure existing enrichments or add / remove as per your business needs. The enrichment properties file path should be provided in the \nproperties.xml\n file.\n\n\nFollowing properties should be set for configuration file path:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.atoapp.enrichments.configFilePath\n\n\nPath of configuration file for enrichments\n\n\nstring\n\n\nenrichments.json\n\n\n\n\n\n\n\n\nGeo Data Enricher\n\n\nGeoData Enrichment operator refers to the \nmaxmind\n database that fetch geo information when you provide the IP address of the transaction location. To run the GeoData Enrichment operator, you must copy the maxmind city database (GeoLite2 City) to HDFS. You can remove this enrichment as well as update properties (e.g. maxmind db path) by configuring enrichment properties. \nPlease mention again as per previous para how to write and set enrichments.json\n\n\nNote:\n Extract \nGeoLite2-City.mmdb\n file to HDFS. Do not copy the ZIP file directly.\n\n\nFollowing properties should be set for the \nUser Profile Enricher\n operator as well as the \nGeo Data Enricher\n operator in the \nenrichments.json\n file.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nstoreType\n\n\nType of data storage\n\n\nstring\n\n\njson_file, geo_mmdb, jdbc\n\n\n\n\n\n\nstoreFields\n\n\nJson array of Name of fields in input objectsa\n\n\nstring\n\n\n[{ \nname\n : \nuserId\n, \ntype\n : \nstring\n },  { \nname\n: \ncustomerType\n, \ntype\n : \nstring\n }]\n\n\n\n\n\n\ninputType\n\n\nType of input object\n\n\nstring\n\n\ncom.datatorrent.ato.schema.UserActivity\n\n\n\n\n\n\noutputType\n\n\nType of output object\n\n\nstring\n\n\ncom.datatorrent.ato.schema.UserActivity\n\n\n\n\n\n\nreuseObject\n\n\nSpecify if object can be reused\n\n\nboolean\n\n\ntrue\n\n\n\n\n\n\nfile\n\n\nPath of the user data file\n\n\nstring\n\n\nato_lookupdata/customers.json\n\n\n\n\n\n\nrefreshInterval\n\n\nTime interval after which cache should be refreshed\n\n\ninteger\n\n\n5000\n\n\n\n\n\n\nlookupFields\n\n\nMain field / key based on which the user data is queried.\n\n\nJSON\n\n\nuserId\n\n\n\n\n\n\nincludeFields\n\n\ncomma seperated Mapping of fields from user data to the fields in JAVA object.\n\n\nJSON\n\n\ncustomer.userId\n:\nuserId\n,\ncustomer.customerType\n:\ncustomerType\n\n\n\n\n\n\n\n\nExample of enrichment.json file\n\n\nFollowing is an example of the \nenrichment.json\n file. You can refer to this example to create the \nenrichment.json\n file.\n\n\n[\n  {\n    \nname\n: \nUserProfileEnricher\n,\n    \nstoreType\n : \njson_file\n,\n    \nstoreFields\n : [\n    { \nname\n : \nuserId\n, \ntype\n : \nstring\n },\n    { \nname\n: \ncustomerType\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustomerAvgSpending\n, \ntype\n : \ndouble\n },\n    { \nname\n : \ncustomerRiskScore\n, \ntype\n : \ndouble\n },\n    { \nname\n : \ncustGender\n, \ntype\n : \nstring\n },\n    { \nname\n: \ncustMaritalStatus\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustIncomeLevel\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustStreet1\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustStreet2\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustCity\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustState\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustCountry\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustPoBox\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustPostalCode\n, \ntype\n : \nstring\n },\n    { \nname\n : \ncustPostalCodeType\n, \ntype\n : \nstring\n },\n    { \nname\n : \nlat\n, \ntype\n : \ndouble\n },\n    { \nname\n : \nlon\n, \ntype\n : \ndouble\n }\n    ],\n    \ninputType\n : \ncom.datatorrent.ato.schema.UserActivity\n,\n    \noutputType\n : \ncom.datatorrent.ato.schema.UserActivity\n,\n    \nreuseObject\n : true,\n    \nproperties\n: {\n    \nfile\n : \nato_lookupdata/customers.json\n,\n    \nrefreshInterval\n : 5000\n    },\n    \nlookupFields\n : {\n    \nuserId\n : \nuserId\n\n    },\n    \nincludeFields\n : {\n    \ncustomer.userId\n:\nuserId\n,\n    \ncustomer.customerType\n:\ncustomerType\n,\n    \ncustomer.customerAvgSpending\n:\ncustomerAvgSpending\n,\n    \ncustomer.customerRiskScore\n:\ncustomerRiskScore\n,\n    \ncustomer.custGender\n:\ncustGender\n,\n    \ncustomer.custMaritalStatus\n:\ncustMaritalStatus\n,\n    \ncustomer.custIncomeLevel\n:\ncustIncomeLevel\n,\n    \ncustomer.custStreet1\n:\ncustStreet1\n,\n    \ncustomer.custStreet2\n:\ncustStreet2\n,\n    \ncustomer.custCity\n:\ncustCity\n,\n    \ncustomer.custState\n:\ncustState\n,\n    \ncustomer.custCountry\n:\ncustCountry\n,\n    \ncustomer.custPoBox\n:\ncustPoBox\n,\n    \ncustomer.custPostalCode\n:\ncustPostalCode\n,\n    \ncustomer.custPostalCodeType\n: \ncustPostalCodeType\n\n    }\n  },\n  {\n    \nname\n: \nGeoDataEnricher\n,\n    \npassThroughOnError\n : true,\n    \nstoreType\n: \ngeo_mmdb\n,\n    \nstoreFields\n: [\n    { \nname\n : \nIP\n, \ntype\n : \nstring\n },\n    { \nname\n: \nCITY\n, \ntype\n: \nstring\n },\n    { \nname\n : \nSUBDIVISION_ISO\n, \ntype\n : \nstring\n },\n    { \nname\n: \nZIPCODE\n, \ntype\n: \nstring\n },\n    { \nname\n: \nCOUNTRY_ISO\n, \ntype\n: \nstring\n },\n    { \nname\n : \nLATITUDE\n, \ntype\n : \ndouble\n },\n    { \nname\n : \nLONGITUDE\n, \ntype\n : \ndouble\n }\n    ],\n    \ninputType\n: \ncom.datatorrent.ato.schema.UserActivity\n,\n    \noutputType\n: \ncom.datatorrent.ato.schema.UserActivity\n,\n    \nreuseObject\n: true,\n    \nproperties\n: {\n    \ndbpath\n: \ncity.mmdb\n,\n    \nrefreshInterval\n: 5000\n    },\n    \nlookupFields\n: {\n    \nIP\n: \ndeviceIP\n\n    },\n    \nincludeFields\n: {\n    \ngeoIp.city\n : \nCITY\n,\n    \ngeoIp.state\n : \nSUBDIVISION_ISO\n,\n    \ngeoIp.zipcode\n : \nZIPCODE\n,\n    \ngeoIp.country\n : \nCOUNTRY_ISO\n,\n    \ngeoIp.latitude\n : \nLATITUDE\n,\n    \ngeoIp.longitude\n : \nLONGITUDE\n\n    }\n  }\n]\n\n\n\n\nRules Executor\n\n\nThe Rules Executor that is the Drools operator provides a method to load rules from:\n\n\n\n\nCEP Workbench\n\n\nHDFS\n\n\n\n\nIf rules are loaded from files on HDFS, you must configure the following property:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\n\n\n\n\n\n\n\n\nrulesDir\n\n\nPath to HDFS from where to load the rules. If this path is set to null, then the operator loads the rules from the classpath.\n\n\nstring\n\n\n\n\n\n\n\n\nIf rules are to be loaded from CEP Workbench, you must specify following properties. Also Refer \nCEP Workbench\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nkieSessionName\n\n\nIf rules are to be loaded from application classpath, then specify the name of the session to use. This is created using CEP Worknbench.\n\n\nstring\n\n\nUserActivity-rules-session\n\n\n\n\n\n\nkiebaseName\n\n\nIf rules are to be loaded from application classpath, then specify the name of the kie base (rule) to use . This is created using CEP Workbench.\n\n\nstring\n\n\nato-rules\n\n\n\n\n\n\n\n\nNote:\n If rules are to be loaded from application classpath, the knowledge jar (KJAR) should be in the classpath.  Refer to \nApplication Configurations\n\n\nAvro Serializer\n\n\nThe following properties should be set for the Avro Serializer operator that is part of the Output module:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.atoapp.schemaName\n\n\nSet schema for the data.\n\n\nstring\n\n\nUserActivity\n\n\n\n\n\n\ntopic\n\n\nSet the kafka topic name\n\n\nstring\n\n\nATO_analyseddata\n\n\n\n\n\n\n\n\nHDFS Output Operator\n\n\nThere are two output operators to write to HDFS:\n\n\n\n\nProcessedActivityDataWriter\n\nThis operator writes all the transactions processed by the application to HDFS.\n\n\nFlaggedActivityFileWriter\n\nThis operator writes only the fraud user activities to the HDFS.\n\n\n\n\nFor details of other properties of FileOutput Operator, please refer the \ndocumentation\n.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\ndt.atoapp.enableOutputOperators\n\n\nThis flag should be set to true if messages are to be written to HDFS\n\n\nboolean\n\n\n\n\n\n\n\n\nfilePath\n\n\nPath of the directory where the output files must be created.\n\n\nstring\n\n\n/user/dtuser/processeddata\n\n\n\n\n\n\noutFileName\n\n\nName of the file on HDFS in which the messages should be written.\n\n\nstring\n\n\nProcessedUserActivity\n\n\n\n\n\n\n\n\nAOO Operator\n\n\nThis operator writes messages to a Kafka topic that are consumed by the OAS (Online Analytics Service). The following properties should be set:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nschema\n\n\nSchema / metadata of the data to be sent to OAS.By default we package \nanalyticsschema.json\n schema to change schema copy your schema file to hdfs and configure, \ndt.atoapp.analytics.resourceFileName\n with your schema file path.\n\n\nstring\n\n\nanalyticsschema.json\n\n\n\n\n\n\nserializerClass\n\n\nProvides information about serializing incoming messages in the form of JAVA objects to send to Kafka\n\n\nstring\n\n\ncom.datatorrent.cep.common.ToStringAnalyticsPojoSerializer\n\n\n\n\n\n\ndisablePartialWindowCheck\n\n\nSet whether to disable partition window check or not.   \nNote\n: By disabling the partition window check duplicate data can be sent to Kafka thereby overriding exactly once guarantees.\n\n\nboolean\n\n\nexample\n\n\n\n\n\n\n\n\nScaling the Application\n\n\nTo handle higher data loads, you can add more partitions of the processing units i.e. operators.\n\n\nUpdate the following properties as per your input load. The following properties must be set for scaling the application:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nUserActivityReceiver.initialPartitionCount\n\n\nPartition count of Kafka data receiver.\n\n\nInteger\n\n\n1\n\n\n\n\n\n\nRulesExecutor.attr.PARTITIONER\n\n\nPartition count of Rule execution operator.\n\n\nInteger\n\n\n1\n\n\n\n\n\n\nFlaggedActivityFileWriter.partitionedFileNameformat\n\n\nFile name format for transaction writer partition.\n\n\nString\n\n\n%s-%04d\n\n\n\n\n\n\nProcessedActivityDataWriter.partitionedFileNameformat\n\n\nFile name format for fraud writer partition.\n\n\nString\n\n\n%s-%04d\n\n\n\n\n\n\nRulesExecutor.port.factsInput.attr.STREAM_CODEC\n\n\nEnsure that all related tuples should go to same partition so that tuples can be co-related across time to do complex event processing.\n\n\n\n\n\n\n\n\n\n\nSet STREAM_CODEC property of factsInput port of RulesExecutor to make sure related tuples go to same partition\n\n\nString\n\n\ncom.datatorrent.cep.common.codec.ConfigurableStreamCodec:userId\n\n\n\n\n\n\n\n\n\n\nLaunching the Application\n\n\nThe Account Takeover application can be launched from the DataTorrent RTS interface.\n\n\nTo launch the application, do the following:\n\n\n\n\nGo to the \nDevelop\n page and upload the application package.\n\n\nSpecify the configuration as per the \nApplication Configuration\n section. \nApplication Configurations\n\n\nLaunch the application. \nDuring the launch process, you can name the configuration and save it for future references. After you launch, you can track the details of the processed data in the application from the \nMonitor\n tab.\n\n\n\n\nGenerate Sample Input\n\n\nFor a test run, you may want to generate sample data.\n\n\nTo generate sample data, do the following:\n\n\n\n\nRun the \ndt-ato-datagen-1.4.0.apa\n application from the DataTorrent RTS interface.\n\n\nSpecify kafka server details and topic name which must be the same as configured for User Activity Receiver.\n\n\n\n\nStoring and Replaying Data\n\n\nA service is available for the ATO application to store and replay events. You can record and replay data from a point in time to evaluate the effectiveness of builds, models, rules, and scenarios before they are put into production. Refer \nStore and Replay\n\n\nSyncing with the Application Backplane\n\n\nApplication Backplane enables multiple applications to be integrated to share insights and actions. ATO application is combined with Fraud Prevention application in real-time. Both these applications remain independent and yet benefit from a network-effect. For more details refer to \nApplication Backplane\n\n\nDashboards\n\n\nThe ATO Prevention application includes the following dashboards:\n\n\n\n\nReal-time Account Take-over Analysis\n\n\nReal-time Account Take-over Operations\n\n\n\n\nReal-time Account Take-over Analysis\n\n\n\nThe analysis dashboard provides a granular set of metrics for analysis of the account activity in ATO application. The following metrics are shown in this dashboard:\n\n\n\n\nAccount Takeover Fraud Prevention Analysis\n\n\nAccount Hacking Breakdown by Channel\n\n\nTotal ATO Frauds\n\n\nATO Fraud Breakdown by Channel\n\n\nFraud Instances by Device\n\n\nATO Frauds\n\n\nFrauds by EventType\n\n\nFraud Rule Matches\n\n\nATO Fraud in the USA (easily customizable for other countries)\n\n\nLogin failures by device type\n\n\n\n\nReal-time Account Take-over Operations\n\n\n\n\nThe operations dashboard provides an operational overview of the application.  The following operational metrics can be viewed from this dashboard:\n\n\n\n\nApplication Latency\n\n\nRule Latency\n\n\nRule Executor Free Memory\n\n\nTotal Number of Process Failures\n\n\nFraud Breakdown by Channel\n\n\nEvent Throughput\n\n\nTop Rules\n\n\nEvents by Device Type\n\n\n\n\nYou can use the OAS dashboards service that is specific to ATO application to visualize various metrics over the dashboards. \nOAS Dashboards\n\n\nFor importing and viewing the dashboards, refer to \ndtDashboard", 
            "title": "Account Takeover Prevention Application"
        }, 
        {
            "location": "/Accounttakeover/#quick-launch-account-takeover-application", 
            "text": "For details about quickly launching the application, refer to  Quick Start Guide - Account Takeover Prevention Application .", 
            "title": "Quick Launch Account Takeover Application"
        }, 
        {
            "location": "/Accounttakeover/#workflow", 
            "text": "The following image depicts the workflow in the ATO application:", 
            "title": "Workflow"
        }, 
        {
            "location": "/Accounttakeover/#setting-the-application", 
            "text": "Before you run the   ATO   application, you must ensure to fulfill the prerequisites and to configure the operators in the DAG.", 
            "title": "Setting the Application"
        }, 
        {
            "location": "/Accounttakeover/#prerequisites", 
            "text": "The following should be installed on the cluster before setting up the application:     Product  Version  Description      Apache Hadoop  2.6.0 and Above  Apache Hadoop is an open-source software framework that is used for distributed storage and processing of dataset of big data using the MapReduce programming model.    DataTorrent RTS  3.10.0  DataTorrent RTS, which is built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion, and distribution features.    Apache Kafka  0.9  Apache Kafka is an open-source stream processing platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/Accounttakeover/#operators-modules", 
            "text": "The following operators/modules are included for the ATO application.     Operator / Module  Description      User Activity Receiver  This Kafka input operator receives current user activity from Syslogs or any other logs. It forwards these logs to downstream operator.    User Activity Parser  This JSON parser operator parses the incoming messages and converts them into plain java objects hereafter referred as tuple for further processing.    User Profile Enricher  This operator gets the relevant JAVA applicable user details corresponding to a unique ID and enriches the tuple. User details can be extracted from JDBC database store or json file on HDFS.  You can configure the operator based on the enrichment data source you choose.Using this operator is optional in an ATO application.    Geo Data Enricher  The application identifies the geolocation of the transaction by performing a lookup of the transaction IP against the external database like Maxmind database.  Using this operator is optional in an ATO application.    Rules Executor  This operator is the Drools Operator. It applies the pre-defined rules to the incoming tuples and takes a suitable action depending on the outcome of the rules applied to a tuple.    Output Module  Output module consists of two operators: Avro Serializer  serializes the output of the Rules Executor (Drools operator) to send to Kafka Output operator.  Kafka Output Operator  sends these events to the specified Kafka topic for consumption by other applications. This publishes the information which can be consumed by Omni-Channel Fraud prevention application.    HDFS Output Operator  This output operator writes messages coming from the Rules Executor to the specified HDFS file path. Using this operator is optional in an ATO application.    OAS Operator  This operator writes messages to a Kafka topic that are consumed by Online Analytics Service (OAS).", 
            "title": "Operators / Modules"
        }, 
        {
            "location": "/Accounttakeover/#configuring-rules", 
            "text": "The application package contains sample rules. However, you can add rules based on your business requirements. These can be configured in Drools supported formats such as .  drl  ,  xls etc . Refer   Authoring Rule Assets  in Drools documentation.  For the Rules Executor, you can configure the rules either from the CEP Workbench or from HDFS.", 
            "title": "Configuring Rules"
        }, 
        {
            "location": "/Accounttakeover/#cep-work-bench", 
            "text": "To configure rules from CEP Workbench,  you must configure the kieBaseName and kieSessionName properties to the application configuration where you are implementing the rules that you have created using CEP Workbench. Refer to  CEP Workbench", 
            "title": "CEP Work Bench"
        }, 
        {
            "location": "/Accounttakeover/#hdfs", 
            "text": "To configure rules from HDFS, do the following:   Create the rules file in one of the format that is supported by Drools and save the  output rule  file onto your local machine.  Copy this rule file into the HDFS folder.  In the Droolsoperator, configure the folder path in the following operator property, to point to HDFS folder containing rules.      Property Name  Description      rulesDir  The path to HDFS from where you can load the rules. If this path is set to null, then operator loads the rules from the classpath.      Restart the application after updating the rules.   Note:  When the folder path is not provided to the Drools operator, the packaged rules are uploaded by default.", 
            "title": "HDFS"
        }, 
        {
            "location": "/Accounttakeover/#configuring-properties", 
            "text": "The properties for the following items must be set for running ATO application:   Kafka  Parser  User Profile Enricher  Geo Data Enricher  Rules Executor  Avro Serializer  HDFS Output Operator  AOO Operator", 
            "title": "Configuring Properties"
        }, 
        {
            "location": "/Accounttakeover/#example-of-enrichmentjson-file", 
            "text": "Following is an example of the  enrichment.json  file. You can refer to this example to create the  enrichment.json  file.  [\n  {\n     name :  UserProfileEnricher ,\n     storeType  :  json_file ,\n     storeFields  : [\n    {  name  :  userId ,  type  :  string  },\n    {  name :  customerType ,  type  :  string  },\n    {  name  :  customerAvgSpending ,  type  :  double  },\n    {  name  :  customerRiskScore ,  type  :  double  },\n    {  name  :  custGender ,  type  :  string  },\n    {  name :  custMaritalStatus ,  type  :  string  },\n    {  name  :  custIncomeLevel ,  type  :  string  },\n    {  name  :  custStreet1 ,  type  :  string  },\n    {  name  :  custStreet2 ,  type  :  string  },\n    {  name  :  custCity ,  type  :  string  },\n    {  name  :  custState ,  type  :  string  },\n    {  name  :  custCountry ,  type  :  string  },\n    {  name  :  custPoBox ,  type  :  string  },\n    {  name  :  custPostalCode ,  type  :  string  },\n    {  name  :  custPostalCodeType ,  type  :  string  },\n    {  name  :  lat ,  type  :  double  },\n    {  name  :  lon ,  type  :  double  }\n    ],\n     inputType  :  com.datatorrent.ato.schema.UserActivity ,\n     outputType  :  com.datatorrent.ato.schema.UserActivity ,\n     reuseObject  : true,\n     properties : {\n     file  :  ato_lookupdata/customers.json ,\n     refreshInterval  : 5000\n    },\n     lookupFields  : {\n     userId  :  userId \n    },\n     includeFields  : {\n     customer.userId : userId ,\n     customer.customerType : customerType ,\n     customer.customerAvgSpending : customerAvgSpending ,\n     customer.customerRiskScore : customerRiskScore ,\n     customer.custGender : custGender ,\n     customer.custMaritalStatus : custMaritalStatus ,\n     customer.custIncomeLevel : custIncomeLevel ,\n     customer.custStreet1 : custStreet1 ,\n     customer.custStreet2 : custStreet2 ,\n     customer.custCity : custCity ,\n     customer.custState : custState ,\n     customer.custCountry : custCountry ,\n     customer.custPoBox : custPoBox ,\n     customer.custPostalCode : custPostalCode ,\n     customer.custPostalCodeType :  custPostalCodeType \n    }\n  },\n  {\n     name :  GeoDataEnricher ,\n     passThroughOnError  : true,\n     storeType :  geo_mmdb ,\n     storeFields : [\n    {  name  :  IP ,  type  :  string  },\n    {  name :  CITY ,  type :  string  },\n    {  name  :  SUBDIVISION_ISO ,  type  :  string  },\n    {  name :  ZIPCODE ,  type :  string  },\n    {  name :  COUNTRY_ISO ,  type :  string  },\n    {  name  :  LATITUDE ,  type  :  double  },\n    {  name  :  LONGITUDE ,  type  :  double  }\n    ],\n     inputType :  com.datatorrent.ato.schema.UserActivity ,\n     outputType :  com.datatorrent.ato.schema.UserActivity ,\n     reuseObject : true,\n     properties : {\n     dbpath :  city.mmdb ,\n     refreshInterval : 5000\n    },\n     lookupFields : {\n     IP :  deviceIP \n    },\n     includeFields : {\n     geoIp.city  :  CITY ,\n     geoIp.state  :  SUBDIVISION_ISO ,\n     geoIp.zipcode  :  ZIPCODE ,\n     geoIp.country  :  COUNTRY_ISO ,\n     geoIp.latitude  :  LATITUDE ,\n     geoIp.longitude  :  LONGITUDE \n    }\n  }\n]", 
            "title": "Example of enrichment.json file"
        }, 
        {
            "location": "/Accounttakeover/#scaling-the-application", 
            "text": "To handle higher data loads, you can add more partitions of the processing units i.e. operators.  Update the following properties as per your input load. The following properties must be set for scaling the application:     Property  Description  Type  Example      UserActivityReceiver.initialPartitionCount  Partition count of Kafka data receiver.  Integer  1    RulesExecutor.attr.PARTITIONER  Partition count of Rule execution operator.  Integer  1    FlaggedActivityFileWriter.partitionedFileNameformat  File name format for transaction writer partition.  String  %s-%04d    ProcessedActivityDataWriter.partitionedFileNameformat  File name format for fraud writer partition.  String  %s-%04d    RulesExecutor.port.factsInput.attr.STREAM_CODEC  Ensure that all related tuples should go to same partition so that tuples can be co-related across time to do complex event processing.      Set STREAM_CODEC property of factsInput port of RulesExecutor to make sure related tuples go to same partition  String  com.datatorrent.cep.common.codec.ConfigurableStreamCodec:userId", 
            "title": "Scaling the Application"
        }, 
        {
            "location": "/Accounttakeover/#launching-the-application", 
            "text": "The Account Takeover application can be launched from the DataTorrent RTS interface.  To launch the application, do the following:   Go to the  Develop  page and upload the application package.  Specify the configuration as per the  Application Configuration  section.  Application Configurations  Launch the application. \nDuring the launch process, you can name the configuration and save it for future references. After you launch, you can track the details of the processed data in the application from the  Monitor  tab.", 
            "title": "Launching the Application"
        }, 
        {
            "location": "/Accounttakeover/#generate-sample-input", 
            "text": "For a test run, you may want to generate sample data.  To generate sample data, do the following:   Run the  dt-ato-datagen-1.4.0.apa  application from the DataTorrent RTS interface.  Specify kafka server details and topic name which must be the same as configured for User Activity Receiver.", 
            "title": "Generate Sample Input"
        }, 
        {
            "location": "/Accounttakeover/#storing-and-replaying-data", 
            "text": "A service is available for the ATO application to store and replay events. You can record and replay data from a point in time to evaluate the effectiveness of builds, models, rules, and scenarios before they are put into production. Refer  Store and Replay", 
            "title": "Storing and Replaying Data"
        }, 
        {
            "location": "/Accounttakeover/#syncing-with-the-application-backplane", 
            "text": "Application Backplane enables multiple applications to be integrated to share insights and actions. ATO application is combined with Fraud Prevention application in real-time. Both these applications remain independent and yet benefit from a network-effect. For more details refer to  Application Backplane", 
            "title": "Syncing with the Application Backplane"
        }, 
        {
            "location": "/Accounttakeover/#dashboards", 
            "text": "The ATO Prevention application includes the following dashboards:   Real-time Account Take-over Analysis  Real-time Account Take-over Operations", 
            "title": "Dashboards"
        }, 
        {
            "location": "/Accounttakeover/#real-time-account-take-over-analysis", 
            "text": "The analysis dashboard provides a granular set of metrics for analysis of the account activity in ATO application. The following metrics are shown in this dashboard:   Account Takeover Fraud Prevention Analysis  Account Hacking Breakdown by Channel  Total ATO Frauds  ATO Fraud Breakdown by Channel  Fraud Instances by Device  ATO Frauds  Frauds by EventType  Fraud Rule Matches  ATO Fraud in the USA (easily customizable for other countries)  Login failures by device type", 
            "title": "Real-time Account Take-over Analysis"
        }, 
        {
            "location": "/Accounttakeover/#real-time-account-take-over-operations", 
            "text": "The operations dashboard provides an operational overview of the application.  The following operational metrics can be viewed from this dashboard:   Application Latency  Rule Latency  Rule Executor Free Memory  Total Number of Process Failures  Fraud Breakdown by Channel  Event Throughput  Top Rules  Events by Device Type   You can use the OAS dashboards service that is specific to ATO application to visualize various metrics over the dashboards.  OAS Dashboards  For importing and viewing the dashboards, refer to  dtDashboard", 
            "title": "Real-time Account Take-over Operations"
        }, 
        {
            "location": "/quickstartlaunchfpa/", 
            "text": "Quick Start Guide - Omni-Channel Fraud Prevention Application\n\n\nSetup Docker and RTS\n\n\n\n\nSetup docker daemon host (preferably same as gateway machine). This supplies the docker images for \nOnline Analytics Service (OAS)\n, \nOAS Dashboards\n which is a customized implementation of Apache Superset, and \nCEP Workbench\n that is a customized Drools Workbench implementation.\n\n\nInstall rts 3.10 bin. In the installation wizard, specify the docker location.\n\n\n\n\n\nLaunching Fraud Prevention Application\n\n\n\n\nNavigate to the \nAppFactory page\n \n \nFinancial Services\n \n \nOmni-Channel Payment Fraud Prevention.\n\n\nIn the DataTorrent Omni Channel Fraud Prevention Application box, click \nimport\n. \n\n\nDownload the application after DataTorrent Omni Channel Fraud Prevention Application package is imported.\n\n\nNavigate to \nDevelop\n \n \nApplication Package\n \n \nData Torrent Omni Channel Fraud Prevention Application.\n Click \nlaunch\n drop-down and select \ndownload package\n. \n\n\nGet the Geolite Maxmind Database (Use Hadoop user or user that has access to Hadoop). Using Bash '\n\nurl http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz -o GeoLite2-City.tar.gz\ntar -zxvf GeoLite2-City.tar.gz \nhdfs dfs put GeoLite2-City*/GeoLite2-City.mmdb city.mmdb\n\n\nGenerate lookup data which will be used by enrichment operators in the DAG.  (Use Hadoop user or any user that has access to Hadoop. Using Bash\n\nmkdir fpa_package\ncd fpa_package\nunzip ../dt-cep-omni-fraud-prevention-app-1.4.0.apa \njava -cp app/*:lib/*:\nhadoop classpath\ncom.datatorrent.cep.transactionGenerator.DumpLookupData lookupdata\n\n\nCreate a New Configuration for the OmniChannelFraudPreventationApp.\n\n\nGo to \nDevelop\n \n \nApplication Configurations\n \n \n+ create new.\n\n\nSelect a Source Application and enter the Configuration Name and then click \nCreate\n. \n\n\nEnter the Required Properties. \n\n\nConfigure the \nCEP Workbench Service\n\n\nOn the configuration page, scroll down.\n \u00a0 - Select the \ndrools-workbench\n and click \nconfigure\n.\n\n\nClick \nsave\n after specifying the configuration.\n\nNote:\n Ensure that the Proxy Address is set correctly.\n\n\nConfigure the \nOnline Analytics Service\n.\n\n\nSelect the \nfpa-online-analytics-service\n and click \nconfigure\n.\n\n\nClick \nsave\n after specifying the configuration.\n\nNote\n :Ensure that the \nKafkaBrokers\n and the \nKafkaTopic\n is set correctly.\n\n\nConfigure the \nOAS Dashboards\n service.\n\n\nSelect \nsuperset-fpa\n and click \nconfigure\n\n\nClick \nsave\n after specifying the configuration.\n  \nNote\n : Ensure to set correct druid_cluster IP and the Proxy Address.\n\n\nConfigure the Dashboards.\n\n\nClick \nconfigure\n.\n\n\nFrom the \nSelect Replacement Applications\n drop down, select the corresponding configuration name for both the dashboards.\n\n\nClick \nSave\n.\n\n\nSave the configuration.\n\n\nClick \nSave.\n\n\nClick \nlaunch\n to launch the application.\n\n\n\n\nLaunch Test Data Generator Application\n\n\n\n\nCreate \nNew Configuration\n for the OmniChannelFraudPreventationDataGenerator.\n\n\nGo to \nDevelop\n \n \nApplication Packages \n + new configuration.\n\n\nAdd \nOptional Properties\n.\n\n\nUnder \nOptional Properties\n , click + \nadd\n and add the required optional properties.\n   \nNote:\n   \nKafka\n topic of the DataGenerator should be same as the \nTransaction Receiver\n topic of the Omni Channel Fraud Prevention Application.\n\n\nClick \nsave\n.\n\n\nClick \nlaunch\n to launch the Data Generator.", 
            "title": "Quick Start Guide - Omni-Channel Fraud Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchfpa/#quick-start-guide-omni-channel-fraud-prevention-application", 
            "text": "", 
            "title": "Quick Start Guide - Omni-Channel Fraud Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchfpa/#setup-docker-and-rts", 
            "text": "Setup docker daemon host (preferably same as gateway machine). This supplies the docker images for  Online Analytics Service (OAS) ,  OAS Dashboards  which is a customized implementation of Apache Superset, and  CEP Workbench  that is a customized Drools Workbench implementation.  Install rts 3.10 bin. In the installation wizard, specify the docker location.", 
            "title": "Setup Docker and RTS"
        }, 
        {
            "location": "/quickstartlaunchfpa/#launching-fraud-prevention-application", 
            "text": "Navigate to the  AppFactory page     Financial Services     Omni-Channel Payment Fraud Prevention.  In the DataTorrent Omni Channel Fraud Prevention Application box, click  import .   Download the application after DataTorrent Omni Channel Fraud Prevention Application package is imported.  Navigate to  Develop     Application Package     Data Torrent Omni Channel Fraud Prevention Application.  Click  launch  drop-down and select  download package .   Get the Geolite Maxmind Database (Use Hadoop user or user that has access to Hadoop). Using Bash ' url http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz -o GeoLite2-City.tar.gz\ntar -zxvf GeoLite2-City.tar.gz \nhdfs dfs put GeoLite2-City*/GeoLite2-City.mmdb city.mmdb  Generate lookup data which will be used by enrichment operators in the DAG.  (Use Hadoop user or any user that has access to Hadoop. Using Bash mkdir fpa_package\ncd fpa_package\nunzip ../dt-cep-omni-fraud-prevention-app-1.4.0.apa \njava -cp app/*:lib/*: hadoop classpath com.datatorrent.cep.transactionGenerator.DumpLookupData lookupdata  Create a New Configuration for the OmniChannelFraudPreventationApp.  Go to  Develop     Application Configurations     + create new.  Select a Source Application and enter the Configuration Name and then click  Create .   Enter the Required Properties.   Configure the  CEP Workbench Service  On the configuration page, scroll down.\n \u00a0 - Select the  drools-workbench  and click  configure .  Click  save  after specifying the configuration. Note:  Ensure that the Proxy Address is set correctly.  Configure the  Online Analytics Service .  Select the  fpa-online-analytics-service  and click  configure .  Click  save  after specifying the configuration. Note  :Ensure that the  KafkaBrokers  and the  KafkaTopic  is set correctly.  Configure the  OAS Dashboards  service.  Select  superset-fpa  and click  configure  Click  save  after specifying the configuration.\n   Note  : Ensure to set correct druid_cluster IP and the Proxy Address.  Configure the Dashboards.  Click  configure .  From the  Select Replacement Applications  drop down, select the corresponding configuration name for both the dashboards.  Click  Save .  Save the configuration.  Click  Save.  Click  launch  to launch the application.", 
            "title": "Launching Fraud Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchfpa/#launch-test-data-generator-application", 
            "text": "Create  New Configuration  for the OmniChannelFraudPreventationDataGenerator.  Go to  Develop     Application Packages   + new configuration.  Add  Optional Properties .  Under  Optional Properties  , click +  add  and add the required optional properties.\n    Note:     Kafka  topic of the DataGenerator should be same as the  Transaction Receiver  topic of the Omni Channel Fraud Prevention Application.  Click  save .  Click  launch  to launch the Data Generator.", 
            "title": "Launch Test Data Generator Application"
        }, 
        {
            "location": "/quickstartlaunchato/", 
            "text": "Quick Start Guide - Account Takeover Prevention Application\n\n\nSetup Docker and RTS\n\n\n\n\nSetup docker daemon host (preferably same as gateway machine). This supplies the docker images for \nOnline Analytics Service\n, \nOAS Dashboards Service\n that is a customized implementation of Apache Superset and \nCEP Workbench\n that is a customized Drools Workbench implementation.\n\n\nInstall rts 3.10 bin. In the installation wizard, specify the docker location.\n\n\n\n\nLaunching Account Takeover Prevention Application\n\n\n\n\nImport the Account Takeover Prevention Application from the AppFactory.\n\n\nIn the DataTorrent Account Takeover Prevention Application box, click \nimport\n. \n\n\nDownload the package, after DataTorrent Account Take Over Prevention Application package is imported.\n\n\nNavigate to \nDevelop\n \n \nApplication Package\n \n \nDataTorrent Account TakeOver Prevention Application\n.\n\n\nClick \nlaunch\n drop-down and select \ndownload package\n.\n\n\nGet the Geolite Maxmind Database (Use Hadoop user or user that has access to Hadoop). Using Bash '\n\nurl http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz -o GeoLite2-City.tar.gz\ntar -zxvf GeoLite2-City.tar.gz \nhdfs dfs put GeoLite2-City*/GeoLite2-City.mmdb city.mmdb\n.\n\n\nGenerate test lookup data which will be used by the enrichment operators in the DAG. Use Hadoop user or user that has access to Hadoop to run following commands:\n\n\n\n\nBash\nmkdir ato_package\ncd ato_package\nunzip ../dt-ato-prevention-application-1.4.0.apa \njava -cp app/*:lib/*:`hadoop classpath` com.datatorrent.ato.userActivityGenerator.DumpLookupData ato_lookupdata\n\n\n\n\n\n\nCreate configuration for ATO\n\n\nNavigate to \nDevelop\n \n \nApplicationPackages\n \n \n+ new configuration\n \n\n\nClick \ncreate\n. \n\n\nEnter the Required Properties. \n\n\nConfigure the \nCEP Workbench Service\n\n\nOn the configuration page, scroll down.\n \u00a0 - Select the \ndrools-workbench\n and click \nconfigure\n.\n\n\nClick \nsave\n after specifying the configuration.\n\n\n\n\nNote:\n Ensure that the Proxy Address is set correctly.\n9. Configure \nOnline Analytics Services\n.\n   - Select the \nato-online-analytics-service\n and click \nconfigure\n. \n\n   - Click \nsave\n after the configuration is set correctly.\n\n\nNote:\n Make sure \nKafkaBrokers\n and the \nKafkaTopic\n are set correctly.\n10. Configure \nOAS Dashboards\n.\n    - Select \nsuperset-ato\n and click \nconfigure\n. \n\n    - Click \nsave\n after the configuration is set correctly.\n\n\nNote\n : Make sure to set correct druid_cluster IP and the Proxy Address. \n11. Configure the Dashboards\n    - Click \nconfigure\n. \n\n    - From the \nSelect Replacement Applications\n drop down, select the correct configuration name for both the Dashboards.\n    - Click \nSave\n. \n12. Save the complete configuration.\n\n\nLaunch Test Data Generator Application\n\n\n\n\nCreate new configuration for the \nUserActivityGenerator\n.\n\n\nGo to \nDevelop\n \n \nApplication Packages\n \n \n+ new configuration\n.\n \n\n\nAdd Optional Properties.\n\n\nIn \nOptional Properties\n , click \n+add\n to add Optional Properties.\n  \n \n\n\n\n\nNote:\n   \nKafka\n topic of the DataGenerator should be same as the \nTransaction Receiver\n topic of the Omni Channel Fraud Prevention Application.\n   - Click \nsave\n.\n   - Click \nlaunch\n to launch the Data Generator.", 
            "title": "Quick Start Guide - Account Takeover Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchato/#quick-start-guide-account-takeover-prevention-application", 
            "text": "", 
            "title": "Quick Start Guide - Account Takeover Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchato/#setup-docker-and-rts", 
            "text": "Setup docker daemon host (preferably same as gateway machine). This supplies the docker images for  Online Analytics Service ,  OAS Dashboards Service  that is a customized implementation of Apache Superset and  CEP Workbench  that is a customized Drools Workbench implementation.  Install rts 3.10 bin. In the installation wizard, specify the docker location.", 
            "title": "Setup Docker and RTS"
        }, 
        {
            "location": "/quickstartlaunchato/#launching-account-takeover-prevention-application", 
            "text": "Import the Account Takeover Prevention Application from the AppFactory.  In the DataTorrent Account Takeover Prevention Application box, click  import .   Download the package, after DataTorrent Account Take Over Prevention Application package is imported.  Navigate to  Develop     Application Package     DataTorrent Account TakeOver Prevention Application .  Click  launch  drop-down and select  download package .  Get the Geolite Maxmind Database (Use Hadoop user or user that has access to Hadoop). Using Bash ' url http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz -o GeoLite2-City.tar.gz\ntar -zxvf GeoLite2-City.tar.gz \nhdfs dfs put GeoLite2-City*/GeoLite2-City.mmdb city.mmdb .  Generate test lookup data which will be used by the enrichment operators in the DAG. Use Hadoop user or user that has access to Hadoop to run following commands:   Bash\nmkdir ato_package\ncd ato_package\nunzip ../dt-ato-prevention-application-1.4.0.apa \njava -cp app/*:lib/*:`hadoop classpath` com.datatorrent.ato.userActivityGenerator.DumpLookupData ato_lookupdata   Create configuration for ATO  Navigate to  Develop     ApplicationPackages     + new configuration    Click  create .   Enter the Required Properties.   Configure the  CEP Workbench Service  On the configuration page, scroll down.\n \u00a0 - Select the  drools-workbench  and click  configure .  Click  save  after specifying the configuration.   Note:  Ensure that the Proxy Address is set correctly.\n9. Configure  Online Analytics Services .\n   - Select the  ato-online-analytics-service  and click  configure .  \n   - Click  save  after the configuration is set correctly.  Note:  Make sure  KafkaBrokers  and the  KafkaTopic  are set correctly.\n10. Configure  OAS Dashboards .\n    - Select  superset-ato  and click  configure .  \n    - Click  save  after the configuration is set correctly.  Note  : Make sure to set correct druid_cluster IP and the Proxy Address. \n11. Configure the Dashboards\n    - Click  configure .  \n    - From the  Select Replacement Applications  drop down, select the correct configuration name for both the Dashboards.\n    - Click  Save . \n12. Save the complete configuration.", 
            "title": "Launching Account Takeover Prevention Application"
        }, 
        {
            "location": "/quickstartlaunchato/#launch-test-data-generator-application", 
            "text": "Create new configuration for the  UserActivityGenerator .  Go to  Develop     Application Packages     + new configuration .    Add Optional Properties.  In  Optional Properties  , click  +add  to add Optional Properties.\n       Note:     Kafka  topic of the DataGenerator should be same as the  Transaction Receiver  topic of the Omni Channel Fraud Prevention Application.\n   - Click  save .\n   - Click  launch  to launch the Data Generator.", 
            "title": "Launch Test Data Generator Application"
        }, 
        {
            "location": "/cep_workbench/", 
            "text": "Introduction\n\n\nCEP Workbench is a \nDataTorrent RTS Service\n that is a customized Drools Workbench implementation. The service provides you with the capability to change the functionality of CEP applications using drools-based rules.\n\n\nThe CEP Workbench service is pre-packaged with applications like Omni-channel Fraud Prevention and Account Takeover Prevention.  From the DT RTS console, you can access CEP Workbench service, create the customized rules, and then apply these rules to the application configuration to change the application behavior.\n\n\nPre-requisites\n\n\nInstall Docker (Version 1.9.1 or greater) and provide the docker host setting by running the installation wizard in cases where Docker is installed on a machine other than the Gateway.  Please note that for automatic artifacts synchronization the Docker container with CEP Workbench and Gateway need to share the same physical host.  For more details, please refer to \nDocker Configuration\n\n\nAccessing CEP Workbench\n\n\nWhen the CEP Workbench service is imported and in a running state, you can access it via a proxy URL from either the application configuration page or from the Service Management page.\n\n\nSteps to access service via Application Configuration page:\n\n\n\n\nIn an application configuration, from the Services section, select the 'drools-workbench' service. The Service Details page is displayed.\n\n\nUnder Proxy URL, click the Web URL. The CEP Workbench service login page is displayed.\n\n\nTo login into the service, use username \nadmin\n and password \nadmin\n.\n\n\n\n\nSteps to access service via the Service Management page:\n\n\n\n\nClick the Settings icon \n located on the upper most right section of the page.\n\n\nSelect Services option. The Services page is displayed with the list of services.\n\n\nClick on the 'drools-workbench' service link. The Service Details page is displayed.\n\n\nUnder Proxy URL, click the Web URL. The CEP Workbench service login page is displayed.\n\n\nTo login into the service, use username \nadmin\n and password \nadmin\n.\n\n\n\n\nConfiguring Rules in CEP Workbench\n\n\nTo configure rules within the CEP Workbench, you must complete the following steps:\n\n\n\n\n\n\nCreate a schema in the Application Configuration\n:\n\n\n\n\nCreate an application configuration for an application.\n\n\nIn the application configuration, create and save a schema. This adds schema in the configuration of the application that you want to launch with the customized rules. \nCreating Schema\n\n\n\n\n\n\n\n\n\n\nCreate a project in Drools\n:\n\n\n\n\nAccess the Drools UI, through proxy URL on the \ndrools-workbench\n service details page.\n\n\nLog in to Drools UI using the default login credentials.\n\n\nClick the \nAuthoring\n tab and select \nProject Authoring\n.\n\n\nIn the Welcome page, click \nNew Project\n.\n\n\nIn the New Project page, enter a project name and description and click \nCreate\n. The new project gets listed under \nProject Authoring\n. For more details, see \nAdd Project\n.\n\n\n\n\n\n\n\n\n\n\nAdd Schema dependency\n:\n\n\n\n\nInside Drools, go to \nAuthoring \n Project Authoring\n and click the name link of the project.\n\n\nIn the Project page, click \nSettings\n.\n\n\nClick the \nProject Settings\n button on the left and select \nDependencies\n. The Dependencies page is displayed.\n\n\nClick \nAdd from Repository button\n. The Artifacts page is displayed and the schema that was created in DT RTS for the application configuration is listed there. The schema is now added as a dependency for the project.\n\n\n\n\n\n\nNote\n: For automatic artifacts synchronization the Docker container with CEP Workbench and Gateway need to share the same physical host and file system.\n\n\n\n\n\n\nAdd Rules file\n:\n\n\n\n\nGo to your Drools project and click \nCreate New Asset\n, select \nDRL file\n from dropdown . A file is created which gets listed in the project.\n\n\nOpen this file, add the rules, and click \nSave\n. For more details, refer to \nCreating Rules\n.\n\n\n\n\n\n\n\n\n\n\nAdd KieBase and KieSessionName to kmodule.xml\n:\n\n\n\n\nInside Drools, go to \nProject page \n Settings\n.\n\n\nClick the \nProject Settings\n button on the left and select Knowledge Bases and Sessions.\n\n\nAdd the KieBase and Kie Session name to kmodule.xml from here.\n\n\nClick \nSave\n.\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and Deploy Project\n:\n\n\n\n\nInside Drools, go to \nAuthoring \n Project Authoring\n and select your project.\n\n\nOn the upper right side, click \nBuild \n Deploy\n button. The Rules Jar is created which becomes automatically available in the application configuration in DT RTS.\n\n\n\n\n\n\n\n\n\n\nAdd Rules Jar to Application Configuration\n:\n\n\n\n\nGo to DT RTS and open the corresponding application configuration wherein you want to add the Rules JAR artifact.\n\n\nUnder \nJAR artifacts\n, click \nadd from artifact library\n.\n\n\nSelect the rule jar that was created in Drools Workbench. The Rules Jar that was created in Drools becomes automatically available in the application configuration in DT RTS.\n\n\n\n\n\n\n\n\n\n\nSpecify Optional Properties\n:\n\n\n\n\nIn the same application configuration, add the following properties in the \nOptional Properties\n section.\n\n\nClick \nSave\n and launch the application.\n\n\n\n\n\n\n\n\n\n\nProperty\n\n\n\n\n\n\n\n\n\n\ndt.operator.FraudRulesExecutor.prop.kieSessionName\n\n\n\n\n\n\ndt.operator.FraudRulesExecutor.prop.kiebaseName\n\n\n\n\n\n\ndt.operator.TransactionValidator.port.input.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.TransactionTransformer.port.input.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.TransactionTransformer.port.output.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.TransactionParser.port.out.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.AccountDataEnricher.port.fraudInput.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.FraudDataAnalyser.port.factsInput.attr.TUPLE_CLASS\n\n\n\n\n\n\ndt.operator.FraudDataAnalyser.fieldExtractors(demo)\n\n\n\n\n\n\ndt.operator.FraudDataAnalyser.fieldExtractors(fraud)\n\n\n\n\n\n\ndt.fraudprevention.enrichments.configFilePath", 
            "title": "CEP Workbench"
        }, 
        {
            "location": "/cep_workbench/#introduction", 
            "text": "CEP Workbench is a  DataTorrent RTS Service  that is a customized Drools Workbench implementation. The service provides you with the capability to change the functionality of CEP applications using drools-based rules.  The CEP Workbench service is pre-packaged with applications like Omni-channel Fraud Prevention and Account Takeover Prevention.  From the DT RTS console, you can access CEP Workbench service, create the customized rules, and then apply these rules to the application configuration to change the application behavior.", 
            "title": "Introduction"
        }, 
        {
            "location": "/cep_workbench/#pre-requisites", 
            "text": "Install Docker (Version 1.9.1 or greater) and provide the docker host setting by running the installation wizard in cases where Docker is installed on a machine other than the Gateway.  Please note that for automatic artifacts synchronization the Docker container with CEP Workbench and Gateway need to share the same physical host.  For more details, please refer to  Docker Configuration", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/cep_workbench/#accessing-cep-workbench", 
            "text": "When the CEP Workbench service is imported and in a running state, you can access it via a proxy URL from either the application configuration page or from the Service Management page.  Steps to access service via Application Configuration page:   In an application configuration, from the Services section, select the 'drools-workbench' service. The Service Details page is displayed.  Under Proxy URL, click the Web URL. The CEP Workbench service login page is displayed.  To login into the service, use username  admin  and password  admin .   Steps to access service via the Service Management page:   Click the Settings icon   located on the upper most right section of the page.  Select Services option. The Services page is displayed with the list of services.  Click on the 'drools-workbench' service link. The Service Details page is displayed.  Under Proxy URL, click the Web URL. The CEP Workbench service login page is displayed.  To login into the service, use username  admin  and password  admin .", 
            "title": "Accessing CEP Workbench"
        }, 
        {
            "location": "/cep_workbench/#configuring-rules-in-cep-workbench", 
            "text": "To configure rules within the CEP Workbench, you must complete the following steps:    Create a schema in the Application Configuration :   Create an application configuration for an application.  In the application configuration, create and save a schema. This adds schema in the configuration of the application that you want to launch with the customized rules.  Creating Schema      Create a project in Drools :   Access the Drools UI, through proxy URL on the  drools-workbench  service details page.  Log in to Drools UI using the default login credentials.  Click the  Authoring  tab and select  Project Authoring .  In the Welcome page, click  New Project .  In the New Project page, enter a project name and description and click  Create . The new project gets listed under  Project Authoring . For more details, see  Add Project .      Add Schema dependency :   Inside Drools, go to  Authoring   Project Authoring  and click the name link of the project.  In the Project page, click  Settings .  Click the  Project Settings  button on the left and select  Dependencies . The Dependencies page is displayed.  Click  Add from Repository button . The Artifacts page is displayed and the schema that was created in DT RTS for the application configuration is listed there. The schema is now added as a dependency for the project.    Note : For automatic artifacts synchronization the Docker container with CEP Workbench and Gateway need to share the same physical host and file system.    Add Rules file :   Go to your Drools project and click  Create New Asset , select  DRL file  from dropdown . A file is created which gets listed in the project.  Open this file, add the rules, and click  Save . For more details, refer to  Creating Rules .      Add KieBase and KieSessionName to kmodule.xml :   Inside Drools, go to  Project page   Settings .  Click the  Project Settings  button on the left and select Knowledge Bases and Sessions.  Add the KieBase and Kie Session name to kmodule.xml from here.  Click  Save .       Build and Deploy Project :   Inside Drools, go to  Authoring   Project Authoring  and select your project.  On the upper right side, click  Build   Deploy  button. The Rules Jar is created which becomes automatically available in the application configuration in DT RTS.      Add Rules Jar to Application Configuration :   Go to DT RTS and open the corresponding application configuration wherein you want to add the Rules JAR artifact.  Under  JAR artifacts , click  add from artifact library .  Select the rule jar that was created in Drools Workbench. The Rules Jar that was created in Drools becomes automatically available in the application configuration in DT RTS.      Specify Optional Properties :   In the same application configuration, add the following properties in the  Optional Properties  section.  Click  Save  and launch the application.      Property      dt.operator.FraudRulesExecutor.prop.kieSessionName    dt.operator.FraudRulesExecutor.prop.kiebaseName    dt.operator.TransactionValidator.port.input.attr.TUPLE_CLASS    dt.operator.TransactionTransformer.port.input.attr.TUPLE_CLASS    dt.operator.TransactionTransformer.port.output.attr.TUPLE_CLASS    dt.operator.TransactionParser.port.out.attr.TUPLE_CLASS    dt.operator.AccountDataEnricher.port.fraudInput.attr.TUPLE_CLASS    dt.operator.FraudDataAnalyser.port.factsInput.attr.TUPLE_CLASS    dt.operator.FraudDataAnalyser.fieldExtractors(demo)    dt.operator.FraudDataAnalyser.fieldExtractors(fraud)    dt.fraudprevention.enrichments.configFilePath", 
            "title": "Configuring Rules in CEP Workbench"
        }, 
        {
            "location": "/oas_dashboards/", 
            "text": "Introduction\n\n\nOAS Dashboards service is a \nDataTorrent RTS Service\n that is a customized implementation of Apache Superset. This service packages application-specific OAS dashboards with a rich set of data visualizations and an easy-to-use interface for exploring and visualizing the application data.  The \nOAS\n provides the neccesary backend infrastructure for OAS Dashboards Service to query for application data used in the dashboard visualizations.\n\n\nThe OAS Dashboards service helps you to visualize real time outcomes, historical trends, real time and historical KPIs, real time operational metrics, etc. for the following DataTorrent RTS applications:\n\n\n\n\nOmni-Channel Fraud Prevention Application\n\n\nAccount Take Over Prevention Application\n\n\n\n\nPre-requisites\n\n\nInstall Docker (Version 1.9.1 or greater) and provide the docker host setting by running the installation wizard in cases where Docker is installed on a machine other than the Gateway.  For more details, please refer to \nDocker Configuration\n\n\nAccessing OAS Dashboards service\n\n\nThe OAS Dashboards service is installed and launched automatically for some RTS applications.  Once installed and running, you can access the OAS dashboards as embedded IFrame widgets inside the application dashboards, or directly via the proxy URL displayed on the service details page.\n\n\nSteps to view service details via application dashboards -\n\n\n\n\nClick the \nMonitor\n tab and open an application.\n\n\nClick the \nvisualize\n drop-down and select one of the available dashboards.  Dashboard will render the IFrame widget with visualizations coming directly from OAS Dashboards service.\n\n\n\n\nSteps to view the service details via Service Management Page -\n\n\n\n\nClick the Settings icon \n located on the upper most right section of the page. \n\n\nClick on the \nServices\n option.\n\n\nClick an OAS Dashboards service from the list of services. The Service Details page is displayed.\n\n\n\n\nManaging OAS Dashboards\n\n\nThe OAS Dashboards proxy URL redirects you to the list of OAS dashboards which is imported for the related application. The following menu items are provided to browse through the dashboards and widgets/slices and OAS cluster connection.\n\n\n\n\n\n\n\n\n\n\nmenu item\n\n\naction\n\n\n\n\n\n\n\n\n\n\nImport dashboards\n\n\nAllows you to import dashboard files (in .pickle format) into the running Superset service\n\n\n\n\n\n\nDruid Clusters\n\n\nLists all Druid clusters connected to the Superset instance. By default, a connection is OAS is already created\n\n\n\n\n\n\nScan New DataSources\n\n\nScans new datasources in the Druid clusters and lists them\n\n\n\n\n\n\nRefresh Druid MetaData\n\n\nRefreshes the metadata and datasource list of Druid cluster connections\n\n\n\n\n\n\nSlices\n\n\nLists the widgets/charts that have been created in the Superset service. You can add/edit/delete any widget from this list\n\n\n\n\n\n\nDashboards\n\n\nLists the dashboards created/imported within the Superset service. You can edit/delete/export dashboards from this list\n\n\n\n\n\n\n\n\nDashboard Controls\n\n\nThere are specific actions that can be performed on the Dashboard. A more descriptive list of these actions can be found below:\n\n\n\n\n\n\n\n\n\n\naction\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nForce refresh the whole dashboard\n\n\nThis action would force a refresh on all widgets on the dashboard\n\n\n\n\n\n\nAdd a new slice to the dashboard\n\n\nThis action enables you to directly add any of the existing slices/widgets to the currently viewed dashboard\n\n\n\n\n\n\nSet refresh interval\n\n\nThis action allows you to set a pre-defined refresh frequency that would force refresh of all slices on the dashboard after the selcted interval\n\n\n\n\n\n\nActive Dashboard filters\n\n\nThis action enables you to apply filters on all slices present inside the dashboard\n\n\n\n\n\n\nAdd CSS\n\n\nThis action allows you to add cutom CSS for the dashboard and save it in a template or use the pre-existing CSS templates\n\n\n\n\n\n\nMail Dashboard\n\n\nThis action enables you to email the Dashboard\n\n\n\n\n\n\nEdit Dashboard properties\n\n\nThis action lets you edit dashboard properties, eg. slices inside dashboard, height, width, etc.\n\n\n\n\n\n\nSave dashboard\n\n\nThis action will save any temporary changes made to the dashboard\n\n\n\n\n\n\n\n\nWidget Controls\n\n\nThere are specific actions that can be performed on each widget/slice on the dashboard. A more descriptive list of these actions can be found below:\n\n\n\n\n\n\n\n\n\n\naction\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nMove chart\n\n\nThis action allows you to move around the widget and set their position\n\n\n\n\n\n\nForce refresh data\n\n\nThis action allows you to force refresh the data being displayed inside the slice/widget\n\n\n\n\n\n\nEdit chart\n\n\nThis action allows you to edit the slice properties like vidualization type, dashboard and different parameters\n\n\n\n\n\n\nExport CSV\n\n\nThis action allows you to export the chart data in CSV format\n\n\n\n\n\n\nExplore chart\n\n\nThis action allows you to view/modify the slice query. For more details, pelase refer to \nexploring your data\n\n\n\n\n\n\nRemove\n\n\nThis action removes the slice/widget from the current dashboard view", 
            "title": "OAS Dashboards"
        }, 
        {
            "location": "/oas_dashboards/#introduction", 
            "text": "OAS Dashboards service is a  DataTorrent RTS Service  that is a customized implementation of Apache Superset. This service packages application-specific OAS dashboards with a rich set of data visualizations and an easy-to-use interface for exploring and visualizing the application data.  The  OAS  provides the neccesary backend infrastructure for OAS Dashboards Service to query for application data used in the dashboard visualizations.  The OAS Dashboards service helps you to visualize real time outcomes, historical trends, real time and historical KPIs, real time operational metrics, etc. for the following DataTorrent RTS applications:   Omni-Channel Fraud Prevention Application  Account Take Over Prevention Application", 
            "title": "Introduction"
        }, 
        {
            "location": "/oas_dashboards/#pre-requisites", 
            "text": "Install Docker (Version 1.9.1 or greater) and provide the docker host setting by running the installation wizard in cases where Docker is installed on a machine other than the Gateway.  For more details, please refer to  Docker Configuration", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/oas_dashboards/#accessing-oas-dashboards-service", 
            "text": "The OAS Dashboards service is installed and launched automatically for some RTS applications.  Once installed and running, you can access the OAS dashboards as embedded IFrame widgets inside the application dashboards, or directly via the proxy URL displayed on the service details page.  Steps to view service details via application dashboards -   Click the  Monitor  tab and open an application.  Click the  visualize  drop-down and select one of the available dashboards.  Dashboard will render the IFrame widget with visualizations coming directly from OAS Dashboards service.   Steps to view the service details via Service Management Page -   Click the Settings icon   located on the upper most right section of the page.   Click on the  Services  option.  Click an OAS Dashboards service from the list of services. The Service Details page is displayed.", 
            "title": "Accessing OAS Dashboards service"
        }, 
        {
            "location": "/oas_dashboards/#managing-oas-dashboards", 
            "text": "The OAS Dashboards proxy URL redirects you to the list of OAS dashboards which is imported for the related application. The following menu items are provided to browse through the dashboards and widgets/slices and OAS cluster connection.      menu item  action      Import dashboards  Allows you to import dashboard files (in .pickle format) into the running Superset service    Druid Clusters  Lists all Druid clusters connected to the Superset instance. By default, a connection is OAS is already created    Scan New DataSources  Scans new datasources in the Druid clusters and lists them    Refresh Druid MetaData  Refreshes the metadata and datasource list of Druid cluster connections    Slices  Lists the widgets/charts that have been created in the Superset service. You can add/edit/delete any widget from this list    Dashboards  Lists the dashboards created/imported within the Superset service. You can edit/delete/export dashboards from this list", 
            "title": "Managing OAS Dashboards"
        }, 
        {
            "location": "/oas_dashboards/#dashboard-controls", 
            "text": "There are specific actions that can be performed on the Dashboard. A more descriptive list of these actions can be found below:      action  description      Force refresh the whole dashboard  This action would force a refresh on all widgets on the dashboard    Add a new slice to the dashboard  This action enables you to directly add any of the existing slices/widgets to the currently viewed dashboard    Set refresh interval  This action allows you to set a pre-defined refresh frequency that would force refresh of all slices on the dashboard after the selcted interval    Active Dashboard filters  This action enables you to apply filters on all slices present inside the dashboard    Add CSS  This action allows you to add cutom CSS for the dashboard and save it in a template or use the pre-existing CSS templates    Mail Dashboard  This action enables you to email the Dashboard    Edit Dashboard properties  This action lets you edit dashboard properties, eg. slices inside dashboard, height, width, etc.    Save dashboard  This action will save any temporary changes made to the dashboard", 
            "title": "Dashboard Controls"
        }, 
        {
            "location": "/oas_dashboards/#widget-controls", 
            "text": "There are specific actions that can be performed on each widget/slice on the dashboard. A more descriptive list of these actions can be found below:      action  description      Move chart  This action allows you to move around the widget and set their position    Force refresh data  This action allows you to force refresh the data being displayed inside the slice/widget    Edit chart  This action allows you to edit the slice properties like vidualization type, dashboard and different parameters    Export CSV  This action allows you to export the chart data in CSV format    Explore chart  This action allows you to view/modify the slice query. For more details, pelase refer to  exploring your data    Remove  This action removes the slice/widget from the current dashboard view", 
            "title": "Widget Controls"
        }, 
        {
            "location": "/oas/", 
            "text": "Online Analytics Service (OAS)\n\n\nOnline Analytics Service (OAS) is an Apex application that is delivered as a service in some of the DataTorrent RTS applications. An example of one such application is \nOmni Channel Fraud Prevention Application\n.  OAS is a \nDruid\n based application that supports querying in real-time on data streams that are populated by a source application such as \nOmni Channel Fraud Prevention Application\n.\n\n\nOAS is integrated with the \nOAS Dashboards Service\n which is a DataTorrent RTS Service that has been built using \nApache Superset\n. The OAS provides the powerful query engine in the backend for the \nOAS Dashboards Service\n which enables impressive visualization on the front-end for any application on the DataTorrent RTS platform. Both OAS and \nOAS Dashboards\n are components of a complete end-to-end Data Analytics solution for actionable insights into real-time data flowing through a DataTorrent RTS application pipeline.\n\n\nOAS is available only with the \nDT Premium\n license\n.\n\n\nWorkflow of OAS\n\n\nThe following image depicts the workflow of OAS.\n\n\n\n\n\n\nOAS is enabled to stream-in data from Apache Kafka. Any datasource can send the applicable data for analysis into a Kafka topic.\n\n\nThe Online Analytics Service takes this real-time data-feed from Apache Kafka, computes and aggregates to generate derived data, and makes it ready for querying.\n\n\nOAS Dashboards service performs queries on OAS in real time using REST APIs via the DT gateway.\n\n\n\n\nOAS can be specified as a required service for an application and managed from the \nServices\n page.\n\n\nPackaging OAS\n\n\nRefer to \nPackaging Services\n for more details.\n\n\nManaging OAS Service\n\n\nRefer to \nManaging Services\n for more details.", 
            "title": "Online Analytics Service (OAS)"
        }, 
        {
            "location": "/oas/#online-analytics-service-oas", 
            "text": "Online Analytics Service (OAS) is an Apex application that is delivered as a service in some of the DataTorrent RTS applications. An example of one such application is  Omni Channel Fraud Prevention Application .  OAS is a  Druid  based application that supports querying in real-time on data streams that are populated by a source application such as  Omni Channel Fraud Prevention Application .  OAS is integrated with the  OAS Dashboards Service  which is a DataTorrent RTS Service that has been built using  Apache Superset . The OAS provides the powerful query engine in the backend for the  OAS Dashboards Service  which enables impressive visualization on the front-end for any application on the DataTorrent RTS platform. Both OAS and  OAS Dashboards  are components of a complete end-to-end Data Analytics solution for actionable insights into real-time data flowing through a DataTorrent RTS application pipeline.  OAS is available only with the  DT Premium  license .", 
            "title": "Online Analytics Service (OAS)"
        }, 
        {
            "location": "/oas/#workflow-of-oas", 
            "text": "The following image depicts the workflow of OAS.    OAS is enabled to stream-in data from Apache Kafka. Any datasource can send the applicable data for analysis into a Kafka topic.  The Online Analytics Service takes this real-time data-feed from Apache Kafka, computes and aggregates to generate derived data, and makes it ready for querying.  OAS Dashboards service performs queries on OAS in real time using REST APIs via the DT gateway.   OAS can be specified as a required service for an application and managed from the  Services  page.", 
            "title": "Workflow of OAS"
        }, 
        {
            "location": "/oas/#packaging-oas", 
            "text": "Refer to  Packaging Services  for more details.", 
            "title": "Packaging OAS"
        }, 
        {
            "location": "/oas/#managing-oas-service", 
            "text": "Refer to  Managing Services  for more details.", 
            "title": "Managing OAS Service"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application administration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools. Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application administration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools. Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/application_configurations/", 
            "text": "Application Configurations\n\n\nAn application configuration is a collection of custom settings applied to an application.\nThese settings may include properties, launch options, artifacts, services, and dashboards.\nApplication Configurations can be thought of as wrappers for Applications, which allow for clean separation between \napplication sources and application runtime configurations.  Configurations are based on \nConfiguration Packages\n and can be created, modified, and deleted without affecting the source application packages.  And once created they can be downloaded, shared, and uploaded, enabling convenient storage and transportation of application settings across multiple environments.\n\n\n\n\nCreating an Application Configuration\n\n\nApplication configurations are created from an existing source application. Multiple\napplication configurations can target the same source application, but each application\nconfiguration must have a unique name.\n\n\nTo create an application configuration:\n\n\n\n\nNavigate to the \nLaunch\n page and click on the \nApplications\n tab.\n\n\nFind the application you want to configure from the list, and click on the dropdown next to the launch button.\n\n\n\n\nFrom the dropdown, select \nnew configuration\n.\n\n\n\n\n\n\n\n\nIn the \nNew Application Configuration\n modal, confirm the \nSource Application\n is correct and name your configuration.\n\n\n\n\nClick \ncreate\n. You will be navigated to the newly created configuration's page.\n\n\n\n\nThere are additional ways to create application configurations:\n\n\n\n\nIn the \nLaunch\n page, using the \nnew configuration\n button above the Applications and Configurations table.\n\n\nIn the \nDevelop \n Application Packages\n page, using the dropdown next to an application's launch button, select \nnew configuration\n.\n\n\nIn the \nDevelop \n Application Configurations\n page, using the \ncreate new\n button above the Configurations table.\n\n\n\n\nLaunching Quickly With Temporary Configurations\n\n\nIf you want to quickly launch an application without saving an application configuration,\nyou can just use the \nlaunch\n button in the \nLaunch\n page. However, applications with\n\nRequired Properties\n must be configured, so a temporary configuration can be used.\n\n\n\n\nWorking with temporary configurations is the same as working with a saved configuration, except\ntemporary configurations will be discarded once you navigate away. Temporary\nconfigurations can be converted to regular application configurations by saving them.\n\n\nWorking With Application Configurations\n\n\n\n\nHeader\n\n\nApplication Details\n\n\nProperties\n\n\nLaunch Options\n\n\nJAR Artifacts\n\n\nFile Artifacts\n\n\nServices\n\n\nDashboards\n\n\n\n\nHeader\n\n\nThe header contains all of the main configuration actions. This is where you can \nlaunch\n, \nsave\n, \ndiscard\n, and more.\n\n\n\n\nThe configuration status is displayed on the top right of the header. The status changes as instances of the configuration are detected. The header displays additional instance-related actions conditionally, like \nkill\n and \nview\n when it finds a \nRUNNING\n instance.\n\n\nActions Menu\n\n\nThe actions (ellipsis) menu contains additional actions like \ncopy\n, \ndownload\n, \ndelete\n, and \nsettings\n, as well as some \napplication package actions\n. If the configuration has a \nRUNNING\n instance, the \napplication instance actions\n section will appear and allow managing the instance.\n\n\n\n\nSettings\n\n\nSelecting \nsettings\n in the actions menu opens the settings modal, which allows you to edit the \nConfiguration Name\n\nand retarget the \nSource Application\n.\n\n\nRetargeting allows you to carry over your configuration settings to another source application. This is useful when working with multiple versions of an application; you can \ncopy\n the configuration, then retarget the source application in the copied configuration.\n\n\nWhen an application configuration's source application is deleted, it becomes orphaned. You can reupload the source application or retarget the configuration to restore functionality.\n\n\nThe settings modal can also be accessed by clicking the \nedit\n button next to the configuration name in the header.\n\n\nApplication Details\n\n\nThe application details section gives an overview of the source application and the package it belongs to. The \napp package\n\nand \napp\n links are useful for quick access to source details. The application DAG can be inspected by clicking-and-dragging to pan the diagram.\n\n\n\n\nIn the case where a configuration becomes orphaned (source application not found), the \napp package\n link will be highlighted red. Clicking it will reveal the Settings modal which you can use to retarget the source application. See more about this in the \nSettings\n section above.\n\n\nProperties\n\n\nApplication Package Properties\n\n\nA list of properties from the source application package. These properties can be overridden in \nOptional Properties\n by using the same property name.\n\n\nNote\n: Package Properties with names or values that are too long to display are cut off. Hover over the table cell to see\nthe rest. To select the entire string including the cut off content, double-click the text in the table cell.\n\n\nRequired Properties\n\n\nA list of properties that must have defined values before the application configuration can be launched.\nThis section only appears if the application has defined required properties.\n\n\n\n\nOptional Properties\n\n\nUse this section to provide additional properties, or to override properties listed\nin \nApplication Package Properties\n.\n\n\nThe entire list of \nApplication Package Properties\n can be added to this section by\nselecting \ndefault properties\n in the \nadd\n dropdown.\n\n\nGarbage collection (GC) logging can be enabled by selecting \nenable gc logging\n in the\n\nadd\n dropdown. This will add an optional property \n\n\nLaunch Options\n\n\nThe launch options section contains environment-specific options, as opposed to application-specific options\nlike \nProperties\n. These options are generated by platform settings (e.g. Hadoop YARN settings)\nand are not meant to be portable.\n\n\n\n\nThe \nQueue to launch this application under\n option allows you to choose a Hadoop YARN queue. See the\n\nHadoop YARN Capacity Scheduler documentation\n\nfor more details.\n\n\nJAR Artifacts\n\n\nJAR artifacts can be added to the application configuration by uploading or adding from the artifact library. New AVRO schemas can also be created. JAR artifacts will be added to the application's classpath.\n\n\n\n\nTo add non-JAR files to the configuration, see \nFile Artifacts\n.\n\n\nAdding From Artifact Library\n\n\nArtifacts can be added to the configuration as a reference to a JAR in the artifact library. As a result, these artifacts are not copied directly into the configuration (.apc).\n\n\nTo add an artifact from the artifact library, click on the \nadd from artifact library\n dropdown and select an artifact.\n\n\nCreating a New Schema\n\n\nNew AVRO schemas can be created and added to the configuration as a reference. Schemas created within application configurations will be stored in the artifact library.\n\n\nTo create a new schema, click on the \nadd\n dropdown under the \nJAR Artifacts\n header and select \nnew schema\n. The Group ID, Artifact ID, Version, and Schema Content fields must be completed.\n\n\nUploading JAR Artifacts\n\n\nTo upload a JAR artifact, click on the \nadd\n dropdown under the \nJAR Artifacts\n header and select \nupload JAR\n.\n\n\nThe entirety of the uploaded JAR artifacts are stored inside the configuration (.apc), and will not be added to the artifact library. Unlike the references stored in the configuration by \nAdding From Artifact Library\n or \nCreating a New Schema\n, these JARs travel along with the configuration when downloaded and shared.\n\n\nNote\n: Make sure you save the configuration after uploading your JAR artifacts. If you decide not to save the configuration, recently uploaded JARs will still be accessible for a limited time in the \nadd\n dropdown's \nrecent uploads\n section.\n\n\nFile Artifacts\n\n\nFiles can be uploaded and added to the application configuration. This is useful for uploading configuration specific datasets.\n\n\n\n\nTo add JAR files to the configuration, see \nJAR Artifacts\n.\n\n\nUploading File Artifacts\n\n\nTo upload a file, click on the \nadd\n dropdown under the \nFile Artifacts\n header and select \nupload\n.\n\n\nLike uploaded JAR artifacts, file artifacts are stored inside the configuration. As a result, these files can be found inside the .apc when downloaded, and are still accessible when the .apc is reuploaded.\n\n\nServices\n\n\nSee the main \nServices\n page for an in-depth explanation of services.\n\n\nBy default, services are pulled in from the source application package. Services can also be added to application configurations. Services in application configurations will be started (or installed first, then started) when the configuration is launched. Some applications have predefined services and will appear in the services list when the configuration is created. Custom services can also be added to the configuration.\n\n\n\n\nAdding a Service\n\n\nTo add a new service to the configuration, click on the \nadd\n dropdown under the \nServices\n header and select \nnew service\n.\n\n\nOtherwise, you can add services defined within app packages by selecting \nfrom app packages\n or any existing installed services by selecting \nfrom installed services\n.\n\n\nOnce an application configuration service is installed, it will appear in the \nServices\n page, which can be accessed by using the cog button on the main navigation bar and selecting \nServices\n.\n\n\nDashboards\n\n\nDashboards are automatically imported (if they have not already been) when the application configuration is launched. These dashboards are pulled in from the source application package and have predefined target applications for each widget data source.\n\n\n\n\nSee the \nPackaged Dashboard\n section in the dtDashboard page for more details.\n\n\nConfiguring a Packaged Dashboard\n\n\nTo configure a packaged dashboard, click the \nconfigure\n button. In the modal, you can change the dashboard name and target applications. Target applications can be either the current application configuration, or other running applications that have data sources.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/application_configurations/#application-configurations", 
            "text": "An application configuration is a collection of custom settings applied to an application.\nThese settings may include properties, launch options, artifacts, services, and dashboards.\nApplication Configurations can be thought of as wrappers for Applications, which allow for clean separation between \napplication sources and application runtime configurations.  Configurations are based on  Configuration Packages  and can be created, modified, and deleted without affecting the source application packages.  And once created they can be downloaded, shared, and uploaded, enabling convenient storage and transportation of application settings across multiple environments.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/application_configurations/#creating-an-application-configuration", 
            "text": "Application configurations are created from an existing source application. Multiple\napplication configurations can target the same source application, but each application\nconfiguration must have a unique name.  To create an application configuration:   Navigate to the  Launch  page and click on the  Applications  tab.  Find the application you want to configure from the list, and click on the dropdown next to the launch button.   From the dropdown, select  new configuration .     In the  New Application Configuration  modal, confirm the  Source Application  is correct and name your configuration.   Click  create . You will be navigated to the newly created configuration's page.   There are additional ways to create application configurations:   In the  Launch  page, using the  new configuration  button above the Applications and Configurations table.  In the  Develop   Application Packages  page, using the dropdown next to an application's launch button, select  new configuration .  In the  Develop   Application Configurations  page, using the  create new  button above the Configurations table.", 
            "title": "Creating an Application Configuration"
        }, 
        {
            "location": "/application_configurations/#launching-quickly-with-temporary-configurations", 
            "text": "If you want to quickly launch an application without saving an application configuration,\nyou can just use the  launch  button in the  Launch  page. However, applications with Required Properties  must be configured, so a temporary configuration can be used.   Working with temporary configurations is the same as working with a saved configuration, except\ntemporary configurations will be discarded once you navigate away. Temporary\nconfigurations can be converted to regular application configurations by saving them.", 
            "title": "Launching Quickly With Temporary Configurations"
        }, 
        {
            "location": "/application_configurations/#working-with-application-configurations", 
            "text": "Header  Application Details  Properties  Launch Options  JAR Artifacts  File Artifacts  Services  Dashboards", 
            "title": "Working With Application Configurations"
        }, 
        {
            "location": "/application_configurations/#header", 
            "text": "The header contains all of the main configuration actions. This is where you can  launch ,  save ,  discard , and more.   The configuration status is displayed on the top right of the header. The status changes as instances of the configuration are detected. The header displays additional instance-related actions conditionally, like  kill  and  view  when it finds a  RUNNING  instance.", 
            "title": "Header"
        }, 
        {
            "location": "/application_configurations/#actions-menu", 
            "text": "The actions (ellipsis) menu contains additional actions like  copy ,  download ,  delete , and  settings , as well as some  application package actions . If the configuration has a  RUNNING  instance, the  application instance actions  section will appear and allow managing the instance.", 
            "title": "Actions Menu"
        }, 
        {
            "location": "/application_configurations/#settings", 
            "text": "Selecting  settings  in the actions menu opens the settings modal, which allows you to edit the  Configuration Name \nand retarget the  Source Application .  Retargeting allows you to carry over your configuration settings to another source application. This is useful when working with multiple versions of an application; you can  copy  the configuration, then retarget the source application in the copied configuration.  When an application configuration's source application is deleted, it becomes orphaned. You can reupload the source application or retarget the configuration to restore functionality.  The settings modal can also be accessed by clicking the  edit  button next to the configuration name in the header.", 
            "title": "Settings"
        }, 
        {
            "location": "/application_configurations/#application-details", 
            "text": "The application details section gives an overview of the source application and the package it belongs to. The  app package \nand  app  links are useful for quick access to source details. The application DAG can be inspected by clicking-and-dragging to pan the diagram.   In the case where a configuration becomes orphaned (source application not found), the  app package  link will be highlighted red. Clicking it will reveal the Settings modal which you can use to retarget the source application. See more about this in the  Settings  section above.", 
            "title": "Application Details"
        }, 
        {
            "location": "/application_configurations/#properties", 
            "text": "", 
            "title": "Properties"
        }, 
        {
            "location": "/application_configurations/#application-package-properties", 
            "text": "A list of properties from the source application package. These properties can be overridden in  Optional Properties  by using the same property name.  Note : Package Properties with names or values that are too long to display are cut off. Hover over the table cell to see\nthe rest. To select the entire string including the cut off content, double-click the text in the table cell.", 
            "title": "Application Package Properties"
        }, 
        {
            "location": "/application_configurations/#required-properties", 
            "text": "A list of properties that must have defined values before the application configuration can be launched.\nThis section only appears if the application has defined required properties.", 
            "title": "Required Properties"
        }, 
        {
            "location": "/application_configurations/#optional-properties", 
            "text": "Use this section to provide additional properties, or to override properties listed\nin  Application Package Properties .  The entire list of  Application Package Properties  can be added to this section by\nselecting  default properties  in the  add  dropdown.  Garbage collection (GC) logging can be enabled by selecting  enable gc logging  in the add  dropdown. This will add an optional property", 
            "title": "Optional Properties"
        }, 
        {
            "location": "/application_configurations/#launch-options", 
            "text": "The launch options section contains environment-specific options, as opposed to application-specific options\nlike  Properties . These options are generated by platform settings (e.g. Hadoop YARN settings)\nand are not meant to be portable.   The  Queue to launch this application under  option allows you to choose a Hadoop YARN queue. See the Hadoop YARN Capacity Scheduler documentation \nfor more details.", 
            "title": "Launch Options"
        }, 
        {
            "location": "/application_configurations/#jar-artifacts", 
            "text": "JAR artifacts can be added to the application configuration by uploading or adding from the artifact library. New AVRO schemas can also be created. JAR artifacts will be added to the application's classpath.   To add non-JAR files to the configuration, see  File Artifacts .", 
            "title": "JAR Artifacts"
        }, 
        {
            "location": "/application_configurations/#adding-from-artifact-library", 
            "text": "Artifacts can be added to the configuration as a reference to a JAR in the artifact library. As a result, these artifacts are not copied directly into the configuration (.apc).  To add an artifact from the artifact library, click on the  add from artifact library  dropdown and select an artifact.", 
            "title": "Adding From Artifact Library"
        }, 
        {
            "location": "/application_configurations/#creating-a-new-schema", 
            "text": "New AVRO schemas can be created and added to the configuration as a reference. Schemas created within application configurations will be stored in the artifact library.  To create a new schema, click on the  add  dropdown under the  JAR Artifacts  header and select  new schema . The Group ID, Artifact ID, Version, and Schema Content fields must be completed.", 
            "title": "Creating a New Schema"
        }, 
        {
            "location": "/application_configurations/#uploading-jar-artifacts", 
            "text": "To upload a JAR artifact, click on the  add  dropdown under the  JAR Artifacts  header and select  upload JAR .  The entirety of the uploaded JAR artifacts are stored inside the configuration (.apc), and will not be added to the artifact library. Unlike the references stored in the configuration by  Adding From Artifact Library  or  Creating a New Schema , these JARs travel along with the configuration when downloaded and shared.  Note : Make sure you save the configuration after uploading your JAR artifacts. If you decide not to save the configuration, recently uploaded JARs will still be accessible for a limited time in the  add  dropdown's  recent uploads  section.", 
            "title": "Uploading JAR Artifacts"
        }, 
        {
            "location": "/application_configurations/#file-artifacts", 
            "text": "Files can be uploaded and added to the application configuration. This is useful for uploading configuration specific datasets.   To add JAR files to the configuration, see  JAR Artifacts .", 
            "title": "File Artifacts"
        }, 
        {
            "location": "/application_configurations/#uploading-file-artifacts", 
            "text": "To upload a file, click on the  add  dropdown under the  File Artifacts  header and select  upload .  Like uploaded JAR artifacts, file artifacts are stored inside the configuration. As a result, these files can be found inside the .apc when downloaded, and are still accessible when the .apc is reuploaded.", 
            "title": "Uploading File Artifacts"
        }, 
        {
            "location": "/application_configurations/#services", 
            "text": "See the main  Services  page for an in-depth explanation of services.  By default, services are pulled in from the source application package. Services can also be added to application configurations. Services in application configurations will be started (or installed first, then started) when the configuration is launched. Some applications have predefined services and will appear in the services list when the configuration is created. Custom services can also be added to the configuration.", 
            "title": "Services"
        }, 
        {
            "location": "/application_configurations/#adding-a-service", 
            "text": "To add a new service to the configuration, click on the  add  dropdown under the  Services  header and select  new service .  Otherwise, you can add services defined within app packages by selecting  from app packages  or any existing installed services by selecting  from installed services .  Once an application configuration service is installed, it will appear in the  Services  page, which can be accessed by using the cog button on the main navigation bar and selecting  Services .", 
            "title": "Adding a Service"
        }, 
        {
            "location": "/application_configurations/#dashboards", 
            "text": "Dashboards are automatically imported (if they have not already been) when the application configuration is launched. These dashboards are pulled in from the source application package and have predefined target applications for each widget data source.   See the  Packaged Dashboard  section in the dtDashboard page for more details.", 
            "title": "Dashboards"
        }, 
        {
            "location": "/application_configurations/#configuring-a-packaged-dashboard", 
            "text": "To configure a packaged dashboard, click the  configure  button. In the modal, you can change the dashboard name and target applications. Target applications can be either the current application configuration, or other running applications that have data sources.", 
            "title": "Configuring a Packaged Dashboard"
        }, 
        {
            "location": "/dtmanage/", 
            "text": "DataTorrent Console (dtManage) Guide\n\n\nThe DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and manage the DataTorrent RTS platform and applications running on your Hadoop cluster.\n\n\nTo download the platform or the VM sandbox, go to \nhttp://www.datatorrent.com/download\n.\n\n\n\n\nThe Console includes the following features:\n\n\n\n\nAppFactory\n\n\nLaunch\n\n\nMonitor\n\n\nVisualize\n\n\nDevelop\n\n\nConfigure\n\n\n\n\nAppFactory\n\n\nThe AppFactory hosts a collection of applications and templates grouped by various industries, that can be imported or downloaded (as .apa files). You can use the applications as they are, or use the templates as a starting point to develop custom applications.\n\n\n\n\nLaunch\n\n\nThe Launch page lists all of the \nApplications\n and \nConfigurations\n available for launching, as well as offering convenient management features.\n\n\n\n\nTwo alternative views are accessible through the \nApplications\n and \nConfigurations\n buttons on the top right of the page. The \nApplications\n view lists all the applications across all the application packages. The \nConfigurations\n view lists all the available application configurations.\n\n\nThe \ninstances\n column lists all the running instances of each application or configuration. Clicking on an instance takes you to the application instance page.\n\n\nUploading Packages and Configurations\n\n\nTo upload an application package (.apa), use the \nupload package\n button in the \nApplications\n view. To upload an application configuration (.apc), use the \nupload configuration\n button in the \nConfigurations\n view.\n\n\nLaunching Applications and Configurations\n\n\nApplications and configurations can be launched using the \nlaunch\n button in the \nactions\n column. This opens a launch modal where you can confirm whether to \nLaunch\n or \nConfigure\n the application.\n\n\nNote\n: Some applications and configurations must be configured before launching because they have incomplete required properties.\n\n\nWhen using the \nConfigure\n button in the \nLaunch Application\n modal, a temporary configuration is used. Temporary configurations are useful for launching and testing quickly without creating extra configurations. Read more about \ntemporary configurations\n on the main Application Configurations page.\n\n\nLaunch Dropdown\n\n\nThe dropdown menu to the right of the \nlaunch\n button contains some convenient management actions and alternative launch options.\n\n\n\n\nWhen working with \nApplications\n, the dropdown provides quick access to related configurations, ability to launch with configuration xml files, and some package management actions.\n\n\nWhen working with \nConfigurations\n, the dropdown provides configuration management actions, links to related configurations, and source management actions.\n\n\nRetargeting Multiple Configurations\n\n\nMultiple configurations can have their source applications retargeted at the same time. This is useful when working with a new application version and you want to migrate a set of existing configurations.\n\n\nTo start, select all of the configurations you want to retarget using the selection checkboxes, then click the \nretarget\n button that shows up above the configurations list.\n\n\nIn the modal, select a new target source application, confirm your changes, and click \nRetarget\n.\n\n\nVisualize\n\n\nSee the \ndtDashboard\n page.\n\n\nMonitor\n\n\nThe Monitor section of the Console can be used to monitor, troubleshoot, and manage running application instances.\n\n\nCluster Overview\n\n\nThe operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.\n\n\n\n\nThe CPU/Memory section shows the cpu cores and memory usage statistics.  The sample above shows that Apex applications are currently using \n10.48 cores\n and \n788 GB\n of memory, and that at one time memory usage peaked at \n991.5 GB\n.\n\n\nThe Applications section shows counts of all current application states.  The sample image shows there are \n6 running\n applications and \n0 pending\n.  It is important that the \npending\n count does not continually show a value other than zero, which may indicate that the cluster lacks available cpu/memory resources to start a new application.\n\n\nThe Performance section shows current statistics such as container and operator counts, tuples processed per seconds and tuples emitted per seconds across all Apex applications.\n\n\nThe Issues section shows warning and error counts.  The count style changes to a clickable button if the value is nonzero.  Additional details about the errors and warnings may be viewed by clicking the error or warning count buttons, or by navigating to the \nSystem Information\n section.\n\n\nThe Services section shows the current running/failed/stopped services.  Only states with nonzero values are shown.  The image above shows there are \n7 running\n services.\n\n\nBelow the cluster overview is a list of running applications.  In this list each column has a filter input just below the header which is useful for locating specific applications when there is a large number of applications running.  Clicking on individual column headers will sort the rows by that column value.  Clicking on a specific application id or name will take you to the instance page for that application (see below).\n\n\nSelecting the \nended apps\n button will include all ended applications that are still in the resource manager history.\n\n\nNotice the \nservices\n designation on the \nato-online-analytics-service\n and \nfpa-online-analytics-service\n applications in the image above.  These are Apex applications running as services.  For more details on services, refer to the \nServices\n section.\n\n\nInstance Page\n\n\nTo get to an application instance page, click on either the app name or the app id in the list of running applications.\n\n\n\n\nAll sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:\n\n\n\n\nThere are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations (\nlogical\n, \nphysical\n, \nphysical-dag-view\n, \nmetric-view\n, \nattempts\n) will suffice. The following is a list of widgets available on an app instance page:\n\n\nApplication Overview Widget\n\n\nAll the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:\n\n\n\n\nThe \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.\n\n\nNote\n: You should not shutdown or kill an Apex application which is running as a service.  If you want to terminate such an application, then you should stop or delete the service.  For more details on stopping and deleting services, refer to the \nServices\n section. \n\n\nThe \nAM logs\n button shows you a dropdown menu where you may find the App Master logs, application log and GC log.  Selecting one of the menu options will take you to the log page where you can analyze the logs.  See the \nViewing Logs\n section for more details.\n\n\nYou can also use the \nset logging level\n button on this widget to specify what logging level gets written to the dt.log files. \n\n\n\n\nYou will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:\n\n\n\n\nStram Events Widget\n\n\nEach application has a stream of notable events that can be viewed with the StrAM Events widget:\n\n\n\n\nSome events have additional information attached to it, which can be viewed by clicking the \"i\" icon in the list:\n\n\n\n\nLogical DAG Widget\n\n\nThis widget visualizes the logical plan of the application being viewed:\n\n\n\n\nAdditionally, by selecting alternatives from the \"Top\" and \"Bottom\" dropdowns, you can cycle through\nvarious metrics aggregated by the logical operators. In the screenshot above, processed tuples per\nsecond and emitted tuples per second are shown.\n\n\nThe rectangle near the bottom right with a miniature representation of the DAG is a scrolling aid and\nis useful when the DAG is very large and does not fit in its entirety in the browser window: You can\npan (i.e. shift the viewport) to a particular area of the DAG by moving the grey box with your pointer\nto the corresponding area of the mini-DAG.\n\n\nClicking on the \"Critical Path\" checkbox, you can enable highlighting of the path of the longest\nlatencies highlighted in red:\n\n\n\n\nSimilarly, clicking on the \"Stream Locality\" checkbox will draw the streams with different dot patterns\nto distinguish the different localities chosen.\n\n\n\n\nPro tip:\n Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.\n\n\n\n\nPhysical DAG Widget\n\n\nThis is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.\n\n\n\n\nSame-colored physical operators in this widget indicates that these operators are in the same container.\n\n\nLogical Operators List Widget\n\n\nThis widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing. \n\n\nOne nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:\n\n\n\n\nPhysical Operators List Widget\n\n\nShows the physical operators in the application and, for each operator, also shows the container\nrunning it. The value in the \"container\" column is a clickable link which takes you to the page\nfor that container. The same link is also present in the \"id\" column of the \"Containers\" widget\ndescribed next.\n\n\nContainers List Widget\n\n\nShows the containers in the application. Selecting a container by clicking on the checkbox\nwill trigger the display of additional buttons which allow you to retrieve logs, fetch info for\ncontainers that have already terminated, retrieve a stack dump, or kill selected containers:\n\n\n\n\nThe Application Master container is the one whose container id ends with \n_000001\n.  The \nAppMaster\n label is shown to the right of the id, as shown in the above screenshot. The entries in the id column are clickable links that take you to the page for a specific container, showing the physical operators hosted in it and the relevant statistics:\n\n\n\n\nLogical Streams List Widget\n\n\nShows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.\n\n\nMetrics Chart\n\n\nShows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.\n\n\nGarbage Collection (GC) Chart by Heap\n\n\nThis chart shows a container's heap memory in use in KB (kilo-bytes) against time. The chart is constructed by plotting and extrapolating in-use heap memory obtained from events in the GC log file of a container which requires GC logging to be enabled as described in \nApplication Configurations\n. The chart shown is for a single container that is selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nGarbage Collection (GC) Log Table\n\n\nThis table shows the garbage collection (GC) events for a group of containers. This table too requires GC logging to be enabled as described in \nApplication Configurations\n. The containers included in the group depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nGarbage Collection (GC) Chart by Duration\n\n\nThis discrete bar chart shows GC event duration in seconds against time for a group of containers. Each bar is of fixed-width but the height denotes the duration of the corresponding GC event. This chart too requires GC logging to be enabled as described in \nApplication Configurations\n. One or more containers are selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:\n\n\n\n\nall application containers in the application view\n\n\nall the containers containing the physical partitions of a logical operator in the logical operator view\n\n\nthe single parent container of a physical operator in the physical operator view\n\n\nthe container itself in the selected container view\n\n\n\n\n\n\nRecording and Viewing Sample Tuples\n\n\nThere is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):\n\n\n\n\n\n\nPro tip:\n Select multiple tuples by holding down the shift key.\n\n\n\n\n\n\nViewing Logs\n\n\nAnother useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:\n\n\n\n\nOnce you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, \"tail\" the log to watch for real-time updates, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:\n\n\n\n\nDevelop\n\n\nApplication packages and application configurations can be viewed and managed in the Develop section. For more information about application packages visit the \nApplication Packages Guide\n.\n\n\n\n\nApplication Packages\n\n\nTo access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operations directly on application packages:\n\n\n\n\nDownload the app package\n\n\nDelete the app package\n\n\nLaunch applications in the app package\n\n\n\n\n\n\nNote:\n If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.\n\n\n\n\nApplication Package Page\n\n\nOnce you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.\n\n\n\n\nAside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package. \n\n\nViewing an Application\n\n\nAll DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications. In addition to the DAG, Package Properties and any Required Properties will be listed on this page.\n\n\n\n\nConfigure\n\n\nThe RTS configuration menu is accessed by the cog button on the top-right corner of the Console. Under the \nSystem Configuration\n section, there are links to various tools to help you configure and troubleshoot your DataTorrent installation. The available menu items may differ depending on your security settings.\n\n\n\n\nSystem Information\n\n\nThis page displays details of the following:\n\n Gateway Information - Displays all the details of the gateway properties that are configured in the current DT RTS installation.\n\n Hadoop Information - Displays the details of Hadoop properties that are configured in the current DT RTS installation.\n* Phone Home Information - Displays the data from the current DT RTS installation, such as usage statistics, which is dynamically aggregated over the time period of 24 hours and sent to the DT servers. The data is aggregated in total time frames that is it includes data from the start of the gateway installation to present.\n\n\n\n\nSystem Configuration\n\n\nThis page shows the system configuration, provides a way to make system changes, and displays any known issues for the DataTorrent RTS installation.\n\n\n\n\nIn addition, you can perform the following actions from this page:\n\n\n\n\nSMTP Configuration - Set up SMTP to be able to send out email alerts and notifications.\n\n\nRestart the Gateway - This button can be used to restart the gateway when the Hadoop configuration or system properties have changed.\n\n\nUsage Reporting - If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.\n\n\nInstallation Wizard - Rerun the initial installation to reconfigure HDFS installation path and Hadoop executable.\n\n\n\n\nSecurity\n\n\nBy default, your installation starts with no security enabled, which may be sufficient on a closed network with a limited set of users. However, it is recommended to use some form of authentication especially for production environments.\n\n\n\n\nDataTorrent RTS supports various authentication methods which can be enabled by following instructions in the \nAuthentication\n section.\n\n\nServices\n\n\nServices represent global, shared, and automatically managed processes. These processes are automatically installed, managed, and monitored by the Gateway, and can be an instance of an Apex application or a Docker container. Applications can rely on any number of services as their dependencies, and all the required services will be automatically installed and launched as needed when the application starts.  For more details refer to the \nServices\n section.\n\n\n\n\nAlerts\n\n\nSystem alerts can be configured to notify users through the Console and emails based on various system and application metrics.\n\n\n\n\nClick on the \n+ create new alert\n button to create an alert.\n\n\n\n\nAn alert consists of\n\n\n\n\na condition (a JavaScript expression)\n\n\na list of recipient email addresses\n\n\na threshold value in milliseconds\n\n\na message, and\n\n\nan enabled/disabled flag\n\n\n\n\nThe gateway periodically (every 5 seconds) processes all enabled alerts by evaluating the condition. If the condition evaluates to \ntrue\n, the alert is said to be \"in effect\".\nIf the condition evaluates to \nfalse\n, the alert is said to be \"out\" (or \"out of effect\"). If the alert stays \"in effect\" for the duration specified as the threshold value,\nthen the alert is triggered and the gateway sends an \"in effect\" email message to all the recipient email addresses.\n\n\nIf a triggered alert goes \"out of effect\" then the gateway immediately sends an \"out of effect\" email message to all the recipient email addresses.\n\n\nThe alert condition is specified as a JavaScript expression which is evaluated in the context of something called \ntopics\n which are described \nhere\n.\n\n\nThe gateway also provides pre-defined alert \"templates\" that allow a user to create alerts for certain common conditions without having to write JavaScript expressions.\n\n\n\n\nClick on the \"Predefined Conditions\" tab and select a template from the drop-down list. Depending on your selection, you will need to provide more values to be filled into the template.\nAs an example, for the \"Application Memory Usage\" template you need to provide the Application Name and Memory values as shown below:\n\n\n\n\nYou can click on the \"Javascript Code\" tab to see the generated JavaScript expression that corresponds to your alert template selection and provided values as shown below:\n\n\n\n\nYou can generate a test email to validate your alert by checking the \"Send Test Email\" check-box and clicking on the blue \"Test\" button. The test email is sent regardless of the true or false result\nof the JavaScript condition, if the evaluation has no errors provided SMTP is configured as described in the Alerts section.\n\n\nLicense\n\n\nUse the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.\n\n\n\n\nUser Profile\n\n\nThe User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:\n\n\n\n\nChange password \n\n\nChange the default home page\n\n\nChange the theme of the console\n\n\nRestore the default options of the console\n\n\n\n\n\n\nUser Management\n\n\nUse this page to manage users and their corresponding roles on your DataTorrent cluster. This page is accessible only to an admin user. You can do the following from this page:\n\n\n\n\nAdd users\n\n\nChange users\u2019 roles\n\n\nChange users\u2019 password\n\n\nDelete users\n\n\nAdd roles\n\n\nEdit role permissions\n\n\nDelete roles\n\n\n\n\n\n\n\n\nNote:\n With most authentication schemes, the admin role cannot be deleted.\n\n\n\n\nInstallation Wizard\n\n\nThe first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:\n\n\n\n\nLocation of the Hadoop executable\n\n\nDFS location where all the DataTorrent files are stored\n\n\nDocker configuration\n\n\nDataTorrent license\n\n\nSummary and review of any remaining configuration items\n\n\n\n\nAt any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.\n\n\n\n\nWhen your Hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard: \n\n\n\n\nKerberos Principal\n: The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.\n\n\nKerberos Keytab\n: The location (path) of the Kerberos keytab file to use on the gateway node's local file system.\n\n\nYARN delegation token lifetime\n: If the value of the \nyarn.resourcemanager.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\nNamenode delegation token lifetime\n: If the value of the \ndfs.namenode.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\n\n\n\n\nNote:\n The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.\n\n\n\n\nDocker configuration is optional.  For more details, refer to the \nDocker Configuration\n section.", 
            "title": "dtManage"
        }, 
        {
            "location": "/dtmanage/#datatorrent-console-dtmanage-guide", 
            "text": "The DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and manage the DataTorrent RTS platform and applications running on your Hadoop cluster.  To download the platform or the VM sandbox, go to  http://www.datatorrent.com/download .   The Console includes the following features:   AppFactory  Launch  Monitor  Visualize  Develop  Configure", 
            "title": "DataTorrent Console (dtManage) Guide"
        }, 
        {
            "location": "/dtmanage/#appfactory", 
            "text": "The AppFactory hosts a collection of applications and templates grouped by various industries, that can be imported or downloaded (as .apa files). You can use the applications as they are, or use the templates as a starting point to develop custom applications.", 
            "title": "AppFactory"
        }, 
        {
            "location": "/dtmanage/#launch", 
            "text": "The Launch page lists all of the  Applications  and  Configurations  available for launching, as well as offering convenient management features.   Two alternative views are accessible through the  Applications  and  Configurations  buttons on the top right of the page. The  Applications  view lists all the applications across all the application packages. The  Configurations  view lists all the available application configurations.  The  instances  column lists all the running instances of each application or configuration. Clicking on an instance takes you to the application instance page.", 
            "title": "Launch"
        }, 
        {
            "location": "/dtmanage/#uploading-packages-and-configurations", 
            "text": "To upload an application package (.apa), use the  upload package  button in the  Applications  view. To upload an application configuration (.apc), use the  upload configuration  button in the  Configurations  view.", 
            "title": "Uploading Packages and Configurations"
        }, 
        {
            "location": "/dtmanage/#launching-applications-and-configurations", 
            "text": "Applications and configurations can be launched using the  launch  button in the  actions  column. This opens a launch modal where you can confirm whether to  Launch  or  Configure  the application.  Note : Some applications and configurations must be configured before launching because they have incomplete required properties.  When using the  Configure  button in the  Launch Application  modal, a temporary configuration is used. Temporary configurations are useful for launching and testing quickly without creating extra configurations. Read more about  temporary configurations  on the main Application Configurations page.", 
            "title": "Launching Applications and Configurations"
        }, 
        {
            "location": "/dtmanage/#launch-dropdown", 
            "text": "The dropdown menu to the right of the  launch  button contains some convenient management actions and alternative launch options.   When working with  Applications , the dropdown provides quick access to related configurations, ability to launch with configuration xml files, and some package management actions.  When working with  Configurations , the dropdown provides configuration management actions, links to related configurations, and source management actions.", 
            "title": "Launch Dropdown"
        }, 
        {
            "location": "/dtmanage/#retargeting-multiple-configurations", 
            "text": "Multiple configurations can have their source applications retargeted at the same time. This is useful when working with a new application version and you want to migrate a set of existing configurations.  To start, select all of the configurations you want to retarget using the selection checkboxes, then click the  retarget  button that shows up above the configurations list.  In the modal, select a new target source application, confirm your changes, and click  Retarget .", 
            "title": "Retargeting Multiple Configurations"
        }, 
        {
            "location": "/dtmanage/#visualize", 
            "text": "See the  dtDashboard  page.", 
            "title": "Visualize"
        }, 
        {
            "location": "/dtmanage/#monitor", 
            "text": "The Monitor section of the Console can be used to monitor, troubleshoot, and manage running application instances.", 
            "title": "Monitor"
        }, 
        {
            "location": "/dtmanage/#cluster-overview", 
            "text": "The operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.   The CPU/Memory section shows the cpu cores and memory usage statistics.  The sample above shows that Apex applications are currently using  10.48 cores  and  788 GB  of memory, and that at one time memory usage peaked at  991.5 GB .  The Applications section shows counts of all current application states.  The sample image shows there are  6 running  applications and  0 pending .  It is important that the  pending  count does not continually show a value other than zero, which may indicate that the cluster lacks available cpu/memory resources to start a new application.  The Performance section shows current statistics such as container and operator counts, tuples processed per seconds and tuples emitted per seconds across all Apex applications.  The Issues section shows warning and error counts.  The count style changes to a clickable button if the value is nonzero.  Additional details about the errors and warnings may be viewed by clicking the error or warning count buttons, or by navigating to the  System Information  section.  The Services section shows the current running/failed/stopped services.  Only states with nonzero values are shown.  The image above shows there are  7 running  services.  Below the cluster overview is a list of running applications.  In this list each column has a filter input just below the header which is useful for locating specific applications when there is a large number of applications running.  Clicking on individual column headers will sort the rows by that column value.  Clicking on a specific application id or name will take you to the instance page for that application (see below).  Selecting the  ended apps  button will include all ended applications that are still in the resource manager history.  Notice the  services  designation on the  ato-online-analytics-service  and  fpa-online-analytics-service  applications in the image above.  These are Apex applications running as services.  For more details on services, refer to the  Services  section.", 
            "title": "Cluster Overview"
        }, 
        {
            "location": "/dtmanage/#instance-page", 
            "text": "To get to an application instance page, click on either the app name or the app id in the list of running applications.   All sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:   There are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations ( logical ,  physical ,  physical-dag-view ,  metric-view ,  attempts ) will suffice. The following is a list of widgets available on an app instance page:", 
            "title": "Instance Page"
        }, 
        {
            "location": "/dtmanage/#application-overview-widget", 
            "text": "All the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:   The \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.  Note : You should not shutdown or kill an Apex application which is running as a service.  If you want to terminate such an application, then you should stop or delete the service.  For more details on stopping and deleting services, refer to the  Services  section.   The  AM logs  button shows you a dropdown menu where you may find the App Master logs, application log and GC log.  Selecting one of the menu options will take you to the log page where you can analyze the logs.  See the  Viewing Logs  section for more details.  You can also use the  set logging level  button on this widget to specify what logging level gets written to the dt.log files.    You will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:", 
            "title": "Application Overview Widget"
        }, 
        {
            "location": "/dtmanage/#stram-events-widget", 
            "text": "Each application has a stream of notable events that can be viewed with the StrAM Events widget:   Some events have additional information attached to it, which can be viewed by clicking the \"i\" icon in the list:", 
            "title": "Stram Events Widget"
        }, 
        {
            "location": "/dtmanage/#logical-dag-widget", 
            "text": "This widget visualizes the logical plan of the application being viewed:   Additionally, by selecting alternatives from the \"Top\" and \"Bottom\" dropdowns, you can cycle through\nvarious metrics aggregated by the logical operators. In the screenshot above, processed tuples per\nsecond and emitted tuples per second are shown.  The rectangle near the bottom right with a miniature representation of the DAG is a scrolling aid and\nis useful when the DAG is very large and does not fit in its entirety in the browser window: You can\npan (i.e. shift the viewport) to a particular area of the DAG by moving the grey box with your pointer\nto the corresponding area of the mini-DAG.  Clicking on the \"Critical Path\" checkbox, you can enable highlighting of the path of the longest\nlatencies highlighted in red:   Similarly, clicking on the \"Stream Locality\" checkbox will draw the streams with different dot patterns\nto distinguish the different localities chosen.   Pro tip:  Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.", 
            "title": "Logical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#physical-dag-widget", 
            "text": "This is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.   Same-colored physical operators in this widget indicates that these operators are in the same container.", 
            "title": "Physical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#logical-operators-list-widget", 
            "text": "This widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing.   One nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:", 
            "title": "Logical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#physical-operators-list-widget", 
            "text": "Shows the physical operators in the application and, for each operator, also shows the container\nrunning it. The value in the \"container\" column is a clickable link which takes you to the page\nfor that container. The same link is also present in the \"id\" column of the \"Containers\" widget\ndescribed next.", 
            "title": "Physical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#containers-list-widget", 
            "text": "Shows the containers in the application. Selecting a container by clicking on the checkbox\nwill trigger the display of additional buttons which allow you to retrieve logs, fetch info for\ncontainers that have already terminated, retrieve a stack dump, or kill selected containers:   The Application Master container is the one whose container id ends with  _000001 .  The  AppMaster  label is shown to the right of the id, as shown in the above screenshot. The entries in the id column are clickable links that take you to the page for a specific container, showing the physical operators hosted in it and the relevant statistics:", 
            "title": "Containers List Widget"
        }, 
        {
            "location": "/dtmanage/#logical-streams-list-widget", 
            "text": "Shows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.", 
            "title": "Logical Streams List Widget"
        }, 
        {
            "location": "/dtmanage/#metrics-chart", 
            "text": "Shows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.", 
            "title": "Metrics Chart"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-chart-by-heap", 
            "text": "This chart shows a container's heap memory in use in KB (kilo-bytes) against time. The chart is constructed by plotting and extrapolating in-use heap memory obtained from events in the GC log file of a container which requires GC logging to be enabled as described in  Application Configurations . The chart shown is for a single container that is selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Chart by Heap"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-log-table", 
            "text": "This table shows the garbage collection (GC) events for a group of containers. This table too requires GC logging to be enabled as described in  Application Configurations . The containers included in the group depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Log Table"
        }, 
        {
            "location": "/dtmanage/#garbage-collection-gc-chart-by-duration", 
            "text": "This discrete bar chart shows GC event duration in seconds against time for a group of containers. Each bar is of fixed-width but the height denotes the duration of the corresponding GC event. This chart too requires GC logging to be enabled as described in  Application Configurations . One or more containers are selectable from the radio buttons shown at the top right corner of the widget. Each container in the radio buttons and the chart is color-coded with the same color. The containers included depend on the context of the widget:   all application containers in the application view  all the containers containing the physical partitions of a logical operator in the logical operator view  the single parent container of a physical operator in the physical operator view  the container itself in the selected container view", 
            "title": "Garbage Collection (GC) Chart by Duration"
        }, 
        {
            "location": "/dtmanage/#recording-and-viewing-sample-tuples", 
            "text": "There is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):    Pro tip:  Select multiple tuples by holding down the shift key.", 
            "title": "Recording and Viewing Sample Tuples"
        }, 
        {
            "location": "/dtmanage/#viewing-logs", 
            "text": "Another useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:   Once you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, \"tail\" the log to watch for real-time updates, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:", 
            "title": "Viewing Logs"
        }, 
        {
            "location": "/dtmanage/#develop", 
            "text": "Application packages and application configurations can be viewed and managed in the Develop section. For more information about application packages visit the  Application Packages Guide .", 
            "title": "Develop"
        }, 
        {
            "location": "/dtmanage/#application-packages", 
            "text": "To access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operations directly on application packages:   Download the app package  Delete the app package  Launch applications in the app package    Note:  If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/dtmanage/#application-package-page", 
            "text": "Once you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.   Aside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package.", 
            "title": "Application Package Page"
        }, 
        {
            "location": "/dtmanage/#viewing-an-application", 
            "text": "All DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications. In addition to the DAG, Package Properties and any Required Properties will be listed on this page.", 
            "title": "Viewing an Application"
        }, 
        {
            "location": "/dtmanage/#configure", 
            "text": "The RTS configuration menu is accessed by the cog button on the top-right corner of the Console. Under the  System Configuration  section, there are links to various tools to help you configure and troubleshoot your DataTorrent installation. The available menu items may differ depending on your security settings.", 
            "title": "Configure"
        }, 
        {
            "location": "/dtmanage/#system-information", 
            "text": "This page displays details of the following:  Gateway Information - Displays all the details of the gateway properties that are configured in the current DT RTS installation.  Hadoop Information - Displays the details of Hadoop properties that are configured in the current DT RTS installation.\n* Phone Home Information - Displays the data from the current DT RTS installation, such as usage statistics, which is dynamically aggregated over the time period of 24 hours and sent to the DT servers. The data is aggregated in total time frames that is it includes data from the start of the gateway installation to present.", 
            "title": "System Information"
        }, 
        {
            "location": "/dtmanage/#system-configuration", 
            "text": "This page shows the system configuration, provides a way to make system changes, and displays any known issues for the DataTorrent RTS installation.   In addition, you can perform the following actions from this page:   SMTP Configuration - Set up SMTP to be able to send out email alerts and notifications.  Restart the Gateway - This button can be used to restart the gateway when the Hadoop configuration or system properties have changed.  Usage Reporting - If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.  Installation Wizard - Rerun the initial installation to reconfigure HDFS installation path and Hadoop executable.", 
            "title": "System Configuration"
        }, 
        {
            "location": "/dtmanage/#security", 
            "text": "By default, your installation starts with no security enabled, which may be sufficient on a closed network with a limited set of users. However, it is recommended to use some form of authentication especially for production environments.   DataTorrent RTS supports various authentication methods which can be enabled by following instructions in the  Authentication  section.", 
            "title": "Security"
        }, 
        {
            "location": "/dtmanage/#services", 
            "text": "Services represent global, shared, and automatically managed processes. These processes are automatically installed, managed, and monitored by the Gateway, and can be an instance of an Apex application or a Docker container. Applications can rely on any number of services as their dependencies, and all the required services will be automatically installed and launched as needed when the application starts.  For more details refer to the  Services  section.", 
            "title": "Services"
        }, 
        {
            "location": "/dtmanage/#alerts", 
            "text": "System alerts can be configured to notify users through the Console and emails based on various system and application metrics.   Click on the  + create new alert  button to create an alert.   An alert consists of   a condition (a JavaScript expression)  a list of recipient email addresses  a threshold value in milliseconds  a message, and  an enabled/disabled flag   The gateway periodically (every 5 seconds) processes all enabled alerts by evaluating the condition. If the condition evaluates to  true , the alert is said to be \"in effect\".\nIf the condition evaluates to  false , the alert is said to be \"out\" (or \"out of effect\"). If the alert stays \"in effect\" for the duration specified as the threshold value,\nthen the alert is triggered and the gateway sends an \"in effect\" email message to all the recipient email addresses.  If a triggered alert goes \"out of effect\" then the gateway immediately sends an \"out of effect\" email message to all the recipient email addresses.  The alert condition is specified as a JavaScript expression which is evaluated in the context of something called  topics  which are described  here .  The gateway also provides pre-defined alert \"templates\" that allow a user to create alerts for certain common conditions without having to write JavaScript expressions.   Click on the \"Predefined Conditions\" tab and select a template from the drop-down list. Depending on your selection, you will need to provide more values to be filled into the template.\nAs an example, for the \"Application Memory Usage\" template you need to provide the Application Name and Memory values as shown below:   You can click on the \"Javascript Code\" tab to see the generated JavaScript expression that corresponds to your alert template selection and provided values as shown below:   You can generate a test email to validate your alert by checking the \"Send Test Email\" check-box and clicking on the blue \"Test\" button. The test email is sent regardless of the true or false result\nof the JavaScript condition, if the evaluation has no errors provided SMTP is configured as described in the Alerts section.", 
            "title": "Alerts"
        }, 
        {
            "location": "/dtmanage/#license", 
            "text": "Use the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.", 
            "title": "License"
        }, 
        {
            "location": "/dtmanage/#user-profile", 
            "text": "The User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:   Change password   Change the default home page  Change the theme of the console  Restore the default options of the console", 
            "title": "User Profile"
        }, 
        {
            "location": "/dtmanage/#user-management", 
            "text": "Use this page to manage users and their corresponding roles on your DataTorrent cluster. This page is accessible only to an admin user. You can do the following from this page:   Add users  Change users\u2019 roles  Change users\u2019 password  Delete users  Add roles  Edit role permissions  Delete roles     Note:  With most authentication schemes, the admin role cannot be deleted.", 
            "title": "User Management"
        }, 
        {
            "location": "/dtmanage/#installation-wizard", 
            "text": "The first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:   Location of the Hadoop executable  DFS location where all the DataTorrent files are stored  Docker configuration  DataTorrent license  Summary and review of any remaining configuration items   At any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.   When your Hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard:    Kerberos Principal : The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.  Kerberos Keytab : The location (path) of the Kerberos keytab file to use on the gateway node's local file system.  YARN delegation token lifetime : If the value of the  yarn.resourcemanager.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.  Namenode delegation token lifetime : If the value of the  dfs.namenode.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.    Note:  The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.   Docker configuration is optional.  For more details, refer to the  Docker Configuration  section.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the \nvisualize\n button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The embedded Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators SnapshotServer (which includes embedded Query Operator) and QueryResult are the operators that serve the data being visualized in the Console.  The SnapshotServer operator takes in data from the TopCounter operator, processes incoming queries, and generates results. The Twitter Hashtag Demo application and all its operators are available in the Apache Malhar repository.\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that provide historical and real-time application statistics and custom metrics data.  You can visualize these metrics the same way as custom Data Sources in an application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from the Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, using the \nvisualize\n button will display a list of existing dashboards that are associated with the application.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\nYou can also navigate back to the application from within the dashboard's menu.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new\n button on the Dashboards screen\n\n\ncreate new dashboard\n option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the \ncreate new\n button\n\n\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with \ncreate new dashboard\n option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to create new dashboard from the visualize menu drop-down list.  The new dashboard will be automatically named based on the application name and saved.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nDashboard Controls\n\n\nDashboards controls are presented as a set of buttons to the right of the dashboard title.\n\n\nView Mode\n\n\nIn view mode, you are not allowed to modify the layout or add/edit widgets, but certain actions\ncan be taken in the dashboard menu.\n\n\n\n\nEdit Mode\n\n\nIn edit mode, you can add/edit widgets, change the dashboard layout, and change dashboard settings.\n\n\n\n\nDashboard Menu\n\n\nThe dashboard menu contains the rest of the dashboard functionality. The \nactions\n section is mostly self-explanatory, but see the \nPresentation Mode\n and \nPresentation Builder\n sections below for more information about presenting your dashboards. The \nassociated applications\n section displays the status of applications associated to the current dashboard and serves as a quick way to jump to those applications. Associated applications are selected based on the data sources of your widgets.\n\n\nNote\n: If an an associated application enters a non-running state (e.g. KILLED, INACTIVE), a warning icon will be displayed on the \ndashboard menu\n button.\n\n\n\n\nDashboard Settings\n\n\nThe dashboard settings interface allows you to change the dashboard name, description, logo image, and \nselect replacement associated applications\n.\n\n\nIt can be accessed from the \ndashboard menu\n using the \nsettings\n option.\n\n\n\n\nReplacing Associated Applications\n\n\nDashboards have widgets that rely on associated applications for data. These associated applications can be replaced in \ndashboard settings\n if the replacement is compatible with the current dashboard.\n\n\nThe replacement application must have a compatible data source. Selecting an application with an incompatible data source will simply skip the replacement process for that application. If the data source matches, but the data schema is incompatible, the widget will attempt to reset its settings to match the new data schema.\n\n\nWhen importing a dashboard, the interface tries its best to preselect the most compatible replacement application. Leaving fields blank means the replacement process for those applications will be skipped.\n\n\nPackaged Dashboards\n\n\nAuto Import When Launching An Application\n\n\nApplication packages may include packaged dashboards which can be imported. Application package developers may select some dashboards to be imported automatically when launching an application, and all packaged dashboards can be imported manually at any time from the Packaged Dashboards page.\n\n\nWhen \nworking with an application configuration\n, the \nDashboards\n section lists the packaged dashboards that will be auto imported when launched. These dashboards can be configured with new names and target applications.\n\n\nNote\n: Auto imports of packaged dashboards only happen if there isn't already an existing dashboard with the same name and owner. They can still be marked for import in the launch interface, but will have to be given a unique name.\n\n\nImport From A Running Application\n\n\nIf a running application has associated packaged dashboards, the packaged dashboards can be imported using the \nvisualize\n button in \nApplication Overview\n.\n\n\n\n\nClicking on a packaged dashboard in the dropdown will open an interface to change the dashboard name, description, logo image, and replacement applications before importing.\n\n\nNote\n: See the \nreplacing associated applications\n section for an explanation about replacement applications.\n\n\n\n\nPressing the \nimport\n button at the bottom of the interface imports the dashboard, and can then be accessed in the \nVisualize\n section.\n\n\nImport From Packaged Dashboards Page\n\n\nFor a list of all packaged dashboards across all your application packages, use the \nimport\n button in the \nVisualize\n section.\n\n\n\n\nThe \nimport\n button brings you to the following page where you can import or download individual packaged dashboards.\n\n\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking the \nadd widget\n button in edit mode, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget.\n\n\n\n\nEach data source supports one or more data schema types, such as snapshot and dimensions.  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nResults are not persisted until you press the \nsave\n button.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use \nsave\n button to persist the desired results.  If the resulting changes should not be saved, using the \ndiscard\n button will revert it to the the original state.\n\n\nControl Widget\n\n\nControl widget can be used to configure settings of one or more widgets on the dashboard.  This is a convenient way to configure multiple widgets settings in one place.  It is also the only way to change widgets settings in presentation mode.  However, settings changes in presentation mode will not be saved.\n\n\nControllable Widgets\n\n\nThe following widgets may be controlled by the Control Widget:\n\n\n\n\nDimensions Schema Data Source Widgets\n\n\nGeo Choropleth\n\n\nGeo Circles\n\n\nLine Chart\n\n\nMulti Bar Chart\n\n\nStacked Area Chart\n\n\n\n\n\n\nSnapshot Schema Data Source Widgets\n\n\nBar Chart\n\n\nHorizontal Bar Chart\n\n\nMulti Color Bar Chart\n\n\nPie Chart\n\n\n\n\n\n\n\n\nControllable Settings\n\n\nThe following settings may be controlled by the Control Widget:\n\n\n\n\nDimensions Schema Data Source Settings\n\n\nCircle Size \n(Geo Circle widget only)\n\n\nColor Intensity \n(Geo Choropleth widget only)\n\n\nGeo Coordinates \n(Geo Circle widget only)\n\n\nKeys Combinations and Aggregates\n\n\nTime Range Selection\n\n\nTooltip Values \n(Geo Choropleth and Circle widgets only)\n\n\n\n\n\n\nSnapshot Schema Data Source Settings\n\n\nField to use as label\n\n\nField to use as quantity\n\n\nSort order\n\n\n\n\n\n\n\n\nAdding Control Widget\n\n\nControl Widget can be added just like any other widget.  See the \nAdding Widgets\n section for more details.\n\n\nAdding Widgets to Control\n\n\nClick the edit button on the Control Widget to show the available widgets to control, and change the controllable widget selections.\n\n\n\n\nThe Cotnrol Widget does not allow selection of dimensional and snapshot schema widgets together. If users happen to choose widgets with both schema types, then the \"OK\" button remains disabled until only one schema type is selected.\n\n\nThe control widget allows selection of widgets with different data sources. Users are warned that some setting sections in the control widget may be disabled if the schemas are incompatible.\n\n\n\n\nWhen widgets to be controlled have incompatible schemas, the incompatible sections remain disabled until mismatched widgets are removed in the control widget settings.  This issue can also be resolved if the widgets with incompatible schemas are removed from the dashboard.\n\n\n\n\nExamples of Control Widget in Action\n\n\nChanging the time range selection for the line and stacked area charts.\n\n\n\n\nChanging dimensional keys and aggregates.\n\n\n\n\nChanging the sorting option.\n\n\n\n\nChanging the geo circle settings.\n\n\n\n\nChanging the geo choropleth settings in presentation mode.\n\n\n\n\nPresentation Mode\n\n\nDashboards can be viewed in a fullscreen mode with main navigation elements removed.\n\n\n\n\nYou can access the Presentation Mode from the dashboard menu:\n\n\n\n\nNote\n: To share your presentation, share the URL while inside Presentation Mode.\n\n\nPresentation Builder\n\n\nThe Presentation Builder can be used to create a presentation with multiple dashboards.\nThe dashboard you launch the Presentation Builder from is the \nhome dashboard\n, which means\nthat dashboard will serve as the starting point of your presentation. To start the presentation,\njust enter Presentation Mode from the home dashboard.\n\n\n\n\nIFrame Widget\n\n\nThe IFrame widget allows user to embed content from another source. The OAS dashboards are embedded inside DataTorrent RTS dashboards using this widget.\n\n\n\n\nIFrame widget supports the following functionalities:\n\n\n\n\nCustom JavaScript execution after iframe has been loaded, provided the iframe src is from the same domain.\n\n\nIframe content can also be opened in a full-view mode in a new tab.\n\n\nIFrame content from a different domain can also be embedded as long the src domain is CORS enabled.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#twitter-example", 
            "text": "The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the  visualize  button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:", 
            "title": "Twitter Example"
        }, 
        {
            "location": "/dtdashboard/#ads-dimension-example", 
            "text": "The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Ads Dimension Example"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The embedded Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators SnapshotServer (which includes embedded Query Operator) and QueryResult are the operators that serve the data being visualized in the Console.  The SnapshotServer operator takes in data from the TopCounter operator, processes incoming queries, and generates results. The Twitter Hashtag Demo application and all its operators are available in the Apache Malhar repository.", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that provide historical and real-time application statistics and custom metrics data.  You can visualize these metrics the same way as custom Data Sources in an application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from the Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, using the  visualize  button will display a list of existing dashboards that are associated with the application.   Below is an example of accessing the data visualization dashboard from a running application.\nYou can also navigate back to the application from within the dashboard's menu.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new  button on the Dashboards screen  create new dashboard  option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the  create new  button     Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with  create new dashboard  option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to create new dashboard from the visualize menu drop-down list.  The new dashboard will be automatically named based on the application name and saved.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#dashboard-controls", 
            "text": "Dashboards controls are presented as a set of buttons to the right of the dashboard title.", 
            "title": "Dashboard Controls"
        }, 
        {
            "location": "/dtdashboard/#view-mode", 
            "text": "In view mode, you are not allowed to modify the layout or add/edit widgets, but certain actions\ncan be taken in the dashboard menu.", 
            "title": "View Mode"
        }, 
        {
            "location": "/dtdashboard/#edit-mode", 
            "text": "In edit mode, you can add/edit widgets, change the dashboard layout, and change dashboard settings.", 
            "title": "Edit Mode"
        }, 
        {
            "location": "/dtdashboard/#dashboard-menu", 
            "text": "The dashboard menu contains the rest of the dashboard functionality. The  actions  section is mostly self-explanatory, but see the  Presentation Mode  and  Presentation Builder  sections below for more information about presenting your dashboards. The  associated applications  section displays the status of applications associated to the current dashboard and serves as a quick way to jump to those applications. Associated applications are selected based on the data sources of your widgets.  Note : If an an associated application enters a non-running state (e.g. KILLED, INACTIVE), a warning icon will be displayed on the  dashboard menu  button.", 
            "title": "Dashboard Menu"
        }, 
        {
            "location": "/dtdashboard/#dashboard-settings", 
            "text": "The dashboard settings interface allows you to change the dashboard name, description, logo image, and  select replacement associated applications .  It can be accessed from the  dashboard menu  using the  settings  option.", 
            "title": "Dashboard Settings"
        }, 
        {
            "location": "/dtdashboard/#replacing-associated-applications", 
            "text": "Dashboards have widgets that rely on associated applications for data. These associated applications can be replaced in  dashboard settings  if the replacement is compatible with the current dashboard.  The replacement application must have a compatible data source. Selecting an application with an incompatible data source will simply skip the replacement process for that application. If the data source matches, but the data schema is incompatible, the widget will attempt to reset its settings to match the new data schema.  When importing a dashboard, the interface tries its best to preselect the most compatible replacement application. Leaving fields blank means the replacement process for those applications will be skipped.", 
            "title": "Replacing Associated Applications"
        }, 
        {
            "location": "/dtdashboard/#packaged-dashboards", 
            "text": "", 
            "title": "Packaged Dashboards"
        }, 
        {
            "location": "/dtdashboard/#auto-import-when-launching-an-application", 
            "text": "Application packages may include packaged dashboards which can be imported. Application package developers may select some dashboards to be imported automatically when launching an application, and all packaged dashboards can be imported manually at any time from the Packaged Dashboards page.  When  working with an application configuration , the  Dashboards  section lists the packaged dashboards that will be auto imported when launched. These dashboards can be configured with new names and target applications.  Note : Auto imports of packaged dashboards only happen if there isn't already an existing dashboard with the same name and owner. They can still be marked for import in the launch interface, but will have to be given a unique name.", 
            "title": "Auto Import When Launching An Application"
        }, 
        {
            "location": "/dtdashboard/#import-from-a-running-application", 
            "text": "If a running application has associated packaged dashboards, the packaged dashboards can be imported using the  visualize  button in  Application Overview .   Clicking on a packaged dashboard in the dropdown will open an interface to change the dashboard name, description, logo image, and replacement applications before importing.  Note : See the  replacing associated applications  section for an explanation about replacement applications.   Pressing the  import  button at the bottom of the interface imports the dashboard, and can then be accessed in the  Visualize  section.", 
            "title": "Import From A Running Application"
        }, 
        {
            "location": "/dtdashboard/#import-from-packaged-dashboards-page", 
            "text": "For a list of all packaged dashboards across all your application packages, use the  import  button in the  Visualize  section.   The  import  button brings you to the following page where you can import or download individual packaged dashboards.", 
            "title": "Import From Packaged Dashboards Page"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking the  add widget  button in edit mode, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget.   Each data source supports one or more data schema types, such as snapshot and dimensions.  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.  Results are not persisted until you press the  save  button.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use  save  button to persist the desired results.  If the resulting changes should not be saved, using the  discard  button will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/dtdashboard/#control-widget", 
            "text": "Control widget can be used to configure settings of one or more widgets on the dashboard.  This is a convenient way to configure multiple widgets settings in one place.  It is also the only way to change widgets settings in presentation mode.  However, settings changes in presentation mode will not be saved.", 
            "title": "Control Widget"
        }, 
        {
            "location": "/dtdashboard/#controllable-widgets", 
            "text": "The following widgets may be controlled by the Control Widget:   Dimensions Schema Data Source Widgets  Geo Choropleth  Geo Circles  Line Chart  Multi Bar Chart  Stacked Area Chart    Snapshot Schema Data Source Widgets  Bar Chart  Horizontal Bar Chart  Multi Color Bar Chart  Pie Chart", 
            "title": "Controllable Widgets"
        }, 
        {
            "location": "/dtdashboard/#controllable-settings", 
            "text": "The following settings may be controlled by the Control Widget:   Dimensions Schema Data Source Settings  Circle Size  (Geo Circle widget only)  Color Intensity  (Geo Choropleth widget only)  Geo Coordinates  (Geo Circle widget only)  Keys Combinations and Aggregates  Time Range Selection  Tooltip Values  (Geo Choropleth and Circle widgets only)    Snapshot Schema Data Source Settings  Field to use as label  Field to use as quantity  Sort order", 
            "title": "Controllable Settings"
        }, 
        {
            "location": "/dtdashboard/#adding-control-widget", 
            "text": "Control Widget can be added just like any other widget.  See the  Adding Widgets  section for more details.", 
            "title": "Adding Control Widget"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets-to-control", 
            "text": "Click the edit button on the Control Widget to show the available widgets to control, and change the controllable widget selections.   The Cotnrol Widget does not allow selection of dimensional and snapshot schema widgets together. If users happen to choose widgets with both schema types, then the \"OK\" button remains disabled until only one schema type is selected.  The control widget allows selection of widgets with different data sources. Users are warned that some setting sections in the control widget may be disabled if the schemas are incompatible.   When widgets to be controlled have incompatible schemas, the incompatible sections remain disabled until mismatched widgets are removed in the control widget settings.  This issue can also be resolved if the widgets with incompatible schemas are removed from the dashboard.", 
            "title": "Adding Widgets to Control"
        }, 
        {
            "location": "/dtdashboard/#examples-of-control-widget-in-action", 
            "text": "Changing the time range selection for the line and stacked area charts.   Changing dimensional keys and aggregates.   Changing the sorting option.   Changing the geo circle settings.   Changing the geo choropleth settings in presentation mode.", 
            "title": "Examples of Control Widget in Action"
        }, 
        {
            "location": "/dtdashboard/#presentation-mode", 
            "text": "Dashboards can be viewed in a fullscreen mode with main navigation elements removed.   You can access the Presentation Mode from the dashboard menu:   Note : To share your presentation, share the URL while inside Presentation Mode.", 
            "title": "Presentation Mode"
        }, 
        {
            "location": "/dtdashboard/#presentation-builder", 
            "text": "The Presentation Builder can be used to create a presentation with multiple dashboards.\nThe dashboard you launch the Presentation Builder from is the  home dashboard , which means\nthat dashboard will serve as the starting point of your presentation. To start the presentation,\njust enter Presentation Mode from the home dashboard.", 
            "title": "Presentation Builder"
        }, 
        {
            "location": "/dtdashboard/#iframe-widget", 
            "text": "The IFrame widget allows user to embed content from another source. The OAS dashboards are embedded inside DataTorrent RTS dashboards using this widget.   IFrame widget supports the following functionalities:   Custom JavaScript execution after iframe has been loaded, provided the iframe src is from the same domain.  Iframe content can also be opened in a full-view mode in a new tab.  IFrame content from a different domain can also be embedded as long the src domain is CORS enabled.", 
            "title": "IFrame Widget"
        }, 
        {
            "location": "/dtgateway/", 
            "text": "dtGateway\n\n\nOne of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind \ndtManage\n. It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS \ninstaller\n.\n\n\ndtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.\n\n\n\n\nThese features are exposed through a \nREST API\n. Here are some of things you can do with the REST API:\n\n\n\n\nGet performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances\n\n\nGet performance metrics and other details of physical and logical operators of each Apex application instance\n\n\nGet performance metrics and other details of individual containers used by each Apex application instance\n\n\nRetrieve container logs\n\n\nDynamically change operator properties, and add and remove operators from the DAG of a running Apex application\n\n\nRecord and retrieve tuples on the fly\n\n\nShutdown a running container or an entire Apex application\n\n\nDynamically change logging level of a container\n\n\nCreate, manage, and view custom system alerts\n\n\nCreate, manage, and interact with dtDashboard\n\n\nCreate, manage, and launch Apex App Packages\n\n\nBasic health checks of the cluster\n\n\n\n\nSecurity\n\n\nWith all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.\n\n\nFor authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.\n\n\nFor information on configuring security see \ndtGateway security\n guide.\n\n\nSystem Alerts\n\n\nSystem Alerts provide a way for users to monitor cluster and application metrics. When an alert condition (written in JavaScript) turns true and stays that way for a configured time interval, dtGateway sends email to the configured list of email addresses. The same is true when the condition turns false. Alerts are created via the \nPUT /ws/v2/systemAlerts/alerts/{name}\n call documented in the \nREST API\n. For more details on System Alerts, please see \nthis document\n.\n\n\nRest API\n\n\nHere is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090\n\n\n$ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n  \ncheckpointStartTime\n: \n1442512091772\n,\n  \ncheckpointTime\n: \n175\n,\n  \ncheckpointTimeMA\n: \n164\n,\n  \nclassName\n: \ncom.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator\n,\n  \ncontainer\n: \ncontainer_e08_1442448722264_14891_01_000017\n,\n  \ncounters\n: null,\n  \ncpuPercentageMA\n: \n0.2039266316727741\n,\n  \ncurrentWindowId\n: \n6195527785184762469\n,\n  \nfailureCount\n: \n0\n,\n  \nhost\n: \nnode22.morado.com:8041\n,\n  \nid\n: \n40\n,\n  \nlastHeartbeat\n: \n1442512100742\n,\n  \nlatencyMA\n: \n5\n,\n  \nlogicalName\n: \nQueryResult\n,\n  \nmetrics\n: {},\n  \nname\n: \nQueryResult\n,\n  \nports\n: [\n    {\n      \nbufferServerBytesPSMA\n: \n0\n,\n      \nname\n: \ninputPort\n,\n      \nqueueSizeMA\n: \n1\n,\n      \nrecordingId\n: null,\n      \ntotalTuples\n: \n6976\n,\n      \ntuplesPSMA\n: \n0\n,\n      \ntype\n: \ninput\n\n    }\n  ],\n  \nrecordingId\n: null,\n  \nrecoveryWindowId\n: \n6195527785184762451\n,\n  \nstatus\n: \nACTIVE\n,\n  \ntotalTuplesEmitted\n: \n0\n,\n  \ntotalTuplesProcessed\n: \n6976\n,\n  \ntuplesEmittedPSMA\n: \n0\n,\n  \ntuplesProcessedPSMA\n: \n20\n,\n  \nunifierClass\n: null\n}\n\n\n\n\nFor the complete spec of the REST API, please refer to dtGateway \nREST API\n.\n\n\nFor information on configuring dtGateway in general, see \nDataTorrent RTS Configuration", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#dtgateway", 
            "text": "One of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind  dtManage . It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS  installer .  dtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.   These features are exposed through a  REST API . Here are some of things you can do with the REST API:   Get performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances  Get performance metrics and other details of physical and logical operators of each Apex application instance  Get performance metrics and other details of individual containers used by each Apex application instance  Retrieve container logs  Dynamically change operator properties, and add and remove operators from the DAG of a running Apex application  Record and retrieve tuples on the fly  Shutdown a running container or an entire Apex application  Dynamically change logging level of a container  Create, manage, and view custom system alerts  Create, manage, and interact with dtDashboard  Create, manage, and launch Apex App Packages  Basic health checks of the cluster", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#security", 
            "text": "With all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.  For authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.  For information on configuring security see  dtGateway security  guide.", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway/#system-alerts", 
            "text": "System Alerts provide a way for users to monitor cluster and application metrics. When an alert condition (written in JavaScript) turns true and stays that way for a configured time interval, dtGateway sends email to the configured list of email addresses. The same is true when the condition turns false. Alerts are created via the  PUT /ws/v2/systemAlerts/alerts/{name}  call documented in the  REST API . For more details on System Alerts, please see  this document .", 
            "title": "System Alerts"
        }, 
        {
            "location": "/dtgateway/#rest-api", 
            "text": "Here is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090  $ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n   checkpointStartTime :  1442512091772 ,\n   checkpointTime :  175 ,\n   checkpointTimeMA :  164 ,\n   className :  com.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator ,\n   container :  container_e08_1442448722264_14891_01_000017 ,\n   counters : null,\n   cpuPercentageMA :  0.2039266316727741 ,\n   currentWindowId :  6195527785184762469 ,\n   failureCount :  0 ,\n   host :  node22.morado.com:8041 ,\n   id :  40 ,\n   lastHeartbeat :  1442512100742 ,\n   latencyMA :  5 ,\n   logicalName :  QueryResult ,\n   metrics : {},\n   name :  QueryResult ,\n   ports : [\n    {\n       bufferServerBytesPSMA :  0 ,\n       name :  inputPort ,\n       queueSizeMA :  1 ,\n       recordingId : null,\n       totalTuples :  6976 ,\n       tuplesPSMA :  0 ,\n       type :  input \n    }\n  ],\n   recordingId : null,\n   recoveryWindowId :  6195527785184762451 ,\n   status :  ACTIVE ,\n   totalTuplesEmitted :  0 ,\n   totalTuplesProcessed :  6976 ,\n   tuplesEmittedPSMA :  0 ,\n   tuplesProcessedPSMA :  20 ,\n   unifierClass : null\n}  For the complete spec of the REST API, please refer to dtGateway  REST API .  For information on configuring dtGateway in general, see  DataTorrent RTS Configuration", 
            "title": "Rest API"
        }, 
        {
            "location": "/services/", 
            "text": "Overview\n\n\nServices represent global, shared, and automatically managed processes.  These processes are automatically installed, managed, and monitored by the Gateway, and can be an instance of an Apex application or a Docker container.  Applications can rely on any number of services as their dependencies, and all the required services will be automatically installed and launched as needed when the application starts.\n\n\nFor example when you launch the \nOmni Channel Fraud Prevention\n application for the first time, the following services are automatically installed and launched along with it:\n\n\n\n\nOnline Analytics Service\n\n\nDrools Workbench\n\n\nOAS Dashboards\n\n\n\n\nServices required by the applications can be defined via JSON service descriptors placed in an application package during the application development.  Users can also add new services and manage existing services via the UI Console using the Services Management or Application Configuration pages.  \n\n\nBelow are three services that are packaged with DT premium applications:\n\n\n\n\nOnline Analytics Service\n - This service provides analytic processing of event streams from various source applications in real-time. This is an Apex application backed by a custom Druid implementation which provides fast in-memory OLAP query support.\n\n\nCEP Workbench\n - This service is a web application and repository which is used to manage Drools assets. It provides the capability to quickly create, edit, and version Drools rules via a web UI, which can in turn be deployed to applications which implement CEP Engine, such as \nOmni Channel Fraud Prevention\n application.\n\n\nOAS Dashboards\n - This service, based on Apache Supersert, provides a rich set of data visualizations with an easy-to-use interface for exploring and visualizing data available via \nOAS\n or other data sources.\n\n\n\n\nManaging Services\n\n\nYou can view and manage installed services using the \nServices\n page. To navigate to the \nServices\n page, follow the steps below:\n\n\n\n\nClick the Settings \n icon located on the upper most right section of the DT RTS console.\n\n\nSelect the \nServices\n menu item from the dropdown menu.  The \nServices\n page is displayed with the list of installed services.\n\n\n\n\nSample services list:\n\n\n\n\nBelow are the descriptions of the services table columns:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nThe service name, which can be clicked to navigate to the service instance page.\n\n\n\n\n\n\nenabled\n\n\nThe service status should be \nRUNNING\n if this field is \ntrue\n (checked).\nThe status should be \nSTOPPED\n if this field is \nfalse\n.\nThe Gateway monitors all \nenabled\n services to make sure they are running.\n\n\n\n\n\n\nstatus\n\n\nThe state of the service.\n\n\n\n\n\n\nstarted\n\n\nThe duration since the service was started.\n\n\n\n\n\n\nuptime\n\n\nNumber of hours the service has been running.\n\n\n\n\n\n\ntype\n\n\nThe service type. Possible values are \ndocker\n and \napex\n.\n\n\n\n\n\n\nactive apps\n\n\nThe active Apex applications that depend on the service.\n\n\n\n\n\n\nmemory\n\n\nMemory allocated by the service.\n\n\n\n\n\n\n\n\n\nBelow are possible service status:\n\n\n\n\n\n\n\n\nStatus\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nINSTALLING\n\n\nThe service is being installed.  This status is typically shown during service download or installation.\n\n\n\n\n\n\nSTOPPED\n\n\nThe service is installed, but not running.\n\n\n\n\n\n\nSTARTING\n\n\nThe service is installed and is starting up.\n\n\n\n\n\n\nRUNNING\n\n\nThe service is installed and running.\n\n\n\n\n\n\nSTOPPING\n\n\nThe service is being stopped.\n\n\n\n\n\n\nREMOVING\n\n\nThe service is being deleted. Once deleted, it should disappear from the table on the \nServices\n page.\n\n\n\n\n\n\nFAILED\n\n\nThe service is installed but failed to start or ended unexpectedly.\n\n\n\n\n\n\n\n\nThe following actions can be performed on this page:\n\n\n\n\nCreating new service\n\n\nImporting packaged services\n\n\nViewing service instance\n\n\nStarting services\n\n\nStopping services\n\n\nCloning a service\n\n\nDeleting services\n\n\n\n\nCreating New Service\n\n\nTo create a new service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nClick the \ncreate new\n button. The \nCreate Service\n dialog is shown.\n\n\nEnter data in the applicable entries.  See sample screen captures below for reference.\n\n\n\n\nSample Docker create service dialog.\n\n\n\n\nSample Apex create service dialog.\n\n\n\n\nCreate Service Dialog Fields\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter the name of the service. This must be a unique name.\n\n\n\n\n\n\nDescription\n\n\nEnter a description about the service.\n(Optional)\n\n\n\n\n\n\nType\n\n\nSelect a service type.\ndocker\n - Docker container as a service.\napex\n - Apex application as a service.\n\n\n\n\n\n\nSource URL\n\n\nSpecify the location of the Docker image or the Apex application image.\n\n\n\n\n\n\nDocker Run\n\n\nEnter the Docker command arguments to be used when the Docker service container starts.\nNote\n: This entry will only be shown if the service type is \ndocker\n.\n(Optional)\n\n\n\n\n\n\nDocker Exec\n\n\nExecute the shell command inside the docker container after it is launched.\nNote\n: This entry will only be shown if the service type is \ndocker\n.\n(Optional)\n\n\n\n\n\n\nApex App Name\n\n\nEnter the application name that exists in the Apex APA image which will be launched when the service starts.\nNote\n: This entry will only be shown if the service type is \napex\n.\n\n\n\n\n\n\nApex Launch Properties\n\n\nEnter Apex launch properties. Click the \nAdd\n button to add additional properties. Enter the names and corresponding values.\nNote\n: This entry will only be shown if the service type is \napex\n.\n(Optional)\n\n\n\n\n\n\nProxy Address\n\n\nPort or host:port to which the Gateway proxy path forwards requests.\n(Optional)\n\n\n\n\n\n\nProxy Request Headers\n\n\nEnter headers to be added to the request made by the Gateway to the proxy destination. Click the \nAdd\n button to add additional headers.\n(Optional)\n\n\n\n\n\n\nProxy Response Replacements\n\n\nEnter the response replacement definitions which represents the text replacement processing to be performed on the response body by the Gateway proxy. Click the \nAdd\n button to add additional replacement definitions.\n(Optional)\n\n\n\n\n\n\n\n\n\n\nClick the \nCreate\n button to create the new service and install it.\n\n\n\n\nFor more details and examples regarding the items in the table above, see the \nServices Property\n section below.\n\n\nImporting Packaged Service\n\n\nPackaged services are pre-defined services included in application packages that are uploaded in the cluster.  These services can be installed as-is or with different settings.\n\n\nTo install a packaged service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nClick the \nimport\n button. The \nImport from Packaged Services\n page is displayed with the list of available packaged services in application packages.\n\n\nClick the \nimport\n button of a service to be imported. An \nImport Packaged Service\n dialog is shown.\n\n\nEdit the applicable entries and click the \nImport\n button to install the service.\n\n\n\n\nViewing Service Instance\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nClick the service name to navigate to the service instance page.\n\n\n\n\nSample service instance page:\n\n\n\n\nThe following sections can be found on the \nService Instance\n page:\n\n\nService Status and Actions\n\n\nThis section shows the \nservice name\n, \ntype\n, \nstatus\n, \nuptime\n and \ncurrently allocated memory\n if the service is running.  It also contains the applicable actionable buttons such as \nview app\n, \nstart\n, \nstop\n, \nedit\n, \ncopy\n and \ndelete\n.  Note that the \nview app\n button is only visible if the service type is \napex\n and the service is running.\n\n\nService Details\n\n\nThis table shows the configuration of the service.  It may also contain the Apex application ID if the service type is \napex\n and the service is running and some metadata keys/values if the service provides such data.  The sample service instance above shows \nQueryIP\n and \nQueryPort\n with values of \n192.168.2.135\n and \n46620\n, respectively.  These metadata keys/values are provided by the service at run time.  This section will also show explicit metadata variables defined in the service descriptor.\n\n\nProxy URL\n\n\nThis section shows the proxy URL that users can use to access data provided by the service through the Gateway proxy.  The Gateway applies proxy request headers and proxy replace string settings when processing this URL requests.\n\n\nDependent Active Apps\n\n\nThis section shows the Apex applications that depend on this service.  The table will show the application ID, application name, application status and the application running username.  Users can click on the application ID or name to navigate to the running Apex application instance page.\n\n\nEdit Service\n\n\nServices are automatically installed when an associated application is launched. However, you can change the settings of the services based on your requirements and restart the services.\n\n\nNote\n: Only \nSTOPPED\n services can be edited.\n\n\nTo edit a service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nSelect a service from the services list and click the \nEdit\n button. The \nEdit Service\n dialog is shown.\n\n\nEdit the settings and click the \nSave\n button. The new settings are saved and will be applied when the service is restarted.\n\n\n\n\nNote\n: You can also edit a service on the service instance page.\n\n\nStarting Services\n\n\nTo start a service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nSelect a service from the services list and click the \nStart\n button.\n\n\n\n\nNote\n: You can also start a service on the service instance page.\n\n\nStopping Services\n\n\nTo stop a service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nSelect a service from the services list and click the \nStop\n button. A \nStop Service\n modal is shown.\n\n\nClick the \nStop\n button to stop the service.\n\n\n\n\nNote\n: You can also stop a service on the service instance page.\n\n\nCloning a Service\n\n\nYou can clone, edit, and save a service configuration as a new service.\n\n\nTo clone a service, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nSelect a service to clone and click the \ncopy\n button.  The \nCreate Service\n dialog is shown with the selected service configurations pre-filled.\n\n\nChange the service name and applicable settings.\nService name must be different from the original service name because it must be unique.\n\n\nClick the \nCreate\n button to create the new service.\nIf the original service is enabled, then the new service will be installed and started.\nIf the original service isn't enabled, then the new service will be installed, but not started.\n\n\n\n\nNote\n: You can also clone a service on the service instance page.\n\n\nDeleting Services\n\n\nServices can be deleted for an application from the Services management page.\n\n\nTo stop or start the services, follow the steps below:\n\n\n\n\nNavigate to the \nServices\n page.\n\n\nSelect a service from the services list and click the \ndelete\n button.  The delete service confirmation modal is shown.\n\n\nClick the \nDelete\n button to confirm that you want to delete the service.\n\n\n\n\nNote\n: You can also delete a service on the service instance page.\n\n\nConfiguring Docker\n\n\nSome applications require services which are run in the Docker containers. For such services, you must install Docker (Version 1.9.1 or greater) on your system. Services can run in Docker installed on a remote system if Docker isn't installed on the system where the Gateway is running.\n\n\nThe Docker version is automatically detected during the DT RTS installation process. That Docker version is shown in the Docker section of the Installation Wizard - Configuration. You can optionally configure the services to run in Docker installed on a remote system.\n\n\nWarning: If the system does not have a compatible version of Docker and the remote Docker host isn't configured, then Docker services will not work.\n\n\nTo configure the remote Docker host, follow the steps below:\n\n\n\n\nOn the DT RTS console, click the settings icon located on the upper most right section of the page and select \nSystem Configuration\n. The \nSystem Configuration\n page is displayed.\n\n\nClick the \nInstallation Wizard\n button.\n\n\nOn the \nWelcome\n page, click the \nContinue\n button.\n\n\n\n\nOn the \nConfiguration\n page, go to the \nDocker\n section and set the following:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDocker host\n\n\nEnter the remote Docker host URL.\nFor example: \nunix:///var/run/docker.sock\n or \nhttp://127.0.0.1:2376\n(Optional)\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nContinue\n and complete the Installation wizard.\n\n\n\n\n\n\nPackaging Services\n\n\nServices and dependent applications can be defined and included in the application package.  This service descriptor is defined in the \nservices.json\n file.  This file is located in the \n/src/main/resources/resources\n directory of your Apex project.  When the project is built and packaged as an APA file, the \nservices.json\n file is placed in the \n/resources\n directory inside the APA file.\n\n\nSample Services File\n\n\nThe following is a sample \nservices.json\n file:\n\n\n{\n  \"services\": [\n    {\n      \"name\": \"superset-service\",\n      \"description\": \"Superset application dashboard service.\",\n      \"type\": \"docker\",\n      \"srcUrl\": \"johnsmith/superset:1.0.0\",\n      \"docker\": {\n        \"run\": \"--add-host cluster:\n -e PORT=9090 -p 28088:8088\"\n      },\n      \"proxy\": {\n        \"address\": \"localhost:28088\",\n        \"followRedirect\": false,\n        \"requestHeaders\": {\n          \"X_PROXY_REMOTE_USER\": \"admin\"\n        },\n        \"replaceStrings\": [\n          {\n            \"matchMime\": \".*text/html.*\",\n            \"matchUrl\": \".*\",\n            \"matchText\": \"href=\\\"/\",\n            \"replaceText\": \"href=\\\"/proxy/services/superset/\"\n          },\n          {\n            \"matchMime\": \".*application/javascript.*\",\n            \"matchUrl\": \".*.entry.js\",\n            \"matchText\": \"\\\"/superset/\",\n            \"replaceText\": \"\\\"/proxy/services/superset/superset/\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"drools-workbench\",\n      \"description\": \"Drools Workbench is the web application and repository to govern Drools assets.\",\n      \"type\": \"docker\",\n      \"srcUrl\": \"jshnsmith/drools-workbench:1.0.0\",\n      \"docker\": {\n        \"run\": \"-d -p 18080:8080 -p 18001:8001\"\n      },\n      \"proxy\": {\n        \"address\": \"localhost:18080/drools-wb\",\n        \"followRedirect\": false\n      }\n    },\n    {\n      \"name\": \"online-analytics-service\",\n      \"description\": \"Online Analytics Service.\",\n      \"type\": \"apex\",\n      \"proxy\": {\n        \"address\": \"${QueryIP}:${QueryPort}\"\n      },\n      \"srcUrl\": \"${dt.gateway.artifactHubLocation}/ws/v1/artifacts/dt-apoxi-oas/1.4.0/download\",\n      \"apex\": {\n        \"appName\": \"Online-Analytics-Service\",\n        \"launchArgs\": {\n          \"apex.app-param.kafkaBrokers\": \"localhost:9092\",\n          \"apex.app-param.kafkaTopic\": \"analytics\"\n        }\n      }\n    }\n  ],\n  \"applications\": [\n    {\n      \"name\": \"MyApexApplication\",\n      \"requiredServices\": [\n        {\n          \"name\": \"online-analytics-service\",\n          \"requiredBeforeLaunch\": \"true\"\n        },\n        {\n          \"name\": \"superset-fpa\"\n        },\n        {\n          \"name\": \"drools-workbench\"\n        }\n      ]\n    }\n  ]\n}\n\n\n\nThe \nservices.json\n file contains two root level properties:\n\n\n\n\nServices Property\n\n\nApplications Property\n\n\n\n\nServices Property\n\n\nService descriptors are defined in the \nservices\n property.  The services property is an array of JSON objects where each object defines a service.\n\n\nService Descriptor Parameters\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nService name, which should be globally unique and only include characters that HDFS file name friendly.\nFor example: \nsuperset-fpa\n, \ndruid_workbench\n, etc.\n\n\n\n\n\n\ndescription\n\n\nstring\n\n\nShort description about the service.\n(Optional)\n\n\n\n\n\n\ntype\n\n\nstring\n\n\nServices type must be one of the following values:\ndocker\n - service is a Docker container.\napex\n - service is an Apex application.\n\n\n\n\n\n\nsrcUrl\n\n\nstring\n\n\nSpecify the name of the Docker image if the service is Docker based or specify the path of the Apex application package if the service is Apex based.\nAn example of a Docker srcUrl: \ndatatorrent/superset-fpa:1.4.0\nAn example of an Apex srcUrl:\n${.dt.gateway.artifactHubLocation}/ws/v1/artifacts/com.datatorrent/\ndt-apoxi-oas/1.4.0-SNAPSHOT/download\nAnother example of an Apex URL: \nfile:///path/to/apppackage.apa\n\n\n\n\n\n\ndocker\n\n\njson object\n\n\nSpecify the Docker details for he service.\nFor example:\n{\n  \"run\": \"-d -p 18080:8080\",\n  \"exec\": \"nginx -t -c ~/mynginx.conf\"\n}\nNote\n: This property is required if the service type is \ndocker\n.\n(Optional)\n\n\n\n\n\n\napex\n\n\njson object\n\n\nSpecify the Apex details for the service.\nFor example:\n{\n  \"appName\": \"OAS\",\n  \"launchArgs\": {\n    ...\n  }\n}\nNote\n: This property is required if the service type is \napex\n.\n(Optional)\n\n\n\n\n\n\nproxy\n\n\njson object\n\n\nSpecify the proxy settings for the service.\nFor example:\n{\n  \"address\": \"localhost\",\n  \"followRedirect\": false,\n  \"requestHeaders\": {\n   ...\n  },\n  \"replaceString\": [\n    ...\n  ]\n}\n(Optional)\n\n\n\n\n\n\nmetadata\n\n\njson object\n\n\nSpecify explicit metadata to use in the service.\nFor example: \n{\n  \"ipaddr\" : \"localhost\",\n  \"port\" : 8080\n}\nWith this metadata defined in the service, we can reference them in the service configuration as \n${superset-fpa.ipaddr}\n and \n${superset-fpa.port}\n, assuming the service name is \nsuperset-fpa\n.\n(Optional)\n\n\n\n\n\n\n\n\nDocker Details\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrun\n\n\nstring\n\n\nSpecify the Docker run command details.\nFor example: \n--add-host druid_cluster:\nGATEWAY_IP\n -e OAS=fpa-online-analytics-service -e PORT=9090 -p 28088:8088\n\n\n\n\n\n\nexec\n\n\nstring\n\n\nSpecify the Docker shell command to execute after the Docker service is started.\nFor example: \nnginx -t -c ~/mynginx.conf\n\n\n\n\n\n\n\n\nApex Details\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nappName\n\n\nstring\n\n\nSpecify the Apex application in the APA to launch.\nFor example: \nOA\n\n\n\n\n\n\nlaunchArgs\n\n\njson object\n\n\nArguments to use during the launching of the Apex service.\nFor example:\n{\n  \"kafkaBrokers\": \"localhost:9092\",\n  \"kafkaTopic\": \"analytics\"\n}\n(Optional)\n\n\n\n\n\n\n\n\nProxy Settings\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\naddress\n\n\nstring\n\n\nHost:port to which the proxy path forwards to.\nFor example: \nlocalhost:28088\n(Optional)\n\n\n\n\n\n\nfollowRedirect\n\n\nboolean\n\n\nIf this property is true, then the Gateway proxy will perform redirect when it sees the HTTP status code 302 in the HTTP response header from the service.  Therefore, the browser surfing the service proxy URL will never encounter the hTTP status code 302.\nWarning\n: Omitting this property or setting it to true may cause a maximum redirect error in the Gateway proxy.\n(Optional, default: true)\n\n\n\n\n\n\nrequestHeaders\n\n\njson object\n\n\nHeaders to be added to the request made by the Gateway to the proxy destination.\nFor example:\n{\n  \"X_PROXY_REMOTE_USER\": \"dtadmin\"\n}\n(Optional)\n\n\n\n\n\n\nreplaceStrings\n\n\narray of json object\n\n\nDefinitions that represents text replacement processing to be performed on the response body by the Gateway proxy.  Regular expression is supported as described in the \nJava Regex Pattern Class\n, which includes capturing group and back references.\n[\n  {\n    \"matchMime\": \"text/html\",\n    \"matchUrl\": \".*.html\",\n    \"matchText\": \"\\\"/static/\",\n    \"replaceText\": \"\\\"/proxy/services/superset/static/\"\n  }\n  ...\n]\n(Optional)\n\n\n\n\n\n\n\n\nReplace Strings Details\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmatchMime\n\n\nstring\n\n\nProcess only for this mime-type.\nFor example: \ntext/html\n(Optional)\n\n\n\n\n\n\nmatchUrl\n\n\nstring\n\n\nProcess only when the URL matches this regular expression pattern.\nFor example: \nacct*\n(Optional)\n\n\n\n\n\n\nmatchText\n\n\nstring\n\n\nText to be matched in the response body.\nFor example: \nhref=\\\"/static/\n\n\n\n\n\n\nreplaceText\n\n\nstring\n\n\nText that replaces the matched-text.\nFor example: \nhref=\\\"/proxy/services/superset-fraud-app/static/\n(Optional, default: '')\n\n\n\n\n\n\n\n\nNote\n: Explicit metadata, implicit and global variables such as \n${superset-fpa.ipaddr}\n, \n${superset-fpa._state}\n, \n${.GATEWAY_CONNECT_ADDRESS}\n, etc. are not currently supported in the replace strings definitions.\n\n\nExample 1:\n\n\n{\n  \"matchMime\": \"text/html\",\n  \"matchUrl\": \".*\\.html\",\n  \"matchText\": \"href=\\\"/static/\",\n  \"replaceText\": \"href=\\\"/proxy/services/superset-fraud-app/static/\"\n}\n\n\n\nThe above example tells the Gateway proxy to process request URLs ending with \n.html\n and the response header mime-type equals \ntext/html\n.  Once the URL and mime-type are a match, then the response body is transformed by replacing every occurrence of \nhref=\"/static/\n with \nhref=\"/proxy/services/superset-fraud-app/static/\n.\n\n\nExample 2:\n\n\n{\n  \"matchMime\": \"text/html\",\n  \"matchUrl\": \".*\",\n  \"matchText\": \"num=([0-9]*)\",\n  \"replaceText\": \"NUM=\\\"$1\\\"\"\n}\n\n\n\nThe above example tells the Gateway to process requests where the response header mime-type equals \ntext/html\n.  Once the mime-type is a match, then the response body is transformed by replacing every occurrence of \nnum=one or more digits\n with \nNUM=\"same digits\"\n.  For example: \nnum=25\n becomes \nNUM=\"25\"\n, \nnum=100\n becomes \nNUM=\"100\"\n, etc.\n\n\nNote\n: The matchUrl in this case will match any URL so it could have been omitted.\n\n\nIn addition to the explicit metadata variables defined in the services, there are implicit and global variables that can be used in the service configuration also.  Implicit variables are specific to the service while global variables are specific to the Gateway.\n\n\nNote\n: These variables are only applicable to the following properties: \nsrcUrl, proxy.address, docker.run, docker.exec and apex.launchArgs values (not names)\n\n\nImplicit Variables\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n_type\n\n\nThis variable should resolve to the service type such as \ndocker\n or \napex\n.  The syntax to reference this variable is \n${superset-fpa._type}\n, assuming the service name is \nsuperset-fpa\n.\n\n\n\n\n\n\n_state\n\n\nThis variable should resolve to the service status.  For a complete list of service status, see the \nservice status table\n in the Manage section. The syntax to reference this variable is \n${superset-fpa._state}\n, assuming the service name is \nsuperset-fpa\n.\n\n\n\n\n\n\n\n\nGlobal Variables\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGATEWAY_CONNECT_ADDRESS\n\n\nThis is the Gateway connection address.  The syntax to reference this variable is \n${.GATEWAY_CONNECT_ADDRESS}\n.\n\n\n\n\n\n\nGATEWAY_ADMIN_USER\n\n\nThis is the Unix user that the Gateway runs as. The syntax to reference this variable is \n${.GATEWAY_ADMIN_USER}\n.\n\n\n\n\n\n\n\n\nApplications Property\n\n\nApplications depending on services are defined in the \napplications\n property.  \n\n\nApplications Parameters\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nApex application name, which exists in the current APA package.\n\n\n\n\n\n\nrequiredServices\n\n\narray of json object\n\n\nList of services in which this application depends on.  If one of the services depends on other services, transitive service dependencies do not need to be specified explicitly.\nFor example:\n[\n  {\n    \"name\": \"superset-fpa\",\n    \"requiredBeforeLaunch\": true,\n    \"transient\": true\n  }\n  ...\n]\n\n\n\n\n\n\n\n\nrequiredServices\n is an array of JSON objects where each object defines a service the application depends on.\n\n\nRequired Services Parameters\n\n\n\n\n\n\n\n\nItem\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nThe service name this application depends on.\n\n\n\n\n\n\nrequiredBeforeLaunch\n\n\nboolean\n\n\nIf this property is set to true, then the application cannot be launched until this service is started.\n(Optional, default: false)\n\n\n\n\n\n\ntransient\n\n\nboolean\n\n\nIf this property is set to true, then it is deleted when the application is killed or shutdown.\n(Optional, default: false)", 
            "title": "Services"
        }, 
        {
            "location": "/services/#overview", 
            "text": "Services represent global, shared, and automatically managed processes.  These processes are automatically installed, managed, and monitored by the Gateway, and can be an instance of an Apex application or a Docker container.  Applications can rely on any number of services as their dependencies, and all the required services will be automatically installed and launched as needed when the application starts.  For example when you launch the  Omni Channel Fraud Prevention  application for the first time, the following services are automatically installed and launched along with it:   Online Analytics Service  Drools Workbench  OAS Dashboards   Services required by the applications can be defined via JSON service descriptors placed in an application package during the application development.  Users can also add new services and manage existing services via the UI Console using the Services Management or Application Configuration pages.    Below are three services that are packaged with DT premium applications:   Online Analytics Service  - This service provides analytic processing of event streams from various source applications in real-time. This is an Apex application backed by a custom Druid implementation which provides fast in-memory OLAP query support.  CEP Workbench  - This service is a web application and repository which is used to manage Drools assets. It provides the capability to quickly create, edit, and version Drools rules via a web UI, which can in turn be deployed to applications which implement CEP Engine, such as  Omni Channel Fraud Prevention  application.  OAS Dashboards  - This service, based on Apache Supersert, provides a rich set of data visualizations with an easy-to-use interface for exploring and visualizing data available via  OAS  or other data sources.", 
            "title": "Overview"
        }, 
        {
            "location": "/services/#managing-services", 
            "text": "You can view and manage installed services using the  Services  page. To navigate to the  Services  page, follow the steps below:   Click the Settings   icon located on the upper most right section of the DT RTS console.  Select the  Services  menu item from the dropdown menu.  The  Services  page is displayed with the list of installed services.   Sample services list:   Below are the descriptions of the services table columns:     Column  Description      name  The service name, which can be clicked to navigate to the service instance page.    enabled  The service status should be  RUNNING  if this field is  true  (checked). The status should be  STOPPED  if this field is  false . The Gateway monitors all  enabled  services to make sure they are running.    status  The state of the service.    started  The duration since the service was started.    uptime  Number of hours the service has been running.    type  The service type. Possible values are  docker  and  apex .    active apps  The active Apex applications that depend on the service.    memory  Memory allocated by the service.     \nBelow are possible service status:     Status  Description      INSTALLING  The service is being installed.  This status is typically shown during service download or installation.    STOPPED  The service is installed, but not running.    STARTING  The service is installed and is starting up.    RUNNING  The service is installed and running.    STOPPING  The service is being stopped.    REMOVING  The service is being deleted. Once deleted, it should disappear from the table on the  Services  page.    FAILED  The service is installed but failed to start or ended unexpectedly.     The following actions can be performed on this page:   Creating new service  Importing packaged services  Viewing service instance  Starting services  Stopping services  Cloning a service  Deleting services", 
            "title": "Managing Services"
        }, 
        {
            "location": "/services/#creating-new-service", 
            "text": "To create a new service, follow the steps below:   Navigate to the  Services  page.  Click the  create new  button. The  Create Service  dialog is shown.  Enter data in the applicable entries.  See sample screen captures below for reference.   Sample Docker create service dialog.   Sample Apex create service dialog.   Create Service Dialog Fields     Item  Description      Name  Enter the name of the service. This must be a unique name.    Description  Enter a description about the service. (Optional)    Type  Select a service type. docker  - Docker container as a service. apex  - Apex application as a service.    Source URL  Specify the location of the Docker image or the Apex application image.    Docker Run  Enter the Docker command arguments to be used when the Docker service container starts. Note : This entry will only be shown if the service type is  docker . (Optional)    Docker Exec  Execute the shell command inside the docker container after it is launched. Note : This entry will only be shown if the service type is  docker . (Optional)    Apex App Name  Enter the application name that exists in the Apex APA image which will be launched when the service starts. Note : This entry will only be shown if the service type is  apex .    Apex Launch Properties  Enter Apex launch properties. Click the  Add  button to add additional properties. Enter the names and corresponding values. Note : This entry will only be shown if the service type is  apex . (Optional)    Proxy Address  Port or host:port to which the Gateway proxy path forwards requests. (Optional)    Proxy Request Headers  Enter headers to be added to the request made by the Gateway to the proxy destination. Click the  Add  button to add additional headers. (Optional)    Proxy Response Replacements  Enter the response replacement definitions which represents the text replacement processing to be performed on the response body by the Gateway proxy. Click the  Add  button to add additional replacement definitions. (Optional)      Click the  Create  button to create the new service and install it.   For more details and examples regarding the items in the table above, see the  Services Property  section below.", 
            "title": "Creating New Service"
        }, 
        {
            "location": "/services/#importing-packaged-service", 
            "text": "Packaged services are pre-defined services included in application packages that are uploaded in the cluster.  These services can be installed as-is or with different settings.  To install a packaged service, follow the steps below:   Navigate to the  Services  page.  Click the  import  button. The  Import from Packaged Services  page is displayed with the list of available packaged services in application packages.  Click the  import  button of a service to be imported. An  Import Packaged Service  dialog is shown.  Edit the applicable entries and click the  Import  button to install the service.", 
            "title": "Importing Packaged Service"
        }, 
        {
            "location": "/services/#viewing-service-instance", 
            "text": "Navigate to the  Services  page.  Click the service name to navigate to the service instance page.   Sample service instance page:   The following sections can be found on the  Service Instance  page:  Service Status and Actions  This section shows the  service name ,  type ,  status ,  uptime  and  currently allocated memory  if the service is running.  It also contains the applicable actionable buttons such as  view app ,  start ,  stop ,  edit ,  copy  and  delete .  Note that the  view app  button is only visible if the service type is  apex  and the service is running.  Service Details  This table shows the configuration of the service.  It may also contain the Apex application ID if the service type is  apex  and the service is running and some metadata keys/values if the service provides such data.  The sample service instance above shows  QueryIP  and  QueryPort  with values of  192.168.2.135  and  46620 , respectively.  These metadata keys/values are provided by the service at run time.  This section will also show explicit metadata variables defined in the service descriptor.  Proxy URL  This section shows the proxy URL that users can use to access data provided by the service through the Gateway proxy.  The Gateway applies proxy request headers and proxy replace string settings when processing this URL requests.  Dependent Active Apps  This section shows the Apex applications that depend on this service.  The table will show the application ID, application name, application status and the application running username.  Users can click on the application ID or name to navigate to the running Apex application instance page.", 
            "title": "Viewing Service Instance"
        }, 
        {
            "location": "/services/#edit-service", 
            "text": "Services are automatically installed when an associated application is launched. However, you can change the settings of the services based on your requirements and restart the services.  Note : Only  STOPPED  services can be edited.  To edit a service, follow the steps below:   Navigate to the  Services  page.  Select a service from the services list and click the  Edit  button. The  Edit Service  dialog is shown.  Edit the settings and click the  Save  button. The new settings are saved and will be applied when the service is restarted.   Note : You can also edit a service on the service instance page.", 
            "title": "Edit Service"
        }, 
        {
            "location": "/services/#starting-services", 
            "text": "To start a service, follow the steps below:   Navigate to the  Services  page.  Select a service from the services list and click the  Start  button.   Note : You can also start a service on the service instance page.", 
            "title": "Starting Services"
        }, 
        {
            "location": "/services/#stopping-services", 
            "text": "To stop a service, follow the steps below:   Navigate to the  Services  page.  Select a service from the services list and click the  Stop  button. A  Stop Service  modal is shown.  Click the  Stop  button to stop the service.   Note : You can also stop a service on the service instance page.", 
            "title": "Stopping Services"
        }, 
        {
            "location": "/services/#cloning-a-service", 
            "text": "You can clone, edit, and save a service configuration as a new service.  To clone a service, follow the steps below:   Navigate to the  Services  page.  Select a service to clone and click the  copy  button.  The  Create Service  dialog is shown with the selected service configurations pre-filled.  Change the service name and applicable settings. Service name must be different from the original service name because it must be unique.  Click the  Create  button to create the new service. If the original service is enabled, then the new service will be installed and started. If the original service isn't enabled, then the new service will be installed, but not started.   Note : You can also clone a service on the service instance page.", 
            "title": "Cloning a Service"
        }, 
        {
            "location": "/services/#deleting-services", 
            "text": "Services can be deleted for an application from the Services management page.  To stop or start the services, follow the steps below:   Navigate to the  Services  page.  Select a service from the services list and click the  delete  button.  The delete service confirmation modal is shown.  Click the  Delete  button to confirm that you want to delete the service.   Note : You can also delete a service on the service instance page.", 
            "title": "Deleting Services"
        }, 
        {
            "location": "/services/#configuring-docker", 
            "text": "Some applications require services which are run in the Docker containers. For such services, you must install Docker (Version 1.9.1 or greater) on your system. Services can run in Docker installed on a remote system if Docker isn't installed on the system where the Gateway is running.  The Docker version is automatically detected during the DT RTS installation process. That Docker version is shown in the Docker section of the Installation Wizard - Configuration. You can optionally configure the services to run in Docker installed on a remote system.  Warning: If the system does not have a compatible version of Docker and the remote Docker host isn't configured, then Docker services will not work.  To configure the remote Docker host, follow the steps below:   On the DT RTS console, click the settings icon located on the upper most right section of the page and select  System Configuration . The  System Configuration  page is displayed.  Click the  Installation Wizard  button.  On the  Welcome  page, click the  Continue  button.   On the  Configuration  page, go to the  Docker  section and set the following:     Field  Description      Docker host  Enter the remote Docker host URL. For example:  unix:///var/run/docker.sock  or  http://127.0.0.1:2376 (Optional)       Click  Continue  and complete the Installation wizard.", 
            "title": "Configuring Docker"
        }, 
        {
            "location": "/services/#packaging-services", 
            "text": "Services and dependent applications can be defined and included in the application package.  This service descriptor is defined in the  services.json  file.  This file is located in the  /src/main/resources/resources  directory of your Apex project.  When the project is built and packaged as an APA file, the  services.json  file is placed in the  /resources  directory inside the APA file.", 
            "title": "Packaging Services"
        }, 
        {
            "location": "/services/#sample-services-file", 
            "text": "The following is a sample  services.json  file:  {\n  \"services\": [\n    {\n      \"name\": \"superset-service\",\n      \"description\": \"Superset application dashboard service.\",\n      \"type\": \"docker\",\n      \"srcUrl\": \"johnsmith/superset:1.0.0\",\n      \"docker\": {\n        \"run\": \"--add-host cluster:  -e PORT=9090 -p 28088:8088\"\n      },\n      \"proxy\": {\n        \"address\": \"localhost:28088\",\n        \"followRedirect\": false,\n        \"requestHeaders\": {\n          \"X_PROXY_REMOTE_USER\": \"admin\"\n        },\n        \"replaceStrings\": [\n          {\n            \"matchMime\": \".*text/html.*\",\n            \"matchUrl\": \".*\",\n            \"matchText\": \"href=\\\"/\",\n            \"replaceText\": \"href=\\\"/proxy/services/superset/\"\n          },\n          {\n            \"matchMime\": \".*application/javascript.*\",\n            \"matchUrl\": \".*.entry.js\",\n            \"matchText\": \"\\\"/superset/\",\n            \"replaceText\": \"\\\"/proxy/services/superset/superset/\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"drools-workbench\",\n      \"description\": \"Drools Workbench is the web application and repository to govern Drools assets.\",\n      \"type\": \"docker\",\n      \"srcUrl\": \"jshnsmith/drools-workbench:1.0.0\",\n      \"docker\": {\n        \"run\": \"-d -p 18080:8080 -p 18001:8001\"\n      },\n      \"proxy\": {\n        \"address\": \"localhost:18080/drools-wb\",\n        \"followRedirect\": false\n      }\n    },\n    {\n      \"name\": \"online-analytics-service\",\n      \"description\": \"Online Analytics Service.\",\n      \"type\": \"apex\",\n      \"proxy\": {\n        \"address\": \"${QueryIP}:${QueryPort}\"\n      },\n      \"srcUrl\": \"${dt.gateway.artifactHubLocation}/ws/v1/artifacts/dt-apoxi-oas/1.4.0/download\",\n      \"apex\": {\n        \"appName\": \"Online-Analytics-Service\",\n        \"launchArgs\": {\n          \"apex.app-param.kafkaBrokers\": \"localhost:9092\",\n          \"apex.app-param.kafkaTopic\": \"analytics\"\n        }\n      }\n    }\n  ],\n  \"applications\": [\n    {\n      \"name\": \"MyApexApplication\",\n      \"requiredServices\": [\n        {\n          \"name\": \"online-analytics-service\",\n          \"requiredBeforeLaunch\": \"true\"\n        },\n        {\n          \"name\": \"superset-fpa\"\n        },\n        {\n          \"name\": \"drools-workbench\"\n        }\n      ]\n    }\n  ]\n}  The  services.json  file contains two root level properties:   Services Property  Applications Property", 
            "title": "Sample Services File"
        }, 
        {
            "location": "/services/#services-property", 
            "text": "Service descriptors are defined in the  services  property.  The services property is an array of JSON objects where each object defines a service.", 
            "title": "Services Property"
        }, 
        {
            "location": "/services/#service-descriptor-parameters", 
            "text": "Item  Type  Description      name  string  Service name, which should be globally unique and only include characters that HDFS file name friendly. For example:  superset-fpa ,  druid_workbench , etc.    description  string  Short description about the service. (Optional)    type  string  Services type must be one of the following values: docker  - service is a Docker container. apex  - service is an Apex application.    srcUrl  string  Specify the name of the Docker image if the service is Docker based or specify the path of the Apex application package if the service is Apex based. An example of a Docker srcUrl:  datatorrent/superset-fpa:1.4.0 An example of an Apex srcUrl: ${.dt.gateway.artifactHubLocation}/ws/v1/artifacts/com.datatorrent/ dt-apoxi-oas/1.4.0-SNAPSHOT/download Another example of an Apex URL:  file:///path/to/apppackage.apa    docker  json object  Specify the Docker details for he service. For example: {   \"run\": \"-d -p 18080:8080\",   \"exec\": \"nginx -t -c ~/mynginx.conf\" } Note : This property is required if the service type is  docker . (Optional)    apex  json object  Specify the Apex details for the service. For example: {   \"appName\": \"OAS\",   \"launchArgs\": {     ...   } } Note : This property is required if the service type is  apex . (Optional)    proxy  json object  Specify the proxy settings for the service. For example: {   \"address\": \"localhost\",   \"followRedirect\": false,   \"requestHeaders\": {    ...   },   \"replaceString\": [     ...   ] } (Optional)    metadata  json object  Specify explicit metadata to use in the service. For example:  {   \"ipaddr\" : \"localhost\",   \"port\" : 8080 } With this metadata defined in the service, we can reference them in the service configuration as  ${superset-fpa.ipaddr}  and  ${superset-fpa.port} , assuming the service name is  superset-fpa . (Optional)", 
            "title": "Service Descriptor Parameters"
        }, 
        {
            "location": "/services/#docker-details", 
            "text": "Item  Type  Description      run  string  Specify the Docker run command details. For example:  --add-host druid_cluster: GATEWAY_IP  -e OAS=fpa-online-analytics-service -e PORT=9090 -p 28088:8088    exec  string  Specify the Docker shell command to execute after the Docker service is started. For example:  nginx -t -c ~/mynginx.conf", 
            "title": "Docker Details"
        }, 
        {
            "location": "/services/#apex-details", 
            "text": "Item  Type  Description      appName  string  Specify the Apex application in the APA to launch. For example:  OA    launchArgs  json object  Arguments to use during the launching of the Apex service. For example: {   \"kafkaBrokers\": \"localhost:9092\",   \"kafkaTopic\": \"analytics\" } (Optional)", 
            "title": "Apex Details"
        }, 
        {
            "location": "/services/#proxy-settings", 
            "text": "Item  Type  Description      address  string  Host:port to which the proxy path forwards to. For example:  localhost:28088 (Optional)    followRedirect  boolean  If this property is true, then the Gateway proxy will perform redirect when it sees the HTTP status code 302 in the HTTP response header from the service.  Therefore, the browser surfing the service proxy URL will never encounter the hTTP status code 302. Warning : Omitting this property or setting it to true may cause a maximum redirect error in the Gateway proxy. (Optional, default: true)    requestHeaders  json object  Headers to be added to the request made by the Gateway to the proxy destination. For example: {   \"X_PROXY_REMOTE_USER\": \"dtadmin\" } (Optional)    replaceStrings  array of json object  Definitions that represents text replacement processing to be performed on the response body by the Gateway proxy.  Regular expression is supported as described in the  Java Regex Pattern Class , which includes capturing group and back references. [   {     \"matchMime\": \"text/html\",     \"matchUrl\": \".*.html\",     \"matchText\": \"\\\"/static/\",     \"replaceText\": \"\\\"/proxy/services/superset/static/\"   }   ... ] (Optional)", 
            "title": "Proxy Settings"
        }, 
        {
            "location": "/services/#replace-strings-details", 
            "text": "Item  Type  Description      matchMime  string  Process only for this mime-type. For example:  text/html (Optional)    matchUrl  string  Process only when the URL matches this regular expression pattern. For example:  acct* (Optional)    matchText  string  Text to be matched in the response body. For example:  href=\\\"/static/    replaceText  string  Text that replaces the matched-text. For example:  href=\\\"/proxy/services/superset-fraud-app/static/ (Optional, default: '')     Note : Explicit metadata, implicit and global variables such as  ${superset-fpa.ipaddr} ,  ${superset-fpa._state} ,  ${.GATEWAY_CONNECT_ADDRESS} , etc. are not currently supported in the replace strings definitions.  Example 1:  {\n  \"matchMime\": \"text/html\",\n  \"matchUrl\": \".*\\.html\",\n  \"matchText\": \"href=\\\"/static/\",\n  \"replaceText\": \"href=\\\"/proxy/services/superset-fraud-app/static/\"\n}  The above example tells the Gateway proxy to process request URLs ending with  .html  and the response header mime-type equals  text/html .  Once the URL and mime-type are a match, then the response body is transformed by replacing every occurrence of  href=\"/static/  with  href=\"/proxy/services/superset-fraud-app/static/ .  Example 2:  {\n  \"matchMime\": \"text/html\",\n  \"matchUrl\": \".*\",\n  \"matchText\": \"num=([0-9]*)\",\n  \"replaceText\": \"NUM=\\\"$1\\\"\"\n}  The above example tells the Gateway to process requests where the response header mime-type equals  text/html .  Once the mime-type is a match, then the response body is transformed by replacing every occurrence of  num=one or more digits  with  NUM=\"same digits\" .  For example:  num=25  becomes  NUM=\"25\" ,  num=100  becomes  NUM=\"100\" , etc.  Note : The matchUrl in this case will match any URL so it could have been omitted.  In addition to the explicit metadata variables defined in the services, there are implicit and global variables that can be used in the service configuration also.  Implicit variables are specific to the service while global variables are specific to the Gateway.  Note : These variables are only applicable to the following properties:  srcUrl, proxy.address, docker.run, docker.exec and apex.launchArgs values (not names)  Implicit Variables     Item  Description      _type  This variable should resolve to the service type such as  docker  or  apex .  The syntax to reference this variable is  ${superset-fpa._type} , assuming the service name is  superset-fpa .    _state  This variable should resolve to the service status.  For a complete list of service status, see the  service status table  in the Manage section. The syntax to reference this variable is  ${superset-fpa._state} , assuming the service name is  superset-fpa .     Global Variables     Item  Description      GATEWAY_CONNECT_ADDRESS  This is the Gateway connection address.  The syntax to reference this variable is  ${.GATEWAY_CONNECT_ADDRESS} .    GATEWAY_ADMIN_USER  This is the Unix user that the Gateway runs as. The syntax to reference this variable is  ${.GATEWAY_ADMIN_USER} .", 
            "title": "Replace Strings Details"
        }, 
        {
            "location": "/services/#applications-property", 
            "text": "Applications depending on services are defined in the  applications  property.", 
            "title": "Applications Property"
        }, 
        {
            "location": "/services/#applications-parameters", 
            "text": "Item  Type  Description      name  string  Apex application name, which exists in the current APA package.    requiredServices  array of json object  List of services in which this application depends on.  If one of the services depends on other services, transitive service dependencies do not need to be specified explicitly. For example: [   {     \"name\": \"superset-fpa\",     \"requiredBeforeLaunch\": true,     \"transient\": true   }   ... ]     requiredServices  is an array of JSON objects where each object defines a service the application depends on.", 
            "title": "Applications Parameters"
        }, 
        {
            "location": "/services/#required-services-parameters", 
            "text": "Item  Type  Description      name  string  The service name this application depends on.    requiredBeforeLaunch  boolean  If this property is set to true, then the application cannot be launched until this service is started. (Optional, default: false)    transient  boolean  If this property is set to true, then it is deleted when the application is killed or shutdown. (Optional, default: false)", 
            "title": "Required Services Parameters"
        }, 
        {
            "location": "/jar_artifacts/", 
            "text": "JAR Artifacts\n\n\nArtifacts can be used to provide libraries, rules, schemas, and custom code to applications.  JAR artifacts follow Apache Maven standards which require groupId, artifactId and version to be specified.  JAR files can be uploaded manually and synchronized automatically from a Maven artifacts directory accessible by the Gateway.  Users can also build new JAR files by creating new schemas using the \nNew Schema\n dialog in the DT RTS Console.  Once the JAR artifacts are added to the DT RTS system, Apex applications can reference them in the \nApplication Configuration\n page.\n\n\nViewing Artifacts\n\n\nArtifacts can be view and managed on the \nJAR Artifacts\n page in the DT RTS Console.  There you can create new schemas and upload JAR files.\n\n\nTo view the \nJAR Artifacts\n page, follow the steps below:\n\n\n\n\nClick the \nDevelop\n link on the top navigation menu.  The Development page is displayed.\n\n\nClick the \nJAR Artifacts\n link on the Development page. The JAR Artifacts page is displayed.\n\n\n\n\nSample JAR artifacts list:\n\n\n\n\nSearching and Filtering\n\n\nThe search field in the upper right side of the page performs a global search across all the fields shown in the table.  Additionally, each column has its own filtering options where filter is performed on that field in addition to the global search and any other active filters.  Hovering over the filter input field displays the tooltip with examples of filter expressions which can be used for the specific filter.  As you enter the search string and column filters, the values are added to the URL parameters, making it possible to bookmark or share the filtered URL with another user.\n\n\nSetting Table Size and Pagination\n\n\nBy default, the table displays 20 artifacts per page.  However, you can change this by clicking on the artifacts count and selecting a different size.\n\nTo the right of the artifact counts are the \nprevious\n and \nnext\n page buttons.  You can click on these buttons to navigate from page to page.  As the artifact offset value changes, the value is added to the URL parameters.  The parameter name for this value is \noffset\n.  If there are many artifacts in the system, you can type an offset number for the \noffset\n parameter to jump directly to that page.\n\n\nArtifacts per page dropdown menu:\n\n\n\n\nTo change the artifacts per page, follow the steps below:\n\n\n\n\nOn the \nJAR Artifacts\n page, click the artifacts count above the artifacts table.  The dropdown menu is shown.\n\n\nSelect the desired \nitems per page\n menu item.  The list is refreshed with the selected count of artifacts per page. The selected artifacts per page should also be added to the URL parameters. This parameter name for this value is \nlimit\n.\n\n\n\n\nAdding New Schemas\n\n\nExisting schemas stored in JAR files can be uploaded by clicking \nadd\n button and selecting \nupload JAR\n option.  Additionally, new schemas can be created by clicking \nadd\n and selecting \nnew schema\n option.  When new schemas are created from the UI, the Gateway packages the schema as a JAR file, and then stores it in the shared artifacts space on the DFS used by DT RTS.  The file names and directory structures follow the Maven directory layout and file naming standards.  New schemas can also be added on the \nApplication Configuration\n page.\n\n\nTo create a new schema, follow the steps below:\n\n\n\n\nOn the \nJAR Artifacts\n page, click the \nadd\n button.  A dropdown menu is shown.\n\n\nClick on the \nnew schema\n menu item.  The \nNew Schema\n dialog is shown.\n\n\n\n\nSample of a new schema dialog:\n\n\n\n\nThe following entries are shown on the \nNew Schema\n dialog:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup ID\n\n\nThis is typically the organization name preceded by \norg\n or \ncom\n.  This value is also used as the top level directory where the JAR file is stored.\nFor example: \norg.apache.maven\n, \ncom.datatorrent\n, etc.\n\n\n\n\n\n\nArtifact ID\n\n\nA name representing this schema.  The schema is built and packaged as a JAR file and this value is used as part of the file name.\nFor example: \nnetlet\n, \njava-xmlbuilder\n, etc.\n\n\n\n\n\n\nVersion\n\n\nThe schema version.  The schema is built and packaged as a JAR file and this value is used as part of the file name.\nFor example: \n1.0.0\n, \n1.0.1\n, etc.\n\n\n\n\n\n\nSchema Content\n\n\nA JSON object structure representing the schema definition.  This JSON structure must comply with the \nApache Avro\n specification.  You can click on the AVRO link on the dialog to open the Apache Avro schema documentation page in a new browser tab for reference. \n{  \"type\": \"enum\",  \"name\": \"states\",  \"symbols\": [\"CA\", \"MA\", \"NV\", \"NY\", \"TX\"]}\n\n\n\n\n\n\n\n\nNote\n: If you enter the Group ID, Artifact ID and Version that match an existing schema or JAR artifact, you will see an error message stating that an artifact with the entered Group ID, Artifact ID and Version already exists.  The \nSave\n button will be disabled until you change one of the fields.\n\n\nUploading JAR File\n\n\nJAR files can be uploaded into the shared artifacts space on the DFS used by DT RTS.  When uploading JAR files, the Gateway searches for a pom.xml file in the JAR file to determine the \ngroupId\n, \nartifactId\n, and \nversion\n (GAV) for the JAR file.  If the JAR file does not have a pom.xml file, then users can enter the group ID, artifact ID and version in the upload dialog and save the JAR file.  If users have to enter the group ID, artifact ID and version, then the Gateway will create an pom.xml file containing this information and store it in the uploaded JAR file.\n\n\nTo upload a JAR file, follow the steps below:\n\n\n\n\nOn the \nJAR Artifacts\n page, click the \nadd\n button.  A dropdown menu is shown.\n\n\nClick on the \nupload JAR\n menu item.  The \nFile Artifact Upload\n dialog is shown.\n\n\nDrag a valid JAR file into the dialog.  The file is uploaded into a temporary storage for the Gateway to process the it.\n\n\n\n\nIf a valid JAR file is uploaded, then a JAR file upload form is shown.\n\n\nSample JAR file upload dialog:\n\n\n\n\n\n\n\n\nEnter the Group ID, Artifact ID and Version if necessary.\n\n\n\n\nClick the \nSave\n button to save the JAR file to the shared artifacts space.\n\n\n\n\nBelow are the descriptions of the fields shown on the dialog:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup ID\n\n\nThis value is found in the \npom.xml\n file inside the JAR file by the Gateway. If the pom.xml file is missing, then the value is empty.  You must enter an appropriate group ID in order to save the JAR file. This is usually the JAR artifact organization name preceded by \norg\n or \ncom\n.  This value is also used as the top level directory where the JAR file.\nFor example: \norg.apache.maven\n, \ncom.datatorrent\n, etc.\n\n\n\n\n\n\nArtifact ID\n\n\nThis value is found in the \npom.xml\n file inside the JAR file by the Gateway.  If the pom.xml file is missing, then the value is empty.  You must enter an appropriate artifact ID in order to save the JAR file.  This value is also used as part of the file name when saving the JAR file to the shared artifacts space.\nFor example: \nnetlet\n, \njava-xmlbuilder\n, etc.\n\n\n\n\n\n\nVersion\n\n\nThis value is found in the \npom.xml\n file inside the JAR file by the Gateway.  If the pom.xml file is missing, then the value is empty.  You must enter an appropriate version in order to save the JAR file.  This value is also used as part of the file name when saving the JAR file to the shared artifacts space.\nFor example: \n1.0.0\n, \n1.0.1\n, etc.\n\n\n\n\n\n\nTags\n\n\nThe Gateway analyzes the JAR file to determine the tags for the JAR file.  Possible tag values are: \nrule\n and \nschema\n.  If the Gateway cannot determine the tags for this JAR file, then an empty field is shown.  When saving the JAR file, the value \nother\n is saved with the JAR file. \nThis field is readonly\n.\n\n\n\n\n\n\nSize\n\n\nThe size of the JAR file. \nThis field is readonly\n.\n\n\n\n\n\n\nPath\n\n\nThe path and file name where the JAR file is temporary stored for processing. \nThis field is readonly\n.\n\n\n\n\n\n\n\n\nAfter uploading a JAR file, the Gateway processes it and shows the information gathers from the JAR file.  You can process a different JAR file by dragging another JAR file into the top section of the dialog.  Alternatively, you can click on the down chevron button to expand that section and click on the \nChoose another file\n button.\n\n\nIf you are loading a JAR file that already exists in the shared artifacts space, you can overwrite the existing JAR file by clicking on the \nReplace\n button on the dialog.\n\n\nSynchronizing JAR Files\n\n\nIn addition to uploading JAR files and creating new schemas manually, Gateway can synchronize the jars from a Maven type \n.m2\n directory available to it on a local file system and copy these jars to shared artifacts space on the DFS.  This can be useful when automating build and delivery processes, or working with a \nCEP Workbench Service\n where new artifacts are created and need to be made available to DT RTS automatically.  You must configure the \ndt.gateway.maven.local.repo\n property in the \ndt-site.xml\n file for this synchronization process to work.\n\n\nTo configure the Gateway to synchronize JAR files on the local file system with shared artifacts on DFS, follow the steps below:\n\n\n\n\nAdd the XML section below to the \ndt-site.xml\n file.\n\n\n\n\nproperty\n\n  \nname\ndt.gateway.maven.local.repo\n/name\n  \n  \nvalue\n/path/of/artifacts/store/on/the/local/fs/.m2\n/value\n\n\n/property\n\n\n\n\n\n\n\nRestart the Gateway.\n\n\n\n\nSample XML to configure JAR files synchronization:\n\n\nproperty\n\n  \nname\ndt.gateway.maven.local.repo\n/name\n\n  \nvalue\n/home/jenkins/.m2\n/value\n\n\n/property\n\n\n\n\n\nThe example above designates the directory named \n/home/jenkins/.m2\n on the file system local to Gateway as the directory where all the Maven JAR artifacts are stored, and which will be synchronized with shared artifacts space on DFS used by DT RTS.", 
            "title": "JAR Artifacts"
        }, 
        {
            "location": "/jar_artifacts/#jar-artifacts", 
            "text": "Artifacts can be used to provide libraries, rules, schemas, and custom code to applications.  JAR artifacts follow Apache Maven standards which require groupId, artifactId and version to be specified.  JAR files can be uploaded manually and synchronized automatically from a Maven artifacts directory accessible by the Gateway.  Users can also build new JAR files by creating new schemas using the  New Schema  dialog in the DT RTS Console.  Once the JAR artifacts are added to the DT RTS system, Apex applications can reference them in the  Application Configuration  page.", 
            "title": "JAR Artifacts"
        }, 
        {
            "location": "/jar_artifacts/#viewing-artifacts", 
            "text": "Artifacts can be view and managed on the  JAR Artifacts  page in the DT RTS Console.  There you can create new schemas and upload JAR files.  To view the  JAR Artifacts  page, follow the steps below:   Click the  Develop  link on the top navigation menu.  The Development page is displayed.  Click the  JAR Artifacts  link on the Development page. The JAR Artifacts page is displayed.   Sample JAR artifacts list:", 
            "title": "Viewing Artifacts"
        }, 
        {
            "location": "/jar_artifacts/#searching-and-filtering", 
            "text": "The search field in the upper right side of the page performs a global search across all the fields shown in the table.  Additionally, each column has its own filtering options where filter is performed on that field in addition to the global search and any other active filters.  Hovering over the filter input field displays the tooltip with examples of filter expressions which can be used for the specific filter.  As you enter the search string and column filters, the values are added to the URL parameters, making it possible to bookmark or share the filtered URL with another user.", 
            "title": "Searching and Filtering"
        }, 
        {
            "location": "/jar_artifacts/#setting-table-size-and-pagination", 
            "text": "By default, the table displays 20 artifacts per page.  However, you can change this by clicking on the artifacts count and selecting a different size. \nTo the right of the artifact counts are the  previous  and  next  page buttons.  You can click on these buttons to navigate from page to page.  As the artifact offset value changes, the value is added to the URL parameters.  The parameter name for this value is  offset .  If there are many artifacts in the system, you can type an offset number for the  offset  parameter to jump directly to that page.  Artifacts per page dropdown menu:   To change the artifacts per page, follow the steps below:   On the  JAR Artifacts  page, click the artifacts count above the artifacts table.  The dropdown menu is shown.  Select the desired  items per page  menu item.  The list is refreshed with the selected count of artifacts per page. The selected artifacts per page should also be added to the URL parameters. This parameter name for this value is  limit .", 
            "title": "Setting Table Size and Pagination"
        }, 
        {
            "location": "/jar_artifacts/#adding-new-schemas", 
            "text": "Existing schemas stored in JAR files can be uploaded by clicking  add  button and selecting  upload JAR  option.  Additionally, new schemas can be created by clicking  add  and selecting  new schema  option.  When new schemas are created from the UI, the Gateway packages the schema as a JAR file, and then stores it in the shared artifacts space on the DFS used by DT RTS.  The file names and directory structures follow the Maven directory layout and file naming standards.  New schemas can also be added on the  Application Configuration  page.  To create a new schema, follow the steps below:   On the  JAR Artifacts  page, click the  add  button.  A dropdown menu is shown.  Click on the  new schema  menu item.  The  New Schema  dialog is shown.   Sample of a new schema dialog:   The following entries are shown on the  New Schema  dialog:     Item  Description      Group ID  This is typically the organization name preceded by  org  or  com .  This value is also used as the top level directory where the JAR file is stored. For example:  org.apache.maven ,  com.datatorrent , etc.    Artifact ID  A name representing this schema.  The schema is built and packaged as a JAR file and this value is used as part of the file name. For example:  netlet ,  java-xmlbuilder , etc.    Version  The schema version.  The schema is built and packaged as a JAR file and this value is used as part of the file name. For example:  1.0.0 ,  1.0.1 , etc.    Schema Content  A JSON object structure representing the schema definition.  This JSON structure must comply with the  Apache Avro  specification.  You can click on the AVRO link on the dialog to open the Apache Avro schema documentation page in a new browser tab for reference.  {  \"type\": \"enum\",  \"name\": \"states\",  \"symbols\": [\"CA\", \"MA\", \"NV\", \"NY\", \"TX\"]}     Note : If you enter the Group ID, Artifact ID and Version that match an existing schema or JAR artifact, you will see an error message stating that an artifact with the entered Group ID, Artifact ID and Version already exists.  The  Save  button will be disabled until you change one of the fields.", 
            "title": "Adding New Schemas"
        }, 
        {
            "location": "/jar_artifacts/#uploading-jar-file", 
            "text": "JAR files can be uploaded into the shared artifacts space on the DFS used by DT RTS.  When uploading JAR files, the Gateway searches for a pom.xml file in the JAR file to determine the  groupId ,  artifactId , and  version  (GAV) for the JAR file.  If the JAR file does not have a pom.xml file, then users can enter the group ID, artifact ID and version in the upload dialog and save the JAR file.  If users have to enter the group ID, artifact ID and version, then the Gateway will create an pom.xml file containing this information and store it in the uploaded JAR file.  To upload a JAR file, follow the steps below:   On the  JAR Artifacts  page, click the  add  button.  A dropdown menu is shown.  Click on the  upload JAR  menu item.  The  File Artifact Upload  dialog is shown.  Drag a valid JAR file into the dialog.  The file is uploaded into a temporary storage for the Gateway to process the it.   If a valid JAR file is uploaded, then a JAR file upload form is shown.  Sample JAR file upload dialog:     Enter the Group ID, Artifact ID and Version if necessary.   Click the  Save  button to save the JAR file to the shared artifacts space.   Below are the descriptions of the fields shown on the dialog:     Item  Description      Group ID  This value is found in the  pom.xml  file inside the JAR file by the Gateway. If the pom.xml file is missing, then the value is empty.  You must enter an appropriate group ID in order to save the JAR file. This is usually the JAR artifact organization name preceded by  org  or  com .  This value is also used as the top level directory where the JAR file. For example:  org.apache.maven ,  com.datatorrent , etc.    Artifact ID  This value is found in the  pom.xml  file inside the JAR file by the Gateway.  If the pom.xml file is missing, then the value is empty.  You must enter an appropriate artifact ID in order to save the JAR file.  This value is also used as part of the file name when saving the JAR file to the shared artifacts space. For example:  netlet ,  java-xmlbuilder , etc.    Version  This value is found in the  pom.xml  file inside the JAR file by the Gateway.  If the pom.xml file is missing, then the value is empty.  You must enter an appropriate version in order to save the JAR file.  This value is also used as part of the file name when saving the JAR file to the shared artifacts space. For example:  1.0.0 ,  1.0.1 , etc.    Tags  The Gateway analyzes the JAR file to determine the tags for the JAR file.  Possible tag values are:  rule  and  schema .  If the Gateway cannot determine the tags for this JAR file, then an empty field is shown.  When saving the JAR file, the value  other  is saved with the JAR file.  This field is readonly .    Size  The size of the JAR file.  This field is readonly .    Path  The path and file name where the JAR file is temporary stored for processing.  This field is readonly .     After uploading a JAR file, the Gateway processes it and shows the information gathers from the JAR file.  You can process a different JAR file by dragging another JAR file into the top section of the dialog.  Alternatively, you can click on the down chevron button to expand that section and click on the  Choose another file  button.  If you are loading a JAR file that already exists in the shared artifacts space, you can overwrite the existing JAR file by clicking on the  Replace  button on the dialog.", 
            "title": "Uploading JAR File"
        }, 
        {
            "location": "/jar_artifacts/#synchronizing-jar-files", 
            "text": "In addition to uploading JAR files and creating new schemas manually, Gateway can synchronize the jars from a Maven type  .m2  directory available to it on a local file system and copy these jars to shared artifacts space on the DFS.  This can be useful when automating build and delivery processes, or working with a  CEP Workbench Service  where new artifacts are created and need to be made available to DT RTS automatically.  You must configure the  dt.gateway.maven.local.repo  property in the  dt-site.xml  file for this synchronization process to work.  To configure the Gateway to synchronize JAR files on the local file system with shared artifacts on DFS, follow the steps below:   Add the XML section below to the  dt-site.xml  file.   property \n   name dt.gateway.maven.local.repo /name   \n   value /path/of/artifacts/store/on/the/local/fs/.m2 /value  /property    Restart the Gateway.   Sample XML to configure JAR files synchronization:  property \n   name dt.gateway.maven.local.repo /name \n   value /home/jenkins/.m2 /value  /property   The example above designates the directory named  /home/jenkins/.m2  on the file system local to Gateway as the directory where all the Maven JAR artifacts are stored, and which will be synchronized with shared artifacts space on DFS used by DT RTS.", 
            "title": "Synchronizing JAR Files"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.\n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows.\n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nMalhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.   Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.  Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.  Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "Malhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Social Media"
        }, 
        {
            "location": "/appbackplane/", 
            "text": "Application Backplane\n\n\nApplication backplane is a mechanism that enables communication between applications. Using this communication channel, the fraud prevention or detection outcomes can be shared across multiple applications so that fraud prevention in one application can signal fraud prevention in another application and further reduce the chance of fraud.\n\n\nThe fraud applications can function independently and still benefit from a network-effect of fraud reduction.\n\n\nSetting the Application Backplane\n\n\nApplication backplane is created based on the following modules that are included in the DAG of the fraud applications.\n\n\n\n\nOutput Module\n\nThis module is in the application that must send outcomes to the application backplane.\n\n\nInput Module\n\nThis module is in the application that must listen to an application backplane.\n\n\n\n\nOutput Module\n\n\nThe output module has the following operators:\n\n\n\n\n\n\n\n\nOperators\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAvro serialization\n\n\nThis operator does the serialization based on the schema set on the module.\n\n\n\n\n\n\nKafka output operator\n\n\nThis operator emits the serialized byte array to Kafka topic whose properties are set on the module.\n\n\n\n\n\n\n\n\nInput Module\n\n\nThe input module has the following operators:\n\n\n\n\n\n\n\n\nOperators\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKafka input operator\n\n\nThis operator emits the byte array from Kafka topic whose properties are set on the module. These properties must be the same ones set on the corresponding output module whose messages are subscribed. You can also indicate which offset to start reading messages from kafka.\n\n\n\n\n\n\nAvro deserialization\n\n\nThis operator does the deserialization based on  the schema set on the module. The schema must be the same one set on corresponding output module whose messages are subscribed. The deserialized class should be in classpath. This will be achieved by querying schema repository to include the jar of the class.\n\n\n\n\n\n\n\n\nSample of Application Backplane Setup\n\n\nThe following image depicts how \nAccount Takeover\n application (ATO) uses the output module as a connector to send the data outcome to a configured Kafka topic.\n\n\nThis outcome can be used by \nOmni Channel Fraud Prevention\n application (\nFP application\n), to obtain enhanced insights.\n\n\nThis is achieved with \nAccountDataEnricher\n operator which is added in the DAG of the \nFP application\n. This operator has the following two input ports:\n\n\n\n\n\nATO input\n : The stream coming from the input module \nAnalyzedActivity\n is connected to this port. The properties set on this module correspond to those used in output module in the \nATO\n application. \n\n\nFP input\n : The resultant stream after product-based enrichment on the transactions, \nEnriched Products\n is connected to this port.\n\n\n\n\nThe enricher maintains a set of users whose accounts have been taken over. This data structure gets updated with fresh activities in \nAnalyzedActivity\n stream. Upon receiving a product enriched payment card transaction on the fraud input port, the enricher checks if the customer's account is taken over, by querying on the membership of the customer's UserID. If it is true, an \nisAccountFlagged\n variable of payment card transaction is set to \ntrue\n.\n\n\nConfiguring Application Backplane for FPA and ATO\n\n\nTo configure application backplane for FP application and ATO application, do the following:\n\n\n\n\nCreate an application configuration for the ATO application. Refer to, \nCreating Application Configuration\n\n\nOpen this configuration and under \nRequired Properties\n \n \nFacts Output Topic\n specify the Kafka topic for the output module.\n\n\nSpecify all the other required properties and then save and launch the application configuration.\n\n\nCreate an application configuration for the FP application.\n\n\nOpen this configuration and under \nRequired Properties\n \n \nAto Facts Output Topic\n specify the same Kafka topic for the input module.\n\n\nSpecify all the other required properties and then save and launch the application configuration.  The flagged outcomes can be received in the kafka topic.", 
            "title": "Application Backplane"
        }, 
        {
            "location": "/appbackplane/#application-backplane", 
            "text": "Application backplane is a mechanism that enables communication between applications. Using this communication channel, the fraud prevention or detection outcomes can be shared across multiple applications so that fraud prevention in one application can signal fraud prevention in another application and further reduce the chance of fraud.  The fraud applications can function independently and still benefit from a network-effect of fraud reduction.", 
            "title": "Application Backplane"
        }, 
        {
            "location": "/appbackplane/#setting-the-application-backplane", 
            "text": "Application backplane is created based on the following modules that are included in the DAG of the fraud applications.   Output Module \nThis module is in the application that must send outcomes to the application backplane.  Input Module \nThis module is in the application that must listen to an application backplane.", 
            "title": "Setting the Application Backplane"
        }, 
        {
            "location": "/appbackplane/#output-module", 
            "text": "The output module has the following operators:     Operators  Description      Avro serialization  This operator does the serialization based on the schema set on the module.    Kafka output operator  This operator emits the serialized byte array to Kafka topic whose properties are set on the module.", 
            "title": "Output Module"
        }, 
        {
            "location": "/appbackplane/#input-module", 
            "text": "The input module has the following operators:     Operators  Description      Kafka input operator  This operator emits the byte array from Kafka topic whose properties are set on the module. These properties must be the same ones set on the corresponding output module whose messages are subscribed. You can also indicate which offset to start reading messages from kafka.    Avro deserialization  This operator does the deserialization based on  the schema set on the module. The schema must be the same one set on corresponding output module whose messages are subscribed. The deserialized class should be in classpath. This will be achieved by querying schema repository to include the jar of the class.", 
            "title": "Input Module"
        }, 
        {
            "location": "/appbackplane/#sample-of-application-backplane-setup", 
            "text": "The following image depicts how  Account Takeover  application (ATO) uses the output module as a connector to send the data outcome to a configured Kafka topic. \nThis outcome can be used by  Omni Channel Fraud Prevention  application ( FP application ), to obtain enhanced insights.  This is achieved with  AccountDataEnricher  operator which is added in the DAG of the  FP application . This operator has the following two input ports:   ATO input  : The stream coming from the input module  AnalyzedActivity  is connected to this port. The properties set on this module correspond to those used in output module in the  ATO  application.   FP input  : The resultant stream after product-based enrichment on the transactions,  Enriched Products  is connected to this port.   The enricher maintains a set of users whose accounts have been taken over. This data structure gets updated with fresh activities in  AnalyzedActivity  stream. Upon receiving a product enriched payment card transaction on the fraud input port, the enricher checks if the customer's account is taken over, by querying on the membership of the customer's UserID. If it is true, an  isAccountFlagged  variable of payment card transaction is set to  true .", 
            "title": "Sample of Application Backplane Setup"
        }, 
        {
            "location": "/appbackplane/#configuring-application-backplane-for-fpa-and-ato", 
            "text": "To configure application backplane for FP application and ATO application, do the following:   Create an application configuration for the ATO application. Refer to,  Creating Application Configuration  Open this configuration and under  Required Properties     Facts Output Topic  specify the Kafka topic for the output module.  Specify all the other required properties and then save and launch the application configuration.  Create an application configuration for the FP application.  Open this configuration and under  Required Properties     Ato Facts Output Topic  specify the same Kafka topic for the input module.  Specify all the other required properties and then save and launch the application configuration.  The flagged outcomes can be received in the kafka topic.", 
            "title": "Configuring Application Backplane for FPA and ATO"
        }, 
        {
            "location": "/storeandreplay/", 
            "text": "About Store and Replay Feature\n\n\nThe Store and Replay Feature enables a user to simulate and view the outcomes of what-if scenarios by storing incoming data that is used later to evaluate and visualize this same data with different rules and business logic. Applications with the capability to store and replay can store the incoming data from the Kafka input operators and then replay the stored data with a different set of rules to visualize the outcome.\n\n\nThe store and replay functions can be enabled by adding specific properties to an application configuration. To prevent potential complications, it is recommended that only one instance of the application be launched for storing data, while a separate instance of the application is then launched to replay a certain time frame of the data stored by the prior application instance. The separate instances can use different configurations and rules to achieve the what-if analysis. When the store-and-replay related configuration properties are specified, the \"storing\" part of the feature is enabled to store the input data to an application's HDFS path.\n\n\nEnabling Data Storage\n\n\nTo store incoming data, you must create an application configuration, add the applicable properties, and launch the application with this configuration. Follow these steps:\n\n\n\n\nCreate an \napplication configuration\n.\n\n\n\n\nOpen this application configuration and add the following optional properties with their corresponding values. Though the UI displays optional properties, these properties are required for enabling the data storage for replay:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.enableArchive\n\n\ntrue\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.enableReplay\n\n\nfalse\n\n\n\n\n\n\n\n\nNote\n : In the above example, \ndt.operator.TransactionReceiver\n is the name of the Kafka input operator. Change this name to the Kafka input operator your application DAG uses.\n\n\n\n\n\n\nClick \nSave\n and then click \nLaunch\n.\n\n\n\n\n\n\nCreating Rules\n\n\nThis step is optional and is used to create alternative rules for your what-if analysis. Please see \nCreating Rules using CEP Workbench\n.\n\n\nEnabling Replay\n\n\nTo replay data, you must create an application configuration, add the applicable properties, apply the required rules, and launch the application with this configuration.  Using the properties, you specify the following:\n\n\n\n\nEnable replay\n: The store function is set to false and the replay is set to true.\n\n\nSrcArchiveAppName\n: Kafka time-offset mapping is read from DB related to SrcArchiveAppName. SrcArchiveAppName is the name of the separate instance of the application that stores the Kafka topic offset in HDFS.\n\n\n\n\nNote\n: The \"Instance name\" is usually a different configured name of the same application.\n\n\n\n\nStart time/endtime\n\nSpecify the start time and end time to begin and stop the replay.\n\n\n\n\nTo enable replay, do the following:\n\n\n\n\nCreate an \napplication configuration\n.\n\n\n\n\nOpen this configuration and add the following optional properties with their corresponding properties:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.enableArchive\n\n\nfalse\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.enableReplay\n\n\ntrue\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.srcArchiveAppName\n\n\nDetectionApp.\"DetectionApp\" is the example name of another app with enableArchive turned on, which has been launched or is running.\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.replayStartTime\n\n\nyyyy-MM-ddTHH:mm:ss.  (For example: 2017-11-20T08:11:00)\n\n\n\n\n\n\ndt.operator.TransactionReceiver.prop.replayEndTime\n\n\nyyyy-MM-ddTHH:mm:ss. (For example: 2017-11-20T18:11:00)\n\n\n\n\n\n\n\n\nNote\n : In the above example, \ndt.operator.TransactionReceiver\n is the name of the Kafka input operator. Change this name to the Kafka input operator your application DAG uses.\n\n\n\n\n\n\nClick \nSave\n and then click \nLaunch\n. You can view the outcome from the \nVisualize\n tab by importing a dashboard for the application instance.\n\n\n\n\n\n\nVisualizing Replay of Data\n\n\nTo visualize replay of data after applying various rules, you can either configure dashboards for this application using \napplication configuration\n or import \ndashboards\n for the application where you have enabled replay.", 
            "title": "Store and Replay Feature"
        }, 
        {
            "location": "/storeandreplay/#about-store-and-replay-feature", 
            "text": "The Store and Replay Feature enables a user to simulate and view the outcomes of what-if scenarios by storing incoming data that is used later to evaluate and visualize this same data with different rules and business logic. Applications with the capability to store and replay can store the incoming data from the Kafka input operators and then replay the stored data with a different set of rules to visualize the outcome.  The store and replay functions can be enabled by adding specific properties to an application configuration. To prevent potential complications, it is recommended that only one instance of the application be launched for storing data, while a separate instance of the application is then launched to replay a certain time frame of the data stored by the prior application instance. The separate instances can use different configurations and rules to achieve the what-if analysis. When the store-and-replay related configuration properties are specified, the \"storing\" part of the feature is enabled to store the input data to an application's HDFS path.", 
            "title": "About Store and Replay Feature"
        }, 
        {
            "location": "/storeandreplay/#enabling-data-storage", 
            "text": "To store incoming data, you must create an application configuration, add the applicable properties, and launch the application with this configuration. Follow these steps:   Create an  application configuration .   Open this application configuration and add the following optional properties with their corresponding values. Though the UI displays optional properties, these properties are required for enabling the data storage for replay:     Property Name  Value      dt.operator.TransactionReceiver.prop.enableArchive  true    dt.operator.TransactionReceiver.prop.enableReplay  false     Note  : In the above example,  dt.operator.TransactionReceiver  is the name of the Kafka input operator. Change this name to the Kafka input operator your application DAG uses.    Click  Save  and then click  Launch .", 
            "title": "Enabling Data Storage"
        }, 
        {
            "location": "/storeandreplay/#creating-rules", 
            "text": "This step is optional and is used to create alternative rules for your what-if analysis. Please see  Creating Rules using CEP Workbench .", 
            "title": "Creating Rules"
        }, 
        {
            "location": "/storeandreplay/#enabling-replay", 
            "text": "To replay data, you must create an application configuration, add the applicable properties, apply the required rules, and launch the application with this configuration.  Using the properties, you specify the following:   Enable replay : The store function is set to false and the replay is set to true.  SrcArchiveAppName : Kafka time-offset mapping is read from DB related to SrcArchiveAppName. SrcArchiveAppName is the name of the separate instance of the application that stores the Kafka topic offset in HDFS.   Note : The \"Instance name\" is usually a different configured name of the same application.   Start time/endtime \nSpecify the start time and end time to begin and stop the replay.   To enable replay, do the following:   Create an  application configuration .   Open this configuration and add the following optional properties with their corresponding properties:     Property Name  Value      dt.operator.TransactionReceiver.prop.enableArchive  false    dt.operator.TransactionReceiver.prop.enableReplay  true    dt.operator.TransactionReceiver.prop.srcArchiveAppName  DetectionApp.\"DetectionApp\" is the example name of another app with enableArchive turned on, which has been launched or is running.    dt.operator.TransactionReceiver.prop.replayStartTime  yyyy-MM-ddTHH:mm:ss.  (For example: 2017-11-20T08:11:00)    dt.operator.TransactionReceiver.prop.replayEndTime  yyyy-MM-ddTHH:mm:ss. (For example: 2017-11-20T18:11:00)     Note  : In the above example,  dt.operator.TransactionReceiver  is the name of the Kafka input operator. Change this name to the Kafka input operator your application DAG uses.    Click  Save  and then click  Launch . You can view the outcome from the  Visualize  tab by importing a dashboard for the application instance.", 
            "title": "Enabling Replay"
        }, 
        {
            "location": "/storeandreplay/#visualizing-replay-of-data", 
            "text": "To visualize replay of data after applying various rules, you can either configure dashboards for this application using  application configuration  or import  dashboards  for the application where you have enabled replay.", 
            "title": "Visualizing Replay of Data"
        }, 
        {
            "location": "/Licensing/", 
            "text": "DataTorrent Licensing\n\n\nDataTorrent offers three types of licenses for subscription:\n\n\n\n\nFree\n\n\nDataTorrent Service Plus\u00ae\n \n\n\nDataTorrent Premium Suite\u00ae\n \n\n\n\n\nEach license type is a combination of the allocated \nLicense Category\n, \nMemory Capacity\n, and \nTerm\n. You can request a new license as well as upload the license file from the \nLicense Information page\n on the DataTorrent RTS UI.\n\n\nNote:\n Existing customers must obtain a new license to upgrade to 3.9. Refer to \nBackward Compatibility\n for more details. \n\n\nAfter the obtained license file is uploaded from the RTS UI, it gets stored in hadoop at the following location:\n\n/user/\nhadoop user\n/datatorrent/licenses.\n For example: \n/user/hduser/datatorrent/licenses\n.\n\n\nThe details of the license capacity as well as the capabilities offered for your subscription are also displayed on the \nLicense Information page\n as shown in these images:\n\n\n\n\nThe following table provides the details of the license offerings for each of the license types:\n\n\n\n\n\n\n\n\nLicense Parameters\n\n\nFree Trial\n\n\nDataTorrent - Service Plus\n\n\nDataTorrent - Premium Suite\n\n\n\n\n\n\n\n\n\n\nMemory Limit\n\n\n40 GB\n\n\nBased on memory requirements\n\n\nBased on memory requirements\n\n\n\n\n\n\nSubscription Term\n\n\n90 Days\n\n\nYearly\n\n\nYearly\n\n\n\n\n\n\nEnvironment Type\n\n\nNon-Production Only\n\n\nProduction and Non-Production (Test, QA, Staging)\n\n\nProduction and Non-Production (Test, QA, Staging)\n\n\n\n\n\n\nTechnical Support SLA\n\n\nDT Community Forum\n\n\nProduction and Non-Production SLAs available\n\n\nProduction and Non-Production SLAs available\n\n\n\n\n\n\nLicense Category (Access to premium applications)\n\n\nNo\n\n\nNo\n\n\nYes\n\n\n\n\n\n\n\n\nThe \nFree Trial\n license can be directly obtained from the \nDataTorrent website\n. For \nDataTorrent Service Plus\n and \nDataTorrent Premium Suite\n licenses, you must contact the DataTorrent Sales Team.\n\n\nLicense Category\n\n\nThe license category determines the type of access that you can have for premium applications and operators that are available in DataTorrent\ns AppFactory.  Based on your business requirements, you can subscribe to any of the following license category:\n\n\n\n\nFree\n\n\nDT Plus\n\n\nDT Premium\n\n\n\n\nWith \nFree\n, you can download limited non-premium applications based on the allocated memory capacity.\n\n\nWith \nDT Plus\n, you can download only non-premium applications and operators that can be used for data ingestion. Premium applications, although visible on the AppFactory, can be downloaded only if you subscribe to \nDataTorrent Premium Suite\n. Customers who are interested in developing their own streaming applications using DataTorrent RTS can subscribe to this category.\n\n\nWith \nDT Premium\n, you can access and download pre-built stream applications and operators such as the Omni-Channel Fraud Prevention application and operators such as Drools-based rule engine for complex event processing.\n\n\nMemory Limit\n\n\nMemory is the capacity allotted to you, based on your license subscription, that can be used to launch the RTS applications.  The total memory consumed by all the running RTS applications defines the memory limit. For example, in a free trial version 40 GB memory limit is allotted and a minimum number of applications can be launched within this  limit.  To launch more applications in the same limit, you must terminate a running application.\n\n\nIn case your memory exceeds beyond the subscribed capacity, the \nLaunch\n, \nVisualize\n, and \nDevelop\n functions get disabled. The streaming applications keep running in the background but you cannot launch any more applications unless the license violation is corrected.\n\n\nYou will receive a warning \n30 minutes\n before these functions gets disabled. To increase the memory limit, you must renew or upgrade the license.\n\n\nTerm\n\n\nThe stipulated period agreed in the subscription is the license term allotted to you. You will be notified with an alert \n30 days prior\n to your license term expiry.  This alert notification is displayed as a banner on top of the RTS UI.\n\n\nWhen the license term expires, the   \nLaunch\n, \nVisualize\n, and \nDevelop\n functions get disabled.  Although the streaming applications keeps running in the background, you cannot launch new applications unless the license is renewed. To increase the license term, you must renew or upgrade the license.\n\n\nBackward Compatibility\n\n\nDataTorrent has updated the software licensing mechanism from RTS 3.9 version. Therefore, to integrate this mechanism, you must obtain a new software license file from DataTorrent or upgrade to version 3.9. Existing customers must contact DataTorrent to obtain an updated license for 3.9.\n\n\nRTS versions 3.9 or later is not backward compatible with previous RTS versions due to older gateway. Therefore, you must reinstall DT RTS before installing the new license. If the gateway is not compatible, the following license error message is displayed:\n\n\nThe License File is Corrupted", 
            "title": "Licensing"
        }, 
        {
            "location": "/Licensing/#datatorrent-licensing", 
            "text": "DataTorrent offers three types of licenses for subscription:   Free  DataTorrent Service Plus\u00ae    DataTorrent Premium Suite\u00ae     Each license type is a combination of the allocated  License Category ,  Memory Capacity , and  Term . You can request a new license as well as upload the license file from the  License Information page  on the DataTorrent RTS UI.  Note:  Existing customers must obtain a new license to upgrade to 3.9. Refer to  Backward Compatibility  for more details.   After the obtained license file is uploaded from the RTS UI, it gets stored in hadoop at the following location: /user/ hadoop user /datatorrent/licenses.  For example:  /user/hduser/datatorrent/licenses .  The details of the license capacity as well as the capabilities offered for your subscription are also displayed on the  License Information page  as shown in these images:   The following table provides the details of the license offerings for each of the license types:     License Parameters  Free Trial  DataTorrent - Service Plus  DataTorrent - Premium Suite      Memory Limit  40 GB  Based on memory requirements  Based on memory requirements    Subscription Term  90 Days  Yearly  Yearly    Environment Type  Non-Production Only  Production and Non-Production (Test, QA, Staging)  Production and Non-Production (Test, QA, Staging)    Technical Support SLA  DT Community Forum  Production and Non-Production SLAs available  Production and Non-Production SLAs available    License Category (Access to premium applications)  No  No  Yes     The  Free Trial  license can be directly obtained from the  DataTorrent website . For  DataTorrent Service Plus  and  DataTorrent Premium Suite  licenses, you must contact the DataTorrent Sales Team.", 
            "title": "DataTorrent Licensing"
        }, 
        {
            "location": "/installation/", 
            "text": "DataTorrent RTS Installation Guide\n\n\nThis guide covers installation of the DataTorrent RTS platform.\n\n\nPlanning\n\n\nInstallation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:\n\n\n\n\nAccessible by users who will launch and manage applications\n\n\nAccessible by all YARN nodes running Apache Apex applications\n\n\n\n\nNote\n: With \ndtGateway security\n configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.\n\n\nRequirements\n\n\n\n\nLinux operating system (tested on CentOS 6.x and Ubuntu 12.04)\n\n\nJava 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)\n\n\nHadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nGoogle Chrome or Safari to access the DataTorrent Console UI\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\n\n\nInstallation\n\n\nComplete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]\n\n\nBinaries are available in /opt/datatorrent/current/bin and links in /usr/bin\n\n\nConfiguration files located in /opt/datatorrent/current/conf\n\n\nLog files located in /var/log/datatorrent\n\n\nDataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\na.  Installing from self-extracting archive (*.bin)\n\n\n    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin\n\n\n\nb.  Installing from RedHat Package Manager archive (*.rpm)\n\n\n  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm\n\n\n\nLimited Local Installation\n\n\nA limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]\n\n\nBinaries are available under $HOME/datatorrent/current/bin\n\n\nConfiguration files located under $HOME/datatorrent/conf\n\n\nLog files located under $HOME/.dt/logs\n\n\n\n\nDataTorrent Gateway running as current user, and managed with dtgateway command\n\n\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\ncurl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin\n\n\n\n\n\n\n\nAdd DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.\n\n\nDATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin\n\n\n\n\n\n\n\nUpgrades\n\n\nDataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.\n\n\nAutomatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.  \n\n\nFull uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.\n\n\nCustomizing Installation\n\n\nVarious options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.\n\n\n./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B \npath\n      Use \npath\n as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U \nuser\n      Use \nuser\n user account for installation.  Default: dtadmin\n-G \ngroup\n     Use \ngroup\n group for installation.  Default: dtadmin ( based on value of \nuser\n )\n-H \npath\n      Use \npath\n for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E \nexpr\n      Adds export \nexpr\n to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway and Apex CLI\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s \nfile\n      Use \nfile\n DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e \nfile\n      Use \nfile\n DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.\n\n\n\nSome Hadoop distributions may require changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):\n\n\nsudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.\n\n\n\nIf JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:\n\n\nsudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path\n\n\n\nIn some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.\n\n\nsudo ./datatorrent-rts*.bin -U myuser\n\n\n\nInstallation Wizard\n\n\nAfter the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.\n\n\nhttp://\ninstallation_host\n:9090/\n\n\n\nThe Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.\n\n\nDataTorrent installation can be verified by running included demo applications.  See \nLaunching Demo Applications\n for details.\n\n\nNote\n: The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.\n\n\nDisaster Recovery Setup\n\n\nIf you have multiple Hadoop clusters for handling the disasters, then you can install multiple instances of the Gateway on the same machine and service account with each gateway pointing to different cluster.\n\n\nInstalling Gateway with dtuser(or any other) User\n\n\n\n\nCreate dtadmin user. This user must have uid \n 1000. (adduser -u 1100 dtadmin).\n\n\nSwitch to dtadmin user.\n\n\nCopy the hadoop \nconf\n dir from both the hadoop clusters to the home of dtadmin user. Name them such that you can differentiate between them (For example, $HOME/hadoop_conf1 and $HOME/hadoop_conf2).\n\n\nDownload the latest version of the DT RTS installable binary file (datatorrent-rts-\n.bin).\n\n\nExport JAVA_HOME, HADOOP_HOME and add in PATH. For example:\n\n\nexport HADOOP_HOME=/opt/hadoop-2.6.5 \n\n\nexport JAVA_HOME=/usr \n\n\nexport PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH\n\n\nInstall the first gateway instance using the following command:\n\n\n\n\n./datatorrent-rts-\nversion\n.bin -E DT_LOG_DIR=$HOME/datatorrent1 -E DT_RUN_DIR=$HOME/datatorrent1 -E HADOOP_CONF_DIR=$HOME/hadoop_conf1 -B $HOME/datatorrent1 -g 9090\n\n\n\n\n\n\nInstall the second gateway instance using the following command:\n\n\n\n\n./datatorrent-rts-\nversion\n.bin -E DT_LOG_DIR=$HOME/datatorrent2 -E DT_RUN_DIR=$HOME/datatorrent2 -E HADOOP_CONF_DIR=$HOME/hadoop_conf2 -B $HOME/datatorrent2 -g 9091\n\n\n\n\nInstalling Gateway with the Root User:\n\n\n\n\nCopy the hadoop \nconf\n dir from both the hadoop clusters to the home of dtadmin user. Name them such that you can differentiate between them (For example, /opt/hadoop_conf1 and /opt/hadoop_conf2).\n\n\nDownload the latest version of the DT RTS installable binary file (datatorrent-rts-\n.bin).\n\n\nExport JAVA_HOME, HADOOP_HOME and add in PATH. For example:\n\n\nexport HADOOP_HOME=/opt/hadoop-2.6.5 \n\n\nexport JAVA_HOME=/usr \n\n\nexport PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH\n\n\nInstall the first gateway instance using the following command:\n\n\n\n\n./datatorrent-rts-\nversion\n.bin -E DT_LOG_DIR=/opt/datatorrent1 -E DT_RUN_DIR=/opt/datatorrent1 -E HADOOP_CONF_DIR=/opt/hadoop_conf1 -B /opt/datatorrent1 -g 9090\n\n\n\n\n\n\nInstall the second gateway instance using the following command:\n\n\n\n\n./datatorrent-rts-\nversion\n.bin -E DT_LOG_DIR=/opt/datatorrent2 -E DT_RUN_DIR=/opt/datatorrent2 -E HADOOP_CONF_DIR=/opt/hadoop_conf2 -B /opt/datatorrent2 -g 9091\n\n\n\n\nNote :\n If you are installing the gateway as a root user, the gateway gets installed globally (or as a service). Hence if you install the second instance after you installing the first,  the command \nservice dtgateway stop\n is used, instead of local script, to stop any previously installed gateways. \nSince gateway services are global, the installer cannot distinguish if the previous gateway service is installed at a different location, even if you have provided a new set of directories in the second install, and thus stops the previous service.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#datatorrent-rts-installation-guide", 
            "text": "This guide covers installation of the DataTorrent RTS platform.", 
            "title": "DataTorrent RTS Installation Guide"
        }, 
        {
            "location": "/installation/#planning", 
            "text": "Installation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:   Accessible by users who will launch and manage applications  Accessible by all YARN nodes running Apache Apex applications   Note : With  dtGateway security  configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.", 
            "title": "Planning"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "Linux operating system (tested on CentOS 6.x and Ubuntu 12.04)  Java 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)  Hadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH  Minimum of 8G RAM available on the Hadoop cluster  Google Chrome or Safari to access the DataTorrent Console UI  Permissions to create HDFS directory for DataTorrent user", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Complete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following   DataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]  Binaries are available in /opt/datatorrent/current/bin and links in /usr/bin  Configuration files located in /opt/datatorrent/current/conf  Log files located in /var/log/datatorrent  DataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command   Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  a.  Installing from self-extracting archive (*.bin)      curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin  b.  Installing from RedHat Package Manager archive (*.rpm)    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#limited-local-installation", 
            "text": "A limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.   DataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]  Binaries are available under $HOME/datatorrent/current/bin  Configuration files located under $HOME/datatorrent/conf  Log files located under $HOME/.dt/logs   DataTorrent Gateway running as current user, and managed with dtgateway command    Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin    Add DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.  DATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin", 
            "title": "Limited Local Installation"
        }, 
        {
            "location": "/installation/#upgrades", 
            "text": "DataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.  Automatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.    Full uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.", 
            "title": "Upgrades"
        }, 
        {
            "location": "/installation/#customizing-installation", 
            "text": "Various options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.  ./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B  path       Use  path  as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U  user       Use  user  user account for installation.  Default: dtadmin\n-G  group      Use  group  group for installation.  Default: dtadmin ( based on value of  user  )\n-H  path       Use  path  for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E  expr       Adds export  expr  to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway and Apex CLI\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s  file       Use  file  DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e  file       Use  file  DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.  Some Hadoop distributions may require changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):  sudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.  If JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:  sudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path  In some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.  sudo ./datatorrent-rts*.bin -U myuser", 
            "title": "Customizing Installation"
        }, 
        {
            "location": "/installation/#installation-wizard", 
            "text": "After the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.  http:// installation_host :9090/  The Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.  DataTorrent installation can be verified by running included demo applications.  See  Launching Demo Applications  for details.  Note : The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/installation/#disaster-recovery-setup", 
            "text": "If you have multiple Hadoop clusters for handling the disasters, then you can install multiple instances of the Gateway on the same machine and service account with each gateway pointing to different cluster.", 
            "title": "Disaster Recovery Setup"
        }, 
        {
            "location": "/installation/#installing-gateway-with-dtuseror-any-other-user", 
            "text": "Create dtadmin user. This user must have uid   1000. (adduser -u 1100 dtadmin).  Switch to dtadmin user.  Copy the hadoop  conf  dir from both the hadoop clusters to the home of dtadmin user. Name them such that you can differentiate between them (For example, $HOME/hadoop_conf1 and $HOME/hadoop_conf2).  Download the latest version of the DT RTS installable binary file (datatorrent-rts- .bin).  Export JAVA_HOME, HADOOP_HOME and add in PATH. For example:  export HADOOP_HOME=/opt/hadoop-2.6.5   export JAVA_HOME=/usr   export PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH  Install the first gateway instance using the following command:   ./datatorrent-rts- version .bin -E DT_LOG_DIR=$HOME/datatorrent1 -E DT_RUN_DIR=$HOME/datatorrent1 -E HADOOP_CONF_DIR=$HOME/hadoop_conf1 -B $HOME/datatorrent1 -g 9090   Install the second gateway instance using the following command:   ./datatorrent-rts- version .bin -E DT_LOG_DIR=$HOME/datatorrent2 -E DT_RUN_DIR=$HOME/datatorrent2 -E HADOOP_CONF_DIR=$HOME/hadoop_conf2 -B $HOME/datatorrent2 -g 9091", 
            "title": "Installing Gateway with dtuser(or any other) User"
        }, 
        {
            "location": "/installation/#installing-gateway-with-the-root-user", 
            "text": "Copy the hadoop  conf  dir from both the hadoop clusters to the home of dtadmin user. Name them such that you can differentiate between them (For example, /opt/hadoop_conf1 and /opt/hadoop_conf2).  Download the latest version of the DT RTS installable binary file (datatorrent-rts- .bin).  Export JAVA_HOME, HADOOP_HOME and add in PATH. For example:  export HADOOP_HOME=/opt/hadoop-2.6.5   export JAVA_HOME=/usr   export PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH  Install the first gateway instance using the following command:   ./datatorrent-rts- version .bin -E DT_LOG_DIR=/opt/datatorrent1 -E DT_RUN_DIR=/opt/datatorrent1 -E HADOOP_CONF_DIR=/opt/hadoop_conf1 -B /opt/datatorrent1 -g 9090   Install the second gateway instance using the following command:   ./datatorrent-rts- version .bin -E DT_LOG_DIR=/opt/datatorrent2 -E DT_RUN_DIR=/opt/datatorrent2 -E HADOOP_CONF_DIR=/opt/hadoop_conf2 -B /opt/datatorrent2 -g 9091  Note :  If you are installing the gateway as a root user, the gateway gets installed globally (or as a service). Hence if you install the second instance after you installing the first,  the command  service dtgateway stop  is used, instead of local script, to stop any previously installed gateways. \nSince gateway services are global, the installer cannot distinguish if the previous gateway service is installed at a different location, even if you have provided a new set of directories in the second install, and thus stops the previous service.", 
            "title": "Installing Gateway with the Root User:"
        }, 
        {
            "location": "/configuration/", 
            "text": "DataTorrent RTS Configuration\n\n\nThis document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit \nhadoop.apache.org\n.\n\n\nInstallation\n\n\nIf you have not installed DataTorrent RTS already, follow the installation instructions in the \ninstallation guide\n.\n\n\n\n\nConfiguration Files\n\n\nSystem configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file \ncustom-env.sh\n can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.\n\n\nDepending on the installation type, these may be located under \n/opt/datatorrent/current/conf\n or \n~/datatorrent/current/conf\n.  See \ninstallation guide\n for details.\n\n\n(install dir)/conf/custom-env.sh\n\n\nThis file can be used to configure behavior of DT Gateway service,\nas well as \napex\n command line utility. \u00a0After adding custom properties\nto this file, dtgateway and Apex CLI utilities need to be restarted for\nchanges to take effect.\n\n\nExample custom-env.sh configuration:\n\n\n# Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m\n\n\n\n\nEnvironment variables available for configuration\n\n\n\n\nDT_GATEWAY_HEAP_MEM\n \n Maximum heap size allocated to DT Gateway service.  Default is 1024m.\n\n\nDT_GATEWAY_DEBUG\n \n Set to 1 to enable additional debug information in the dtgateway.log\n\n\nDT_CLASSPATH\n \n Classpath used to load additional jars or properties for Apex CLI and dtgateway\n\n\nDT_LOG_DIR\n \n Directory for log files\n\n\nDT_RUN_DIR\n \n Directory for process id and other temporary files\ncreated at run time\n\n\n\n\n(user home)/.dt/dt-site.xml\n\n\nThis file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.\n\n\nExample of a single property configuration in dt-site.xml:\n\n\nconfiguration\n\n  \nproperty\n\n      \nname\ndt.operator.MyCustomStore.host\n/name\n\n      \nvalue\n192.168.2.35\n/value\n\n  \n/property\n\n   \u2026\n\n/configuration\n\n\n\n\n\nGateway Configuration Properties\n\n\n\n\ndt.gateway.listenAddress\n - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090\n\n\ndt.gateway.autoPublishInterval\n - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.\n\n\ndt.gateway.sslKeystorePath\n - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the \ndtGateway Security\n document)\n\n\ndt.gateway.sslKeystorePassword\n - The password of the SSL key store (See the \ndtGateway Security\n document)\n\n\ndt.gateway.allowCrossOrigin\n - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.\n\n\ndt.gateway.authentication.(OPTION)\n - Determines the scheme of Hadoop security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.http.authentication\n - Determines the scheme of DT Gateway HTTP security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.staticResourceDirectory\n - The document root directory where the DT Gateway should serve from for the /static HTTP path.\n\n\ndt.dockerMinSupportedVersion\n - Sets the minimum supported docker server version for running docker services. Default is 1.9.1.\n\n\n\n\nApplication Configuration Properties\n\n\nFor a complete list of configurable application properties see the Attributes\u00a0section below.\n\n\n\n\nResources Management and Performance Tuning\n\n\nThe platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via \nREST API\n, \nApex CLI\n, and \ndtManage\n.\n\n\nThe platform is also responsible for\n\n\n\n\nHonoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.\n\n\nHonoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.\n\n\n\n\nSTRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).\n\n\nCPU\n\n\nCPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.\n\n\nNetwork\n\n\nNetwork usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.\n\n\nThe platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.\n\n\nRAM\n\n\nSTRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.\n\n\nSpike Management\n\n\nStreaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.\n\n\nThe platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.\n\n\nPartitioning\n\n\nPartitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in\n\nApplication development\n and \nOperator Development\n.\n\n\nStream Modes\n\n\nThe platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.\n\n\n\n\nTHREAD_LOCAL\n: All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.\n\n\nCONTAINER_LOCAL\n: All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in memory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, persistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.\n\n\nNODE_LOCAL\n: All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.\n\n\nRACK_LOCAL\n: All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.\n\n\nUnspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.\n\n\n\n\n\n\n\n\nMulti-Tenancy and Security\n\n\nThe platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications.\n\nThe default security for the streaming application is Kerberos based.\n\n\nKerberos Authentication\n\n\nKerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.\n\n\nCLI Configuration\n\n\nThe DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. The security configuration for the CLI program is described in the Apache Apex security document available \nhere\n.\n\n\nDT Gateway Configuration\n\n\nDT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. In a Hadoop cluster with Kerberos security enabled additional configuration is needed for this service to communicate with Hadoop. This is described in \nGateway Security\n.\n\n\nConsole Authentication\n\n\nAccess to the UI console can be secured by having users authenticate before they can access the contents of the console. Different authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to \nGateway Security\n for details of how to configure this.\n\n\nRun-Time Management\n\n\nUnlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include\n\n\n\n\nRuntime metrics and stats on various components of the\n    application including aggregated metrics\n\n\nAbility to change the logical plan, physical plan, and\n    execution plan of the application\n\n\nAbility to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)\n\n\nIntegration of STRAM state with Zookeeper (in later versions)\n\n\nDebugger, charting, and other tools triggered at run time\n\n\n\n\nDynamic Functional Modifications\n\n\nPlatform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe \napex\n tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.\n\n\nFrom an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).\n\n\nExamples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream\n\n\nAny change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude\n\n\n\n\nChange in attributes that triggers STRAM to change the number\n    of physical operators.\n\n\nRuntime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals\n\n\nDirect request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate\n\n\n\n\nAny change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include\n\n\n\n\nChanges in attributes that need a new execution plan\n\n\nChanges to stream modes\n\n\nNode recovery from an outage\n\n\n\n\nRuntime Code\n\n\nSTRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.\n\n\nLoad\n\n\nSTRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.\n\n\nUptime\n\n\nNode recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (preemption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.\n\n\n\n\nAttributes\n\n\nAttribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.\n\n\nThere are three kinds of attributes\n\n\n\n\nApplication attributes\n\n\nOperator attributes\n\n\nPort attributes\n\n\n\n\nFor implementation details look at the \njavadocs\n. Some very common attributes are\n\n\n\n\nApplication: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See \nContext.DAGContext\n\n\nOperators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See \nContext.OperatorContext\n\n\nPorts: Queue capacity, auto_record, partition parallel, etc.\n    See \nContext.PortContext\n\n\n\n\nThese attributes are available via the \nContext\n class and can be\naccessed in the \nsetup\n call of an operator.\n\n\nApplication Configuration\n\n\nStarting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the\n\nApplication Packages\n\u00a0for details.\n\n\nAdjusting Logging Levels\n\n\nApplication Logging\n\n\nLogging levels for specific classes or groups of classes can be raised or\nlowered at runtime from \ndtManage\n application view with the\n\nSet Log Levels\n button. \u00a0Explicit class paths or patterns like\n\norg.apache.hadoop.*\n\u00a0or \ncom.datatorrent.*\n\u00a0can be used to adjust logging to\nvalid \nlog4j\n levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.\n\n\nFor permanent changes to logging levels, lines similar to these can be inserted\ninto \ndt-site.xml\n. To specify multiple patterns, use a comma-separated list.\n\n\nproperty\n\n  \nname\ndt.loggers.level\n/name\n\n  \nvalue\ncom.datatorrent.*:DEBUG,org.apache.*:INFO\n/value\n\n\n/property\n\n\n\n\n\nFull DEBUG logging can be enabled by adding these lines:\n\n\nproperty\n\n  \nname\ndt.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\n\nIntegration of Gateway and Apex with 3rd party log aggregation tools\n\n\nIntroduction\n\n\nDuring execution Gateway and Apex generate event log records in order to provide an audit trail that can be used to understand the activity of the system and to diagnose problems. Typically the event log records are stored into a local file system and later can be used for analysis and diagnostic.\nGateway also provides an universal ability to pass and store Gateway and Apex event log records to 3rd party sources and use some external tools to store the log events and do querying and reporting. In order to make it the user should configure logger appender in Gateway configuration files.\n\n\nConfiguration of Logger Appenders\n\n\nGateway and Apex Client processes are running on the machine node where Gateway instance was installed. So the configuration of  logger appenders can be done via the regular log4j properties (datatorrent/releases/3.8.0/conf/dtgateway.log4j.properties).\n\n\nExample of configuration log4j properties for Socket Appender:lizy@datatorrent.com\n\n\nlog4j.rootLogger=${dt.root.logger.level},tcp\n...\nlog4j.appender.tcp=org.apache.log4j.net.SocketAppender\nlog4j.appender.tcp.RemoteHost=logstashnode1\nlog4j.appender.tcp.Port=5400\nlog4j.appender.tcp.ReconnectionDelay=10000\nlog4j.appender.tcp.LocationInfo=true\n\n\n\n\nThe configuration of logger appenders for Apex Application Master and Containers can be done via the regular attribute property \u201capex.attr.LOGGER_APPENDER\u201d that can be defined in the configuration file dt-site.xml (global, local and user) or the static and runtime application properties.\n\n\nSyntax of logger appender attribute value:\n\n\n{comma-separated-appender-names};{comma-separated-appenders-properties}\n\n\n\n\nExample of configuration of the logger appender attribute for Socket  Appender:\n\n\n  \nproperty\n\n    \nname\napex.attr.LOGGER_APPENDER\n/name\n\n    \nvalue\ntcp;log4j.appender.tcp=org.apache.log4j.net.SocketAppender,log4j.appender.tcp.RemoteHost=logstashnode1,log4j.appender.tcp.Port=5400,log4j.appender.tcp.ReconnectionDelay=10000,log4j.appender.tcp.LocationInfo=true\n/value\n\n  \n/property\n\n\n\n\n\nIntegration with ElasticSearch and Splunk\n\n\nThe user can use different ways to store event log records to an external data source. But we would recommend to use the following scenario.\nGateway and Apex can be configured to use Socket Appender to send logger events to Logstash. And Logstash can deploy event log records to any output data sources. For instance the following picture shows the integration workflow with ElasticSearch and Splunk.\n\n\n\n\nExample of  Logstash configuration:\n\n\ninput {  getting of looger events from Socket Appender\n  log4j {\n    mode =\n \nserver\n\n    port =\n 5400\n    type =\n \nlog4j\n\n  }\n}\n\nFilter{  transformation of looger events to event log records\n  mutate {\n    remove_field =\n [ \n@version\n,\npath\n,\ntags\n,\nhost\n,\ntype\n,\nlogger_name\n ]\n    rename =\n { \napex.user\n =\n \nuser\n }\n    rename =\n { \napex.application\n =\n \napplication\n }\n    rename =\n { \napex.containerId\n =\n \ncontainerId\n }\n    rename =\n { \napex.applicationId\n =\n \napplicationId\n }\n    rename =\n { \napex.node\n =\n \nnode\n }\n    rename =\n { \napex.service\n =\n \nservice\n }\n    rename =\n { \ndt.node\n =\n \nnode\n }\n    rename =\n { \ndt.service\n =\n \nservice\n }\n    rename =\n { \npriority\n =\n \nlevel\n }\n    rename =\n { \ntimestamp\n =\n \nrecordTime\n }\n   }\n   date {\n    match =\n [ \nrecordTime\n, \nUNIX\n ]\n    target =\n \nrecordTime\n\n  }\n}\n\noutput {\n  elasticsearch {  putting of event log records to ElasticSearch cluster\n  hosts =\n [\nesnode1:9200\n,\nesnode2:9200\n,\nesnode3:9200\n]\n    index =\n \napexlogs-%{+YYYY-MM-dd}\n\n    manage_template =\n false\n  }\n\n  tcp {  putting of event log records to Splunk\n   host =\n \nsplunknode\n\n   mode =\n \nclient\n\n   port =\n 15000\n   codec =\n \njson_lines\n\n }\n}\n\n\n\n\nElasticSearch users can use Kibana reporting tool for analysis and diagnostic. Splunk users can use Splunkweb.\n\n\n\n\nLinks to 3rd party tools:\n\n\n\n\nLogstash: \nwww.elastic.co/products/logstash\n \n\n\nElasticSearch: \nwww.elastic.co/products/elasticsearch\n\n\nKibana: \nwww.elastic.co/products/kibana\n\n\nSplunk: \nwww.splunk.com\n\n\n\n\nCustom log4j Properties for Application Packages\n\n\nThere are two ways of setting custom \nlog4j\n properties in an Apex application\npackage\n\n\n\n\n\n\nAt the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:\n\n\nproperty\n\n  \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n  \nvalue\n-Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nAt an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:\n\n\nproperty\n\n  \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n  \nvalue\n -Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nMake sure that the file \ncustom_log4j.properties\n is part of your application\njar and is located under \nsrc/main/resources\n.  Some examples of custom log4j\nproperties files follow.\n\n\n\n\n\n\nWriting to a file\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n\n\n\n\n\n\nWriting to Console\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\n\n\n\n\n\n\n\ndtGateway Logging\n\n\nDT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).\n\n\nexport DT_GATEWAY_DEBUG=1\n\n\n\nHadoop Tuning\n\n\nYARN vmem-pmem ratio tuning\n\n\nAfter performing a new installation, sometimes the following\nmessage is displayed while launching an application:\n\n\nApplication application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001\n\n\n\n\nTo fix this \nyarn.nodemanager.vmem-pmem-ratio\n in \nyarn-site.xml\n should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:\n\n\nproperty\n\n   \ndescription\nRatio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n   \n/description\n\n   \nname\nyarn.nodemanager.vmem-pmem-ratio\n/name\n\n   \nvalue\n10\n/value\n\n \n/property", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#datatorrent-rts-configuration", 
            "text": "This document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit  hadoop.apache.org .", 
            "title": "DataTorrent RTS Configuration"
        }, 
        {
            "location": "/configuration/#installation", 
            "text": "If you have not installed DataTorrent RTS already, follow the installation instructions in the  installation guide .", 
            "title": "Installation"
        }, 
        {
            "location": "/configuration/#configuration-files", 
            "text": "System configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file  custom-env.sh  can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.  Depending on the installation type, these may be located under  /opt/datatorrent/current/conf  or  ~/datatorrent/current/conf .  See  installation guide  for details.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/configuration/#install-dirconfcustom-envsh", 
            "text": "This file can be used to configure behavior of DT Gateway service,\nas well as  apex  command line utility. \u00a0After adding custom properties\nto this file, dtgateway and Apex CLI utilities need to be restarted for\nchanges to take effect.  Example custom-env.sh configuration:  # Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m  Environment variables available for configuration   DT_GATEWAY_HEAP_MEM    Maximum heap size allocated to DT Gateway service.  Default is 1024m.  DT_GATEWAY_DEBUG    Set to 1 to enable additional debug information in the dtgateway.log  DT_CLASSPATH    Classpath used to load additional jars or properties for Apex CLI and dtgateway  DT_LOG_DIR    Directory for log files  DT_RUN_DIR    Directory for process id and other temporary files\ncreated at run time", 
            "title": "(install dir)/conf/custom-env.sh"
        }, 
        {
            "location": "/configuration/#user-homedtdt-sitexml", 
            "text": "This file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.  Example of a single property configuration in dt-site.xml:  configuration \n   property \n       name dt.operator.MyCustomStore.host /name \n       value 192.168.2.35 /value \n   /property \n   \u2026 /configuration", 
            "title": "(user home)/.dt/dt-site.xml"
        }, 
        {
            "location": "/configuration/#gateway-configuration-properties", 
            "text": "dt.gateway.listenAddress  - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090  dt.gateway.autoPublishInterval  - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.  dt.gateway.sslKeystorePath  - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the  dtGateway Security  document)  dt.gateway.sslKeystorePassword  - The password of the SSL key store (See the  dtGateway Security  document)  dt.gateway.allowCrossOrigin  - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.  dt.gateway.authentication.(OPTION)  - Determines the scheme of Hadoop security authentication (See the  dtGateway Security  document).  dt.gateway.http.authentication  - Determines the scheme of DT Gateway HTTP security authentication (See the  dtGateway Security  document).  dt.gateway.staticResourceDirectory  - The document root directory where the DT Gateway should serve from for the /static HTTP path.  dt.dockerMinSupportedVersion  - Sets the minimum supported docker server version for running docker services. Default is 1.9.1.", 
            "title": "Gateway Configuration Properties"
        }, 
        {
            "location": "/configuration/#application-configuration-properties", 
            "text": "For a complete list of configurable application properties see the Attributes\u00a0section below.", 
            "title": "Application Configuration Properties"
        }, 
        {
            "location": "/configuration/#resources-management-and-performance-tuning", 
            "text": "The platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via  REST API ,  Apex CLI , and  dtManage .  The platform is also responsible for   Honoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.  Honoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.   STRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).", 
            "title": "Resources Management and Performance Tuning"
        }, 
        {
            "location": "/configuration/#cpu", 
            "text": "CPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.", 
            "title": "CPU"
        }, 
        {
            "location": "/configuration/#network", 
            "text": "Network usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.  The platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.", 
            "title": "Network"
        }, 
        {
            "location": "/configuration/#ram", 
            "text": "STRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.", 
            "title": "RAM"
        }, 
        {
            "location": "/configuration/#spike-management", 
            "text": "Streaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.  The platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.", 
            "title": "Spike Management"
        }, 
        {
            "location": "/configuration/#partitioning", 
            "text": "Partitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in Application development  and  Operator Development .", 
            "title": "Partitioning"
        }, 
        {
            "location": "/configuration/#stream-modes", 
            "text": "The platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.   THREAD_LOCAL : All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.  CONTAINER_LOCAL : All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in memory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, persistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.  NODE_LOCAL : All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.  RACK_LOCAL : All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.  Unspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.", 
            "title": "Stream Modes"
        }, 
        {
            "location": "/configuration/#multi-tenancy-and-security", 
            "text": "The platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications. \nThe default security for the streaming application is Kerberos based.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/configuration/#kerberos-authentication", 
            "text": "Kerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/configuration/#cli-configuration", 
            "text": "The DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. The security configuration for the CLI program is described in the Apache Apex security document available  here .", 
            "title": "CLI Configuration"
        }, 
        {
            "location": "/configuration/#dt-gateway-configuration", 
            "text": "DT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. In a Hadoop cluster with Kerberos security enabled additional configuration is needed for this service to communicate with Hadoop. This is described in  Gateway Security .", 
            "title": "DT Gateway Configuration"
        }, 
        {
            "location": "/configuration/#console-authentication", 
            "text": "Access to the UI console can be secured by having users authenticate before they can access the contents of the console. Different authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to  Gateway Security  for details of how to configure this.", 
            "title": "Console Authentication"
        }, 
        {
            "location": "/configuration/#run-time-management", 
            "text": "Unlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include   Runtime metrics and stats on various components of the\n    application including aggregated metrics  Ability to change the logical plan, physical plan, and\n    execution plan of the application  Ability to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)  Integration of STRAM state with Zookeeper (in later versions)  Debugger, charting, and other tools triggered at run time", 
            "title": "Run-Time Management"
        }, 
        {
            "location": "/configuration/#dynamic-functional-modifications", 
            "text": "Platform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe  apex  tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.  From an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).  Examples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream  Any change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude   Change in attributes that triggers STRAM to change the number\n    of physical operators.  Runtime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals  Direct request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate   Any change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include   Changes in attributes that need a new execution plan  Changes to stream modes  Node recovery from an outage", 
            "title": "Dynamic Functional Modifications"
        }, 
        {
            "location": "/configuration/#runtime-code", 
            "text": "STRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.", 
            "title": "Runtime Code"
        }, 
        {
            "location": "/configuration/#load", 
            "text": "STRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.", 
            "title": "Load"
        }, 
        {
            "location": "/configuration/#uptime", 
            "text": "Node recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (preemption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.", 
            "title": "Uptime"
        }, 
        {
            "location": "/configuration/#attributes", 
            "text": "Attribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.  There are three kinds of attributes   Application attributes  Operator attributes  Port attributes   For implementation details look at the  javadocs . Some very common attributes are   Application: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See  Context.DAGContext  Operators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See  Context.OperatorContext  Ports: Queue capacity, auto_record, partition parallel, etc.\n    See  Context.PortContext   These attributes are available via the  Context  class and can be\naccessed in the  setup  call of an operator.", 
            "title": "Attributes"
        }, 
        {
            "location": "/configuration/#application-configuration", 
            "text": "Starting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the Application Packages \u00a0for details.", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/configuration/#adjusting-logging-levels", 
            "text": "", 
            "title": "Adjusting Logging Levels"
        }, 
        {
            "location": "/configuration/#application-logging", 
            "text": "Logging levels for specific classes or groups of classes can be raised or\nlowered at runtime from  dtManage  application view with the Set Log Levels  button. \u00a0Explicit class paths or patterns like org.apache.hadoop.* \u00a0or  com.datatorrent.* \u00a0can be used to adjust logging to\nvalid  log4j  levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.  For permanent changes to logging levels, lines similar to these can be inserted\ninto  dt-site.xml . To specify multiple patterns, use a comma-separated list.  property \n   name dt.loggers.level /name \n   value com.datatorrent.*:DEBUG,org.apache.*:INFO /value  /property   Full DEBUG logging can be enabled by adding these lines:  property \n   name dt.attr.DEBUG /name \n   value true /value  /property", 
            "title": "Application Logging"
        }, 
        {
            "location": "/configuration/#integration-of-gateway-and-apex-with-3rd-party-log-aggregation-tools", 
            "text": "", 
            "title": "Integration of Gateway and Apex with 3rd party log aggregation tools"
        }, 
        {
            "location": "/configuration/#introduction", 
            "text": "During execution Gateway and Apex generate event log records in order to provide an audit trail that can be used to understand the activity of the system and to diagnose problems. Typically the event log records are stored into a local file system and later can be used for analysis and diagnostic.\nGateway also provides an universal ability to pass and store Gateway and Apex event log records to 3rd party sources and use some external tools to store the log events and do querying and reporting. In order to make it the user should configure logger appender in Gateway configuration files.", 
            "title": "Introduction"
        }, 
        {
            "location": "/configuration/#configuration-of-logger-appenders", 
            "text": "Gateway and Apex Client processes are running on the machine node where Gateway instance was installed. So the configuration of  logger appenders can be done via the regular log4j properties (datatorrent/releases/3.8.0/conf/dtgateway.log4j.properties).  Example of configuration log4j properties for Socket Appender:lizy@datatorrent.com  log4j.rootLogger=${dt.root.logger.level},tcp\n...\nlog4j.appender.tcp=org.apache.log4j.net.SocketAppender\nlog4j.appender.tcp.RemoteHost=logstashnode1\nlog4j.appender.tcp.Port=5400\nlog4j.appender.tcp.ReconnectionDelay=10000\nlog4j.appender.tcp.LocationInfo=true  The configuration of logger appenders for Apex Application Master and Containers can be done via the regular attribute property \u201capex.attr.LOGGER_APPENDER\u201d that can be defined in the configuration file dt-site.xml (global, local and user) or the static and runtime application properties.  Syntax of logger appender attribute value:  {comma-separated-appender-names};{comma-separated-appenders-properties}  Example of configuration of the logger appender attribute for Socket  Appender:     property \n     name apex.attr.LOGGER_APPENDER /name \n     value tcp;log4j.appender.tcp=org.apache.log4j.net.SocketAppender,log4j.appender.tcp.RemoteHost=logstashnode1,log4j.appender.tcp.Port=5400,log4j.appender.tcp.ReconnectionDelay=10000,log4j.appender.tcp.LocationInfo=true /value \n   /property", 
            "title": "Configuration of Logger Appenders"
        }, 
        {
            "location": "/configuration/#integration-with-elasticsearch-and-splunk", 
            "text": "The user can use different ways to store event log records to an external data source. But we would recommend to use the following scenario.\nGateway and Apex can be configured to use Socket Appender to send logger events to Logstash. And Logstash can deploy event log records to any output data sources. For instance the following picture shows the integration workflow with ElasticSearch and Splunk.   Example of  Logstash configuration:  input {  getting of looger events from Socket Appender\n  log4j {\n    mode =   server \n    port =  5400\n    type =   log4j \n  }\n}\n\nFilter{  transformation of looger events to event log records\n  mutate {\n    remove_field =  [  @version , path , tags , host , type , logger_name  ]\n    rename =  {  apex.user  =   user  }\n    rename =  {  apex.application  =   application  }\n    rename =  {  apex.containerId  =   containerId  }\n    rename =  {  apex.applicationId  =   applicationId  }\n    rename =  {  apex.node  =   node  }\n    rename =  {  apex.service  =   service  }\n    rename =  {  dt.node  =   node  }\n    rename =  {  dt.service  =   service  }\n    rename =  {  priority  =   level  }\n    rename =  {  timestamp  =   recordTime  }\n   }\n   date {\n    match =  [  recordTime ,  UNIX  ]\n    target =   recordTime \n  }\n}\n\noutput {\n  elasticsearch {  putting of event log records to ElasticSearch cluster\n  hosts =  [ esnode1:9200 , esnode2:9200 , esnode3:9200 ]\n    index =   apexlogs-%{+YYYY-MM-dd} \n    manage_template =  false\n  }\n\n  tcp {  putting of event log records to Splunk\n   host =   splunknode \n   mode =   client \n   port =  15000\n   codec =   json_lines \n }\n}  ElasticSearch users can use Kibana reporting tool for analysis and diagnostic. Splunk users can use Splunkweb.   Links to 3rd party tools:   Logstash:  www.elastic.co/products/logstash    ElasticSearch:  www.elastic.co/products/elasticsearch  Kibana:  www.elastic.co/products/kibana  Splunk:  www.splunk.com", 
            "title": "Integration with ElasticSearch and Splunk"
        }, 
        {
            "location": "/configuration/#custom-log4j-properties-for-application-packages", 
            "text": "There are two ways of setting custom  log4j  properties in an Apex application\npackage    At the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:  property \n   name dt.attr.CONTAINER_JVM_OPTIONS /name \n   value -Dlog4j.configuration=custom_log4j.properties /value  /property     At an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:  property \n   name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n   value  -Dlog4j.configuration=custom_log4j.properties /value  /property     Make sure that the file  custom_log4j.properties  is part of your application\njar and is located under  src/main/resources .  Some examples of custom log4j\nproperties files follow.    Writing to a file  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}    Writing to Console  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout", 
            "title": "Custom log4j Properties for Application Packages"
        }, 
        {
            "location": "/configuration/#dtgateway-logging", 
            "text": "DT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).  export DT_GATEWAY_DEBUG=1", 
            "title": "dtGateway Logging"
        }, 
        {
            "location": "/configuration/#hadoop-tuning", 
            "text": "", 
            "title": "Hadoop Tuning"
        }, 
        {
            "location": "/configuration/#yarn-vmem-pmem-ratio-tuning", 
            "text": "After performing a new installation, sometimes the following\nmessage is displayed while launching an application:  Application application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001  To fix this  yarn.nodemanager.vmem-pmem-ratio  in  yarn-site.xml  should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:  property \n    description Ratio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n    /description \n    name yarn.nodemanager.vmem-pmem-ratio /name \n    value 10 /value \n  /property", 
            "title": "YARN vmem-pmem ratio tuning"
        }, 
        {
            "location": "/dtgateway_security/", 
            "text": "DataTorrent Gateway Security\n\n\nDataTorrent Gateway is a service that provides the backend functionality\nfor the DataTorrent UI Console and processes the web service requests from it. The service provides real-time information about running applications, allows changes to applications, launches new applications among various other operations. Refer to \ndtGateway\n for details on the Gateway service and \ndtManage\n\u00a0for the UI Console.\n\n\nBroadly security in Gateway can be classified into two categories, frontend security and backend security. Frontend security deals with access to Gateway service which mainly involves securing the web service calls. This includes aspects such as user authentication and authorization. The backend security deals with security aspects when Gateway is communicating with a secure Hadoop infrastructure.\n\n\nAfter installation of DataTorrent RTS both these aspects can be configured manually as described in the following sections although work is being done to enable this configuration in the UI Console during installation itself and post installation.\n\n\nKerberos Secure Mode\n\n\nKerberos is the de-facto authentication mechanism supported in Hadoop. When secure mode is enabled in Hadoop, requests from clients to Hadoop are authenticated using Kerberos. In this mode Gateway service needs Kerberos credentials to communicate with Hadoop. The credentials should match the user that the DT Gateway service is running under.\n\n\nIn a multi-user installation DT Gateway is typically running as\nuser \ndtadmin\n and the Kerberos credentials specified should be for this\nuser. They are specified in the \ndt-site.xml\n configuration file located in the config folder under the installation which is typically \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install). For a single user installation where gateway is running as the user, the Kerberos credentials will be the user\u2019s credentials.\n\n\nThe snippet below shows how the credentials can be specified in the\nconfiguration file.\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.principal\n/name\n\n        \nvalue\nkerberos-principal-of-gateway-user\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.keytab\n/name\n\n        \nvalue\nabsolute-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\nLong running applications\n\n\nIn secure mode, long running applications have additional requirements. Refer to the Token Refresh section in the Apache Apex security \ndocument\n.\n\n\nAuthentication\n\n\nDataTorrent Gateway has support for authentication and when it is configured users have to authenticate before they can access the UI Console. Various authentication mechanisms are supported and this gives enterprises the flexibility to extend their existing authentication mechanism already in use within the enterprise to Gateway. It also supports roles, mapping of groups or roles from the external authentication mechanism to roles and supports role based authorization.\n\n\nThe different authentication mechanisms supported by Gateway are\n\n\n\n\nPassword Authentication\n\n\nLDAP Authentication\n\n\nKerberos Authentication\n\n\nJAAS Authentication\n for Active Directory, PAM, etc\n\n\n\n\nJAAS is a extensible authentication framework that supports different types of authentication mechanisms by plugging in an appropriate module.\n\n\nPassword Authentication\n\n\nPassword security is simple to set up and is ideal for a small to medium set of users. It comes with role-based access control, so users can be assigned roles, and roles can be assigned granular permissions (see \nUser Management\n). This is the only authentication mechanism available that does not depend on any external systems. The users will be managed locally by the Gateway. When enabled, all users will be presented with the login prompt before being able to use the DT Console.\n\n\nTo set up password security, on the \nSecurity Configuration\n page select \nPassword\n from the \nAuthentication Type\n dropdown, and save. Allow the Gateway to restart.\n\n\n\n\nWhen the Gateway has restarted, you should be prompted for username and password. Log in as the default admin user \ndtadmin\n with password \ndtadmin\n.\n\n\n\n\nOnce authenticated, active username and an option to log out is presented in the top right corner of the DT Console screen.\n\n\n\n\nAdditional users and roles can be created and managed on the \nUser Management\n page.\n\n\nNote\n: Don't forget to change your \ndtadmin\n user's password!\n\n\nPassword Authentication via dt-site.xml\n\n\nPassword authentication can alternatively be configured outside the Console by performing following two steps:\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n    \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\npassword\n/value\n\n    \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway. If running Gateway in local mode use \ndtgateway restart\n instead.\n\n\nsudo service dtgateway restart\n\n\n\n\n\n\n\nLDAP Authentication\n\n\nLDAP is a directory based authentication mechanism used in many enterprises. If your organization uses LDAP for authentication, the LDAP security option is ideal for giving your existing users access to RTS, with role-based access control and group mapping features.\n\n\nThere are four variations for configuring LDAP authentication:\n\n\n\n\nIdentity\n\n\nProvide the parent DN of your users and \nspecify the RDN attribute of users\n.\n\n\nUsers will authenticate using their RDN attribute value as their username.\n\n\n\n\n\n\nAnonymous \n User Search Filter\n\n\nProvide the parent DN of your users and \nspecify a search filter to identify users\n.\n\n\nUsers will authenticate using an appropriate username that matches the parameters defined in the search filter.\n\n\n\n\n\n\nIdentity and Anonymous Search\n\n\nProvide the parent DN of your users, \nspecify the RDN attribute of users, and a search filter\n.\n\n\nUsers will authenticate using their RDN attribute value as their username, as well as the parameters defined in the search filter.\n\n\n\n\n\n\nNon-Anonymous Search with Group Support\n\n\nProvide basic DN info for users, DN and password of a user able to perform a non-anonymous search, and optional group support info.\n\n\nWith \ngroup support disabled\n, users need to be added in User Management before logging in, unless a default role is set. Users authenticate with their User Id Attribute value as their username.\n\n\nWith \ngroup support enabled\n, users do \nnot\n have to be added in User Management before logging in. Users authenticate with their User Id Attribute value as their username. They will be assigned roles mapped to their LDAP group. For example, user Peter is part of GroupA (admin) and GroupB (developer, operator); Peter will be assigned roles admin, developer, and operator upon login.\n\n\n\n\n\n\n\n\nWhen group support is not configured, users must be assigned a role before they are able to log in, unless a default role is set. This means users can be restricted from logging in (blacklisted) by removing all of their roles. Default roles will not be reapplied after a user's initial login, and therefore will not interfere with restricting users.\n\n\nNote\n: If migrating from \nPassword\n mode, the existing users will be carried over as \"local users\" and can still login as if in \nPassword\n mode. It is recommended to keep only the \ndtadmin\n user and delete the rest. This is because local users cannot be added or deleted once \nLDAP\n mode is activated.\n\n\n\n\nAfter setting up LDAP in the Security Configuration page, the \nLDAP Users\n section will appear in the User Management page. If you have group support enabled, the \nLDAP Groups\n section will also appear. Existing users (carried over from \nPassword\n mode), will be placed in the \nLocal Users\n sections. Local users cannot be added or deleted in LDAP mode, but their roles and passwords can be modified.\n\n\n\n\nTo configure LDAP via dt-site.xml, check out the \nJAAS Authentication - LDAP\n section below.\n\n\nActive Directory Authentication\n\n\nActive Directory (AD) is a directory based authentication mechanism for users in Microsoft Windows domains. Active Directory supports role-based access control and group mapping features.\n\n\nTo configure Active Directory, follow the steps to configure \nLDAP\n with the \nNon-Anonymous Search with Group Support\n variation. The only difference is the additional User Object Class field.\n\n\n\n\nUser Management for Active Directory behaves the same as LDAP with the \nNon-Anonymous Search with Group Support\n variation.\n\n\nTo configure Active Directory via dt-site.xml, check out the \nJAAS Authentication - Active Directory\n section below.\n\n\nKerberos Authentication\n\n\nKerberos authentication can optionally be enabled for Hadoop web access.\nWhen configured, all web browser access to Hadoop management consoles are Kerberos authenticated. Web services are also Kerberos authenticated. This authentication is performed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.\n\n\nKerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by logging in to kerberos system in a terminal emulator using kinit. Then, user launches a browser to access the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.\n\n\nWhen this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the \nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nAdditional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the \nConfiguration\n. This\nauthentication can be set up using the following steps.\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install)\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\nkerberos\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.principal\n/name\n\n    \nvalue\n{kerberos-principal-of-web-service}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.keytab\n/name\n\n    \nvalue\n{absolute-path-to-keytab-file}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.token.validity\n/name\n\n    \nvalue\n{authentication-token-validity-in-seconds}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.cookie.domain\n/name\n\n    \nvalue\n{http-cookie-domain-for-authentication-token}\n/value\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.cookie.path\n/name\n\n    \nvalue\n{http-cookie-path}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.signature.secret\n/name\n\n    \nvalue\n{absolute-path-of-secret-file-for-signing-authentication-tokens} \n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nNote that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property\n\ndt.gateway.http.authentication.type\n should be replaced with the\nappropriate values for your setup.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nJAAS Authentication\n\n\nJAAS or Java Authentication and Authorization Service is a pluggable\nand extensible mechanism for authentication. It is an authentication framework\nwhere the actual authentication is performed by a JAAS login module plugin which can be configured using a configuration file. \n\n\nThe general configuration steps for enabling any JAAS based authentication mechanism is described below. Subsequent sections will cover the specific configuration details for LDAP, Active Directory and PAM. However JAAS is not limited to just these three mechanisms. Any other JAAS compatible mechanism can be used by specifying the appropriate module in the configuration. Also if there isn't a JAAS module available for an authentication mechanism a new one can be developed using the JAAS API and used here.\n\n\nBelow are the general steps for configuring a JAAS authentication mechanism\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n (or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n    ...\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.type\n/name\n\n      \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n      \nvalue\nname-of-jaas-module\n/value\n\n  \n/property\n\n    ...\n\n/configuration\n\n\n\n\nThe \ndt.gateway.http.authentication.jaas.name\n property specifies the login module to use with JAAS and the next step explains the process for configuring it.\n\n\n\n\n\n\nIf the login module requires a custom callback handler it needs to be specified. What a callback handler is and how it can be specified is described in the next section \nCallback Handlers\n.\n\n\n\n\n\n\nThe name of the login module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be \n/home/dtadmin/.java.login.config\n, if running as a\n    regular user it would be \n~/.java.login.config\n. The sample\n    configurations for LDAP and PAM are shown in the next sections.\n\n\n\n\n\n\nThe classes for the login module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.\n\n\nThe following step shows how to do this\n\n\na.  Edit the \ncustom-env.sh\n configuration file, typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n(when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nSimilar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the\n\nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.  \n\n\nCallback Handlers\n\n\nIn JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. These callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS provides a default callback handler that handles the common callbacks but it may\nnot be sufficient for all login modules like in the Active Directory case described below.\n\n\nRTS also supports specification of a custom callback handler to handle custom callbacks of a login module. The custom callback handler can be specified using the property \ndt.gateway.http.authentication.jaas.callback.class.name\n and when this property is not specified the default callback handler is used. The property can be specified in the \ndt-site.xml\n configuration file and as follows\n\n\nconfiguration\n\n    ...\n    \nproperty\n \n            \nname\ndt.gateway.http.authentication.jaas.callback.class.name\n/name\n\n        \nvalue\nfull-class-name-of-callback\n/value\n\n    \n/property\n\n    ...\n\n/configuration\n\n\n\n\n\nCustom callback handlers can be implemented by extending the default callback handler so they get the support for the common callbacks or they can be implemented from scratch. The Jetty callback handler described below also extends the default callback handler. The default callback handler implementation is implemented in the \ncom.datatorrent.lib.security.auth.callback.DefaultCallbackHandler\n class available in \ncom.datatorrent:dt-library\n artifact.\n\n\nLDAP\n\n\nAn alternative way to enable LDAP authentication, instead of \nLDAP Authentication\n section above, is to follow the JAAS configuration steps described above with the following specific details for the individual steps.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JAAS module for LDAP. You can choose a name like \nldap\n that is appropriate for the current scheme. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nldap\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.\n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate LDAP settings. There are different JAAS authentication modules available for LDAP. One of them is supplied by default in Java. It is the Sun LdapLoginModule class. A sample configuration when using this module is shown below\n\n\nldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};\n\n\n\nNote that the first string before the open brace, in this case\n\nldap\n must match the jaas name specified in step 1. The first property\n\ncom.sun.security.auth.module.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The fields that appear next are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.\n\n\nThe setting \nauthIdentity\n is used to derive the identity of the user that will be used for authentication with the server. Here it is specifying the pattern to derive the LDAP distinguished name (dn) for the user that will be used along with the password supplied by the user for authentication with the LDAP server. The \n{USERNAME}\n variable in the setting will be replaced by the username specified by the user. \n\n\nRefer to the \njavadoc\n for all the configuration options available with this module.\n\n\n\n\n\n\nFor step 4, no extra jars are needed as the module is available in Java\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nActive Directory\n\n\nActive Directory is used when authenticating users in Microsoft Windows domains. The authentication protocol includes Microsoft's implementation of Kerberos as well as LDAP. The recommended way to configure Active Directory is described in \nActive Directory Authentication\n, in this section we will look at an alternative way with JAAS.\n\n\nFollow the JAAS configuration steps described above with the following specific details.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JAAS module. You can choose a name like \nad\n that is appropriate for the current scheme. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nad\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks. \n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured with\n   the appropriate Active Directory settings in the .java.login.config file. \n\n\nIn active directory authentication the user id is typically specified as \nusername@domain\n and sometimes just \nusername\n. Once the user is authenticated the user id needs to be mapped to the LDAP node for the user. This is done by specifying a search filter criteria which is then used to search for the user node in the directory tree starting from a specified base.\n\n\nBelow is an example showing a sample configuration for active directory authentication with the Sun LdapLoginModule\n\n\nad {\n    com.sun.security.auth.module.LdapLoginModule required\n    userProvider=\"ldap://ad.server.com/cn=users,dc=domain,dc=com\"\n    userFilter=\"samAccountName={USERNAME}\"\n    authIdentity=\"{USERNAME}@my.domain.com\";\n};\n\n\n\nThe setting \nuserFilter\n specifies the search criteria to look for the user in the Active Directory tree. The \n{USERNAME}\n variable in the setting is replaced with the username supplied by the user. The setting \nauthIdentity\n specifies the pattern to derive the user id with the domain from the specified username.\n\n\nThe search filter can have more complex logical expressions like the one shown below\n\n\nuserFilter=\"(\n(|(samAccountName={USERNAME})(userPrincipalName={USERNAME})(cn={USERNAME}))(objectClass=user))\"\n\n\n\nAn Active Directory or a LDAP browser client can be used to browse the Active Directory to determine the correct filter to use find the users.\n\n\n\n\n\n\nFor step 4, no extra jars are needed as the module is available in Java\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nJetty module \n binding\n\n\nSometimes a single \nauthIdentity\n pattern like the one used above cannot be used to identity the users because of the way they are configured in the Active Directory, for an enterprise. An example is the case where multiple domains are being used and a single domain name cannot be specified in the \nauthIdentity\n. \n\n\nIn those cases a fixed identity called the root identity or a system identity is used to connect to the server. Then the user node is searched with the search criteria and supplied username, from the node the user directory name is extracted and it is used along with the supplied password for the authentication.\n\n\nA different JAAS login module that supports the system identity is needed as the Sun LdapLoginModule does not support this functionality. Jetty implements one such login module. The steps to configure this module are as follows.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, the name \nad\n can be used.\n\n\n\n\n\n\nFor step 2, a special callback handler is needed to handle the Jetty module callbacks as they are not the common callbacks. There is an implementation of the callback handler for Jetty that is provided by DataTorrent RTS. It is the \ncom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n class in the \ncom.datatorrent:dt-contrib\n artifact.\n\n\nAs explained in the \nCallback Handlers\n section above, the callback handler can be specified in the \ndt-site.xml\n config file using a property as shown below\n\n\nconfiguration\n\n    ...\n    \nproperty\n \n        \nname\n\n            dt.gateway.http.authentication.jaas.callback.class.name\n    \n/name\n\n        \nvalue\n\n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n        \n/value\n\n    \n/property\n\n    ...  \n\n/configuration\n\n\n\n\n\n\n\n\nFor step 3, the configuration for the Jetty module is specified in the \n.java.login.config\n file. A sample configuration for this module is shown below\n\n\nad {\n    org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\"\n    hostname=\"ad.server.com\"\n    port=\"389\"\n    bindDn=\"serviceId@my.domain.com\"\n    bindPassword=\"Password1\"\n    forceBindingLogin=\"true\"\n    userBaseDn=\"DC=my,DC=domain,DC=com\"\n    userIdAttribute=\"samAccountName\"\n    userObjectClass=\"person\";\n};\n\n\n\nThe property \norg.eclipse.jetty.plus.jaas.spi.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP authentication. The properties \nbindDn\n and \nbindPassword\n specify the directory name and password for the system identity respectively. The \nuserIdAttribute\n and \nuserObjectClass\n settings are used as search criteria to search for the user directory node under the \nuserBaseDn\n path in the directory.\n\n\nRefer to the \njavadoc\n for all the configuration options available with this module.\n\n\n\n\n\n\nThe Gateway service in DataTorrent RTS 3.0 is compatible with Jetty 8. The Jetty login module in this version is in the \njetty-plus\n jar. The following version \njetty-plus-8.1.10.v20130312.jar\n is known to work but other compatible versions should work as well. The jar file can be obtained from the Jetty project or from \nmaven central\n. \n\n\nAlong with the jetty jar the dt-contrib jar containing the callback handler and it's dependencies should also be included. It is available on the DataTorrent maven server \nhere\n.  Pick the version matching the DataTorrent RTS version you are using. The dt-contrib artifact has a dependency to \ncom.datatorrent:dt-library\n artifact. It is also available on the maven server \nhere\n.\n\n\nThe jars can be specified in the gateway script file described in step 4 above as follows\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jetty-plus-8.1.10.v20130312.jar:path/to/dt-contrib-\nversion\n.jar:path/to/dt-library-\nversion\n.jar\n\n\n\n\n\n\n\nRestart the gateway as described in step 5\n\n\n\n\n\n\nPAM\n\n\nPAM (Pluggable Authentication Module) is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules, \nthey can be used in Gateway as well.\n\n\nPAM is implemented in C language and has a C API. JPam is a Java PAM bridge\nthat uses JNI to interface with PAM. It is available at\n\nJPAM website\n and the website also has detailed documentation on how to install and set it up. JPam has a JAAS login module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS login module is available.\n\n\nTo enable JPAM follow the JAAS configuration steps described above with the following specific details.\n\n\n\n\n\n\nJPAM has to be first installed on the system. Please follow the\n    installation instructions from the JPAM website.\n\n\n\n\n\n\nFor step 1 of the JAAS authentication \nconfiguration process\n, pick a name for the JPAM authentication module. You can choose a  name like \nnet-sf-jpam\n which JPAM typically uses as the PAM configuration name. This can be done by specifying the value of the \ndt.gateway.http.authentication.jaas.name\n property to be \nnet-sf-jpam\n. This name should now be configured with the appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks \n\n\n\n\n\n\nFor step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate JPAM settings . A\n    sample configuration is shown below\n\n\nnet-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case,\n\nnet-sf-jpam\n must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM related settings. The setting \nserviceName\n specifies the PAM service which would need to be further configured in /etc/pam.d/net-sf-jpam to specify the PAM modules to use. Refer to PAM documentation on how to configure a PAM service with PAM modules. If using Linux local accounts system-auth could be specified as the PAM module in this file. The above settings are only provided as a reference example and a different name can be chosen for \nserviceName\n.\n\n\n\n\n\n\nFor step 4, add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format \nJPam-\nversion\n.jar\n where\n   \nversion\n denotes the version, version 1.1 has been tested.\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/JPam-\nversion\n.jar\n\n\n\n\n\n\n\nRestart the gateway as described in step 5 above    \n\n\n\n\n\n\nGroup Support\n\n\nFor group support such as using LDAP groups for authorization refer to\nthe \nAuthorization using external roles\n section below.\n\n\nHadoop user mapping\n\n\nWhen authentication is enabled, applications are launched on the Hadoop cluster, by default, as the user matching the user name of the authenticated user. This mapping behavior, to select the user to use on the Hadoop side, is configurable and different options are available. To configure the behavior use the following setting\n\n\nproperty\n\n  \nname\ndt.gateway.hadoop.user.strategy\n/name\n\n  \nvalue\nSTRATEGY\n/value\n\n\n/property\n\n\n\n\n\nThe \nSTRATEGY\n in the property above identifies the mapping behavior and the following options are available\n\n\nAuthenticated user\n\n\nTo specify this behavior, use \nAUTH_USER\n as the value for \nSTRATEGY\n. As explained earlier, this leads to applications being launched with the same user name as the authenticated user. This is the default behavior even when the property is not specified.\n\n\nIn Kerberos secure mode, DT Gateway would still connect to Hadoop by using its Kerberos credentials, namely credentials of the user the DT Gateway service is running under (default \ndtadmin\n) but impersonate the authenticated user, to launch the application. This impersonation requires additional configuration on Hadoop side and it is explained in the Hadoop Configuration sub-section under the Impersonation section in the Apache Apex security \ndocument\n.\n\n\nGateway user\n\n\nTo specify this behavior, use \nGATEWAY_USER\n as the value for \nSTRATEGY\n. In this mode, the Hadoop user is the same as the DT Gateway service user (default \ndtadmin\n) and no impersonation is necessary.\n\n\nSpecified user\n\n\nTo specify this behavior, use \nSPECIFIED_USER\n as the value for \nSTRATEGY\n. In this mode, a specific user name can be specified for the Hadoop user and it is used no matter who the authenticated user is. The Hadoop user name is specified by an additional property\n\n\nproperty\n\n  \nname\ndt.gateway.hadoop.user.name\n/name\n\n  \nvalue\nusername\n/value\n\n\n/property\n\n\n\n\n\nThe impersonation behavior described in the \nAuthenticated user\n section above applies here as well and so do the requirements mentioned there.\n\n\nAuthorization\n\n\nWhen any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.\n\n\nPermissions\n\n\nThe list of all possible permissions in the DT Gateway is as follow:\n\n\nACCESS_RM_PROXY\n\n\nAllow HTTP proxying requests to YARN\u2019s Resource Manager REST API\n\n\nEDIT_GLOBAL_CONFIG\n\n\nEdit global settings\n\n\nEDIT_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\nLAUNCH_APPS\n\n\nLaunch Apps\n\n\nMANAGE_LICENSES\n\n\nManage DataTorrent RTS licenses\n\n\nMANAGE_OTHER_USERS_APPS\n\n\nManage (e.g. edit, kill, etc) applications launched by other users\n\n\nMANAGE_OTHER_USERS_APP_PACKAGES\n\n\nManage App Packages uploaded by other users  \n\n\nMANAGE_ROLES\n\n\nManage roles (create/delete roles, or assign permissions to roles)\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nManage system alerts\n\n\nMANAGE_USERS\n\n\nManage users (create/delete users, change password)\n\n\nUPLOAD_APP_PACKAGES\n\n\nUpload App Packages\n\n\nVIEW_GLOBAL_CONFIG\n\n\nView global settings   \n\n\nVIEW_LICENSES\n\n\nView DataTorrent RTS licenses\n\n\nVIEW_OTHER_USERS_APPS\n\n\nView applications launched by others\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nView App Packages uploaded by other users\n\n\nVIEW_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\nVIEW_SYSTEM_ALERTS\n\n\nView system alerts\n\n\nDefault Roles\n\n\nDataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.\n\n\nAdmin\n\n\nAn administrator of DataTorrent RTS is intended to be able to install,\nmanage \n modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.\n\n\nOperator\n\n\nOperators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.\n\n\nHere is the list of default permissions given to operators\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_OTHER_USERS_APPS\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nNote that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.  \n\n\nDeveloper\n\n\nDevelopers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.\n\n\nHere is the list of default permissions given to developers\n\n\nLAUNCH_APPS\n\n\nUPLOAD_APP_PACKAGES\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nApp Permissions and App Package Permissions\n\n\nUsers can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.\n\n\nThe default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.\n\n\nThe Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the \nDT Gateway REST API document\n and \nhere\n for examples on how to use the REST API.\n\n\nViewing and Managing Auth in the Console\n\n\nViewing User Profile\n\n\nAfter you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.\n\n\n\nAdministering Auth\n\n\nFrom the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:\n\n\n\n\nCreate new users\n\n\nDelete users\n\n\nChange existing users\u2019 passwords\n\n\nAssign roles to users\n\n\nCreate roles\n\n\nAssign permissions to roles\n\n\n\n\n\n\nDataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.\n\n\nAuthorization using external roles\n\n\nWhen using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections \nKerberos roles\n and\n\nJAAS roles\n. Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the \nExternal Role Mapping\n section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.\n\n\nKerberos roles\n\n\nWhen Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form \nuser/group@DOMAIN\n\nthe group portion is used as the external role and no additional\nconfiguration is necessary.\n\n\nJAAS roles\n\n\nTo use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the JAAS plugin login module. If the module supports roles then some of these principals are for roles and these role principals need to be identified from the list. Additional configuration is needed to do\nthis and it is specific to the login module implementation. Specifically the Java class name identifying the role principal is needed. This can be specified using the \ndt.gateway.http.authentication.jaas.role.class.name\n property in the \ndt-site.xml\n configuration file as shown below\n\n\nconfiguration\n\n    ...\n    \nproperty\n\n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n        \nvalue\nfull-class-name-of-role\n/value\n\n    \n/property\n\n    ...\n\n/configuration\n\n\n\n\n\nLDAP Groups\n\n\nWhen using LDAP with JAAS, to utilize the LDAP roles, a LDAP login module supporting roles should be used. Any LDAP module that supports roles can be used. Jetty login module has support for roles. Refer to \nJetty module \n binding\n section above for more details about this module. The configuration steps are as follows.\n\n\n\n\n\n\nThe Jetty login module returns the roles in role principal classes. The class name identifying the role is \norg.eclipse.jetty.plus.jaas.JAASRole\n. This should be specified using the \ndt.gateway.http.authentication.jaas.role.class.name\n property in the \ndt-site.xml\n configuration file as described in the section above. \n\n\nAlso as described in the \nJetty module \n binding\n section a custom callback handler is needed for the Jetty login module.\n\n\nA sample configuration file with all the properties is shown below\n\n\nconfiguration\n\n...\n  \nproperty\n\n          \nname\ndt.gateway.http.authentication.type\n/name\n\n          \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n         \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n         \nvalue\nldap\n/value\n\n  \n/property\n\n  \nproperty\n  \n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n        \nvalue\norg.eclipse.jetty.plus.jaas.JAASRole\n/value\n\n  \n/property\n\n  \n/property\n\n        \nname\n\n                dt.gateway.http.authentication.jaas.callback.class.name\n        \n/name\n\n        \nvalue\n\n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n        \n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nIf the system identity is used then the Jetty login module can be used as is and the jar dependencies in the step below can be specified as described in the \nJetty module \n binding\n section. \n\n\nHowever, if system identity is not used something different needs to be done. An issue was discovered with the Jetty login module supplied with Jetty 8 that prevented LDAP authentication to be successful even when the user credentials were correct. DataTorrent has a fix for this and is providing the login module with the fix in a separate package called \ndt-auth\n. The class name for the module is \ncom.datatorrent.auth.jetty.JettyLdapLoginModule\n. The \ndt-auth\n project along with the source can be found here \nAuth\n. DataTorrent is working on submitting this fix back to Jetty project so that it gets back into the main source.\n\n\nThe JAAS configuration file as described in \nLDAP\n section under \nEnabling JAAS Auth\n should be configured to specify the ldap settings for roles. A sample configuration containing role settings for the \ndt-auth\n login module is shown below. \n\n\nldap {\n    com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" port=\"389\"\n    authenticationMethod=\"simple\" \n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\"cn\"\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\";\n};\n\n\n\nThe \nroleNameAttribute\n and \nroleBaseDn\n settings are used to identify the role and the \nuserRdnAttribute\n setting is used the identify the users that belong to the role. The values for these settings are dependent on attributes names are being used in your LDAP directory server.\n\n\nSimilar configuration can be used when the original Jetty login module is being used with the system id, the module class would be Jetty login module class and the binding settings would be specified as described in the \nJetty module \n binding\n section.\n\n\nRefer to the \njavadoc\n for the role and other configuration options available with this module.\n\n\n\n\n\n\nIf the Jetty login module is being used as is then the path specification instructions described in the \nJetty module \n binding\n section can be used. \n\n\nIf the login module containing the DataTorrent fix, dt-auth, is being used then it's jar along with the Jetty module dependencies containing the role class, the dt-contrib jar containing the custom callback handler along with its dependencies should all be made available for Gateway. The jars can be obtained from the \nDataTorrent Auth\n project.\n\n\nPlease follow the instructions in the above url to obtain the project jar files. After obtaining the jar files they can be specified in the gateway script file as\n\n\nexport DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway as described earlier\n\n\n\n\n\n\nExternal Role Mapping\n\n\nExternal role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps\n\n\n\n\n\n\nIn the configuration folder typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is\n\n\nadmins:admin\nstaff:developer\n\n\n\nThis maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.\n\n\n\n\n\n\nRestart the Gateway as described earlier\n\n\n\n\n\n\nAdministering Using Command Line\n\n\nYou can also utilize the \ndtGateway REST API\n (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.\n\n\nLog in as admin:\n\n\n% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login\n\n\n\nThis curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar\n\n\nChanging the admin password:\n\n\n% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin\n\n\n\nThis uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.\n\n\nAdding a second admin user:\n\n\n% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.\n\n\nAdding a user in the developer role:\n\n\n% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.\n\n\nListing all users:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users\n\n\n\nGetting info for a specific user:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command returns the information about the user \u201cjohn\u201d.\n\n\nRemoving a user:\n\n\n% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command removes the user \u201cjane\u201d.\n\n\nSetting up SSL Keystore in Gateway\n\n\nAn SSL keystore is needed for your gateway to enable HTTPS in the gateway and to configure SMTP with password authentication. SMTP configuration is needed for the gateway to \nsend email alerts as described in \nSystem Alerts\n. \n\n\nFollow the steps below to set up the keystore.\n\n\n\n\n\n\nGenerate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here: \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n.\nNote down the keystore password and full path of the keystore that you need to provide in the next step.\n\n\n\n\n\n\nAdd two properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n (or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \n!-- this is the full path to the SSL keystore you created --\n\n      \nname\ndt.gateway.sslKeystorePath\n/name\n\n      \nvalue\n{/path/to/keystore}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \n!-- this is the keystore password --\n\n      \nname\ndt.gateway.sslKeystorePassword\n/name\n\n      \nvalue\n{keystore-password}\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nPerform any additional steps required for \"Enabling HTTPS\" or \"SMTP Password Encryption\" use-case as described below.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n(when running Gateway in local mode use \ndtgateway restart\n command)\n\n\n\n\n\n\nEnabling HTTPS in Gateway\n\n\nTo enable HTTPS in the Gateway after setting up the keystore as described above, you have to add the following\nproperty to the \ndt-site.xml\n configuration file\n\n\n    \nconfiguration\n\n    ...\n      \nproperty\n\n        \nname\ndt.attr.GATEWAY_USE_SSL\n/name\n\n        \nvalue\ntrue\n/value\n\n      \n/property\n\n    ...\n    \n/configuration\n\n\n\n\nSetting up a Key for SMTP Password Encryption\n\n\nAdd another key to the keystore created above using the instructions mentioned in \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n. For example:\n\n\nkeytool -genkey -alias smtpenc-alias -keyalg RSA -keypass \nkey password\n -storepass \nstore password\n -keystore gwkeystore.jks\n\n\n\n\nNote down the alias you used for the key (smtpenc-alias in the above command). Remember to use the same key password and \nstore password as the ones you used in the previous command where you created the keystore.\n\n\nAdd the following property to \ndt-site.xml\n to indicate the alias to be used for SMTP Password Encryption:\n\n\n    \nconfiguration\n\n    ...\n     \nproperty\n\n       \nname\ndt.gateway.ssl.alias.password.encryption\n/name\n\n       \nvalue\nsmtpenc-alias\n/value\n\n     \n/property\n\n    ...\n    \n/configuration", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway_security/#datatorrent-gateway-security", 
            "text": "DataTorrent Gateway is a service that provides the backend functionality\nfor the DataTorrent UI Console and processes the web service requests from it. The service provides real-time information about running applications, allows changes to applications, launches new applications among various other operations. Refer to  dtGateway  for details on the Gateway service and  dtManage \u00a0for the UI Console.  Broadly security in Gateway can be classified into two categories, frontend security and backend security. Frontend security deals with access to Gateway service which mainly involves securing the web service calls. This includes aspects such as user authentication and authorization. The backend security deals with security aspects when Gateway is communicating with a secure Hadoop infrastructure.  After installation of DataTorrent RTS both these aspects can be configured manually as described in the following sections although work is being done to enable this configuration in the UI Console during installation itself and post installation.", 
            "title": "DataTorrent Gateway Security"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-secure-mode", 
            "text": "Kerberos is the de-facto authentication mechanism supported in Hadoop. When secure mode is enabled in Hadoop, requests from clients to Hadoop are authenticated using Kerberos. In this mode Gateway service needs Kerberos credentials to communicate with Hadoop. The credentials should match the user that the DT Gateway service is running under.  In a multi-user installation DT Gateway is typically running as\nuser  dtadmin  and the Kerberos credentials specified should be for this\nuser. They are specified in the  dt-site.xml  configuration file located in the config folder under the installation which is typically  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install). For a single user installation where gateway is running as the user, the Kerberos credentials will be the user\u2019s credentials.  The snippet below shows how the credentials can be specified in the\nconfiguration file.  property \n         name dt.gateway.authentication.principal /name \n         value kerberos-principal-of-gateway-user /value  /property  property \n         name dt.gateway.authentication.keytab /name \n         value absolute-path-to-keytab-file /value  /property", 
            "title": "Kerberos Secure Mode"
        }, 
        {
            "location": "/dtgateway_security/#long-running-applications", 
            "text": "In secure mode, long running applications have additional requirements. Refer to the Token Refresh section in the Apache Apex security  document .", 
            "title": "Long running applications"
        }, 
        {
            "location": "/dtgateway_security/#authentication", 
            "text": "DataTorrent Gateway has support for authentication and when it is configured users have to authenticate before they can access the UI Console. Various authentication mechanisms are supported and this gives enterprises the flexibility to extend their existing authentication mechanism already in use within the enterprise to Gateway. It also supports roles, mapping of groups or roles from the external authentication mechanism to roles and supports role based authorization.  The different authentication mechanisms supported by Gateway are   Password Authentication  LDAP Authentication  Kerberos Authentication  JAAS Authentication  for Active Directory, PAM, etc   JAAS is a extensible authentication framework that supports different types of authentication mechanisms by plugging in an appropriate module.", 
            "title": "Authentication"
        }, 
        {
            "location": "/dtgateway_security/#password-authentication", 
            "text": "Password security is simple to set up and is ideal for a small to medium set of users. It comes with role-based access control, so users can be assigned roles, and roles can be assigned granular permissions (see  User Management ). This is the only authentication mechanism available that does not depend on any external systems. The users will be managed locally by the Gateway. When enabled, all users will be presented with the login prompt before being able to use the DT Console.  To set up password security, on the  Security Configuration  page select  Password  from the  Authentication Type  dropdown, and save. Allow the Gateway to restart.   When the Gateway has restarted, you should be prompted for username and password. Log in as the default admin user  dtadmin  with password  dtadmin .   Once authenticated, active username and an option to log out is presented in the top right corner of the DT Console screen.   Additional users and roles can be created and managed on the  User Management  page.  Note : Don't forget to change your  dtadmin  user's password!", 
            "title": "Password Authentication"
        }, 
        {
            "location": "/dtgateway_security/#password-authentication-via-dt-sitexml", 
            "text": "Password authentication can alternatively be configured outside the Console by performing following two steps:    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n     property \n     name dt.gateway.http.authentication.type /name \n     value password /value \n     /property \n... /configuration     Restart the Gateway. If running Gateway in local mode use  dtgateway restart  instead.  sudo service dtgateway restart", 
            "title": "Password Authentication via dt-site.xml"
        }, 
        {
            "location": "/dtgateway_security/#ldap-authentication", 
            "text": "LDAP is a directory based authentication mechanism used in many enterprises. If your organization uses LDAP for authentication, the LDAP security option is ideal for giving your existing users access to RTS, with role-based access control and group mapping features.  There are four variations for configuring LDAP authentication:   Identity  Provide the parent DN of your users and  specify the RDN attribute of users .  Users will authenticate using their RDN attribute value as their username.    Anonymous   User Search Filter  Provide the parent DN of your users and  specify a search filter to identify users .  Users will authenticate using an appropriate username that matches the parameters defined in the search filter.    Identity and Anonymous Search  Provide the parent DN of your users,  specify the RDN attribute of users, and a search filter .  Users will authenticate using their RDN attribute value as their username, as well as the parameters defined in the search filter.    Non-Anonymous Search with Group Support  Provide basic DN info for users, DN and password of a user able to perform a non-anonymous search, and optional group support info.  With  group support disabled , users need to be added in User Management before logging in, unless a default role is set. Users authenticate with their User Id Attribute value as their username.  With  group support enabled , users do  not  have to be added in User Management before logging in. Users authenticate with their User Id Attribute value as their username. They will be assigned roles mapped to their LDAP group. For example, user Peter is part of GroupA (admin) and GroupB (developer, operator); Peter will be assigned roles admin, developer, and operator upon login.     When group support is not configured, users must be assigned a role before they are able to log in, unless a default role is set. This means users can be restricted from logging in (blacklisted) by removing all of their roles. Default roles will not be reapplied after a user's initial login, and therefore will not interfere with restricting users.  Note : If migrating from  Password  mode, the existing users will be carried over as \"local users\" and can still login as if in  Password  mode. It is recommended to keep only the  dtadmin  user and delete the rest. This is because local users cannot be added or deleted once  LDAP  mode is activated.   After setting up LDAP in the Security Configuration page, the  LDAP Users  section will appear in the User Management page. If you have group support enabled, the  LDAP Groups  section will also appear. Existing users (carried over from  Password  mode), will be placed in the  Local Users  sections. Local users cannot be added or deleted in LDAP mode, but their roles and passwords can be modified.   To configure LDAP via dt-site.xml, check out the  JAAS Authentication - LDAP  section below.", 
            "title": "LDAP Authentication"
        }, 
        {
            "location": "/dtgateway_security/#active-directory-authentication", 
            "text": "Active Directory (AD) is a directory based authentication mechanism for users in Microsoft Windows domains. Active Directory supports role-based access control and group mapping features.  To configure Active Directory, follow the steps to configure  LDAP  with the  Non-Anonymous Search with Group Support  variation. The only difference is the additional User Object Class field.   User Management for Active Directory behaves the same as LDAP with the  Non-Anonymous Search with Group Support  variation.  To configure Active Directory via dt-site.xml, check out the  JAAS Authentication - Active Directory  section below.", 
            "title": "Active Directory Authentication"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-authentication", 
            "text": "Kerberos authentication can optionally be enabled for Hadoop web access.\nWhen configured, all web browser access to Hadoop management consoles are Kerberos authenticated. Web services are also Kerberos authenticated. This authentication is performed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.  Kerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by logging in to kerberos system in a terminal emulator using kinit. Then, user launches a browser to access the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.  When this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the  External Role Mapping  in the  Authorization using external roles  section below for that.  Additional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the  Configuration . This\nauthentication can be set up using the following steps.    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install)  configuration \n...\n   property \n     name dt.gateway.http.authentication.type /name \n     value kerberos /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.principal /name \n     value {kerberos-principal-of-web-service} /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.keytab /name \n     value {absolute-path-to-keytab-file} /value \n   /property \n   property \n     name dt.gateway.http.authentication.token.validity /name \n     value {authentication-token-validity-in-seconds} /value \n   /property \n   property \n     name dt.gateway.http.authentication.cookie.domain /name \n     value {http-cookie-domain-for-authentication-token} /value \n   property \n     name dt.gateway.http.authentication.cookie.path /name \n     value {http-cookie-path} /value \n   /property \n   property \n     name dt.gateway.http.authentication.signature.secret /name \n     value {absolute-path-of-secret-file-for-signing-authentication-tokens}  /value \n   /property  /configuration   Note that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property dt.gateway.http.authentication.type  should be replaced with the\nappropriate values for your setup.    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/dtgateway_security/#jaas-authentication", 
            "text": "JAAS or Java Authentication and Authorization Service is a pluggable\nand extensible mechanism for authentication. It is an authentication framework\nwhere the actual authentication is performed by a JAAS login module plugin which can be configured using a configuration file.   The general configuration steps for enabling any JAAS based authentication mechanism is described below. Subsequent sections will cover the specific configuration details for LDAP, Active Directory and PAM. However JAAS is not limited to just these three mechanisms. Any other JAAS compatible mechanism can be used by specifying the appropriate module in the configuration. Also if there isn't a JAAS module available for an authentication mechanism a new one can be developed using the JAAS API and used here.  Below are the general steps for configuring a JAAS authentication mechanism    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  (or  ~/datatorrent/current/conf  for local install).  configuration \n    ...\n   property \n       name dt.gateway.http.authentication.type /name \n       value jaas /value \n   /property \n   property \n       name dt.gateway.http.authentication.jaas.name /name \n       value name-of-jaas-module /value \n   /property \n    ... /configuration   The  dt.gateway.http.authentication.jaas.name  property specifies the login module to use with JAAS and the next step explains the process for configuring it.    If the login module requires a custom callback handler it needs to be specified. What a callback handler is and how it can be specified is described in the next section  Callback Handlers .    The name of the login module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be  /home/dtadmin/.java.login.config , if running as a\n    regular user it would be  ~/.java.login.config . The sample\n    configurations for LDAP and PAM are shown in the next sections.    The classes for the login module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.  The following step shows how to do this  a.  Edit the  custom-env.sh  configuration file, typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart \n(when running Gateway in local mode use   dtgateway restart  command)    Similar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the External Role Mapping  in the  Authorization using external roles  section below for that.", 
            "title": "JAAS Authentication"
        }, 
        {
            "location": "/dtgateway_security/#callback-handlers", 
            "text": "In JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. These callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS provides a default callback handler that handles the common callbacks but it may\nnot be sufficient for all login modules like in the Active Directory case described below.  RTS also supports specification of a custom callback handler to handle custom callbacks of a login module. The custom callback handler can be specified using the property  dt.gateway.http.authentication.jaas.callback.class.name  and when this property is not specified the default callback handler is used. The property can be specified in the  dt-site.xml  configuration file and as follows  configuration \n    ...\n     property  \n             name dt.gateway.http.authentication.jaas.callback.class.name /name \n         value full-class-name-of-callback /value \n     /property \n    ... /configuration   Custom callback handlers can be implemented by extending the default callback handler so they get the support for the common callbacks or they can be implemented from scratch. The Jetty callback handler described below also extends the default callback handler. The default callback handler implementation is implemented in the  com.datatorrent.lib.security.auth.callback.DefaultCallbackHandler  class available in  com.datatorrent:dt-library  artifact.", 
            "title": "Callback Handlers"
        }, 
        {
            "location": "/dtgateway_security/#ldap", 
            "text": "An alternative way to enable LDAP authentication, instead of  LDAP Authentication  section above, is to follow the JAAS configuration steps described above with the following specific details for the individual steps.    For step 1 of the JAAS authentication  configuration process , pick a name for the JAAS module for LDAP. You can choose a name like  ldap  that is appropriate for the current scheme. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  ldap . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.    For step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate LDAP settings. There are different JAAS authentication modules available for LDAP. One of them is supplied by default in Java. It is the Sun LdapLoginModule class. A sample configuration when using this module is shown below  ldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};  Note that the first string before the open brace, in this case ldap  must match the jaas name specified in step 1. The first property com.sun.security.auth.module.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The fields that appear next are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.  The setting  authIdentity  is used to derive the identity of the user that will be used for authentication with the server. Here it is specifying the pattern to derive the LDAP distinguished name (dn) for the user that will be used along with the password supplied by the user for authentication with the LDAP server. The  {USERNAME}  variable in the setting will be replaced by the username specified by the user.   Refer to the  javadoc  for all the configuration options available with this module.    For step 4, no extra jars are needed as the module is available in Java    Restart the gateway as described in step 5", 
            "title": "LDAP"
        }, 
        {
            "location": "/dtgateway_security/#active-directory", 
            "text": "Active Directory is used when authenticating users in Microsoft Windows domains. The authentication protocol includes Microsoft's implementation of Kerberos as well as LDAP. The recommended way to configure Active Directory is described in  Active Directory Authentication , in this section we will look at an alternative way with JAAS.  Follow the JAAS configuration steps described above with the following specific details.    For step 1 of the JAAS authentication  configuration process , pick a name for the JAAS module. You can choose a name like  ad  that is appropriate for the current scheme. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  ad . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks.     For step 3, the JAAS name specified above should be configured with\n   the appropriate Active Directory settings in the .java.login.config file.   In active directory authentication the user id is typically specified as  username@domain  and sometimes just  username . Once the user is authenticated the user id needs to be mapped to the LDAP node for the user. This is done by specifying a search filter criteria which is then used to search for the user node in the directory tree starting from a specified base.  Below is an example showing a sample configuration for active directory authentication with the Sun LdapLoginModule  ad {\n    com.sun.security.auth.module.LdapLoginModule required\n    userProvider=\"ldap://ad.server.com/cn=users,dc=domain,dc=com\"\n    userFilter=\"samAccountName={USERNAME}\"\n    authIdentity=\"{USERNAME}@my.domain.com\";\n};  The setting  userFilter  specifies the search criteria to look for the user in the Active Directory tree. The  {USERNAME}  variable in the setting is replaced with the username supplied by the user. The setting  authIdentity  specifies the pattern to derive the user id with the domain from the specified username.  The search filter can have more complex logical expressions like the one shown below  userFilter=\"( (|(samAccountName={USERNAME})(userPrincipalName={USERNAME})(cn={USERNAME}))(objectClass=user))\"  An Active Directory or a LDAP browser client can be used to browse the Active Directory to determine the correct filter to use find the users.    For step 4, no extra jars are needed as the module is available in Java    Restart the gateway as described in step 5", 
            "title": "Active Directory"
        }, 
        {
            "location": "/dtgateway_security/#jetty-module-binding", 
            "text": "Sometimes a single  authIdentity  pattern like the one used above cannot be used to identity the users because of the way they are configured in the Active Directory, for an enterprise. An example is the case where multiple domains are being used and a single domain name cannot be specified in the  authIdentity .   In those cases a fixed identity called the root identity or a system identity is used to connect to the server. Then the user node is searched with the search criteria and supplied username, from the node the user directory name is extracted and it is used along with the supplied password for the authentication.  A different JAAS login module that supports the system identity is needed as the Sun LdapLoginModule does not support this functionality. Jetty implements one such login module. The steps to configure this module are as follows.    For step 1 of the JAAS authentication  configuration process , the name  ad  can be used.    For step 2, a special callback handler is needed to handle the Jetty module callbacks as they are not the common callbacks. There is an implementation of the callback handler for Jetty that is provided by DataTorrent RTS. It is the  com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler  class in the  com.datatorrent:dt-contrib  artifact.  As explained in the  Callback Handlers  section above, the callback handler can be specified in the  dt-site.xml  config file using a property as shown below  configuration \n    ...\n     property  \n         name \n            dt.gateway.http.authentication.jaas.callback.class.name\n     /name \n         value \n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n         /value \n     /property \n    ...   /configuration     For step 3, the configuration for the Jetty module is specified in the  .java.login.config  file. A sample configuration for this module is shown below  ad {\n    org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\"\n    hostname=\"ad.server.com\"\n    port=\"389\"\n    bindDn=\"serviceId@my.domain.com\"\n    bindPassword=\"Password1\"\n    forceBindingLogin=\"true\"\n    userBaseDn=\"DC=my,DC=domain,DC=com\"\n    userIdAttribute=\"samAccountName\"\n    userObjectClass=\"person\";\n};  The property  org.eclipse.jetty.plus.jaas.spi.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP authentication. The properties  bindDn  and  bindPassword  specify the directory name and password for the system identity respectively. The  userIdAttribute  and  userObjectClass  settings are used as search criteria to search for the user directory node under the  userBaseDn  path in the directory.  Refer to the  javadoc  for all the configuration options available with this module.    The Gateway service in DataTorrent RTS 3.0 is compatible with Jetty 8. The Jetty login module in this version is in the  jetty-plus  jar. The following version  jetty-plus-8.1.10.v20130312.jar  is known to work but other compatible versions should work as well. The jar file can be obtained from the Jetty project or from  maven central .   Along with the jetty jar the dt-contrib jar containing the callback handler and it's dependencies should also be included. It is available on the DataTorrent maven server  here .  Pick the version matching the DataTorrent RTS version you are using. The dt-contrib artifact has a dependency to  com.datatorrent:dt-library  artifact. It is also available on the maven server  here .  The jars can be specified in the gateway script file described in step 4 above as follows  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jetty-plus-8.1.10.v20130312.jar:path/to/dt-contrib- version .jar:path/to/dt-library- version .jar    Restart the gateway as described in step 5", 
            "title": "Jetty module &amp; binding"
        }, 
        {
            "location": "/dtgateway_security/#pam", 
            "text": "PAM (Pluggable Authentication Module) is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules, \nthey can be used in Gateway as well.  PAM is implemented in C language and has a C API. JPam is a Java PAM bridge\nthat uses JNI to interface with PAM. It is available at JPAM website  and the website also has detailed documentation on how to install and set it up. JPam has a JAAS login module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS login module is available.  To enable JPAM follow the JAAS configuration steps described above with the following specific details.    JPAM has to be first installed on the system. Please follow the\n    installation instructions from the JPAM website.    For step 1 of the JAAS authentication  configuration process , pick a name for the JPAM authentication module. You can choose a  name like  net-sf-jpam  which JPAM typically uses as the PAM configuration name. This can be done by specifying the value of the  dt.gateway.http.authentication.jaas.name  property to be  net-sf-jpam . This name should now be configured with the appropriate settings as described in the next step.    For step 2, no special callback handler needs to be specified as the default callback handler can handle the callbacks     For step 3, the JAAS name specified above should be configured in the .java.login.config file with the appropriate JPAM settings . A\n    sample configuration is shown below  net-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};    Note that the first string before the open brace, in this case, net-sf-jpam  must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM related settings. The setting  serviceName  specifies the PAM service which would need to be further configured in /etc/pam.d/net-sf-jpam to specify the PAM modules to use. Refer to PAM documentation on how to configure a PAM service with PAM modules. If using Linux local accounts system-auth could be specified as the PAM module in this file. The above settings are only provided as a reference example and a different name can be chosen for  serviceName .    For step 4, add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format  JPam- version .jar  where\n    version  denotes the version, version 1.1 has been tested.  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/JPam- version .jar    Restart the gateway as described in step 5 above", 
            "title": "PAM"
        }, 
        {
            "location": "/dtgateway_security/#group-support", 
            "text": "For group support such as using LDAP groups for authorization refer to\nthe  Authorization using external roles  section below.", 
            "title": "Group Support"
        }, 
        {
            "location": "/dtgateway_security/#hadoop-user-mapping", 
            "text": "When authentication is enabled, applications are launched on the Hadoop cluster, by default, as the user matching the user name of the authenticated user. This mapping behavior, to select the user to use on the Hadoop side, is configurable and different options are available. To configure the behavior use the following setting  property \n   name dt.gateway.hadoop.user.strategy /name \n   value STRATEGY /value  /property   The  STRATEGY  in the property above identifies the mapping behavior and the following options are available", 
            "title": "Hadoop user mapping"
        }, 
        {
            "location": "/dtgateway_security/#authenticated-user", 
            "text": "To specify this behavior, use  AUTH_USER  as the value for  STRATEGY . As explained earlier, this leads to applications being launched with the same user name as the authenticated user. This is the default behavior even when the property is not specified.  In Kerberos secure mode, DT Gateway would still connect to Hadoop by using its Kerberos credentials, namely credentials of the user the DT Gateway service is running under (default  dtadmin ) but impersonate the authenticated user, to launch the application. This impersonation requires additional configuration on Hadoop side and it is explained in the Hadoop Configuration sub-section under the Impersonation section in the Apache Apex security  document .", 
            "title": "Authenticated user"
        }, 
        {
            "location": "/dtgateway_security/#gateway-user", 
            "text": "To specify this behavior, use  GATEWAY_USER  as the value for  STRATEGY . In this mode, the Hadoop user is the same as the DT Gateway service user (default  dtadmin ) and no impersonation is necessary.", 
            "title": "Gateway user"
        }, 
        {
            "location": "/dtgateway_security/#specified-user", 
            "text": "To specify this behavior, use  SPECIFIED_USER  as the value for  STRATEGY . In this mode, a specific user name can be specified for the Hadoop user and it is used no matter who the authenticated user is. The Hadoop user name is specified by an additional property  property \n   name dt.gateway.hadoop.user.name /name \n   value username /value  /property   The impersonation behavior described in the  Authenticated user  section above applies here as well and so do the requirements mentioned there.", 
            "title": "Specified user"
        }, 
        {
            "location": "/dtgateway_security/#authorization", 
            "text": "When any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.", 
            "title": "Authorization"
        }, 
        {
            "location": "/dtgateway_security/#permissions", 
            "text": "The list of all possible permissions in the DT Gateway is as follow:", 
            "title": "Permissions"
        }, 
        {
            "location": "/dtgateway_security/#access_rm_proxy", 
            "text": "Allow HTTP proxying requests to YARN\u2019s Resource Manager REST API", 
            "title": "ACCESS_RM_PROXY"
        }, 
        {
            "location": "/dtgateway_security/#edit_global_config", 
            "text": "Edit global settings", 
            "title": "EDIT_GLOBAL_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#edit_other_users_config", 
            "text": "Edit other users\u2019 settings", 
            "title": "EDIT_OTHER_USERS_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#launch_apps", 
            "text": "Launch Apps", 
            "title": "LAUNCH_APPS"
        }, 
        {
            "location": "/dtgateway_security/#manage_licenses", 
            "text": "Manage DataTorrent RTS licenses", 
            "title": "MANAGE_LICENSES"
        }, 
        {
            "location": "/dtgateway_security/#manage_other_users_apps", 
            "text": "Manage (e.g. edit, kill, etc) applications launched by other users", 
            "title": "MANAGE_OTHER_USERS_APPS"
        }, 
        {
            "location": "/dtgateway_security/#manage_other_users_app_packages", 
            "text": "Manage App Packages uploaded by other users", 
            "title": "MANAGE_OTHER_USERS_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#manage_roles", 
            "text": "Manage roles (create/delete roles, or assign permissions to roles)", 
            "title": "MANAGE_ROLES"
        }, 
        {
            "location": "/dtgateway_security/#manage_system_alerts", 
            "text": "Manage system alerts", 
            "title": "MANAGE_SYSTEM_ALERTS"
        }, 
        {
            "location": "/dtgateway_security/#manage_users", 
            "text": "Manage users (create/delete users, change password)", 
            "title": "MANAGE_USERS"
        }, 
        {
            "location": "/dtgateway_security/#upload_app_packages", 
            "text": "Upload App Packages", 
            "title": "UPLOAD_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#view_global_config", 
            "text": "View global settings", 
            "title": "VIEW_GLOBAL_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#view_licenses", 
            "text": "View DataTorrent RTS licenses", 
            "title": "VIEW_LICENSES"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_apps", 
            "text": "View applications launched by others", 
            "title": "VIEW_OTHER_USERS_APPS"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_app_packages", 
            "text": "View App Packages uploaded by other users", 
            "title": "VIEW_OTHER_USERS_APP_PACKAGES"
        }, 
        {
            "location": "/dtgateway_security/#view_other_users_config", 
            "text": "Edit other users\u2019 settings", 
            "title": "VIEW_OTHER_USERS_CONFIG"
        }, 
        {
            "location": "/dtgateway_security/#view_system_alerts", 
            "text": "View system alerts", 
            "title": "VIEW_SYSTEM_ALERTS"
        }, 
        {
            "location": "/dtgateway_security/#default-roles", 
            "text": "DataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.", 
            "title": "Default Roles"
        }, 
        {
            "location": "/dtgateway_security/#admin", 
            "text": "An administrator of DataTorrent RTS is intended to be able to install,\nmanage   modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.", 
            "title": "Admin"
        }, 
        {
            "location": "/dtgateway_security/#operator", 
            "text": "Operators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.  Here is the list of default permissions given to operators  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_OTHER_USERS_APPS  VIEW_OTHER_USERS_APP_PACKAGES  VIEW_SYSTEM_ALERTS  Note that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.", 
            "title": "Operator"
        }, 
        {
            "location": "/dtgateway_security/#developer", 
            "text": "Developers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.  Here is the list of default permissions given to developers  LAUNCH_APPS  UPLOAD_APP_PACKAGES  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_SYSTEM_ALERTS", 
            "title": "Developer"
        }, 
        {
            "location": "/dtgateway_security/#app-permissions-and-app-package-permissions", 
            "text": "Users can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.  The default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.  The Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the  DT Gateway REST API document  and  here  for examples on how to use the REST API.", 
            "title": "App Permissions and App Package Permissions"
        }, 
        {
            "location": "/dtgateway_security/#viewing-and-managing-auth-in-the-console", 
            "text": "", 
            "title": "Viewing and Managing Auth in the Console"
        }, 
        {
            "location": "/dtgateway_security/#viewing-user-profile", 
            "text": "After you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.", 
            "title": "Viewing User Profile"
        }, 
        {
            "location": "/dtgateway_security/#administering-auth", 
            "text": "From the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:   Create new users  Delete users  Change existing users\u2019 passwords  Assign roles to users  Create roles  Assign permissions to roles    DataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.", 
            "title": "Administering Auth"
        }, 
        {
            "location": "/dtgateway_security/#authorization-using-external-roles", 
            "text": "When using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections  Kerberos roles  and JAAS roles . Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the  External Role Mapping  section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.", 
            "title": "Authorization using external roles"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-roles", 
            "text": "When Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form  user/group@DOMAIN \nthe group portion is used as the external role and no additional\nconfiguration is necessary.", 
            "title": "Kerberos roles"
        }, 
        {
            "location": "/dtgateway_security/#jaas-roles", 
            "text": "To use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the JAAS plugin login module. If the module supports roles then some of these principals are for roles and these role principals need to be identified from the list. Additional configuration is needed to do\nthis and it is specific to the login module implementation. Specifically the Java class name identifying the role principal is needed. This can be specified using the  dt.gateway.http.authentication.jaas.role.class.name  property in the  dt-site.xml  configuration file as shown below  configuration \n    ...\n     property \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n         value full-class-name-of-role /value \n     /property \n    ... /configuration", 
            "title": "JAAS roles"
        }, 
        {
            "location": "/dtgateway_security/#ldap-groups", 
            "text": "When using LDAP with JAAS, to utilize the LDAP roles, a LDAP login module supporting roles should be used. Any LDAP module that supports roles can be used. Jetty login module has support for roles. Refer to  Jetty module   binding  section above for more details about this module. The configuration steps are as follows.    The Jetty login module returns the roles in role principal classes. The class name identifying the role is  org.eclipse.jetty.plus.jaas.JAASRole . This should be specified using the  dt.gateway.http.authentication.jaas.role.class.name  property in the  dt-site.xml  configuration file as described in the section above.   Also as described in the  Jetty module   binding  section a custom callback handler is needed for the Jetty login module.  A sample configuration file with all the properties is shown below  configuration \n...\n   property \n           name dt.gateway.http.authentication.type /name \n           value jaas /value \n   /property \n   property \n          name dt.gateway.http.authentication.jaas.name /name \n          value ldap /value \n   /property \n   property   \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n         value org.eclipse.jetty.plus.jaas.JAASRole /value \n   /property \n   /property \n         name \n                dt.gateway.http.authentication.jaas.callback.class.name\n         /name \n         value \n            com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n         /value \n   /property \n... /configuration     If the system identity is used then the Jetty login module can be used as is and the jar dependencies in the step below can be specified as described in the  Jetty module   binding  section.   However, if system identity is not used something different needs to be done. An issue was discovered with the Jetty login module supplied with Jetty 8 that prevented LDAP authentication to be successful even when the user credentials were correct. DataTorrent has a fix for this and is providing the login module with the fix in a separate package called  dt-auth . The class name for the module is  com.datatorrent.auth.jetty.JettyLdapLoginModule . The  dt-auth  project along with the source can be found here  Auth . DataTorrent is working on submitting this fix back to Jetty project so that it gets back into the main source.  The JAAS configuration file as described in  LDAP  section under  Enabling JAAS Auth  should be configured to specify the ldap settings for roles. A sample configuration containing role settings for the  dt-auth  login module is shown below.   ldap {\n    com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" port=\"389\"\n    authenticationMethod=\"simple\" \n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\"cn\"\n    contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\";\n};  The  roleNameAttribute  and  roleBaseDn  settings are used to identify the role and the  userRdnAttribute  setting is used the identify the users that belong to the role. The values for these settings are dependent on attributes names are being used in your LDAP directory server.  Similar configuration can be used when the original Jetty login module is being used with the system id, the module class would be Jetty login module class and the binding settings would be specified as described in the  Jetty module   binding  section.  Refer to the  javadoc  for the role and other configuration options available with this module.    If the Jetty login module is being used as is then the path specification instructions described in the  Jetty module   binding  section can be used.   If the login module containing the DataTorrent fix, dt-auth, is being used then it's jar along with the Jetty module dependencies containing the role class, the dt-contrib jar containing the custom callback handler along with its dependencies should all be made available for Gateway. The jars can be obtained from the  DataTorrent Auth  project.  Please follow the instructions in the above url to obtain the project jar files. After obtaining the jar files they can be specified in the gateway script file as  export DT_CLASSPATH=${DT_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway as described earlier", 
            "title": "LDAP Groups"
        }, 
        {
            "location": "/dtgateway_security/#external-role-mapping", 
            "text": "External role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps    In the configuration folder typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is  admins:admin\nstaff:developer  This maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.    Restart the Gateway as described earlier", 
            "title": "External Role Mapping"
        }, 
        {
            "location": "/dtgateway_security/#administering-using-command-line", 
            "text": "You can also utilize the  dtGateway REST API  (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.", 
            "title": "Administering Using Command Line"
        }, 
        {
            "location": "/dtgateway_security/#log-in-as-admin", 
            "text": "% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login  This curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar", 
            "title": "Log in as admin:"
        }, 
        {
            "location": "/dtgateway_security/#changing-the-admin-password", 
            "text": "% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin  This uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.", 
            "title": "Changing the admin password:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-second-admin-user", 
            "text": "% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john  This command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.", 
            "title": "Adding a second admin user:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-user-in-the-developer-role", 
            "text": "% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane  This command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.", 
            "title": "Adding a user in the developer role:"
        }, 
        {
            "location": "/dtgateway_security/#listing-all-users", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users", 
            "title": "Listing all users:"
        }, 
        {
            "location": "/dtgateway_security/#getting-info-for-a-specific-user", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john  This command returns the information about the user \u201cjohn\u201d.", 
            "title": "Getting info for a specific user:"
        }, 
        {
            "location": "/dtgateway_security/#removing-a-user", 
            "text": "% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane  This command removes the user \u201cjane\u201d.", 
            "title": "Removing a user:"
        }, 
        {
            "location": "/dtgateway_security/#setting-up-ssl-keystore-in-gateway", 
            "text": "An SSL keystore is needed for your gateway to enable HTTPS in the gateway and to configure SMTP with password authentication. SMTP configuration is needed for the gateway to \nsend email alerts as described in  System Alerts .   Follow the steps below to set up the keystore.    Generate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here:  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html .\nNote down the keystore password and full path of the keystore that you need to provide in the next step.    Add two properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  (or  ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n     !-- this is the full path to the SSL keystore you created -- \n       name dt.gateway.sslKeystorePath /name \n       value {/path/to/keystore} /value \n   /property \n   property \n     !-- this is the keystore password -- \n       name dt.gateway.sslKeystorePassword /name \n       value {keystore-password} /value \n   /property \n... /configuration     Perform any additional steps required for \"Enabling HTTPS\" or \"SMTP Password Encryption\" use-case as described below.    Restart the Gateway by running  sudo service dtgateway restart  (when running Gateway in local mode use  dtgateway restart  command)", 
            "title": "Setting up SSL Keystore in Gateway"
        }, 
        {
            "location": "/dtgateway_security/#enabling-https-in-gateway", 
            "text": "To enable HTTPS in the Gateway after setting up the keystore as described above, you have to add the following\nproperty to the  dt-site.xml  configuration file       configuration \n    ...\n       property \n         name dt.attr.GATEWAY_USE_SSL /name \n         value true /value \n       /property \n    ...\n     /configuration", 
            "title": "Enabling HTTPS in Gateway"
        }, 
        {
            "location": "/dtgateway_security/#setting-up-a-key-for-smtp-password-encryption", 
            "text": "Add another key to the keystore created above using the instructions mentioned in  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html . For example:  keytool -genkey -alias smtpenc-alias -keyalg RSA -keypass  key password  -storepass  store password  -keystore gwkeystore.jks  Note down the alias you used for the key (smtpenc-alias in the above command). Remember to use the same key password and \nstore password as the ones you used in the previous command where you created the keystore.  Add the following property to  dt-site.xml  to indicate the alias to be used for SMTP Password Encryption:       configuration \n    ...\n      property \n        name dt.gateway.ssl.alias.password.encryption /name \n        value smtpenc-alias /value \n      /property \n    ...\n     /configuration", 
            "title": "Setting up a Key for SMTP Password Encryption"
        }, 
        {
            "location": "/dtgateway_systemalerts/", 
            "text": "DT Gateway System Alerts\n\n\nThe DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.\n\n\nAlerts and Topics\n\n\nAs described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.\n\n\nHere is an example for simple JavaScript condition; we can make a REST call to create an\nalert named \nxyz\n which emails to \nsomeone@company.com\n when the number of running\napplications is greater than 5 for at least 60 seconds; the disabled flag indicates whether the alert is enabled or disabled; the JSON object is the payload:\n\n\nPUT /ws/v2/systemAlerts/alerts/xyz\n\n\n{\n  \ncondition\n:\n_topic['cluster.metrics'].numAppsRunning \n 5\n,\n  \nemail\n:\nsomeone@company.com\n,\n  \ndescription\n: \nNo.of apps running is more than 5\n,\n  \ntimeThresholdMillis\n:\n60000\n,\n  \ndisabled\n:\nfalse\n\n}\n\n\n\n\nWhen the number of running applications is greater than 5 for more than 60 seconds with the alert status being enabled, a simple\nemail will be sent to \nsomeone@company.com\n, stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect. If an alert is disabled, an email will be sent to indicate the change.\n\n\nThe condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform \n_topic[\ntopic_name\n]\n.\nThe topic names can be looked up using the \nGET /ws/v2/systemAlerts/topicData\n REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as \nGet\n, \nPut\n, and \nDelete\n):\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\n{\n  \napplications.\napplicationId\n.logicalOperators\n: {...},\n  \napplications.\napplicationId\n.physicalOperators\n: {...},\n  \napplications.\napplicationId\n.containers\n: {...},\n  \napplications.\napplicationId\n: {...}\n  \ncluster.metrics\n: {...},\n  \napplications\n: {...}\n}\n\n\n\n\nwhere \napplicationId\n refers to the application id of a running application\n(which typically has the form \napplication_1482319446115_4314\n). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.\n\n\nConfiguring SMTP for Email\n\n\nSMTP needs to be configured for the gateway to be able to send alert emails via an \n\nSMTP server\n. Make\na note of the following steps before attempting to configure SMTP in the gateway.\n\n\n\n\nmake sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).\n\n\nif the SMTP server supports or requires \nTLS or SSL encryption\n,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish \ntrust\n.\n\n\ndetermine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).\n\n\nif you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.\n\n\ndetermine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.\n\n\ndetermine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.\n\n\nin case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described\n\nhere\n. The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.\n\n\n\n\nYou can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request. \n\n\nPUT /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\npassword\n: \nsecret_password\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nContents of the JSON Object:\n\n\n\n\n\n\n\n\nJSON Key\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nthe SMTP hostname\n\n\n\n\n\n\nport\n\n\nthe SMTP port on the SMTP server\n\n\n\n\n\n\nfromAddr\n\n\nthe \"from-address\" described above i.e. the address from which the alert email is sent\n\n\n\n\n\n\nfromName\n\n\nthe \"from-name\" described above i.e. the user friendly string for the \"from-address\"\n\n\n\n\n\n\nencryptionType\n\n\nthe \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"\n\n\n\n\n\n\nauthType\n\n\nthe \"authentication-type\" value described above i.e. \"password\" or \"none\"\n\n\n\n\n\n\nusername\n\n\nthe \"SMTP-username\" value described above\n\n\n\n\n\n\npassword\n\n\nthe \"SMTP-password\" value described above\n\n\n\n\n\n\ntestAddr\n\n\nthe \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.\n\n\n\n\n\n\n\n\nA note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.\n\n\nGET /ws/v2/config/smtp\n\n\n{\n\nhost\n: \nsmtp.gmail.com\n,\n\nport\n: \n587\n,\n\nfromAddr\n: \nno-reply@mydomain.com\n,\n\nfromName\n: \nDo Not Reply\n,\n\nencryptionType\n: \ntls\n,\n\nauthType\n: \npassword\n,\n\nusername\n: \nsmtp-user@mydomain.com\n,\n\ntestAddr\n: \ntestuser@yourdomain.com\n\n}\n\n\n\n\nThe GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.\n\n\nAuthorization Support for Alerts and Topics\n\n\nStarting with RTS 3.8, access to alerts will be restricted based on alert owner. A user can view and manage only his own alerts. A user with admin role will have complete access to all alerts including those of other users.\n\n\nA user will also have restricted access to topics. A user can only view and access topics for applications started by him. Access to topics \"cluster.metrics\" and \"applications\" will be restricted to users who have permission to view global configuration(VIEW_GLOBAL_CONFIG). A user with admin role will have access to all topics of all users as well as cluster.metrics and applications.\n\n\nWith the above restriction on topic data, the alert condition provided while creating system alerts can only utilize alert topics which are accessible to alert owner. Hence, a non-admin user A cannot create alerts with conditions using topic data of application(s) started by another user B.\n\n\nManaging and viewing alerts\n\n\nThe following operations are available in the Gateway REST API:\n\n\n\n\n\n\nCreate an alert\n\n\n\n\n\n\nDelete an alert\n\n\n\n\n\n\nGet alert history\n\n\n\n\n\n\nView content and status of alerts\n\n\n\n\n\n\nView all the current data in the \n_topic\n array\n\n\n\n\n\n\nCreating an alert\n\n\nTo create an alert, the user needs to specify the alert name, condition,\nemail address, duration in milliseconds and alert status using disabled flag. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.\n\n\nComplex JavaScript Expressions :\n\n\nThe following example shows how to issue a REST request to create an alert named\n\nWordCountAppNotRunning\n which emails to \nphil@company.com\n and \nmike@company.com\n\nwhen the \nWordCount\n app is not in the \nRUNNING\n state for at least 60 seconds.\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \n_topic['applications.application_1480063135007_0543']['state'] != 'RUNNING'\n,\n  \nemail\n:\nphil@company.com, mike@company.com\n,\n  \ndescription\n: \nWordCount Application is not running\n,\n  \ntimeThresholdMillis\n:\n60000\n,\n  \ndisabled\n:\nfalse\n\n}\n\n\n\n\nThe above alert works for the current invocation of the \nWordCount\n application; however,\nwhen the application is restarted, a new application \nid\n is generated for which this alert\nwill no longer work. To avoid having to update the application \nid\n in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application \nid\n for the given application as shown below:\n\n\nvar alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i \n appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;\n\n\n\n\nThe JavaScript expression must however be written as a single line; tools such as\n\njavascriptcompressor\n are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:\n\n\nPUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning\n\n\n{\n  \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert;\n,\n   \nemail\n:\nphil@company.com, mike@company.com\n,\n   \ndescription\n: \nWordCount Application is not running\n,\n   \ntimeThresholdMillis\n:\n60000\n,\n   \ndisabled\n:\nfalse\n\n}\n\n\n\n\nDeleting an alert\n\n\nWe can delete an alert with the DELETE REST API request:\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nAlerts history\n\n\nWe can get the alerts history with the GET REST API request:\n\n\nGET /ws/v2/systemAlerts/history\n\n\nIt returns a result of the following form:\n\n\n{\n\nhistory\n: [{\n    \nname\n: \nxyz\n,\n    \ninTime\n: \n1481235199598\n,\n    \noutTime\n: \n1481235251088\n,\n    \nmessage\n: \nNo.of apps running is more than 5\n\n},{\n    \nname\n: \ncheckCsvParserNotRunning\n,\n    \ninTime\n: \n1481235251087\n,\n    \noutTime\n: \n1481235549648\n,\n    \nmessage\n: \nApplication is not running\n\n}],\n}\n\n\n\n\nThe alert history comprises the alert name, time it became active (\ninTime\n), time it\nbecame inactive (\noutTime\n) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.\n\n\nViewing the content and the status of alerts\n\n\nWe can get the content and status of alerts with the GET REST API web request:\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nIt returns a result of the following form:\n\n\n{\n \nname\n: \ncheckLatencyApp\n,\n \ncondition\n: \nvar alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i\nappsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i\noperators.length);++i){if(operators[i].latencyMA\n50){alert=true;break}}}alert;\n,\n \nemail\n: \nphil@company.com, mike@company.com\n,\n \ndescription\n: \nchecking latency \n 50\n,\n \ntimeThresholdMillis\n: \n10000\n,\n \ndisabled\n:\nfalse\n,\n \nalertStatus\n: {\n   \nisInAlert\n: true,\n   \ninTime\n: \n1481264665450\n,\n   \nemailSent\n: true,\n   \nmessage\n: \nchecking latency \n 50\n\n  }\n}\n\n\n\n\nThe result includes the alert name, condition, email addresses, description, duration and enabled/disabled status.\nIt also includes alert status info such as \nisInAlert\n which indicates whether it is still active\nor not, \ninTime\n which represents the time the alert became active, \nemailSent\n which,\nas the name suggests, indicates if email was sent, and \nmessage\n which is similar to the\ndescription.\n\n\nViewing all the current data in the \n_topic\n array\n\n\nThis section is already covered under \nAlerts and Topics\n .", 
            "title": "System Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#dt-gateway-system-alerts", 
            "text": "The DT Gateway allows the user to create system alerts using JavaScript expressions.\nValues in the expressions can come from PubSub websocket topics and include values\nlike system metrics, application metrics, and custom application counters.\nAlert configurations are stored in the Hadoop cluster and therefore persist across\ngateway restarts; however, some state information about the alert (such as: whether\nit is active or not and a historical record of when it was triggered in the past)\nwill be lost when the gateway is restarted, since it is stored in memory.", 
            "title": "DT Gateway System Alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-and-topics", 
            "text": "As described above the trigger for an alert is a JavaScript expression potentially\ninvolving a variety of metrics. When the expression evaluates to true and remains so\nfor a configured duration, the alert becomes active and email is sent to a configured\nlist of addresses. Likewise, when the condition turns false, the alert becomes inactive\nand another email about the state change is sent.  Here is an example for simple JavaScript condition; we can make a REST call to create an\nalert named  xyz  which emails to  someone@company.com  when the number of running\napplications is greater than 5 for at least 60 seconds; the disabled flag indicates whether the alert is enabled or disabled; the JSON object is the payload:", 
            "title": "Alerts and Topics"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertsxyz", 
            "text": "{\n   condition : _topic['cluster.metrics'].numAppsRunning   5 ,\n   email : someone@company.com ,\n   description :  No.of apps running is more than 5 ,\n   timeThresholdMillis : 60000 ,\n   disabled : false \n}  When the number of running applications is greater than 5 for more than 60 seconds with the alert status being enabled, a simple\nemail will be sent to  someone@company.com , stating that the alert is in effect, and when\nthe number of running applications drops below 6, another email will be sent, stating that\nthe alert is no longer in effect. If an alert is disabled, an email will be sent to indicate the change.  The condition is a simple JavaScript expression which the user can build\nfrom various system or application-specific values. These values are available as fields\nof JavaScript objects representing WebSocket topics obtained with expressions of the\nform  _topic[ topic_name ] .\nThe topic names can be looked up using the  GET /ws/v2/systemAlerts/topicData  REST API\ncall shown below (please note that alert names may contain special characters such as\nspaces, slashes and percents but they must be URL-encoded before making REST API calls\nsuch as  Get ,  Put , and  Delete ):", 
            "title": "PUT /ws/v2/systemAlerts/alerts/xyz"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2systemalertstopicdata", 
            "text": "{\n   applications. applicationId .logicalOperators : {...},\n   applications. applicationId .physicalOperators : {...},\n   applications. applicationId .containers : {...},\n   applications. applicationId : {...}\n   cluster.metrics : {...},\n   applications : {...}\n}  where  applicationId  refers to the application id of a running application\n(which typically has the form  application_1482319446115_4314 ). These topics\nexist for every running application. The alert condition can refer to multiple such topic values and\ncan be any valid JavaScript expressions that return a boolean. A comma separated list of\nemail addresses can be specified.", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_systemalerts/#configuring-smtp-for-email", 
            "text": "SMTP needs to be configured for the gateway to be able to send alert emails via an  SMTP server . Make\na note of the following steps before attempting to configure SMTP in the gateway.   make sure there is an SMTP server in the network the gateway will have access to. Note down the\nSMTP server's hostname and port number (please see the note below regarding encryption-type).  if the SMTP server supports or requires  TLS or SSL encryption ,\ndetermine the encryption-type for the communication between the gateway and the SMTP server. The values for\nencryption-type are \"tls\", \"ssl\" or \"none\" (i.e. no encryption). In case \"tls\" or \"ssl\" is used,\nensure appropriate certificates are installed to establish  trust .  determine the authentication-type for your SMTP server. The only supported values are \"password\" (i.e. authentication using\nusername and password) or \"none\" (i.e. no authentication).  if you choose the \"password\" authentication-type, you will need the SMTP-username and SMTP-password to be used\nby the gateway to authenticate with the SMTP server.  determine the \"fromAddr\" and the \"fromName\" for the emails sent by the gateway which the email recipient\nwill see as the sender of the emails. Note that for certain SMTP servers the \"fromAddr\" and the SMTP-username\nmay need to match or else the latter overrides the former.  determine a \"test-address\" where you can receive emails to verify validity of the SMTP configuration.  in case you choose the \"password\" authentication-type, you will need to set up an SSL keystore as described here . The gateway uses the keystore to encrypt and\ndecrypt your SMTP-password.   You can use the following APIs to set or retrieve the SMTP configuration. Note that the JSON sample following the\nPUT request is the payload of the request.", 
            "title": "Configuring SMTP for Email"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , password :  secret_password , testAddr :  testuser@yourdomain.com \n}  Contents of the JSON Object:     JSON Key  Value      host  the SMTP hostname    port  the SMTP port on the SMTP server    fromAddr  the \"from-address\" described above i.e. the address from which the alert email is sent    fromName  the \"from-name\" described above i.e. the user friendly string for the \"from-address\"    encryptionType  the \"encryption-type\" value described above i.e. \"tls\", \"ssl\" or \"none\"    authType  the \"authentication-type\" value described above i.e. \"password\" or \"none\"    username  the \"SMTP-username\" value described above    password  the \"SMTP-password\" value described above    testAddr  the \"test-address\" value described above. The gateway sends a test email to this address when you use this API to set the SMTP configuration.     A note about password: if you omit the \"password\" value in your JSON object in the API, the gateway will use the existing saved password\nso the client does not need to include the password in subsequent API calls. Note that the GET API (described below) never returns the\npassword of the SMTP configuration, hence the DataTorrent console is able to use this feature without either displaying or requiring the \nuser to re-enter the previous password. Note that the JSON sample following the GET request is the value returned from the GET request.", 
            "title": "PUT /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#get-wsv2configsmtp", 
            "text": "{ host :  smtp.gmail.com , port :  587 , fromAddr :  no-reply@mydomain.com , fromName :  Do Not Reply , encryptionType :  tls , authType :  password , username :  smtp-user@mydomain.com , testAddr :  testuser@yourdomain.com \n}  The GET API returns the existing SMTP configuration in the gateway. As mentioned above, the SMTP-password\nvalue is never returned for security reasons.", 
            "title": "GET /ws/v2/config/smtp"
        }, 
        {
            "location": "/dtgateway_systemalerts/#authorization-support-for-alerts-and-topics", 
            "text": "Starting with RTS 3.8, access to alerts will be restricted based on alert owner. A user can view and manage only his own alerts. A user with admin role will have complete access to all alerts including those of other users.  A user will also have restricted access to topics. A user can only view and access topics for applications started by him. Access to topics \"cluster.metrics\" and \"applications\" will be restricted to users who have permission to view global configuration(VIEW_GLOBAL_CONFIG). A user with admin role will have access to all topics of all users as well as cluster.metrics and applications.  With the above restriction on topic data, the alert condition provided while creating system alerts can only utilize alert topics which are accessible to alert owner. Hence, a non-admin user A cannot create alerts with conditions using topic data of application(s) started by another user B.", 
            "title": "Authorization Support for Alerts and Topics"
        }, 
        {
            "location": "/dtgateway_systemalerts/#managing-and-viewing-alerts", 
            "text": "The following operations are available in the Gateway REST API:    Create an alert    Delete an alert    Get alert history    View content and status of alerts    View all the current data in the  _topic  array", 
            "title": "Managing and viewing alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#creating-an-alert", 
            "text": "To create an alert, the user needs to specify the alert name, condition,\nemail address, duration in milliseconds and alert status using disabled flag. As explained above, the condition\ncan refer to values in various topic objects including system metrics,\napplication metrics, and custom application counters and must yield a Boolean\nvalue.  Complex JavaScript Expressions :  The following example shows how to issue a REST request to create an alert named WordCountAppNotRunning  which emails to  phil@company.com  and  mike@company.com \nwhen the  WordCount  app is not in the  RUNNING  state for at least 60 seconds.", 
            "title": "Creating an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning", 
            "text": "{\n   condition :  _topic['applications.application_1480063135007_0543']['state'] != 'RUNNING' ,\n   email : phil@company.com, mike@company.com ,\n   description :  WordCount Application is not running ,\n   timeThresholdMillis : 60000 ,\n   disabled : false \n}  The above alert works for the current invocation of the  WordCount  application; however,\nwhen the application is restarted, a new application  id  is generated for which this alert\nwill no longer work. To avoid having to update the application  id  in the condition each\ntime the application restarts, we can write a more complex JavaScript expression to find\nthe application  id  for the given application as shown below:  var alert = false;\nvar appId = undefined;\nvar appsInfo = _topic['applications'].apps;\n\nfor(i = 0; i   appsInfo.length; i++)\n{\n    if (appsInfo[i].name == 'WordCount')\n    {\n        appId = appsInfo[i].id;\n        break;\n    }\n}\nif (appId != undefined)\n{\n    alert  = _topic['applications.' + appId]['state']  !=  'RUNNING' ;\n}\nalert;  The JavaScript expression must however be written as a single line; tools such as javascriptcompressor  are useful for this purpose.\nAny HTML in the expression also needs to be escaped. Here is new alert\nwith the revised complex expression compressed to a single line:", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#put-wsv2systemalertsalertswordcountappnotrunning_1", 
            "text": "{\n   condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='WordCount'){appId=appsInfo[i].id;break}}if(appId!=undefined){alert=_topic['applications.'+appId]['state']!='RUNNING'}alert; ,\n    email : phil@company.com, mike@company.com ,\n    description :  WordCount Application is not running ,\n    timeThresholdMillis : 60000 ,\n    disabled : false \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/WordCountAppNotRunning"
        }, 
        {
            "location": "/dtgateway_systemalerts/#deleting-an-alert", 
            "text": "We can delete an alert with the DELETE REST API request:  DELETE /ws/v2/systemAlerts/alerts/{name}", 
            "title": "Deleting an alert"
        }, 
        {
            "location": "/dtgateway_systemalerts/#alerts-history", 
            "text": "We can get the alerts history with the GET REST API request:  GET /ws/v2/systemAlerts/history  It returns a result of the following form:  { history : [{\n     name :  xyz ,\n     inTime :  1481235199598 ,\n     outTime :  1481235251088 ,\n     message :  No.of apps running is more than 5 \n},{\n     name :  checkCsvParserNotRunning ,\n     inTime :  1481235251087 ,\n     outTime :  1481235549648 ,\n     message :  Application is not running \n}],\n}  The alert history comprises the alert name, time it became active ( inTime ), time it\nbecame inactive ( outTime ) and message. The alert history is obtained through the gateway,\nso whenever gateway is restarted the alerts history gets cleared.", 
            "title": "Alerts history"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-the-content-and-the-status-of-alerts", 
            "text": "We can get the content and status of alerts with the GET REST API web request:  GET /ws/v2/systemAlerts/alerts/{name}  It returns a result of the following form:  {\n  name :  checkLatencyApp ,\n  condition :  var alert=false;var appId=undefined;var appsInfo=_topic['applications'].apps;for(i=0;i appsInfo.length;i++){if(appsInfo[i].name=='xyzApp'){appId=appsInfo[i].id;break}}if(appId!=undefined){var expTopic='applications.'+appId+'.physicalOperators';var operators=_topic[expTopic].operators;for(i=0;(i operators.length);++i){if(operators[i].latencyMA 50){alert=true;break}}}alert; ,\n  email :  phil@company.com, mike@company.com ,\n  description :  checking latency   50 ,\n  timeThresholdMillis :  10000 ,\n  disabled : false ,\n  alertStatus : {\n    isInAlert : true,\n    inTime :  1481264665450 ,\n    emailSent : true,\n    message :  checking latency   50 \n  }\n}  The result includes the alert name, condition, email addresses, description, duration and enabled/disabled status.\nIt also includes alert status info such as  isInAlert  which indicates whether it is still active\nor not,  inTime  which represents the time the alert became active,  emailSent  which,\nas the name suggests, indicates if email was sent, and  message  which is similar to the\ndescription.", 
            "title": "Viewing the content and the status of alerts"
        }, 
        {
            "location": "/dtgateway_systemalerts/#viewing-all-the-current-data-in-the-_topic-array", 
            "text": "This section is already covered under  Alerts and Topics  .", 
            "title": "Viewing all the current data in the _topic array"
        }, 
        {
            "location": "/apexcli/", 
            "text": "Apache Apex Command Line Interface\n\n\nApex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications.\n\nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of Apex CLI is to provide scope, by connecting and executing commands in a context\nof specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.\n\n\n\n\nLaunch or kill applications\n\n\nView system metrics including load, throughput, latency, etc.\n\n\nStart or stop tuple recording\n\n\nRead operator, stream, port properties and attributes\n\n\nWrite to operator properties\n\n\nDynamically change the application logical plan\n\n\nCreate custom macros\n\n\n\n\nApex CLI Commands\n\n\nApex CLI can be launched by running following command:\n\n\napex\n\n\n\nHelp on all commands is available via \u201chelp\u201d command in the CLI\n\n\nGlobal Commands\n\n\nGLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf \napp package configuration file\n        Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives \ncomma separated list of archives\n    Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf \nconfiguration file\n                      Specify an\n                                                            application\n                                                            configuration file.\n            -D \nproperty=value\n                             Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files \ncomma separated list of files\n          Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars \ncomma separated list of libjars\n      Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId \napplication id\n                 Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue \nqueue name\n                             Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file\n\n\n\n\nCommands after connecting to an application\n\n\nCOMMANDS WHEN CONNECTED TO AN APP (via connect \nappid\n) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName \nproperty name\n    The name of the property whose\n                                             value needs to be retrieved\n            -waitTime \nwait time\n            How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\nget-container-stacktrace container-id\n    Shows the stack trace of all the threads in the container\n\n\n\n\nCommands when changing the logical plan\n\n\nCOMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change\n\n\n\n\nExamples\n\n\nAn example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.\n\n\napex\n begin-macro add-console-output\nmacro\n begin-logical-plan-change\nmacro\n create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro\n create-stream stream_$1 $2 $3 $1 in\nmacro\n submit\n\n\n\n\nThen execute the \nadd-console-output\n macro like this\n\n\napex\n add-console-output xyz opername portname\n\n\n\n\nThis macro then expands to run the following command\n\n\nbegin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit\n\n\n\n\nNote\n:  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "Apex CLI"
        }, 
        {
            "location": "/apexcli/#apache-apex-command-line-interface", 
            "text": "Apex CLI, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications. \nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of Apex CLI is to provide scope, by connecting and executing commands in a context\nof specific application.  Apex CLI enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.   Launch or kill applications  View system metrics including load, throughput, latency, etc.  Start or stop tuple recording  Read operator, stream, port properties and attributes  Write to operator properties  Dynamically change the application logical plan  Create custom macros", 
            "title": "Apache Apex Command Line Interface"
        }, 
        {
            "location": "/apexcli/#apex-cli-commands", 
            "text": "Apex CLI can be launched by running following command:  apex  Help on all commands is available via \u201chelp\u201d command in the CLI", 
            "title": "Apex CLI Commands"
        }, 
        {
            "location": "/apexcli/#global-commands", 
            "text": "GLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf  app package configuration file         Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives  comma separated list of archives     Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf  configuration file                       Specify an\n                                                            application\n                                                            configuration file.\n            -D  property=value                              Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files  comma separated list of files           Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars  comma separated list of libjars       Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId  application id                  Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue  queue name                              Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file", 
            "title": "Global Commands"
        }, 
        {
            "location": "/apexcli/#commands-after-connecting-to-an-application", 
            "text": "COMMANDS WHEN CONNECTED TO AN APP (via connect  appid ) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName  property name     The name of the property whose\n                                             value needs to be retrieved\n            -waitTime  wait time             How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\nget-container-stacktrace container-id\n    Shows the stack trace of all the threads in the container", 
            "title": "Commands after connecting to an application"
        }, 
        {
            "location": "/apexcli/#commands-when-changing-the-logical-plan", 
            "text": "COMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change", 
            "title": "Commands when changing the logical plan"
        }, 
        {
            "location": "/apexcli/#examples", 
            "text": "An example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.  apex  begin-macro add-console-output\nmacro  begin-logical-plan-change\nmacro  create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro  create-stream stream_$1 $2 $3 $1 in\nmacro  submit  Then execute the  add-console-output  macro like this  apex  add-console-output xyz opername portname  This macro then expands to run the following command  begin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit  Note :  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "Examples"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "===============================\n\n\nOnline Analytics Service (OAS)\n\n\nHandling High Ingestion Rate in OAS\n\n\nUsually, OAS consumes the Kafka topic data as soon as it is available from upstream. However, if it cannot cope with the incoming rate, there can be failures in the \nInput\n operator. \nTo avoid such issues, the following approaches are suggested:\n\n\n\n\nOlapParser Operator partitioning\n\n\n\n\nOlapParser operator can be partitioned, if the ingestion rate is very high. For example,  for creating 4 partitions, the property      \ndt.operator.OlapParser.attr.PARTITIONER\n can be used with value as \ncom.datatorrent.common.partitioner.StatelessPartitioner:4\n\n\n\n\nIncrease Retention period for kafka topic\n\n\n\n\nIf OAS is overloaded and not processing the data at the same rate as upstream, the retention period for the kafka topic can be increased. This gives sufficient time for OAS to process all the topic data.\n\n\n\n\nSpecify 'auto.offset.reset' consumer property\n\n\n\n\nThere can be cases where OAS is unable to keep pace with the upstream and the older data in Kafka topic gets expired because of  the  retention policy that is set before getting processed by OAS. In such cases the OAS \nInput\n operator may fail. To avoid this failure, the consumer property \ndt.operator.Input.prop.consumerProps(auto.offset.reset)\n can be set in OAS with value as \nearliest\n. With this property, in case of older topic data expiry, the offset used for reading the data is \nearliest\n  that is whichever oldest offset that is currently available with the topic. This avoids the Input operator failure but also involves some loss of data.\n\nCaution\n: This is not the recommended approach,  as it may result in data loss without any notification.\n\n\nDownload\n\n\nWhere can I get DataTorrent RTS software?\n\n\nDataTorrent RTS software can be downloaded from \nhttps://www.datatorrent.com/download/\n\n\nThe following deployment options are available for downloading DataTorrent RTS:\n- \nDataTorrent RTS - Sandbox Appliance\n\n- \nDataTorrent RTS - Installable Binary\n\n- \nDataTorrent RTS - Cloud Instance\n\n\nWhat are the DT licenses that can be obtained with subscription?\n\n\nRefer to \nhttp://docs.datatorrent.com/Licensing/#datatorrent-licensing\n\n\nHow do I confirm the package downloaded correctly?\n\n\nYou can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:\n\n\n# md5sum \nDT_RTS_Package\n\n\n\n\nCompare the result with MD5 sum posted on the product download page.\n\n\nHow do I download the DataTorrent RTS package using CLI?\n\n\nUse following curl command to download DataTorrent RTS package:\n\n\ncurl -LSO\u00a0\nDT_RTS_download_link\n\n\n\n\nWe recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.\n\n\nWhat are the prerequisites of DataTorrent RTS ?\n\n\nDataTorrent RTS platform has following Hadoop cluster requirements:\n\n\n\n\nOperating system supported by Hadoop distribution\n\n\nHadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable\n\n\nJava 7 or 8 as supported by Hadoop distribution\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\nGoogle Chrome or Safari to access dtManage (DataTorrent UI\n    console)\n\n\n\n\nWhere can I start from after downloading DataTorrent RTS?\n\n\n\n\nAfter successful download of DataTorrent RTS, make sure all prerequisites are satisfied.\n\n\nYou will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to \ninstallation guide\n\n\nOnce installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.\n\n\n\n\nWhat are the supported Hadoop distribution by DataTorrent RTS?\n\n\nDataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nHadoop Distribution\n\n\nSupported Version\n\n\n\n\n\n\nAmazon EMR\n\n\nHadoop 2.4 and higher\n\n\n\n\n\n\nApache Hadoop\n\n\nHadoop 2.2 and higher\n\n\n\n\n\n\nCloudera\n\n\nCDH 5.0 and higher\n\n\n\n\n\n\nHortonworks\n\n\nHDP 2.0 and higher\n\n\n\n\n\n\nMapR\n\n\n4.0 and higher\n\n\n\n\n\n\nMicrosoft\n\n\nHDInsight\n\n\n\n\n\n\nPivotal\n\n\n2.1 and higher\n\n\n\n\n\n\n\n\n\nWhat is the Datatorrent Sandbox?\n\n\nDataTorrent Sandbox is a deployment option that provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The latest version of DataTorrent RTS is pre-installed on it along with all the Hadoop services required to launch and run the included demo applications. See also http://docs.datatorrent.com/sandbox/\n\n\nWhere do I get DataTorrent Sandbox download link?\n\n\nSandbox can be downloaded from \ndatatorrent.com/download\n\n\nWhat are the system requirements for sandbox deployment?\n\n\nThe system requirements for Sandbox deployment are as follows:\n-  \nVirtualBox\n 4.3 or greater installed.\n-  6GB RAM or greater available for Sandbox VM.\n\n\nWhat are the DataTorrent RTS package content details in sandbox?\n\n\n\n\nUbuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)\n\n\nLubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)\n\n\nApache Apex, Apache Apex-Malhar\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtDashboard\n\n\nDemo Applications\n\n\n\n\nWhy does the browser console on the sandbox say \nHDFS Not Ready\n ?\n\n\nThe HDFS services take a few minutes to start; the console needs all of\nthose services to be up and running and until that occurs, it displays this\nwarning. If the normal console does not appear after a few minutes, please run\nthe \njps\n command to see which services may \nnot\n be running, for example:\n\n\ndtadmin@dtbox:~/tmp$ jps\n1407 DTGateway\n4145 Jps\n2830 NodeManager\n3059 ResourceManager\n2650 NameNode\n\n\n\n\nHere we see that the \nDataNode\n is not running. In this case, stop all\nHDFS services (using, for example the script shown in the\n\nsandbox page\n. Then, remove everything\nunder these directories:\n\n\n/sfw/hadoop/shared/data/hdfs/datanode/current\n/sfw/hadoop/shared/data/hdfs/namenode/current\n\n\n\nNow reformat HDFS with \nhdfs namenode -format\n\nand finally, restart all HDFS services.\n\n\nIf all services are running and the normal console still does not appear,\nplease run following commands:\n\n\nhdfs dfsadmin -safemode leave\nhdfs fsck -delete\n\n\n\n\nIf HDFS detects that some files are corrupted (perhaps due to an earlier improper shutdown)\nit will not exit the initial safemode automatically;\nthe commands above exit safemode manually and delete corrupted files.\n\n\nHow do I get specific DT version ?\n\n\nYou can find archive list of various DataTorrent RTS versions at the bottom of each product download page.\n\n\nWhere can I request new / upgrade current license?\n\n\nPlease follow the instructions at \nLicense Upgrade\n\n\nWhere do I find product documentation?\n\n\nPlease refer to: \nDataTorrent Documentation\n\n\nWhere can I learn more about Apache Apex?\n\n\nYou can refer Apex page for more details: \nApache Apex\n\n\nDo you need help?\n\n\nYou can contact us at \nhttps://www.datatorrent.com/contact\n\n\nInstallation\n\n\nMinimum hardware requirements, what happens if certain minimum configuration requirement has not been met?\n\n\nMinimum of 8G RAM is required on the Hadoop cluster.\n\n\nWhat happens if java is not installed?\n\n\nFollowing message can be seen when Java is not available on the system\n\n\nError: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.\n\n\n\nInstall Java 7 from package manager of Linux Distribution and try running installer again.\n\n\nWhat happens if Hadoop is not installed?\n\n\nInstallation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect Hadoop binary (/usr/bin/hadoop) \n DFS location.\n\n\n\n\nInstall Hadoop > 2.2.0 and update the configuration parameters above.\n\n\nHow do I check if Hadoop is installed and running correctly?\n\n\nFollowing commands can be used to confirm installed Hadoop version and if Hadoop services are running.\n\n\n$ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager\n\n\n\nWhat happens if the downloaded file is corrupted?\n\n\nMD5 checksum will result in the following error:\n\n\n\u201cVerifying archive integrity...Error in MD5 checksums: \nMD5 checksum\n is different from \nMD5 checksum\n\u201d.\n\n\n\nDownloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use \ncurl\n instead of \nwget\n.\n\n\nWhy do I see the following permissions errors?\n\n\nDuring installation following error message will be seen on screen\n\n\n\n\nThese typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:\n\n\n$ hadoop dfs -ls /user/\nUSER\n/datatorrent\n$ hadoop dfs -mkdir /user/\nUSER\n/datatorrent  \n$ hadoop dfs -chown \nUSER\n /user/\nUSER\n/datatorrent\n\n\n\nUpgrade\n\n\nLicense agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0.\n\n\nIf your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See \ninstallation guide\n for details.\n\n\nConfiguration\n\n\nConfiguring Memory\n\n\nConfiguring Operator Memory:\n\n\nOperator memory for an operator can be configured in one of the following two ways:\n\n\n1 Using the same default values for all the operators: \n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.attr.MEMORY_MB\n/name\n\n  \nvalue\n2048\n/value\n\n\n/property\n\n\n\n\nThis would set 2GB as size of all the operators in the given application.\n\n\n2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator \nOp\n.\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.attr.MEMORY_MB\n/name\n\n  \nvalue\n8192\n/value\n\n\n/property\n\n\n\n\nThe amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.\n\n\nConfiguring Buffer Server Memory:\n\n\nThere is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:\n\n\n1 Using the same default values for all the output ports of all the operators\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n128\n/value\n\n\n/property\n\n\n\n\nThis sets 128Mb as buffer memory for all the output ports of all the operators.\n\n\n2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port \np\n of an operator \nOp\n:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.port.p.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n1024\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 512 MB\n\n\nCalculating Container memory:\n\n\nFollowing formula is used to calculate the container memory.\n\n\nContainer Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.\n\n\n\nSometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between\n\n\n[yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]\n\n\n\nThese values can be found in yarn configuration\n\n\nConfiguring Application Master Memory:\n\n\nApplication Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.attr.MASTER_MEMORY_MB\n/name\n\n  \nvalue\n4096\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers\n\n\nDevelopment\n\n\nHadoop dependencies conflicts\n\n\nYou have to make sure that the Hadoop jars are not bundled with the application package o/w they may conflict with the versions available in Hadoop classpath. Here are some of the ways to exclude Hadoop dependencies from the application package\n\n\n\n\n\n\nIf your application is directly dependent on the Hadoop jars, make sure that the scope of the dependency is \nprovided\n. For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml\n\n\ndependency\n\n  \ngroupId\norg.apache.hadoop\n/groupId\n\n  \nartifactId\nhadoop-common\n/artifactId\n\n  \nversion\n2.2.0\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\n\n\n\n\n\nIf your application has transitive dependency on Hadoop jars, make sure that Hadoop jars are excluded from the transitive dependency and added back as application dependency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows\n\n\ndependency\n\n  \ngroupId\n/groupId\n\n  \nartifactId\n/artifactId\n\n  \nversion\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\norg.apache.hadoop\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\n\n\n\n\n\n\nSerialization considerations\n\n\nAn Apex application needs to satisfy serializability requirements on operators and tuples as described below.\n\n\nOperators\n\n\nAfter an application is launched, the DAG is serialized using a combination of\n\nJava Serialization\n and\n\nKryo\n and then the DAG is\ntransferred over the network from the launching node to the application master node.\n\n\nCheckpointing also involves serializing and persisting an operator state to a store and deserializing \nfrom the store in case of recovery. The platform uses Kryo serialization in this case. Kryo imposes\nadditional requirements on an operator Java class to be de-serializable. For more details check out\nthis \npage\n.\n\n\nTuples\n\n\nTuples are serialized (and deserialized) according to the specified stream codec when transmitted between Yarn containers.\nWhen no stream codec is specified, Apex uses the default stream codec that relies on the \n\nKryo\n serialization library to\nserialize and deserialize tuples. A custom stream codec can be specified to use a different serialization\nframework.\n\n\nThread and container local streams don't use a stream codec, so tuples don't need to be serializable in such cases.\n\n\nTroubleshooting serialization issues\n\n\nThere is no guaranteed way to uncover serialization issues in your code. An operator may emit a problematic tuple\nonly in very rare and hard to reproduce conditions while testing. Kryo deserialization problem in an operator will \nnot be uncovered until the recovery time, and at that point it is most likely too late. It is recommended to unit\ntest an operator's ability to restore itself properly similar to this \n\nexample\n\n\nTo exercise tuple serialization, run your application in \n\nlocal mode\n that could uncover many tuple\nserialization problems. Use the \nApexCLI\n to launch your application with\nthe \n-local\n option to run it in local mode. The application will fail at a point when the platform is unable to serialize\nor deserialize a tuple,and the relevant exception will be logged on the console or a log file as described in the \n\nKryo exception\n section. Check out that section further for\nhints about troubleshooting serialization issues.\n\n\nTransient members\n\n\nCertain data members of an operator do not need to be serialized or deserialized during deployment or \ncheckpointing/recovery because they are \ntransient\n\nin nature and do not represent stateful data. Developers should judiciously use the \n\ntransient\n keyword for declaring\nsuch non-stateful members of operators (or members of objects that are indirectly members of operators) \nso that the platform skips serialization of such members and serialization/deserialization errors are \nminimized. Transient members are further described in the context of the operator life-cycle \n\nhere\n. Typical examples of\ntransient data members are database or network connection objects which need to be \ninitialized before they are used in a process, so they are never persisted across process invocations.\n\n\nGetting this message in STRAM logs. Is anything wrong in my code?\n\n\n2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)\n\n\n\nComing soon.\n\n\nDebugging\n\n\nHow to remote debug gateway service?\n\n\nUpdate Hadoop OPTS variable by running,\n\n\nexport HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\"\n\n\n\nHow to setup DEBUG level while running an application?\n\n\nAdd the following property to your properties file:\n\n\nproperty\n\n  \nname\ndt.application.\nAPP-NAME\n.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nMy gateway is throwing the following exception.\n\n\n  ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...\n\n\n\nCheck if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.\n\n\nWhen Apex is running in secure mode, YARN logs get flooded with several thousand messages per second.\n\n\nPlease make sure that the kerberos principal user name has an account with the\nsame user id on the cluster nodes.\n\n\nApplication throwing following Kryo exception.\n\n\n  com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):\n\n\n\nThis means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception\n\n\n\n\nAdd default constructor to the type in question.\n\n\n\n\nUsing \ncustom serializer\n for the type in question. Some existing alternative serializers can be found at \nhttps://github.com/magro/kryo-serializers\n. A custom serializer can be used as follows:\n\n\n2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.\n\n\n@FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType\n\n\n\nKryo will use this CustomSerializer to serialize and deserialize type SomeType. If\nSomeType is a Map or Collection, there are some special annotations @BindMap and\n@BindCollection; please see \nhere\n.\n\n\n2.2 Using the @DefaultSerializer annotation on the class, for example:\n\n\n@DefaultSerializer(JavaSerializer.class)\npublic class SomeClass ...\n\n\n\n2.3 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:\n\n\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec\nT\n extends com.datatorrent.lib.codec.KryoSerializableStreamCodec\nT\n\n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}\n\n\n\nLet's say there is an Operator \nCustomOperator\n with an input port \ninput\n that expects type SomeType. Following is how to use above defined custom stream codec\n\n\nCustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec\nSomeType\n codec = new CustomSerializableStreamCodec\nSomeType\n();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);\n\n\n\nThis works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways. \n\n\n\n\n\n\nLog analysis\n\n\nThere are multiple ways to adjust logging levels.  For details see \nconfiguration guide\n.\n\n\nHow to check STRAM logs\n\n\nYou can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.  \n\n\nIn dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.  \n\n\nAlternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.\n\n\nyarn logs -applicationId \napplicationId\n\n\n\n\nHow to check application logs\n\n\nOn dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.\n\n\n\n\nHow to check killed operator\u2019s state\n\n\nOn dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.\n\n\n\n\nHow to search for particular any application or container?\n\n\nIn applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.\n\n\nHow do I search within logs?\n\n\nOnce you navigate to the logs page,  \n\n\n\n\nDownload log file to search using your preferred editor  \n\n\nuse \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d\n\n\n\n\nLaunching Applications\n\n\nApplication goes from accepted state to Finished(FAILED) state\n\n\nCheck if your application name conflicts with any of the already running\napplications in your cluster. Apex does not allow two applications with\nthe same name to run simultaneously.\nYour STRAM logs will have following error:\n\n\u201cForced shutdown due to Application master failed due to application\n\\\nappId> with duplicate application name \\\nappName> by the same user\n\\\nuser name> is already started.\u201d  \n\n\nConstraintViolationException during application launch\n\n\nCheck if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException. \u00a0  \n\n\nEvents\n\n\nHow to check container failures\n\n\nIn StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.\n\n\nHow to search within events\n\n\nYou can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.\n\n\ntail vs range mode\n\n\ntail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.\n\n\nWhat is \u201cfollowing\u201d button in events pane\n\n\nWhen we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.\n\n\nHow do I get a heap dump when a container gets an OutOfMemoryError ?\n\n\nThe JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely \n-XX:+HeapDumpOnOutOfMemoryError\n and \n-XX:HeapDumpPath=/tmp/op.heapdump\n.\nTo add them to a specific operator, use this stanza in your configuration file\nwith \nOPERATOR_NAME\n replaced by the actual name of an operator:\n\n\n    \nproperty\n\n      \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nTo add them to all your containers, add this stanza to your configuration file:\n\n\n    \nproperty\n\n      \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nWith these options, when an \nOutOfMemoryError\n occurs, the JVM writes the heap dump to the\nfile \n/tmp/op.heapdump\n; you'll then need to retrieve the file from the node on which the\noperator was running.\n\n\nYou can use the tool \njmap\n (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe \n-F\n option; here is a sample invocation on the sandbox:\n\n\ndtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created\n\n\n\nThe heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as\n\njhat\n or\n\nMAT\n to analyze it.\n\n\nThe former (\njhat\n) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:\n\n\ntmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.\n\n\n\nIt is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available\n\nhere\n.\n\n\nComing Soon\n\n\n\n\nConnection Refused Exception\n\n\nClassNotFound Exception\n\n\nLaunching apa vs jar\n\n\nDAG validation failed\n\n\nMultiple gateways running simultaneously, app not launched.\n\n\nHDFS in safe mode\n\n\nApplication stays in accepted state\n\n\nSome containers do not get resources (specially in case of repartition)\n\n\nInsufficient memory set to operator causes operator kill continuously.\n\n\n\n\nWhy is the number of events same/different at input and output port of each operator?\n\n\n\n\n\n\nShutdown vs kill option\n\n\n\n\nWhy shutdown doesn\u2019t work? (if some containers are not running)\n\n\nCan I kill multiple applications at same time?\n\n\nKilling containers vs killing application\n\n\nSTRAM failures (during define partitions)\n\n\nThread local + partition parallel configuration\n\n\nWhat to do when downstream operators are slow than the input  operators.\n\n\nI am seeing high latency, what to do?\n\n\nconf option in Apex CLI\n\n\nApplication keeps restarting (has happened once due to license agent during upgrade)\n\n\nOperator getting killed after every 60 secs (Timeout issue)\n\n\nHow to change commit frequency\n\n\nDifference between exactly once, at least once and at most once\n\n\nThread local vs container local vs node local\n\n\nCluster nodes not able to access edge node where Gateway is running\n\n\n\n\nDevelopers not sure when to process incoming tuples in end window or when to do it in process function of operator\n\n\n\n\n\n\nHow partitioning works\n\n\n\n\nHow the data is partitioned between different partitions.\n\n\nHow to use stream-codec\n\n\nData on which ports is partitioned? By default default partitioner partitions data on first port.\n\n\nHow to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).\n\n\n\n\n\n\n\n\npom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes Hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc\n\n\n\n\n\n\nExactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.\n\n\n\n\n\n\nHow to check checkpoint size. (large checkpoint size cause instability in the DAG).\n\n\n\n\nHow to add custom metrics and metric aggregator.\n\n\nExample of how to implement dynamic partitioning.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#online-analytics-service-oas", 
            "text": "", 
            "title": "Online Analytics Service (OAS)"
        }, 
        {
            "location": "/troubleshooting/#handling-high-ingestion-rate-in-oas", 
            "text": "Usually, OAS consumes the Kafka topic data as soon as it is available from upstream. However, if it cannot cope with the incoming rate, there can be failures in the  Input  operator. \nTo avoid such issues, the following approaches are suggested:   OlapParser Operator partitioning   OlapParser operator can be partitioned, if the ingestion rate is very high. For example,  for creating 4 partitions, the property       dt.operator.OlapParser.attr.PARTITIONER  can be used with value as  com.datatorrent.common.partitioner.StatelessPartitioner:4   Increase Retention period for kafka topic   If OAS is overloaded and not processing the data at the same rate as upstream, the retention period for the kafka topic can be increased. This gives sufficient time for OAS to process all the topic data.   Specify 'auto.offset.reset' consumer property   There can be cases where OAS is unable to keep pace with the upstream and the older data in Kafka topic gets expired because of  the  retention policy that is set before getting processed by OAS. In such cases the OAS  Input  operator may fail. To avoid this failure, the consumer property  dt.operator.Input.prop.consumerProps(auto.offset.reset)  can be set in OAS with value as  earliest . With this property, in case of older topic data expiry, the offset used for reading the data is  earliest   that is whichever oldest offset that is currently available with the topic. This avoids the Input operator failure but also involves some loss of data. Caution : This is not the recommended approach,  as it may result in data loss without any notification.", 
            "title": "Handling High Ingestion Rate in OAS"
        }, 
        {
            "location": "/troubleshooting/#download", 
            "text": "", 
            "title": "Download"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-get-datatorrent-rts-software", 
            "text": "DataTorrent RTS software can be downloaded from  https://www.datatorrent.com/download/  The following deployment options are available for downloading DataTorrent RTS:\n-  DataTorrent RTS - Sandbox Appliance \n-  DataTorrent RTS - Installable Binary \n-  DataTorrent RTS - Cloud Instance", 
            "title": "Where can I get DataTorrent RTS software?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-dt-licenses-that-can-be-obtained-with-subscription", 
            "text": "Refer to  http://docs.datatorrent.com/Licensing/#datatorrent-licensing", 
            "title": "What are the DT licenses that can be obtained with subscription?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-confirm-the-package-downloaded-correctly", 
            "text": "You can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:  # md5sum  DT_RTS_Package   Compare the result with MD5 sum posted on the product download page.", 
            "title": "How do I confirm the package downloaded correctly?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-download-the-datatorrent-rts-package-using-cli", 
            "text": "Use following curl command to download DataTorrent RTS package:  curl -LSO\u00a0 DT_RTS_download_link   We recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.", 
            "title": "How do I download the DataTorrent RTS package using CLI?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-prerequisites-of-datatorrent-rts", 
            "text": "DataTorrent RTS platform has following Hadoop cluster requirements:   Operating system supported by Hadoop distribution  Hadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable  Java 7 or 8 as supported by Hadoop distribution  Minimum of 8G RAM available on the Hadoop cluster  Permissions to create HDFS directory for DataTorrent user  Google Chrome or Safari to access dtManage (DataTorrent UI\n    console)", 
            "title": "What are the prerequisites of DataTorrent RTS ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-start-from-after-downloading-datatorrent-rts", 
            "text": "After successful download of DataTorrent RTS, make sure all prerequisites are satisfied.  You will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to  installation guide  Once installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.", 
            "title": "Where can I start from after downloading DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-supported-hadoop-distribution-by-datatorrent-rts", 
            "text": "DataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.       Hadoop Distribution  Supported Version    Amazon EMR  Hadoop 2.4 and higher    Apache Hadoop  Hadoop 2.2 and higher    Cloudera  CDH 5.0 and higher    Hortonworks  HDP 2.0 and higher    MapR  4.0 and higher    Microsoft  HDInsight    Pivotal  2.1 and higher", 
            "title": "What are the supported Hadoop distribution by DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-is-the-datatorrent-sandbox", 
            "text": "DataTorrent Sandbox is a deployment option that provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The latest version of DataTorrent RTS is pre-installed on it along with all the Hadoop services required to launch and run the included demo applications. See also http://docs.datatorrent.com/sandbox/", 
            "title": "What is the Datatorrent Sandbox?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-get-datatorrent-sandbox-download-link", 
            "text": "Sandbox can be downloaded from  datatorrent.com/download", 
            "title": "Where do I get DataTorrent Sandbox download link?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-system-requirements-for-sandbox-deployment", 
            "text": "The system requirements for Sandbox deployment are as follows:\n-   VirtualBox  4.3 or greater installed.\n-  6GB RAM or greater available for Sandbox VM.", 
            "title": "What are the system requirements for sandbox deployment?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-datatorrent-rts-package-content-details-in-sandbox", 
            "text": "Ubuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)  Lubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)  Apache Apex, Apache Apex-Malhar  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtDashboard  Demo Applications", 
            "title": "What are the DataTorrent RTS package content details in sandbox?"
        }, 
        {
            "location": "/troubleshooting/#why-does-the-browser-console-on-the-sandbox-say-hdfs-not-ready", 
            "text": "The HDFS services take a few minutes to start; the console needs all of\nthose services to be up and running and until that occurs, it displays this\nwarning. If the normal console does not appear after a few minutes, please run\nthe  jps  command to see which services may  not  be running, for example:  dtadmin@dtbox:~/tmp$ jps\n1407 DTGateway\n4145 Jps\n2830 NodeManager\n3059 ResourceManager\n2650 NameNode  Here we see that the  DataNode  is not running. In this case, stop all\nHDFS services (using, for example the script shown in the sandbox page . Then, remove everything\nunder these directories:  /sfw/hadoop/shared/data/hdfs/datanode/current\n/sfw/hadoop/shared/data/hdfs/namenode/current  Now reformat HDFS with  hdfs namenode -format \nand finally, restart all HDFS services.  If all services are running and the normal console still does not appear,\nplease run following commands:  hdfs dfsadmin -safemode leave\nhdfs fsck -delete  If HDFS detects that some files are corrupted (perhaps due to an earlier improper shutdown)\nit will not exit the initial safemode automatically;\nthe commands above exit safemode manually and delete corrupted files.", 
            "title": "Why does the browser console on the sandbox say HDFS Not Ready ?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-specific-dt-version", 
            "text": "You can find archive list of various DataTorrent RTS versions at the bottom of each product download page.", 
            "title": "How do I get specific DT version ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-request-new-upgrade-current-license", 
            "text": "Please follow the instructions at  License Upgrade", 
            "title": "Where can I request new / upgrade current license?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-find-product-documentation", 
            "text": "Please refer to:  DataTorrent Documentation", 
            "title": "Where do I find product documentation?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-learn-more-about-apache-apex", 
            "text": "You can refer Apex page for more details:  Apache Apex", 
            "title": "Where can I learn more about Apache Apex?"
        }, 
        {
            "location": "/troubleshooting/#do-you-need-help", 
            "text": "You can contact us at  https://www.datatorrent.com/contact", 
            "title": "Do you need help?"
        }, 
        {
            "location": "/troubleshooting/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/troubleshooting/#minimum-hardware-requirements-what-happens-if-certain-minimum-configuration-requirement-has-not-been-met", 
            "text": "Minimum of 8G RAM is required on the Hadoop cluster.", 
            "title": "Minimum hardware requirements, what happens if certain minimum configuration requirement has not been met?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-java-is-not-installed", 
            "text": "Following message can be seen when Java is not available on the system  Error: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.  Install Java 7 from package manager of Linux Distribution and try running installer again.", 
            "title": "What happens if java is not installed?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-hadoop-is-not-installed", 
            "text": "Installation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect Hadoop binary (/usr/bin/hadoop)   DFS location.   Install Hadoop > 2.2.0 and update the configuration parameters above.", 
            "title": "What happens if Hadoop is not installed?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-check-if-hadoop-is-installed-and-running-correctly", 
            "text": "Following commands can be used to confirm installed Hadoop version and if Hadoop services are running.  $ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager", 
            "title": "How do I check if Hadoop is installed and running correctly?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-the-downloaded-file-is-corrupted", 
            "text": "MD5 checksum will result in the following error:  \u201cVerifying archive integrity...Error in MD5 checksums:  MD5 checksum  is different from  MD5 checksum \u201d.  Downloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use  curl  instead of  wget .", 
            "title": "What happens if the downloaded file is corrupted?"
        }, 
        {
            "location": "/troubleshooting/#why-do-i-see-the-following-permissions-errors", 
            "text": "During installation following error message will be seen on screen   These typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:  $ hadoop dfs -ls /user/ USER /datatorrent\n$ hadoop dfs -mkdir /user/ USER /datatorrent  \n$ hadoop dfs -chown  USER  /user/ USER /datatorrent", 
            "title": "Why do I see the following permissions errors?"
        }, 
        {
            "location": "/troubleshooting/#upgrade", 
            "text": "", 
            "title": "Upgrade"
        }, 
        {
            "location": "/troubleshooting/#license-agent-errors-cause-problems-during-upgrade-from-datatorrent-rts-20-to-30", 
            "text": "If your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See  installation guide  for details.", 
            "title": "License agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0."
        }, 
        {
            "location": "/troubleshooting/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/troubleshooting/#configuring-memory", 
            "text": "", 
            "title": "Configuring Memory"
        }, 
        {
            "location": "/troubleshooting/#configuring-operator-memory", 
            "text": "Operator memory for an operator can be configured in one of the following two ways:  1 Using the same default values for all the operators:   property \n   name dt.application. APPLICATION_NAME .operator.*.attr.MEMORY_MB /name \n   value 2048 /value  /property   This would set 2GB as size of all the operators in the given application.  2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator  Op .  property \n   name dt.application. APPLICATION_NAME .operator.Op.attr.MEMORY_MB /name \n   value 8192 /value  /property   The amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.", 
            "title": "Configuring Operator Memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-buffer-server-memory", 
            "text": "There is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:  1 Using the same default values for all the output ports of all the operators  property \n   name dt.application. APPLICATION_NAME .operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n   value 128 /value  /property   This sets 128Mb as buffer memory for all the output ports of all the operators.  2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port  p  of an operator  Op :  property \n   name dt.application. APPLICATION_NAME .operator.Op.port.p.attr.BUFFER_MEMORY_MB /name \n   value 1024 /value  /property   Default value for this attribute is 512 MB", 
            "title": "Configuring Buffer Server Memory:"
        }, 
        {
            "location": "/troubleshooting/#calculating-container-memory", 
            "text": "Following formula is used to calculate the container memory.  Container Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.  Sometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between  [yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]  These values can be found in yarn configuration", 
            "title": "Calculating Container memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-application-master-memory", 
            "text": "Application Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:  property \n   name dt.application. APPLICATION_NAME .attr.MASTER_MEMORY_MB /name \n   value 4096 /value  /property   Default value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers", 
            "title": "Configuring Application Master Memory:"
        }, 
        {
            "location": "/troubleshooting/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/troubleshooting/#hadoop-dependencies-conflicts", 
            "text": "You have to make sure that the Hadoop jars are not bundled with the application package o/w they may conflict with the versions available in Hadoop classpath. Here are some of the ways to exclude Hadoop dependencies from the application package    If your application is directly dependent on the Hadoop jars, make sure that the scope of the dependency is  provided . For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml  dependency \n   groupId org.apache.hadoop /groupId \n   artifactId hadoop-common /artifactId \n   version 2.2.0 /version \n   scope provided /scope  /dependency     If your application has transitive dependency on Hadoop jars, make sure that Hadoop jars are excluded from the transitive dependency and added back as application dependency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows  dependency \n   groupId /groupId \n   artifactId /artifactId \n   version /version \n   exclusions \n     exclusion \n       groupId org.apache.hadoop /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency", 
            "title": "Hadoop dependencies conflicts"
        }, 
        {
            "location": "/troubleshooting/#serialization-considerations", 
            "text": "An Apex application needs to satisfy serializability requirements on operators and tuples as described below.", 
            "title": "Serialization considerations"
        }, 
        {
            "location": "/troubleshooting/#operators", 
            "text": "After an application is launched, the DAG is serialized using a combination of Java Serialization  and Kryo  and then the DAG is\ntransferred over the network from the launching node to the application master node.  Checkpointing also involves serializing and persisting an operator state to a store and deserializing \nfrom the store in case of recovery. The platform uses Kryo serialization in this case. Kryo imposes\nadditional requirements on an operator Java class to be de-serializable. For more details check out\nthis  page .", 
            "title": "Operators"
        }, 
        {
            "location": "/troubleshooting/#tuples", 
            "text": "Tuples are serialized (and deserialized) according to the specified stream codec when transmitted between Yarn containers.\nWhen no stream codec is specified, Apex uses the default stream codec that relies on the  Kryo  serialization library to\nserialize and deserialize tuples. A custom stream codec can be specified to use a different serialization\nframework.  Thread and container local streams don't use a stream codec, so tuples don't need to be serializable in such cases.", 
            "title": "Tuples"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting-serialization-issues", 
            "text": "There is no guaranteed way to uncover serialization issues in your code. An operator may emit a problematic tuple\nonly in very rare and hard to reproduce conditions while testing. Kryo deserialization problem in an operator will \nnot be uncovered until the recovery time, and at that point it is most likely too late. It is recommended to unit\ntest an operator's ability to restore itself properly similar to this  example  To exercise tuple serialization, run your application in  local mode  that could uncover many tuple\nserialization problems. Use the  ApexCLI  to launch your application with\nthe  -local  option to run it in local mode. The application will fail at a point when the platform is unable to serialize\nor deserialize a tuple,and the relevant exception will be logged on the console or a log file as described in the  Kryo exception  section. Check out that section further for\nhints about troubleshooting serialization issues.", 
            "title": "Troubleshooting serialization issues"
        }, 
        {
            "location": "/troubleshooting/#transient-members", 
            "text": "Certain data members of an operator do not need to be serialized or deserialized during deployment or \ncheckpointing/recovery because they are  transient \nin nature and do not represent stateful data. Developers should judiciously use the  transient  keyword for declaring\nsuch non-stateful members of operators (or members of objects that are indirectly members of operators) \nso that the platform skips serialization of such members and serialization/deserialization errors are \nminimized. Transient members are further described in the context of the operator life-cycle  here . Typical examples of\ntransient data members are database or network connection objects which need to be \ninitialized before they are used in a process, so they are never persisted across process invocations.", 
            "title": "Transient members"
        }, 
        {
            "location": "/troubleshooting/#getting-this-message-in-stram-logs-is-anything-wrong-in-my-code", 
            "text": "2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)  Coming soon.", 
            "title": "Getting this message in STRAM logs. Is anything wrong in my code?"
        }, 
        {
            "location": "/troubleshooting/#debugging", 
            "text": "", 
            "title": "Debugging"
        }, 
        {
            "location": "/troubleshooting/#how-to-remote-debug-gateway-service", 
            "text": "Update Hadoop OPTS variable by running,  export HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\"", 
            "title": "How to remote debug gateway service?"
        }, 
        {
            "location": "/troubleshooting/#how-to-setup-debug-level-while-running-an-application", 
            "text": "Add the following property to your properties file:  property \n   name dt.application. APP-NAME .attr.DEBUG /name \n   value true /value  /property", 
            "title": "How to setup DEBUG level while running an application?"
        }, 
        {
            "location": "/troubleshooting/#my-gateway-is-throwing-the-following-exception", 
            "text": "ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...  Check if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.", 
            "title": "My gateway is throwing the following exception."
        }, 
        {
            "location": "/troubleshooting/#when-apex-is-running-in-secure-mode-yarn-logs-get-flooded-with-several-thousand-messages-per-second", 
            "text": "Please make sure that the kerberos principal user name has an account with the\nsame user id on the cluster nodes.", 
            "title": "When Apex is running in secure mode, YARN logs get flooded with several thousand messages per second."
        }, 
        {
            "location": "/troubleshooting/#application-throwing-following-kryo-exception", 
            "text": "com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):  This means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception   Add default constructor to the type in question.   Using  custom serializer  for the type in question. Some existing alternative serializers can be found at  https://github.com/magro/kryo-serializers . A custom serializer can be used as follows:  2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.  @FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType  Kryo will use this CustomSerializer to serialize and deserialize type SomeType. If\nSomeType is a Map or Collection, there are some special annotations @BindMap and\n@BindCollection; please see  here .  2.2 Using the @DefaultSerializer annotation on the class, for example:  @DefaultSerializer(JavaSerializer.class)\npublic class SomeClass ...  2.3 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:  import java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec T  extends com.datatorrent.lib.codec.KryoSerializableStreamCodec T \n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}  Let's say there is an Operator  CustomOperator  with an input port  input  that expects type SomeType. Following is how to use above defined custom stream codec  CustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec SomeType  codec = new CustomSerializableStreamCodec SomeType ();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);  This works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways.", 
            "title": "Application throwing following Kryo exception."
        }, 
        {
            "location": "/troubleshooting/#log-analysis", 
            "text": "There are multiple ways to adjust logging levels.  For details see  configuration guide .", 
            "title": "Log analysis"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-stram-logs", 
            "text": "You can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.    In dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.    Alternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.  yarn logs -applicationId  applicationId", 
            "title": "How to check STRAM logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-application-logs", 
            "text": "On dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.", 
            "title": "How to check application logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-killed-operators-state", 
            "text": "On dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.", 
            "title": "How to check killed operator\u2019s state"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-for-particular-any-application-or-container", 
            "text": "In applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.", 
            "title": "How to search for particular any application or container?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-search-within-logs", 
            "text": "Once you navigate to the logs page,     Download log file to search using your preferred editor    use \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d", 
            "title": "How do I search within logs?"
        }, 
        {
            "location": "/troubleshooting/#launching-applications", 
            "text": "", 
            "title": "Launching Applications"
        }, 
        {
            "location": "/troubleshooting/#application-goes-from-accepted-state-to-finishedfailed-state", 
            "text": "Check if your application name conflicts with any of the already running\napplications in your cluster. Apex does not allow two applications with\nthe same name to run simultaneously.\nYour STRAM logs will have following error: \n\u201cForced shutdown due to Application master failed due to application\n\\ appId> with duplicate application name \\ appName> by the same user\n\\ user name> is already started.\u201d", 
            "title": "Application goes from accepted state to Finished(FAILED) state"
        }, 
        {
            "location": "/troubleshooting/#constraintviolationexception-during-application-launch", 
            "text": "Check if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException.", 
            "title": "ConstraintViolationException during application launch"
        }, 
        {
            "location": "/troubleshooting/#events", 
            "text": "", 
            "title": "Events"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-container-failures", 
            "text": "In StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.", 
            "title": "How to check container failures"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-within-events", 
            "text": "You can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.", 
            "title": "How to search within events"
        }, 
        {
            "location": "/troubleshooting/#tail-vs-range-mode", 
            "text": "tail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.", 
            "title": "tail vs range mode"
        }, 
        {
            "location": "/troubleshooting/#what-is-following-button-in-events-pane", 
            "text": "When we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.", 
            "title": "What is \u201cfollowing\u201d button in events pane"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-a-heap-dump-when-a-container-gets-an-outofmemoryerror", 
            "text": "The JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely  -XX:+HeapDumpOnOutOfMemoryError  and  -XX:HeapDumpPath=/tmp/op.heapdump .\nTo add them to a specific operator, use this stanza in your configuration file\nwith  OPERATOR_NAME  replaced by the actual name of an operator:       property \n       name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   To add them to all your containers, add this stanza to your configuration file:       property \n       name dt.attr.CONTAINER_JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   With these options, when an  OutOfMemoryError  occurs, the JVM writes the heap dump to the\nfile  /tmp/op.heapdump ; you'll then need to retrieve the file from the node on which the\noperator was running.  You can use the tool  jmap  (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe  -F  option; here is a sample invocation on the sandbox:  dtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created  The heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as jhat  or MAT  to analyze it.  The former ( jhat ) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:  tmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.  It is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available here .", 
            "title": "How do I get a heap dump when a container gets an OutOfMemoryError ?"
        }, 
        {
            "location": "/troubleshooting/#coming-soon", 
            "text": "Connection Refused Exception  ClassNotFound Exception  Launching apa vs jar  DAG validation failed  Multiple gateways running simultaneously, app not launched.  HDFS in safe mode  Application stays in accepted state  Some containers do not get resources (specially in case of repartition)  Insufficient memory set to operator causes operator kill continuously.   Why is the number of events same/different at input and output port of each operator?    Shutdown vs kill option   Why shutdown doesn\u2019t work? (if some containers are not running)  Can I kill multiple applications at same time?  Killing containers vs killing application  STRAM failures (during define partitions)  Thread local + partition parallel configuration  What to do when downstream operators are slow than the input  operators.  I am seeing high latency, what to do?  conf option in Apex CLI  Application keeps restarting (has happened once due to license agent during upgrade)  Operator getting killed after every 60 secs (Timeout issue)  How to change commit frequency  Difference between exactly once, at least once and at most once  Thread local vs container local vs node local  Cluster nodes not able to access edge node where Gateway is running   Developers not sure when to process incoming tuples in end window or when to do it in process function of operator    How partitioning works   How the data is partitioned between different partitions.  How to use stream-codec  Data on which ports is partitioned? By default default partitioner partitions data on first port.  How to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).     pom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes Hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc    Exactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.    How to check checkpoint size. (large checkpoint size cause instability in the DAG).   How to add custom metrics and metric aggregator.  Example of how to implement dynamic partitioning.", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/release_notes/", 
            "text": "DataTorrent RTS Release Notes\n\n\nVersion: 3.10\n\n\nCompanies want to gain quick insights and take action on impacting events at the speed of their businesses. DataTorrent's Real-time Streaming (RTS) platform makes it easy to deliver fast data analytics applications that drive business outcomes using the latest innovation in data science and machine learning. It integrates best-of-breed open source technology innovation to provide all the features that a business needs to develop and deliver best-in-class, fast data applications.\n\n\nDataTorrent RTS 3.10 version includes the following new features, resolved issues, and known issues.\n\n\nFeatures\n\n\nThe key features of DataTorrent RTS 3.10 release are as follows:\n\n\n\n\nNew RTS Platform features\n\n\nDataTorrent's RTS Apoxi\u2122 framework with added features\n\n\nNew and enhanced AppFactory applications\n\n\nMiscellaneous features\n\n\n\n\n\n\nRTS Platform\n\n\nThe following RTS Platform features are included in this release:\n\n\n\n\nSupport for OLAP with Druid\n\n\nExpanded Support for Machine Learning and AI\n\n\n\n\n\n\nSupport for Real-time OLAP with Druid\n\n\nProvides customers with the ability to slice and dice data in real-time to get the information needed to compute and compare it to historical trends. This capability is delivered as a pre-built component that tightly integrates Apex, Druid, and Superset for real-time plus historical dashboards and visualizations.   Druid is an open-source project that is hardened and delivered as part of the RTS platform so that a separate, dedicated Druid cluster is not required.\n\n\n\n\nExpanded Support for Machine Learning and AI\n\n\nDataTorrent RTS 3.10 helps customers capitalize on the value from the latest innovations in machine learning, AI, and data science.\n\n\nPython Support\n\n\nRTS is extended to support the machine-scoring models that are written in Python.\n\n\nSupport for PMML Based Machine Scoring Models\n\n\nRTS also provides support for machine scoring models based on PMML which is an emerging standard.\n\n\n\n\nDataTorrent's RTS Apoxi\u2122 Framework\n\n\nRTS Apoxi framework gives businesses operational readiness at any scale by providing the tooling required to assemble, manage, and operate fast data applications on any infrastructure. Apoxi is a framework that uniquely binds application building blocks or micro data services to create optimized, pre-built apps. It can also integrate independent applications to allow them to operate as one.\n\n\nWith Apoxi, DataTorrent can combine multiple micro data services, each with its own capability and operability, while also preserving the independence of different services, i.e. ingest and enrich, train and prepare, archive and persist, analyze and act.\n\n\nFor developers and data engineers, Apoxi saves time by providing facilities for streaming data visualization, lifecycle management, what-if data replay, and a built-in backplane for rapid integration.  Apoxi helps DevOps teams push applications to production with confidence by providing the management tooling, enterprise integrations, and operational metrics needed to meet production SLAs 24x7x365.\n\n\nThe following features of RTS Apoxi framework are included in 3.10 release:\n\n\n\n\nStore and Replay\n\n\nDrools Workbench\n\n\nApplication Backplane\n\n\nAzure HDI Deployment\n\n\nNew iFrame Widget\n\n\nConfiguration Artifacts for Schemas and Rules\n\n\nEnhanced RTS UI for Launch and Application Configuration\n\n\n\n\n\n\nStore and Replay\n\n\nStore and Replay helps customers push to production with confidence. Customers can record and replay data from a specific point in time to evaluate the effectiveness of builds, models, and scenarios before they are put into production thereby removing the guesswork about what outcomes can occur.\n\n\n\n\nDrools Workbench\n\n\nDrools Workbench integration makes it easier to modify complex event processing (CEP) rules and push new rules to production. The workbench enables customers to import data schema and visually edit and manage complex rules, easily using an intuitive graphical user interface.\n\n\n\n\nApplication Backplane\n\n\nApplication Backplane enables multiple applications to be simply and consistently integrated to share insights and actions. Combining numerous applications in real-time can result in significantly better outcomes, while still enabling separately managed applications to remain independent and benefit from a network-effect.\nFor example, you can integrate a fraud prevention application, such as Payment Card Fraud Prevention, with another fraud prevention application so that bothapplications see a net reduction in fraud by alerting each other whenever a fraud is detected in one.\n\n\n\n\nAzure HDI Deployment\n\n\nRTS now supports deployment to an Azure HDInsights Hadoop Cluster thereby enabling you to deploy RTS on Microsoft cloud. You can build streaming analytics applications and reduce the time to value with support for Azure Event Hubs as a source of incoming stream data.\n\n\n\n\nNew iFrame Widget\n\n\nNow you can create rich dashboards that include visualizations from 3rd sources so that all your insights can be displayed in one, easy-to-use interface for exploring and visualizing data.   You can see this new capability in action as part of the new Online Analytics Service that provides real-time and historical visualizations, drill-down, query, etc.\n\n\n\n\nConfiguration Artifacts for Schemas and Rules\n\n\nDT RTS UI now allows you to include configuration artifacts which refers to schemas, libraries, rules and custom code that can be provided to an application through an application configuration. These artifacts can be uploaded manually and synchronized automatically from a Maven artifacts directory that is accessible by the Gateway. During the launch of an application, these artifacts are deployed as Application Configuration (*.apc) files.\n\n\n\n\nEnhanced RTS UI for Launch and Application Configuration\n\n\nThe RTS UI is enhanced to include user-friendly screens for creating application configurations that are composed of multiple, separate services - enabling them to function as one end-to-end application.\n\n\nAppFactory \n\n\nThe AppFactory includes an extensive library of popular open-source and proprietary operators for data input/output, enrichment, analytical functions, and more. These operators are further combined into pre-tested, streaming micro data services for common patterns such as data ingestion, real-time synchronization, anomaly detection, machine scoring, etc.\n\n\nThe following features are included in AppFactory for 3.10 release:\n\n\n\n\nOmni-Channel Fraud Prevention Application V2\n\n\nAccount Takeover Prevention Application\n\n\nNew Use Case: Retail Recommender PoC\n\n\nMicro Data Service (MDS): Online Analytics Service\n\n\nEnhanced Drools Operator\n\n\n\n\n\n\nOmni-Channel Fraud Prevention Application V2\n\n\nThe newest version of DataTorrent's Omni-channel Payment Fraud Prevention application integrates with the Druid OLAP component for real-time online analytical processing and enhanced historical trend analysis. This latest application also includes a reference architecture for integration with a variety of machine-trained analytical models for enhanced fraud prevention.\n\n\nThe improvements in this version of the Fraud Prevention application include the following:\n\n\n\n\nSupport for stateful rules via Drools operator which allows storing and using a state across multiple transactions.\n\n\nIntegrated with OAS which enables real-time analytics.\n\n\nSupport for rule editing with CEP Workbench.\n\n\nIntegrated with Apache Superset UI which lets you visualize and analyze trends on real-time and historical dashboards.\n\n\n\n\n\n\nOnline Account Takeover Prevention\n\n\nRTS introduces \nAccount Takeover Prevention (ATO) application\n, a reference application that enables customers to prevent online account takeover and fraud by processing, enriching, analyzing, and acting on multiple streams of account event information in real-time.\n\n\nAdditional key features in this application include:\n\n\n\n\nIntegration with Online Analytics Service for OLAP trend analysis.\n\n\nSupport for rule editing with Drools Workbench.\n\n\nIntegration with Apache Superset UI for visualization.\n\n\n\n\n\n\nNew Use Case: Retail Recommender - Reference Architecture\n\n\nReal-time, personalized product and service recommendations drive additional revenue for retail and ecommerce companies. DataTorrent's Retail Recommender provides a reference architecture that produces product recommendations in real-time by operationalizing the latest innovations in machine-learning.\n\n\nKey features in this application include:\n\n\n\n\nIntegration with Online Analytics Service for OLAP trend analysis\n\n\nIntegration of Spark MLlib.\n\n\nIntegration with Apache Superset UI for visualization\n\n\n\n\nFor more details, refer to Retail Recommender Reference Architecture in our \nAppFactory\n\n\n\n\nOnline Analytics Service (OAS)\n\n\nOnline Analytics Service (OAS) is delivered as a pre-built component that integrates Apex, Druid, and Superset for real-time BI dashboards and visualizations. OAS provides real-time as well as historical trend analysis, visualization, drill down, and query.\n\n\nRTS enables OAS to receive event data from multiple data sources into a single Kafka topic while making the data available for analysis. RTS also provides a seamless way for target applications to push the data into Kafka topics for consumption by OAS.\n\n\n\n\nEnhanced Drools Operator\n\n\nThe Drools operator is enhanced to provide support for stateful rules execution. The updated Drools operator is used in the latest version of Fraud Prevention application as well as in the Account Takeover Prevention application.\n\n\n\n\nMiscellaneous Features\n\n\nThe following miscellaneous features are included in this release:\n\n\n\n\nEncrypted StreamCodec\n\n\nDynamic Usage Statistics Reporting via Phone Home\n\n\n\n\n\n\nEncrypted StreamCodec\n\n\nEncrypted StreamCodec adds multiple implementations of StreamCodec and StatefulStreamCodec, which provides support for encryption of tuples before sending them downstream. All implementations are based on AES and secure RPC when launching operators in the cluster. The shared secret key is distributed to all the operators during deployment.\n\n\n\n\nDynamic Usage Statistics Reporting via Phone Home\n\n\nDynamic usage statistics are reported to DataTorrent via the phone home feature. This aids DataTorrent to provide better support and to analyze when you are running out of data capacity. It also helps you to understand your RTS usage patterns over time.\n\n\nUpgrade Information\n\n\nIn case you want to use Services in DataTorrent RTS applications, you must install Docker on your system. However, this is not mandatory for upgrading to RTS 3.10 version.\n\n\nResolved Issues\n\n\nThe following resolved issues are included in this release:\n\n\n\n\nAdded a new recovery mode REUSE_INSTANCE to reuse operator instance across failures without reloading from checkpoint in certain cases. Now when the upstream operator is killed, the downstream operator does not reload the data from checkpoint from HDFS. Instead it uses the state in memory as-is. (SPOI-11219)\n\n\nSupport for option to use per user application directory in HDFS in secure mode. (SPOI-12134)\n\n\nOut of order window ID was received in the begin window after recovery from a failure.  (SPOI-12633)\n\n\nSupport is provided for UI Active Directory. (SPOI-12663)\n\n\nContainer request is not sent, when the node report checks for the memory requested by the container. (SPOI-12708)\n\n\nWith anti-affinity STRAM, enough containers are not received from the resource manager to run all the operators. (SPOI-12791)\n\n\nFiles configured by the user were not available after RTS upgrade. (SPOI-12794)\n\n\nProvide new permission that is VIEW_LOGS which determines if user can view the log files of the application instances. (SPOI-12936)\n\n\nAuthentication for Active Directory failed for domain users. (SPOI-12998)\n\n\nOption provided to run snapshot server in stand-alone mode. (SPOI-13031)\n\n\nSMTP configuration does not work without SSL. ( SPOI-13256)\n\n\n\n\nKnown Issues\n\n\nThe following known issues are included in RTS 3.10:\n\n\n\n\nKafkaExactlyOnceOutputOperator\n does not work with Kerberised cluster. (SPOI-13182)\n\n\nOperator in fraud-prevention App dies while attempting to record a sample. (SPOI-13194)\n\n\nThe java script validation cannot terminate a java script, if it is running longer than the timeout period. (SPOI-12265)\n\n\nApplication does not shutdown even after consuming the end offset in Kafka. ( SPOI-12955)\n\n\nOAS is unable to handle multiple intervals in select query. (SPOI-12865)\n\n\n\n\nDT services with secure docker setup does not work. (SPOI-13104)\n\n\n\n\nWorkaround:\n Docker in SSL mode requires mutual TLS (MTLS) which is not supported. Make sure your docker daemon uses either UNIX sockets or plain TCP sockets\n\n\n\n\n\n\n\n\nMultiple redirections occur when logging into drools workbench through proxy URL. (SPOI-13254)\n\n\n\n\nWorkaround\n : Click the \nBack\n button on the browser to go to the Drools WorkBench login page. You can also close the current tab and click the Proxy URL once again to go to the login page.\n\n\n\n\n\n\n\n\nTransient services are not deleted when the corresponding app is terminated. (SPOI-12983)\n\n\n\n\nSchema Aware operators should not perform validations in setter methods. (SPOI-13095)\n\n\nProxy URL for newly created Superset service (from UI) does not work. (SPOI-13454)\n\n\nGATEWAY_CONNECT_ADDRESS parameter is misspelt as GATEWAY_CONNECT_ADDRESSS. (SPOI-13449)\n\n\nFraud App: Physical DAG is broken (SPOI-13388)\n\n\nSecurity vulnerability is caused due to Key tab copy and incorrect permission issues. (SPOI-13366)\n\n\nApplication fails when both replay and archive properties are set to True. ( SPOI-13202)\nRelease date: Feb 21, 2018\n\n\n\n\nVersion: 3.9.2\n\n\nRelease date: Dec 6, 2017\n\n\nThe following issues are fixed in DataTorrent RTS 3.9.2 release:\n\n\nResolved Issues\n\n\n[SPOI-12791]\n\n\nIn the application when the anti-affinity is enabled, the application master gets containers from the Resource Manager for sometime only.  After this no more containers are received even after periodically requesting the STRAM for containers. Thus the functioning of the application gets disrupted.\n\n\n[SPOI-12794]\n\n\nWhile upgrading to RTS 3.9.1 with the latest rpm, the installation do not remain seamless. The LDAP configurations of the user are cleared from conf directory and the configurations had to be manually set up again. \n\n\n[SPOI-12795]\n\n\nMetrics platform was not supported with DT-Plus license and was only available with DT_Premium license as part of 3.9.1 release. \n\n\n[SPOI-12832]\n\n\nThe application exits due to \njava.lang.NullPointer\n exception in the application master. \n\n\n[SPOI-12855]\n\n\nCassandraStore class only supports single node.  The CassandraStore must be enabled to support more than one cassandra nodes.\n\n\n[SPOI-12820]\n\n\nA new License API must be added to check the license restriction inside the Apex application container.\n\n\n[SPOI-12898]\n\n\nLicense is required for deploying RTS 3.9.2 on Sandbox. \n\n\n[SPOI-12843]\n\n\nThe App Metrics platform must be updated with the new license checking code to support DT Premium or DT Plus license category.\n\n\n[SPOI-12842]\n\n\nThe Drools CEP Rule Engine operator must be updated with the new license checking code to support DT Premium or DT Plus license category.\n\n\n[SPOI-12841]\n\n\nThe Omni-channel Fraud Prevention v1 premium application must be updated with the new license checking code that supports DT Premium or DT Plus license category.\n\n\n[SPOI-12932]\n\n\nFor Alert configurations, an e-mail address with hypen(-) character is not accepted. \n\n\n[SPOI-12888]\n\n\nThe metrics data is generated even when the platform license was not set.\n\n\n[SPOI-12872]\n\n\nisValidLicense API\n returns as \nTrue\n in cases where the license is not even configured.\n\n\n[SPOI-12867]\n\n\nThe method \ncontext.getAttributes().get()\n, that is used in License API, do not function properly in getting the application name property in the Apex Container code. This method must be replaced with \ncontext.getValue()\n method.\n\n\n[SPOI-12811]\n\n\nThe filtering of auto complete list in tags do not input correctly.\n\n\n[SPOI-12774]\n\n\nIf the key value contains a space in the key-combination, then that space is converted into hypen(-).\nFor example: If the key is \nLas Vegas\n, then it gets converted to \nLas-Vegas\n.\n\n\n[SPOI-12749]\n\n\nNullpointer exception in found in FSWindowDataManager.\n\n\nVersion: 3.9.1\n\n\nRelease date: Oct 10, 2017\n\n\nFeatures\n\n\nThe following features and enhancements are included in the DataTorrent RTS 3.9.1 release:\n\n\nPremium Applications Licensing\n\n\nDataTorrent applications and operators are now offered under the following license categories:\n\n\n\n\nDataTorrent Premium Suite\u00ae\n\n\nDataTorrent Service Plus\u00ae \n\n\n\n\nThese categories determine the type of access that you have for the premium applications and operators that are available in DataTorrent's AppFactory.\n\n\nWith \nDT Premium\n, you can access  and download premium applications from the DataTorrent AppFactory such as the Omni-Channel Fraud Prevention application and operators such as Drools-based rule engine for complex event processing. \n\n\nWith \nDT Plus\n, you can download only non-premium applications and operators that can be used for data ingestion.  Premium applications, although visible on the AppFactory, can be downloaded only if you subscribe to DataTorrent Premium Suite.\n\n\nIn this release, licensing support is extended to \nMetrics module\n and to \nOmni-Channel Fraud Prevention\n application.\n\n\nPython Support\n\n\nDataTorrent now provides support to Python for building applications in Apex. You can create a DAG with Python as high level API support.\n\n\nSingle Value Widget\n\n\nThe Single Value widget can be now added to a dashboard of Application Templates.\n\n\nHistorical Time Range Results\n\n\nThe Historical time range selection results now includes the  \nFrom\n and \nTo\n values.\n\n\nEnhancements to Import Packaged Dashboards Section\n\n\nThe following enhancements are done to the \nImport Packaged Dashboards\n section of the \nLaunch \n Application \n dialog box:\n\n\n\n\nOn the UI, the default selection for the replacement application is now the same as the currently launching application.\n\n\nThe \nName\n field is now changed to \nDashboard Name\n\n\nA detailed description is now provided for the \nSelect Replacement Applications\n field. \n\n\nThe purpose of the application state can be viewed when you hover over the dashboard name. \n\n\nThe name of the user who packaged the dashboards is now no longer visible against the name of the dashboard. \n\n\n\n\nArtifact Query API Filter\n\n\nThe artifact query API now supports expansion to all versions, including older ones, as well as specifying exact RTS Version and limiting the results. \n\n\nDataTorrent RTS Defect Fixes\n\n\nThe following resolved issues are included in the DataTorrent RTS 3.9.1 release:\n\n\nDefects\n\n\n\n\n[SPOI-11219] - Add a new recovery mode where the operator instance before a failure event can be reused in certain cases without reloading from checkpoint.\n\n\n[SPOI-12665] - Autometric values of an operator is showing wrongly in App master.\n\n\n[SPOI-12633] - Operator fails with catastrophic error message.\n\n\n\n\nVersion: 3.9\n\n\nRelease date: Aug 23, 2017\n\n\nSummary\n\n\nApplication  Dashboards\n\n\nThese dashboards help to deliver a complete application experience, showcasing the business-relevant metrics customers\u2019 desire.\n\n\nDataTorrent\u2019s Applications Dashboards are 1) easy to create from library of widgets 2) show business relevant, real-time analytics and 3) can be easily associated with one or more streaming application(s). Now customers can construct the specific view that THEY want to see in one place.\n\n\nApplication Metrics and Visualization\n\n\nCustomers can create real-time metrics that are relevant to the overall streaming analytics application, so that business-relevant metrics can be easily computed and visualized. \n\n\nDataTorrent\u2019s Application Metrics \n Visualization capabilities enable customers to define not just operational metrics related to data processing, but now they can capture, store, and show the business relevant metrics too.  This is key to being able to show dashboards that are relevant to specific business problems (for ex: percentage of transactions where fraud occurs, fraud trends in real-time, or fraud breakdown by channels).\n\n\nDataTorrent AppFactory\n\n\nDataTorrent has evolved the AppHub, making it even easier for customers to solve problems and understand how real-time streaming analytics can make a difference for their business.  \n\n\nDataTorrent AppFactory is a centralized marketplace for streaming analytics applications that:\nSolve industry specific streaming analytics problems i.e. fraud prevention to prepare the data so that you can make quick/informed business decisions help customers deliver value quickly without requiring incremental DIY work\n\n\nThe AppFactory is arranged both by vertical application and Application Suites making it very intuitive for the customer to identify what might be most relevant/critical for their business needs.\n\n\nAdditionally, customers can see the Use Cases, Reference Architectures, and pre-built applications for download to get a full view of what\u2019s possible in the DataTorrent AppFactory.\n\n\nAt GA, DataTorrent AppFactory contains the following Application Suites:\nOmni-channel Payment Fraud Prevention Application Suite\nContinuous Big Data Cloud Sync Application Suite\nContinuous Big Data Archival Application Suite\nContinuous Big Data Preparation Application Suite\nContinuous Big Data Sync Application Suite\n\n\nAppFactory is a marketplace of big data streaming analytics use cases, reference architectures, and downloadable applications that help you to make a positive impact on your business as quickly as possible. You can search by industry or technology to quickly find what is most relevant to your needs.\n\n\nOmni-channel Payment Fraud Prevention Application Suite\n\n\nPrevent payment fraud in real-time on all transactions across all your channels.\nDataTorrent\u2019s Omni-channel Payment Fraud Prevention Application delivers real-time, data-in-motion analytics built for 24/7/365 production with sub-second latency, self-healing, enterprise-grade reliability and a scale-out architecture built with a complete pipeline that includes real-time business and operational visualizations.\n\n\nContinuous Big Data Cloud Sync Application Suite\n\n\nContinuous Big Data sync of on-premise and cloud infrastructures for availability, compliance, and archival.\nAllows customers to create various data storages within and across on-premise and cloud which can be continuously synced, with no data loss.\n\n\nContinuous Big Data Archival Application Suite\n\n\nContinuous archival of Big Data for compliance and business continuity.\nEnables customers fast backup of large volumes of data with low latency, and dynamic scaling features. \n\n\nContinuous Big Data Preparation Application Suite\n\n\nStreaming Big Data ingestion to prepare your data for insight and action.\nRenders your data \u201cdecision ready\u201d as close as possible to the time of creation, serving the business with continuous, clean, consistent, enriched data in the desired business template.   \n\n\nContinuous Big Data Sync Application Suite\n\n\nContinuous delivery of Big Data to your Data Lake.\nAllows customers to create an Enterprise Data Lake which delivers continuous, clean, and consistent data while ensuring no data loss occurs during data movement. \n\n\nAdditional product features that further increase a customer\u2019s time to value include:\n\n\nOperator Library Improvements\n\n\nWith every release, DataTorrent hardens the operators we ship with our applications and templates, continually adding to the Open Source community.\n\n\nWith RTS 3.9, we\u2019ve included the creation of an input operator for the latest version of Kafka so we now support Kafka 0.10.1.\n\n\nSchema Support for Applications\nThis feature makes it easier to change the fields that are supported in your data.\n\n\nWith DataTorrent\u2019s Basic Schema Support for applications, it is easier to change the schema that are supported by the data.  Now users are able to associate fields to the data being processed by a DAG in order to associate this pipeline with a set of fields. When the fields are changed, the whole pipeline is updated accordingly.\n\n\nLicensing\n\n\nEvery customer including eval and free edition users will need a new license file to use the RTS 3.9.0 product.\n\n\nIn 3.9, we have updated our software licensing mechanism. Customers, including those using our Free license, will be required to obtain a new software license file from DataTorrent in order to use or upgrade to version 3.9.  Existing customers can visit Upgrade License to obtain a new license when ready.\n\n\nCustomers who download our sandbox environment or request a free or evaluation license will automatically receive a 3.9-compatible license.\n\n\nSecurity\n\n\nSecurity hardening enhancements in 3.9.0 enable users to configure LDAP Security directly in the product. While available previously, the additional functionality in our User Interface now makes LDAP configuration even easier, saving customers time.\n\n\nCentralized Log Aggregation\n\n\nTroubleshooting just got easier with this feature that enables a centralized facility for log aggregation.\n\n\nDataTorrent RTS 3.9 Support for 3rd Party Log Aggregation makes it even easier for customers to troubleshoot while in production with indexing, search and correlation, and a centralized facility for log aggregation. You can now integrate Elasticsearch, Logstash, Kibana (ELK) or Splunk with RTS out of the box.  \n\n\nAdditional Features of RTS 3.9.0\n\n\nApache Beam Support\n\n\nThis feature is focused on making data processing easier, faster and less costly.\nApache Beam Support, aka Google DataFlow is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows and also data ingestion and integration flows, supporting EIPs and DSLs. This dataflow pipeline simplifies the mechanics of large-scale batch and streaming processing.\n\n\nBasic Batch Processing Support\n\n\nThis feature enables data to be read and processed in batch mode.\nDataTorrent\u2019s Batch Processing Support allows customers to use their batch-oriented architectures in order to integrate with support for external scheduling (i.e. through cron, Oozie, etc.).\n\n\nDeprecated Features of RTS 3.9.0\n\n\nThis section lists features and capabilities that have been either removed or planned for removal in DataTorrent RTS.\n\n\n\n\nApplication Data Tracker (ADT):  This feature will no longer be supported after the RTS 3.9.0 release.   ADT is determined to be not sufficiently scalable and requires execution of a separate application that is not directly built into the RTS platform.  ADT is replaced with the new Application Metrics feature.  The new Application Metrics feature available in RTS 3.9.0 provides the required scalability, is easier for developers to use, and does not require execution of a separate application.  \n\n\n\n\nApex Features\n\n\n\n\n[APEXCORE-594] Plugin support in Apex\n\n\n[APEXCORE-579] Custom control tuple support\n\n\n[APEXCORE-563] Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.\n\n\n[APEXCORE-655] Support RELEASE as archetype version when creating a project\n\n\n[APEXCORE-662] Raise StramEvent for heartbeat miss\n\n\n\n\nApex Bugs\n\n\n\n\n[APEXCORE-674] DTConfiguration utility class ValueEntry access level was changed\n\n\n[APEXCORE-663] Application restart not working.\n\n\n[APEXCORE-648] Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()\n\n\n\n\nDataTorrent RTS Bug Fixes\n\n\nBugs\n\n\n\n\n[SPOI-8784] - Restarting application with originalAppId for long running app takes long time\n\n\n[SPOI-9203] - AppHub \"check for updates\" option says 'no updated versions' and then displays updated packages\n\n\n[SPOI-10409] - Updating app packages using \"check for updates\" option from AppHub gives wrong notification as undefined\n\n\n[SPOI-10786] - Checkbox cannot be unchecked in the Widget Options screen\n\n\n[SPOI-10989] - Gateway insists on using IP address for GATEWAY_CONNECT_ADDRESS\n\n\n[SPOI-11143] - Launching PiDemoAppData with optional properties throws 500 Server Error\n\n\n[SPOI-11235] - Stram Search Event Does Not Return Correct Result\n\n\n[SPOI-11239] - Prevent Page Freeze in Monitor Application Page\n\n\n[SPOI-11312] - Container log files contain \"\\t\" characters instead of a tab\n\n\n[SPOI-11322] - AppMaster node is mentioned as \"N/A\" in the Failure Message\n\n\n[SPOI-11381] - Search for \"heap reduction %\" column in GC Log Table does not work correctly\n\n\n[SPOI-11425] - When operator fails recursively, few StartOperator/StartContainer events are grouped incorrectly\n\n\n[SPOI-11446] - Search for status column in \"Logical Operators\" widget does not work\n\n\n[SPOI-11482] - dtDashboard should ask for \"save changes\" while moving away from unsaved dashboard\n\n\n[SPOI-11485] - Cannot switch back to \"Name-Value\" once clicked on \"JSON\" on Application Configuration details page\n\n\n[SPOI-11498] - After machine reboot, DTGateway fails to get the uploaded license.\n\n\n[SPOI-11523] - physical DAG view has become unresponsive\n\n\n[SPOI-11580] - Shows the empty long description when uploading the existing app template with different version\n\n\n[SPOI-11622] - Fix semantic versioning for ALL Application Templates\n\n\n[SPOI-11784] - Getting Javascript error when viewing application summary - \"Unable to render dag due to TypeError: Cannot read property 'appInstance' of undefined\"\n\n\n[SPOI-11791] - Application Package upload fails with \"Unexpected EOF reached\" error\n\n\n[SPOI-11793] - Too many CLOSE_WAIT entries in netstat output when running applications with dashboards\n\n\n[SPOI-11805] - AppHub timestamps do not follow App Package timestamp standards\n\n\n[SPOI-11809] - Issues with Bar-Charts on Dashboard\n\n\n[SPOI-11851] - 'Reset Position' option for DAG on Application details page does not work\n\n\n[SPOI-11858] - Incorrect metrics are reported for the application\n\n\n[SPOI-11859] - Metrics are not updated in real-time for long running applications\n\n\n[SPOI-11860] - 'dt.metrics.baseDir' should default to \"DFS Root Directory/metrics\" instead of hard coded \"datatorrent/metrics\"\n\n\n[SPOI-11869] - Gateway throws FileNotFoundException for applications without any metrics data stores\n\n\n[SPOI-11870] - Issues faced while demoing app template(s)\n\n\n[SPOI-11884] - Logical and Physical Dag widgets shrink when clicking on the left and right border\n\n\n[SPOI-11889] - DTX-SNAPSHOT build packages apex core jar from different versions.\n\n\n[SPOI-11897] - Launching Database to Database Sync App gives SQL syntax error\n\n\n[SPOI-11915] - No options are shown under 'Predefined Conditions' dropdown while creating new alert\n\n\n[SPOI-11929] - Table widget should not have \"Chart Axes\" settings\n\n\n[SPOI-11930] - Historical App Metrics data in table widget does not get refreshed even when \"Load data continuously\" option is enabled\n\n\n[SPOI-11931] - Table widget issue in selecting check boxes for aggregates\n\n\n[SPOI-11932] - Misalignment in \"Time Range Selection\" option in table widget\n\n\n[SPOI-11933] - Time Range Selection option for Table widget returns less number of entries than that of requested\n\n\n[SPOI-11934] - Unable to list widgets for \"Snapshot :: App Stats\" data source\n\n\n[SPOI-11936] - Stram events doesn't scroll to bottom on page load\n\n\n[SPOI-11939] - \"Field to visualize\" option in Trend widget does not show any options in dropdown\n\n\n[SPOI-11959] - latency:AVG values are not displayed in table widget for Historical :: App Stats\n\n\n[SPOI-11961] - Improvement: \"failureCount\" for 'Historical :: Operator Stats' should be SUM and not LAST\n\n\n[SPOI-11962] - Mouseover text in stacked area chart does not change for historical data points\n\n\n[SPOI-11968] - Application search from app details page does not work\n\n\n[SPOI-11969] - All selected aggregate values for a metric are not displayed on dashboard\n\n\n[SPOI-11973] - Chart legends are not shown if number of fields to visualize is more than 12\n\n\n[SPOI-11974] - Gateway showing invalid license when server time changed\n\n\n[SPOI-11981] - Invalid license due to HDFS not ready in Sandbox.\n\n\n[SPOI-12025] - Historical data is not fetched for all the requested minute intervals\n\n\n[SPOI-12032] - Required properties are missing\n\n\n[SPOI-12046] - Gateway should exclude registering metrics datasource which are not enabled for metrics\n\n\n[SPOI-12047] - Gateway throws exceptions while getting restarted after security configuration\n\n\n[SPOI-12049] - App Metric Aggregation throws NPE when user code is called\n\n\n[SPOI-12076] - Launching application throws FileNotFoundException in gateway log for 'datatorrent/apps/${appId}/permissions.json'\n\n\n[SPOI-12079] - Too many NullPointerExceptions in dtgateway log when app connected to dashboard is killed\n\n\n[SPOI-12080] - Too many NullPointerExceptions in dtgateway log when time range for historical data is set to 'All'\n\n\n[SPOI-12081] - Table widget for historical data should resize to number of rows available for display\n\n\n[SPOI-12082] - Grouping by root cause feature is not available\n\n\n[SPOI-12086] - 'logicalOperatorName' column is blank for historical OperatorStats table widget\n\n\n[SPOI-12089] - Time Format used for displaying and querying table widget data should be same\n\n\n[SPOI-12091] - Widget data source selection not working with filtering\n\n\n[SPOI-12092] - Obfuscate public key value in RTS jar\n\n\n[SPOI-12100] - Specifying which metrics to write throws UnsupportedOperationException\n\n\n[SPOI-12102] - Snapshot schema is not refreshing\n\n\n[SPOI-12113] - Fix bullet chart\n\n\n[SPOI-12115] - Opening hdfs-line-copy app on AppHub gives FileNotFoundException in logs\n\n\n[SPOI-12116] - Retention policy for terminated apps does not work on 3.9.0\n\n\n[SPOI-12117] - Data sources get sorted only after adding first widget\n\n\n[SPOI-12124] - Gateway should not read the list datasources from metrics platform every sec\n\n\n[SPOI-12125] - Too many warning messages in gateway log with IOException for changed Blocklist\n\n\n[SPOI-12126] - Too many warning messages in gateway logs with InterruptedException while submitting Auto publish executor\n\n\n[SPOI-12127] - Issues in HDFSStoreReader and Record Data structure\n\n\n[SPOI-12129] - X-axis labels for Stacked Area chart are misaligned\n\n\n[SPOI-12140] - App level metric processor delivers empty metrics for an operator\n\n\n[SPOI-12141] - Clicking on physical operator details page for hdfs-line-copy app hangs UI\n\n\n[SPOI-12152] - Long running apps with metric data give \"Too many open files\" exception in apex log\n\n\n[SPOI-12153] - User should not be allowed to exit the modal when dashboard name is blank\n\n\n[SPOI-12159] - 'create new dashboard' option on app details page overwrites existing dashboard with same name\n\n\n[SPOI-12161] - Dashboards for restarted apps get stuck at original app data\n\n\n[SPOI-12164] - Configuration issues modal text overflow\n\n\n[SPOI-12167] - AppMaster fails with OutOfMemoryError for apps with metric data\n\n\n[SPOI-12170] - RTS installation fails with NoClassDefFoundError for jackson libraries\n\n\n[SPOI-12178] - ConcurrentModificationException in GroupingManager\n\n\n[SPOI-12181] - Launching from app builder provides invalid notification\n\n\n[SPOI-12186] - Properties for some of the operators are not displayed\n\n\n[SPOI-12192] - Unavailable data sources are shown while adding widgets\n\n\n[SPOI-12193] - Metrics platform does not honour \"dt.write.metrics\" property\n\n\n[SPOI-12199] - Metrics lib writer should include jackson libraries are runtime dependencies\n\n\n[SPOI-12216] - Clicking on logical operators link in breadcrumb does not show any list for searching\n\n\n[SPOI-12217] - Clicking on module name in logical DAG gives 404\n\n\n[SPOI-12219] - Make sure all dashboard widgets have the debug settings\n\n\n[SPOI-12223] - Filtering on data sources should be done on app name and data source name only\n\n\n[SPOI-12224] - Historical time range selection gives wrong number of records\n\n\n[SPOI-12257] - Date time selection in Control widget has issues\n\n\n[SPOI-12258] - ProductDataEnricher Operator gives \u201cUnable to retrieve result \u201d\n\n\n[SPOI-12260] - Application name is altered when control widget settings are applied to more than one applications\n\n\n[SPOI-12261] - App Metrics should not expose \"appUser\" and \"appName\" keys as selectable dimensions for [HISTORICAL] data sources\n\n\n[SPOI-12268] - When \"time.from\" and \"time.to\" are specified, \"time.latestNumBuckets\" should NOT be required.\n\n\n[SPOI-12275] - Package Properties - value of a field (long list) is only partially visible.\n\n\n[SPOI-12276] - Metrcis plugin throws IllegalStateException\n\n\n[SPOI-12286] - Time range selection UI for Historical data is off\n\n\n[SPOI-12287] - Querying with Historical time range does not return the data to widget\n\n\n[SPOI-12290] - Geo circle widget does not honor custom fields for lon and lat\n\n\n[SPOI-12293] - Historical time range selection settings are not preserved on refresh\n\n\n[SPOI-12294] - Manually editing the value in Historical time range selection changes datetime format\n\n\n[SPOI-12297] - Show all link in notification service causes page error\n\n\n[SPOI-12302] - AppFactory - styling issues\n\n\n[SPOI-12306] - Wrong sorting order for artifacts listing\n\n\n[SPOI-12321] - Unable to view data on dashboards which after import on launch\n\n\n[SPOI-12322] - X button to delete chosen application in Import Packaged Dashboard is hidden\n\n\n[SPOI-12323] - Historical time range selection settings are not preserved\n\n\n[SPOI-12332] - AppFactory - after importing appPackage, buttons should be refreshed automatically\n\n\n[SPOI-12339] - Unable to retarget datasource in dashboard settings after adding widget\n\n\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-11483] -  For FAILED applications, container state is shown as ACTIVE. Yarn returns containers of failed application as active. \n\n\nWorkaround\n - Check for app status first.\n\n\n\n\nVersion: 3.8.1\n\n\nRelease date: Aug 14, 2017\n\n\nSummary\n\n\nThis minor release primarily addresses issues related to installation of DataTorrent RTS on a Hadoop cluster configured for secure mode.\n\n\nRTS Bug Fixes\n\n\n[SPOI-11694] : Fresh system-wide install fails with \"DFS Installation Location failed to load\" error\n[SPOI-11846] : Gateway fails when launching application in secure mode without authentication enabled\n[SPOI-11848] : Installation wizard fails on the hadoop configuration screen in secure mode\n[SPOI-11849] : The UI calls to retrieve properties in the wizard fail\n[SPOI-12047] : Gateway throws exceptions while getting restarted after security configuration\n\n\nApache Apex Bug Fixes\n\n\n[APEXCORE-737] : Buffer server may stop processing tuples when backpressure is enabled\n[APEXCORE-745] : AppMaster does not shut down because numRequestedContainers becomes negative\n\n\nVersion: 3.8.0\n\n\nRelease date: Apr 18, 2017\n\n\nSummary\n\n\nApplication Templates (AppHub)\n\n\nPre-built data ingestion templates speed time-to-production\n\n\nDeploy DataTorrent RTS application templates on a Hadoop distribution either on-premises or on the cloud. As part of this release, DataTorrent is providing AWS - EMR deployment script option for each available application template. As a result, development is simplified, enabling developers to more quickly and easily unlock value for customers. DataTorrent is focused on the goals of reducing complexity and removing dependency on Hadoop deployment, and this release represents progress in that direction.   \n\n\nApplication Configurations\n\n\nSimplifying customized application launches\n\n\nUsers can start with a single Application Package and create multiple Application Configurations to launch and run the applications on different environments (for instance, in test and development).  And, efficiencies can be realized across business units: one Application Package could have multiple configurations for multiple internal units.  Each Application Configuration introduces a safety feature, which ensures that only one instance of Application Configuration can run at a time.  The status whether Application Configuration is running or not, and controls to launch and stop the application instance are provided in the Application Configuration view.  This set of features improves management, adds safety, and increases transparency when managing and launching applications.\n\n\nDebugging\n\n\nImproving log visualizations to speed up debugging\n\n\nStrAM Event Grouping\n\n\nThe StrAM Events widget helps from development and operations perspectives to visualize notable events from the application launch and throughout its ongoing run.  With this release, StrAM events widgets now offers better readability by organizing these events into related groups.  For example, when multiple downstream operators are re-deployed due to a container failure. All events triggered by the system to restore normal function will be grouped under a single root event which caused the restarts.  With this improved readability, the user can quickly identify failure causes and then drill down into the logs for each event.\n\n\nGarbage Collection Widgets\n\n\nWith this release, developers can more easily visualize garbage collection data trends\u2014as opposed to sifting through logs. Three widgets are available for GC visualizations:\n\n\n\n\nGarbage collection log chart by heap.  Visualizes when memory is allocated and deallocated \n\n\nGarbage collection log table.  List memory allocation and deallocation details\n\n\nGarbage collection log chart by duration.  Visualizes how long it takes to deallocate memory \n\n\n\n\nLog Tailing \n Search\n\n\nUsers can follow the logs as they are generated (tailing) with the RTS UI Console.  In this release, users can now also perform searches even when tailing to focus on specific events and exclude the noise.\n\n\nAlert Templates\n\n\nSimplifying and expanding alert functionality\n\n\nIn 3.7.0, RTS introduced monitoring with alerts that a DevOps engineer could set up based on specific conditions. The 3.8.0 release continues to simplify and expand this functionality by adding:\n\n\n\n\nNine predefined system alert templates, including cluster and application memory usage, application status, and active container count, killed container alerts, etc.\n\n\nOption to disable alerts without deleting them.\n\n\nThe ability to configure custom SMTP settings for sending alert emails, instead of relying on Gateway\u2019s local node sendmail facility.\n\n\n\n\nSecurity\n\n\nSecurity enhancement in 3.8.0 applies to RTS deployment on secure Hadoop with Kerberos enabled. User's own Kerberos credentials can now be used directly by RTS to launch applications.  It is better from a security perspective.\n\n\nThe previous model involved using a single system user with Kerberos credential (Hadoop impersonation)  to launch applications. That requires access to the system Kerberos credential in order to refresh tokens before they expire.  With user\u2019s own Kerberos credential, that is no longer the case. \n\n\nLicensing\n\n\nWith the release of 3.8.0, DataTorrent is updating and simplifying its licensing policy.  What has changed?  \n\n\nStarting with 3.8, the Community Edition is no longer available. We are replacing the Community Edition with Free Edition.  Community Edition limited the features available to you, such as security.   Now with Free Edition you have access to all the features and tools of RTS up to a 128GB processing limit.\n\n\nPlease refer to the DataTorrent website for additional details.\n\n\nLicensing FAQ\n\n\nI have a Community Edition license. Is that edition still available?\n\nStarting with 3.8, the Community Edition is no longer available.   You can continue to use the Community Edition with RTS version 3.7.\n\n\nHow is license memory consumption calculated?\n\nLicense memory consumption is the sum of all running applications as can be seen in Configuration - License Information and Monitor.\n\n\nWhat will happen when my memory consumption exceed my license limit?\n\nYou\u2019ll receive a warning, which is shown for 30 minutes before most RTS features will be disabled. All existing applications will continue to run. Should you need to upgrade, you can easily contact DataTorrent for a new license.\n\n\nHow will I know when my license is going to expire and what happens if the license expires?\n\nWe provide warnings at 30 days and 7 days before expiration date.  When a license expires, most RTS features will be immediately disabled. All existing applications will continue to run. You can easily contact DataTorrent for a new license.  \n\n\nWhat can I do once RTS is locked (either because my license memory has been exceeded or its expiration date has passed)?\n\nUsers can view running applications and enter new license details to unlock RTS. The following capabilities are still accessible:\n\n Configure - System Configuration is available \n\n Configure - License Information is available\n\n Configure - Installation Wizard is available\n\n Monitor - Application kill, inspect and shutdown are available\n\n Monitor - Application Overview (shutdown and kill only) per application is available\n\n Monitor - Containers (kill only)\n\n AppHub (download only, no import)\n\n Learn\n\n\nCan I reduce the number of applications and return to compliance?\n\nYes, you can do so if you have exceeded your memory capacity, assuming that your license has not expired\n\n\nCan I reduce the memory usage of an application and bring it back to license compliance?\n\nYes, assuming that your license has not expired.  Please note that you will have to restart the application.\n\n\nI have an enterprise license for my production cluster. Can I use the Free Edition in a non- production cluster?\n\nYes. It\u2019s worth noting that only community support is available for the Free Edition. Please visit the DataTorrent User group for RTS-related questions: https://groups.google.com/forum/#!aboutgroup/dt-users\n\n\nFor the Apache Apex mailing list and meetups information, please go to\nhttps://apex.apache.org/community.html#mailing-lists\n\n\nCan I buy DataTorrent support for Free Edition?\n\nUnfortunately, no. DataTorrent support is sold as part of our Enterprise Edition. If you\u2019re seeking support, you may consider upgrading.\n\n\nIs Application Master Container memory consumption included in the calculation for processing capacity?\n\nYes. Application Master Container consumes 1 GB by default and every application has its own Application Master. If application is not running, it does not run as well. It grows based on customer application build. Memory requirements increases along with the size of logical and physical DAG. Partitioners of an operator run in AppMaster.\n\n\nAdditional Features of 3.8.0\n\n\nMultiple gateway support\n\n\nThis allows simultaneous multiple gateway operations and increase fault tolerance due to management console failure. \n\n\nWhen there are multiple gateways (usually for High Availability), different developers may access them at the same time, with different or same user accounts. These activities will often result in simultaneous modification of the same resource stored in HDFS, and invalidate cache entries on each client. For example: When developer A tries to save a configuration package and developer B has edited and saved the same package, developer A will get an error. Developer A would then have to manually merge the differences. This release introduces a new file-based locking mechanism with HTTP ETag header to handle that scenario.\nKnown limitation: Alerts and visualization works correctly with single gateway only.\n\n\nRetain metric selections when returning to Monitor - Physical/Logical view\n\n\nBetter user experience since RTS will keep what users have selected to view (e.g. metrics) as they go from one screen to another. \n\n\nDataTorrent Apex Core fork\n\n\nRTS 3.8.0 is bundled with Apache Apex Core 3.5.0 plus forty feature and fixes that will be part of Apache Apex Core 3.6.0. Apex Core commits from Apr 3, 2017 will be included in RTS 3.9.0\n\n\nNew Feature\n\n\n\n\n[APEXCORE-579]        Custom control tuple support\n\n\n[APEXCORE-563]        Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.\n\n\n\n\nImprovements\n\n\n\n\n[APEXCORE-676]        Show description for DefaultProperties only when user requests it\n\n\n[APEXCORE-655]        Support RELEASE as archetype version when creating a project\n\n\n[APEXCORE-611]        StrAM Event Log Levels\n\n\n[APEXCORE-605]        Suppress bootstrap compiler warning\n\n\n[APEXCORE-592]        Returning description field in defaultProperties during apex cli call get-app-package-info\n\n\n[APEXCORE-572]        Remove dependency on hadoop-common test.jar\n\n\n[APEXCORE-570]        Prevent upstream operators from getting too far ahead when downstream operators are slow\n\n\n[APEXCORE-522]        Promote singleton usage pattern for String2String, Long2String and other StringCodecs\n\n\n[APEXCORE-426]        Support work preserving AM recovery\n\n\n[APEXCORE-294]        Graceful application shutdown\n\n\n[APEXCORE-143]        Graceful shutdown of test applications\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-662]        Raise StramEvent for heartbeat miss\n\n\n\n\nDependency Upgrade\n\n\n\n\n[APEXCORE-656]        Upgrade org.apache.httpcomponents.httpclient\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-674]        DTConfiguration utility class ValueEntry access level was changed\n\n\n[APEXCORE-663]        Application restart not working.\n\n\n[APEXCORE-648]        Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()\n\n\n[APEXCORE-645]        StramLocalCluster does not wait for master thread termination\n\n\n[APEXCORE-644]        get-app-package-operators with parent option does not work\n\n\n[APEXCORE-636]        Ability to refresh tokens using user's own Kerberos credentials in a managed environment where the application is launched using an admin with impersonation\n\n\n[APEXCORE-634]        Apex Platform unable to set unifier attributes for modules in DAG\n\n\n[APEXCORE-627]        Unit test AtMostOnceTest intermittently fails\n\n\n[APEXCORE-624]        Shutdown does not work because of incorrect logic in the AppMaster\n\n\n[APEXCORE-617]        InputNodeTest intermittently fails with ConcurrentModificationException\n\n\n[APEXCORE-616]        Application fails to start Kerberised cluster\n\n\n[APEXCORE-610]        Avoid multiple getBytes() calls in Tuple.writeString\n\n\n[APEXCORE-608]        Streaming Containers use stale RPC proxy after connection is closed\n\n\n[APEXCORE-598]        Embedded mode execution does not use APPLICATION_PATH for checkpointing\n\n\n[APEXCORE-597]        BufferServer needs to shut down all created execution services\n\n\n[APEXCORE-596]        Committed method on operators not called when stream locality is THREAD_LOCAL\n\n\n[APEXCORE-595]        Master incorrectly updates committedWindowId when all partitions are terminated.\n\n\n[APEXCORE-593]        apex cli get-app-package-info could not retrieve properties defined in properties.xml\n\n\n[APEXCORE-591]        SubscribeRequestTuple has wrong buffer size when mask is zero\n\n\n[APEXCORE-585]        Latency should be calculated only after the first window has been complete\n\n\n[APEXCORE-583]        Buffer Server LogicalNode should not be reused by Subscribers\n\n\n[APEXCORE-558]        Do not use yellow color to display command strings in help output\n\n\n[APEXCORE-504]        Possible race condition in StreamingContainerAgent.getStreamCodec()\n\n\n[APEXCORE-471]        Requests for container allocation are not re-submitted\n\n\n\n\nDataTorrent RTS Bug Fixes\n\n\n\n\n[SPOI-10021]  DTX Logical Operator page - BufferServerReadBytesPSMA and BufferServerWriteBytesPSMA to be removed\n\n\n[SPOI-10107]  Application service returns DAG which is null\n\n\n[SPOI-10118]  Upon launch of application, application details do not show up.\n\n\n[SPOI-10153]  Add System Properties \"change\" button should be warm in color\n\n\n[SPOI-10208]  Container state for failed attempt of app is shown as RUNNING\n\n\n[SPOI-10266]  \"host\" information is not available in appattempts API call\n\n\n[SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens\n\n\n[SPOI-10331]  Delete user modal gives error when clicked outside the frame\n\n\n[SPOI-10332]  Cancelling delete user action throws TypeError in developer console\n\n\n[SPOI-10335]  Jersey throwing exceptions \n excessive logging when WADL is enabled\n\n\n[SPOI-10355]  Update Buttons and Text\n\n\n[SPOI-10357]  Redirect User to Login Page\n\n\n[SPOI-10363]  CheckPermission should not throw exception when auth is not enabled.\n\n\n[SPOI-10416]  dtAssemble does not show connection between operators correctly\n\n\n[SPOI-10421]  Fix oauth login\n\n\n[SPOI-10438]  Hide Top Nav Menu Dropdown When Item is Clicked\n\n\n[SPOI-10463]  Enhance Date/Time Picker for StrAM Events Date Range\n\n\n[SPOI-10587]  Implement Date/Time Picker for Dashboard Widget\n\n\n[SPOI-10588]  Change Button Label to Close During Launching\n\n\n[SPOI-10862]  Multiple containers are labelled as AppMaster after dynamic partitioning\n\n\n[SPOI-10866]  Time Range Selection Saved Settings Not Loaded Correctly\n\n\n[SPOI-10894]  dtAssemble - Inspector contents not showing up consistently\n\n\n[SPOI-10911]  determine AppMaster container by id that contains _000001 instead of by including 0 operators\n\n\n[SPOI-10963]  appInstance page - fail to show \"packagedDashboard\" which is included in appPackage that appInstance is launched from\n\n\n[SPOI-10970]  AppHub on gateway does not load in HTTPS\n\n\n[SPOI-10996]  Subscribers/DataListeners may not be scheduled to execute even when they have data to process\n\n\n[SPOI-10997]  BufferServer needs to shut down all created execution services\n\n\n[SPOI-11000]  Upgrade org.apache.httpcomponents.httpclient\n\n\n[SPOI-11024]  Alerts Icon Issue\n\n\n[SPOI-11057]  Restart of the apps are failing\n\n\n[SPOI-11108]  DAG View Javascript Error\n\n\n[SPOI-11127]  Enhance \"lastNbytes\" to behave like \"tail\" command\n\n\n[SPOI-11142]  Unable to launch app if another app with default APPLICATION_NAME is already running\n\n\n[SPOI-11152]  Avoid usage of Apache Apex engine core class com.datatorrent.stram.client.DTConfiguration.ValueEntry\n\n\n[SPOI-11163]  Cannot launch application from application details page\n\n\n[SPOI-11164]  \"Add default properties\" option under \"Specify launch properties\" is missing\n\n\n[SPOI-11168]  License API Does Not Return Latest State Information\n\n\n[SPOI-11179]  Update Root Cause Failure to Use Newer Object Structure\n\n\n[SPOI-11185]  Invalid license expiration message sent by Gateway\n\n\n[SPOI-11186]  AppHub should be visible if license is invalid\n\n\n[SPOI-11188]  App Packages search does not work for \"format\" column\n\n\n[SPOI-11190]  Toggling \"system apps\" option does not show ended system apps\n\n\n[SPOI-11192]  Search for \"lifetime\" column on Monitor screen does not work\n\n\n[SPOI-11193]  Search for \"memory\" column on Monitor screen does not work\n\n\n[SPOI-11194]  Search on \"state\" column on Monitor page is not alphabetical\n\n\n[SPOI-11197]  Search on locality/source/sinks columns under Streams widget on logical tab does not work\n\n\n[SPOI-11198]  Search for \"allocated mem\" and \"free memory\" under Containers widget does not work\n\n\n[SPOI-11199]  \"download file\" option for empty container log files should be disabled\n\n\n[SPOI-11200]  Search for \"allocated mem\" and \"started time\" under Containers widget  for app attempt does not work\n\n\n[SPOI-11208]  DTgateway install screen messed up\n\n\n[SPOI-11210]  On entering corrupt license file error message should be a proper one\n\n\n[SPOI-11212]  Trailing and non trailing search should have same string\n\n\n[SPOI-11213]  Unable to save SMTP configuration using gmail\n\n\n[SPOI-11214]  Launching application with \u00ef\u00bf\u00bc\"Enable Garbage Collection\" throws 404\n\n\n[SPOI-11215]  ADMIN_NOT_CONFIGURED warning is only shown to Dev user instead of admin\n\n\n[SPOI-11220]  Fresh RTS installation fails because of blank response from \"/ws/v2/config\" api call\n\n\n[SPOI-11222]  StackTrace feature not available from physical tab containers widget\n\n\n[SPOI-11223]  Show \"Password change\" warning for dtadmin only\n\n\n[SPOI-11231]  AppHub fails to load previous package versions\n\n\n[SPOI-11234]  Show all AppHub package versions option is missing in list view\n\n\n[SPOI-11236]  Extend application-level gc.log API to take in new parameter \"descendingOrder\" (false/true)\n\n\n[SPOI-11237]  Angular not resolving certain dtText-wrapped expressions in Console modals\n\n\n[SPOI-11238]  AppHub Check for Updates Does Not Show In All Cases\n\n\n[SPOI-11242]  \"Upload package\" option on Application Packages should not be available with invalid/no license\n\n\n[SPOI-11265]  Invalid message displayed when no license is uploaded\n\n\n[SPOI-11266]  Alert notification is delayed\n\n\n[SPOI-11270]  System Alert history is empty after relogin\n\n\n[SPOI-11271]  Developer user cannot create system alert\n\n\n[SPOI-11281]  .class file generated by tuple schema manager is invalid\n\n\n[SPOI-11291]  Creating clone of JSON application gives 500 Server Error\n\n\n[SPOI-11303]  Creating clone of JSON application gives 500 Server Error (UI)\n\n\n[SPOI-11304]  App package load errors after migrating from 3.7 to 3.8\n\n\n[SPOI-11307]  Search for \"lifetime\" column on Monitor screen does not work\n\n\n[SPOI-11318]  Copy To Clipboard option from Failure Message modal does not work\n\n\n[SPOI-11323]  Upgrade from 3.7.0 evaluation edition to 3.8.0 retains old license\n\n\n[SPOI-11325]  Selecting KILLED applications and then disabling ended apps, shows shutdown/kill options\n\n\n[SPOI-11326]  Clean install of 3.8.0 comes with no license\n\n\n[SPOI-11327]  StramEvents are grouped incorrectly\n\n\n[SPOI-11366]  Copy to clipboard not working when viewing stram event stack trace\n\n\n[SPOI-11367]  Wrong 'uptime' value in Application Overview\n\n\n[SPOI-11368]  Log and info icons should be right aligned in stram events\n\n\n[SPOI-11369]  Socket is unsubscribed by Console when critical path is not checked\n\n\n[SPOI-11371]  \"source package\" column in Application Configurations table is not sortable\n\n\n[SPOI-11372]  Unable to view gc.log on UI\n\n\n[SPOI-11375]  Wrong stream locality is shown in Physical DAG widget\n\n\n[SPOI-11377]  Incorrect GC stats for logical operators if they are connected by CONTAINER_LOCAL stream\n\n\n[SPOI-11379]  Heap reduction percentage is negative for some GC events\n\n\n[SPOI-11382]  UI hangs when trying to change settings for GC Log Chart widgets\n\n\n[SPOI-11383]  Selecting a container in GC Log Chart widgets throws error in Developer console\n\n\n[SPOI-11417]  Memory usage of App Data Tracker is not counted against license\n\n\n[SPOI-11418]  Event grouping: UI should ignore groupId 0 \n null\n\n\n[SPOI-11421]  Unable to use Security Configuration feature with free license\n\n\n[SPOI-11422]  Admin user should not be able to delete its own user account\n\n\n[SPOI-11424]  Show tooltip for version string in application overview widget\n\n\n[SPOI-7887]   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}] should return error instead of success where there is error \"Failed to load\"\n\n\n[SPOI-8248]   Packaged dashboards do not reconnect with new app instances\n\n\n[SPOI-8477]   Upgrade License Opens in dtManage window, it should be in opened up in new window\n\n\n[SPOI-8610]   Disable editing operator properties which are of Object type\n\n\n[SPOI-9375]   Uptime values shown on UI are out of whack immediately after app is launched\n\n\n[SPOI-9474]   Failed to restart DT application in MapR secure cluster\n\n\n[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors\n\n\n[SPOI-9945]   Top navigation menu is wrapping into two lines\n\n\n\n\nVersion: 3.7.1\n\n\nRelease date: Feb 28, 2017\n\n\nSummary\n\n\nThis is primarily for users who install RTS in a Kerberized cluster as application fails to launch in 3.7.0. This release fixes the issue. \n\n\nOther issues that are also fixed:\n\n\n\n\nIn 3.6.0, users have the option to override certain properties from the config file. This ability was missing in 3.7.0.\n\n\nGateway fails to recognize AppDataTracker application and it continuously relaunched on a Kerberized cluster.\n\n\nOn a Kerberized cluster, when the configuration parameters are specified in the \ndt-site.xml\n file in the user's home directory, the installation wizard does not allow the user to continue with the installation.\n\n\n\n\nAppendix\n\n\nBug Fixes\n\n\n\n\n[SPOI-10698] - Allow Custom Properties with Config XML file while launching an application\n\n\n[SPOI-10737] - AppDataTracker application relaunches continuously and fails in a Kerberized cluster.\n\n\n[SPOI-10932] - Installation Wizard does not allow to complete gateway configuration\n\n\n[SPOI-10773] - Application fails to run in fully enabled Kerberized mode (APEXCORE-616 - https://issues.apache.org/jira/browse/APEXCORE-616)\n\n\n\n\nVersion: 3.7.0\n\n\nRelease date: Dec 30, 2016\n\n\nSummary\n\n\nThe new features on this release are functionalities that will ease debugging an application and administering application alerts in production. \n\n\nOperation related features for a Dev Ops role:  \n\n\n\n\nManage and see a history of previous alerts so that users can be aware of potential issues before they become critical. \n\n\nView operator ID(s) and name(s) in the dtManage-Physical-Container list table to quickly identify what operators are in each container. \n\n\nFilter matching tuple recordings by searching across tuple recording data.\n\n\n\n\nDebugging features: When trying to troubleshoot or debug a distributed system, these capabilities allow users to quickly identify problem areas and easily drill down into relevant details (i.e. logs). \n\n\n\n\nNotify users when log files have been removed\n\n\nNew Application Attempts section under Monitor. It is located along other views such as logical and physical\n\n\nNew Application Master logs in Application Overview section\n\n\nNew log button shortcut in Stram Events and Physical Operator section\n\n\nShow dead container logs in Running applications. That is to show history of physical operator containers and logs in the physical operator view. For each previous incarnation, there are start time, end time, link to corresponding logs, root cause with error code, and recovery window id.\n\n\nShow the same details for killed or finished application like running application view.\n\n\nShow container history as default in Physical Operator view\n\n\nNew historic count field in Physical Operator view\n\n\nGet thread dump from a container. It is useful to analyze issues such as \"stuck operator\", and obtain statistics from the running JVM. In production environments users often don't have direct access to the machines, thus making it available through the REST API will help. \n\n\nOption to auto tail container logs. When viewing a container log via the UI, there is an option to periodically poll for more data (i.e. \"tail -f\" effect).\n\n\n\n\ndtAssemble\n\n\n\n\nNew validate button so that user can validate DAG without having to save. \n\n\nRemove auto save function. Save will be initiated by user only.\n\n\nSupport custom JSON input for tuple schema creation. This is particularly useful when user needs to add a large number of fields. \n\n\n\n\ndtDashboard\n\n\n\n\nNew gauge widget\n\n\n\n\nAppHub\n\n\n\n\nContinued to refine application templates in AppHub (introduced in RTS 3.6.0).  \n\n\n\n\nRTS 3.7.0 is based on Apache Apex Core 3.5.0 (released Dec 19, 2016) and Apache Apex Malhar 3.6.0 (released Dec 8, 2016).\n\n\nApache Apex Core 3.5.0\n\n\nThis release upgrades the Apache Hadoop YARN dependency from 2.2 to 2.6. The community determined that current users run on versions equal or higher than 2.6 and Apex can now take advantage of more recent capabilities of YARN. The release contains a number of important bug fixes and operability improvements. \nChange log: https://github.com/apache/apex-core/blob/v3.5.0/CHANGELOG.md\n\n\nApache Apex Malhar 3.6.0\n\n\nThe release adds first iteration of SQL support via Apache Calcite. Features include SELECT, INSERT, INNER JOIN with non-empty equi join condition, WHERE clause, SCALAR functions that are implemented in Calcite, custom scalar functions. Endpoint can be file, Kafka or internal streaming port for both input and output. CSV format is implemented for both input and output. See examples for usage of the new API.\n\n\nThe windowed state management has been improved (WindowedOperator). There is now an option to use spillable data structures for the state storage. This enables the operator to store large states and perform efficient checkpointing.\n\n\nThere was also benchmarking on WindowedOperator with the spillable data structures. From the result, the community significantly improved how objects are serialized and reduced garbage collection considerably in the Managed State layer. Work is still in progress for purging state that is not needed any more and further improving the performance of Managed State that the spillable data structures depend on. More information about the windowing support can be found at http://apex.apache.org/docs/malhar/operators/windowedOperator/.\n\n\nThis release also adds a new, alternative Cassandra output operator (non-transactional, upsert based) and support for fixed length file format to the enrichment operator. \nChange log: https://github.com/apache/apex-malhar/blob/v3.6.0/CHANGELOG.md\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors\n\n\n[SPOI-9923]   Custom panes' on operator monitoring page should have character limits for titles\n\n\n[SPOI-9924]   Can not escape the \"custom\" pane name\n\n\n[SPOI-9925]   Limit number of custom panes\n\n\n[SPOI-9947]   JDBC Poll Input operator processes extra records when its container is killed\n\n\n[SPOI-9948]   JDBC Poll Input operator does not process new records when they are inserted while the app is processing the existing records\n\n\n[SPOI-9965]   Restarting the KILLED application with JDBC Poll Input operator plays the duplicate data\n\n\n[SPOI-10046]  Deleting a property directly creates tuple schema with remaining properties\n\n\n[SPOI-10049]  Limit the number of characters in role name \n\n\n[SPOI-10107]  Application service returns dag which is null\n\n\n[SPOI-10151]  Add System Properties modal should validate the properties\n\n\n[SPOI-10153]  Add System Properties \"change\" button should be warm in color\n\n\n[SPOI-10154]  Rerun Install wizard allows extension of trial\n\n\n[SPOI-10158]  Logical/Physical plan view does not retain metric selections in drop-down\n\n\n[SPOI-10165]  Container logs, dt.log files produced by chklogs.py have HTML escapes\n\n\n[SPOI-10202]  About API call gives out information without authentication.\n\n\n[SPOI-10203]  User can set any non existent package name \n\n\n[SPOI-10208]  Container state for failed attempt of app is shown as RUNNING\n\n\n[SPOI-10267]  If number of alerts are huge, gateway starts slowing down\n\n\n[SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens\n\n\n[SPOI-10275]  Alert contains an exception trace\n\n\n[SPOI-10292]  Delay in cluster metrics when the authentication is enabled\n\n\n[SPOI-10315]  \"requires Apex version\" information is missing for latest app packages on AppHub\n\n\n[SPOI-10332]  Canceling delete user action throws TypeError in developer console\n\n\n[SPOI-10333]  Configuration issue modal content goes out of modal and is not scrollable\n\n\n[SPOI-10334]  App restart doesn't take to new page\n\n\n[SPOI-10373]  FinishedTime/EndTime information is available only after application if killed/shutdown for CDH\n\n\n[SPOI-10379]  Enable Reporting button should have cool colors\n\n\n[SPOI-10385]  The app name field should not be editable at launch time\n\n\n[SPOI-10389]  UI Console should not allow creation of config pkg with special characters\n\n\n[SPOI-10396]  Application configuration should require save before launch\n\n\n[SPOI-10398]  UI says \"An error occurred while fetching data\" immediately after launching apps (intermittent)\n\n\n[SPOI-10399]  New permission do not reflect unless user logs out\n\n\n[SPOI-10401]  Deleted user can do any operations\n\n\n[SPOI-10406]  Application Configurations upload modal title should say \"Application Configuration Upload\"\n\n\n[SPOI-10409]  Updating app packages using \"check for updates\" option from AppHub gives wrong notification\n\n\n[SPOI-10410]  Deleting a property while creating tuple schema gives error in developer console\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-7720]   DT Hub shows just one version of an application\n\n\n[SPOI-10182]  dtAssemble - if there is unsaved change, keep launch button enabled which will pop up a dialog box asking save-and-launch when being clicked.\n\n\n[SPOI-8879]   Support custom JSON input for tuple schema creation\n\n\n[SPOI-9877]   Alerts History\n\n\n[SPOI-9876]   Alerts Notification\n\n\n[SPOI-9875]   Alerts Management\n\n\n[SPOI-8570]   Ability to filter tuple recording\n\n\n[SPOI-9525]   UI for dtDebug - Logs\n\n\n[SPOI-8499]   Create diagnostic tool for analyzing RM and container logs\n\n\n[SPOI-10364]  Update launch and configuration package views for simplified properties\n\n\n[SPOI-9772]   App Launch properties\n\n\n[SPOI-8764]   UI Console Configuration Packages support\n\n\n[SPOI-8611]   Support gateway configuration changes in UI\n\n\n[SPOI-10323]  Always show User profile even when license type is not enterprise\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-9785]   send GA events instead of page views for apphub page events\n\n\n[SPOI-10290]  Certification tool - RTS Installer Support for tool\n\n\n[SPOI-10179]  dtAssemble should warn about the unsaved changes when navigating away\n\n\n[SPOI-8989]   Show operator names under Physical -\n Containers\n\n\n[SPOI-10163]  dtDebug utilities README should have list of requirements  \n\n\n[SPOI-9881]   appAttempt page - Containers table - \"containerLogsUrl\" column - change it from showing a hyperlink to \"logs\" button.\n\n\n[SPOI-7343]   Ability to obtain a thread dump from a container\n\n\n[SPOI-3553]   Option to auto-tail container logs\n\n\n[SPOI-9133]   Gateway restart modal and button has soothing colors\n\n\n[SPOI-9132]   Gateway restart UI button has soothing colors\n\n\n[SPOI-9105]   Refactor security validation in console (consolidate resolve:{} from many places into one place)\n\n\n[SPOI-9047]   Make Package Upload Message clickable\n\n\n[SPOI-8766]   Make Physical DAG has the same metric selection (Top dropdown, Bottom dropdown) like Logical DAG does.\n\n\n[SPOI-8645]   Logical DAG, Physical DAG - show spinner in panel instead of blank panel before graph has been rendered and displayed.\n\n\n[SPOI-8644]   Do not show graph options(Show/Hide Stream Locality, Reset Position, Top dropdown, Bottom dropdown) until graph has been rendered and displayed.\n\n\n[SPOI-7810]   dtManage: Number of failures for an operator should have 'number search' option instead of 'string search'\n\n\n[SPOI-7277]   Ability to upload configuration file during app launch ( like -conf in dtcli )\n\n\n[SPOI-9418]   ConfigPackages backend support\n\n\n[SPOI-9400]   Change licensing for RTS to be managed/displayed in GB instead of MB\n\n\n[SPOI-9086]   Add support for DIGEST enabled Hadoop web services environment\n\n\n[SPOI-7963]   Show container stack trace in dtManage\n\n\n[SPOI-10230]  containerLogsUrl shown in appattempt table can be pretty-printed\n\n\n\n\nTask\n\n\n\n\n[SPOI-9291]   Calcite\n\n\n[SPOI-8027]   TBD: Apex Java high level API - Aggregation Part 1\n\n\n[SPOI-9769]   Tiles for apps on AppHub\n\n\n[SPOI-7965]   Productize certification tool to size RTS\n\n\n[SPOI-6350]   Gauge widget\n\n\n[SPOI-7433]   Update all relevant docs with AppHub\n\n\n[SPOI-9693]   Add Validate button in dtAssemble\n\n\n[SPOI-9688]   Remove autosave from dtAssemble\n\n\n[SPOI-9971]   Verify that alerts can only be sent by mail\n\n\n[SPOI-9364]   dtDebug Logs backend feature\n\n\n[SPOI-9695]   Launch application not using config package name\n\n\n[SPOI-9413]   Permission changes for tenancy \n\n\n[SPOI-7966]   Provide user ability to configure security through dtManage UI (only password option)\n\n\n[SPOI-8736]   dtManage should alert user when there's a potential Hadoop config issue\n\n\n[SPOI-9575]   Create demo app\n\n\n[SPOI-9021]   State management benchmark\n\n\n[SPOI-8933]   Change Megh repository to ASL  \n\n\n[SPOI-8865]   Operator Maturity Framework - Cassandra Output\n\n\n[SPOI-8788]   Operator Maturity Framework - Enhancement of FS Output Operator\n\n\n[SPOI-9966]   App-templates misc \n\n\n[SPOI-9774]   Update Database to HDFS app template \n\n\n[SPOI-9234]   AppHub - App Pipeline creation with continuous iteration\n\n\n[SPOI-10063]  Create new apex core build based on master\n\n\n[SPOI-9859]   Log retrieval tool\n\n\n[SPOI-9043]   Requirements discussion on Batch Support - Definition of Batch, Scheduling of Batch DAG, State of Batch, Replay of Batch and Monitoring of Batch  \n\n\n[SPOI-8990]   DAG Editor - unable to drag a connection stream from a port if having restriction DISABLE_APP_EDIT_STRUCTURE\n\n\n[SPOI-9972]   Document DT Gateway System Alerts\n\n\n[SPOI-9970]   Modify access to allow Admin (and ONLY admin) to set alerts\n\n\n[SPOI-9653]   Plan and implement (1 item in v1)for Gateway Alerts\n\n\n[SPOI-10261]  Show friendly message when user logs files have been removed.\n\n\n[SPOI-9163]   Refactor configPackages to configurations\n\n\n[SPOI-8223]   Troubleshooting improvements in dtManage\n\n\n[SPOI-9382]   QA - Operator Maturity Framework - AbstractFileInputOperator\n\n\n[SPOI-8885]   Operator Testing (Operator Maturity Framework)\n\n\n[SPOI-9483]   Superuser role and oAuth cleanups\n\n\n[SPOI-8996]   Add authentication configuration web services spc to gateway REST api doc\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-5852]   App Package page has a single word \"ago\" for modification time after importing pi demo\n\n\n[SPOI-6651]   Launching App from UI ignores APPLICATION_NAME attribute defined in properties.xml file\n\n\n[SPOI-7062]   dtHub UI - tags column - filter - searching from the beginning of a tag.\n\n\n[SPOI-7652]   AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings\n\n\n[SPOI-8039]   UI says \"An error occurred fetching data.\" after launching the application\n\n\n[SPOI-8349]   User should not be able to delete the default apps in ingestion-solution package\n\n\n[SPOI-8379]   Property editor for array of enum is not rendered correctly\n\n\n[SPOI-8489]   Application_Name attribute from the config file is not honored.\n\n\n[SPOI-8507]   Unable to launch an AppDataTracker application imported from dtHub\n\n\n[SPOI-8516]   DataTorrent rpm version inconsistency\n\n\n[SPOI-8522]   Unable to set roles while creating user in secure environment\n\n\n[SPOI-8523]   Users can kill the app even if privileges get revoked in secure environment \n\n\n[SPOI-8531]   Multiple MachineData demos are available at dtHub\n\n\n[SPOI-8534]   README.html for sandbox contains references to 'malhar-users'\n\n\n[SPOI-8536]   DT RTS gateway log floods with WARN message\n\n\n[SPOI-8542]   Installation: User home directory is not created by default\n\n\n[SPOI-8566]   Tuple Recording Modal Fixes\n\n\n[SPOI-8622]   Exception in retrieving app state in certification when application has not yet reached running state\n\n\n[SPOI-8630]   \"merge\" configurations option for appPackages also creates new applications\n\n\n[SPOI-8717]   Updating sandbox generates errors\n\n\n[SPOI-8781]   Buffer server metrics not available for physical operator in Metrics Chart\n\n\n[SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available\n\n\n[SPOI-8888]   Unable to see imported/uploaded/running applications on DT UI in SSL enable envornment \n\n\n[SPOI-8995]   Running through the unit tests in dtx creates a residual file\n\n\n[SPOI-9007]   Kryo Exception while re-deploying the DimensionsComputationFlexibleSingleSchemaPOJO operator\n\n\n[SPOI-9127]   Wrong notification provided by dtConsole when package upload is failed\n\n\n[SPOI-9134]   Gateway restart modal has incorrect focus\n\n\n[SPOI-9135]   Gateway restart modal should be horizontally and vertically aligned\n\n\n[SPOI-9140]   dtConsole shows \"Failed to parse\" error when 'Monitor' tab is refreshed\n\n\n[SPOI-9152]   Application package link is not working\n\n\n[SPOI-9153]   Hyperlink not required on AppPackage tab\n\n\n[SPOI-9161]   Recordings rest API gives wrong number of totalTuples\n\n\n[SPOI-9199]   Error running application due to YARN API exception\n\n\n[SPOI-9200]   dt-site.xml has a misguiding warning\n\n\n[SPOI-9245]   \nSet logging level\n cannot delete the set logs\n\n\n[SPOI-9431]   Disable tenant option in TenancyFilter\n\n\n[SPOI-9496]   Use the AppPackageOwner field instead of logged in user, while working with configPackages.\n\n\n[SPOI-9503]   AbstractFileInputOperator does not honor filePatternRegexp parameter\n\n\n[SPOI-9580]   APEXMALHAR-2314 Improper functioning in partitioning of sequentialFileRead property of FSRecordReader\n\n\n[SPOI-9658]   Apps are not filtered correctly using tags column on AppHub \n\n\n[SPOI-9660]   AppHub navigation tab is still seen as dtHub\n\n\n[SPOI-9696]   Prevent Navigation in DAG Diagram When Dragging Image Around\n\n\n[SPOI-9738]   DAG Diagram Doesn't Display Sometimes\n\n\n[SPOI-9783]   Operators stay in PENDING_DEPLOY\n\n\n[SPOI-9787]   Configuration package spelling error\n\n\n[SPOI-9790]   Recording Tuple Dialog Display Bug\n\n\n[SPOI-9791]   Fix log line format\n\n\n[SPOI-9793]   Can not start gateway after installation\n\n\n[SPOI-9794]   Can not start dtcli after installation \n\n\n[SPOI-9908]   Container StackTrace is not functioning\n\n\n[SPOI-9914]   Container buttons should be contextual based on state\n\n\n[SPOI-9916]   Kafka Input Operator (0.9) validation app is missing from QA/test-apps repository\n\n\n[SPOI-9933]   Unable to run JDBC Poll app on latest SNAPSHOT build (3.7.0)\n\n\n[SPOI-9934]   Notification History links, when clicked modal doesn't get closed\n\n\n[SPOI-9939]   Links to log files should not be restricted to enterprise edition\n\n\n[SPOI-9941]   Gateway password security errors\n\n\n[SPOI-9974]   Unable to upload packages to the gateway\n\n\n[SPOI-9977]   Unable to launch applications using apex CLI, gives ClassNotFoundException\n\n\n[SPOI-10002]  Config Package Page Not Showing Saved Properties\n\n\n[SPOI-10016]  Config package upload fails\n\n\n[SPOI-10018]  Unnecessary check boxes present for applications under Develop tab\n\n\n[SPOI-10020]  Tuple recording feature is not working\n\n\n[SPOI-10036]  Last modified time for imported packages is always shown with additional 2 minutes.\n\n\n[SPOI-10043]  User should not be allowed to save tupleSchema with blank values\n\n\n[SPOI-10044]  Editing existing tuple schema gives TypeError\n\n\n[SPOI-10046]  Deleting a property while creating tuple schema directly creates final schema with remaining properties\n\n\n[SPOI-10047]  Tuple schema created using JSON input does not take latest JSON as input\n\n\n[SPOI-10050]  Can not record samples\n\n\n[SPOI-10051]  Delete roles modal should have warm colors\n\n\n[SPOI-10052]  Restore roles modal should have warm colors\n\n\n[SPOI-10054]  Launch application for configuration package not using local settings for application naming\n\n\n[SPOI-10056]  Malhar-angular-table temporary fix for application packages and app properties lists\n\n\n[SPOI-10065]  User is allowed to \"Add System Property\" with blank value\n\n\n[SPOI-10068]  dtGateway script doesn't return correct status when gateway is down\n\n\n[SPOI-10080]  Issues with containers table from UI console\n\n\n[SPOI-10110]  AppPackage get info should not be used to show the configPackage apps\n\n\n[SPOI-10143]  StramEvents API gives 500 error\n\n\n[SPOI-10147]  \"cluster/metrics\" API gives 500 error\n\n\n[SPOI-10148]  Add system properties has weird titles\n\n\n[SPOI-10150]  Change system properties modal button should not be in cool colors\n\n\n[SPOI-10152]  Disable appdatatracker modal buttons should be in warm color\n\n\n[SPOI-10155]  AppDataTracker can not be enabled\n\n\n[SPOI-10156]  Clicking on \"ended apps\" and \"system apps\" multiple times shows multiple shadows of \"system apps\"\n\n\n[SPOI-10157]  Inspect port UI hangs\n\n\n[SPOI-10159]  Can not upload application packages\n\n\n[SPOI-10181]  killed applications - (1) \"AM Logs\" dropdown is empty. (2) AppMaster container does not have purple label in id column. \n\n\n[SPOI-10195]  Selected schema doesn't show the fields in the schema\n\n\n[SPOI-10200]  Button for \"Delete logging level\" is misaligned\n\n\n[SPOI-10209]  Link is missing for \"originalTrackingUrl\" field on currently running app attempt\n\n\n[SPOI-10228]  Sorting by \nhost\n in the Physical Plan -\n Containers tab is not working\n\n\n[SPOI-10229]  \"attempts\" tab for dtDebug is available in community edition\n\n\n[SPOI-10233]  Application attempts API does not return startedTime and finishedTime for FAILED attempts\n\n\n[SPOI-10235]  Kill Application Master container modal should have warm colors\n\n\n[SPOI-10236]  Kill selected container title has unwanted text\n\n\n[SPOI-10242]  Configuration Packages missing\n\n\n[SPOI-10257]  An error is shown for a while while launching Application Configurations\n\n\n[SPOI-10258]  Application package launch modal shows wrong \"Use configuration file\" option instead of \"Use configuration package\" \n\n\n[SPOI-10259]  Security configuration page on console has illegible content\n\n\n[SPOI-10260]  Links in Alert configuration modal are broken\n\n\n[SPOI-10277]  Stacktrace is not fully shown in alerts detail\n\n\n[SPOI-10279]  Update app hub description.\n\n\n[SPOI-10300]  ValidateApplication \n PutApplication should have CheckViewPermission instead of checkModifyPermission\n\n\n[SPOI-10343]  UI errors while displaying containers in Physical tab\n\n\n[SPOI-10350]  Jackson jars are missing from the RTS after Hadoop upgrade to 2.6 causing API failures\n\n\n[SPOI-10356]  Update Buttons Labels\n\n\n[SPOI-10358]  Schemas in ConfigPackages\n\n\n[SPOI-10359]  Schema in ConfigPackage Permission on 2 APIs should be reduced to view\n\n\n[SPOI-10361]  Upload of configPackage failed\n\n\n[SPOI-10381]  Unable to create configPackage from java application\n\n\n[SPOI-10387]  Unable to launch application configurations from \"Application Configuration details\" page\n\n\n[SPOI-10390]  Use Configuration Package Should be Enabled\n\n\n\n\nVersion: 3.6.0\n\n\nRelease date: Nov 9, 2016\n\n\nSummary\n\n\nDataTorrent RTS releases AppHub, a repository of application templates for various Big Data use cases. The key of this release is that RTS now have an infrastructure to distribute application templates easily. Developers can reduce the time to develop Big Data applications using templates. There are five templates in this release with many more to come. \n\n\n\n\nHDFS Sync \n\n\nAmazon S3 to HDFS Sync\n\n\nKafka to HDFS Sync\n\n\nHDFS to HDFS Line Copy\n\n\nHDFS to Kafka Sync\n\n\n\n\nAppendix\n\n\nImprovement\n\n\n\n\n[SPOI-9136] - Enforce DefaultOutputPort.emit() or Sink.put() thread affinity\n\n\n\n\nTask\n\n\n\n\n[SPOI-9118] - Publish App Templates for Ingestion on AppHub\n\n\n[SPOI-9419] - Update AppHub API to include the markdown content\n\n\n[SPOI-9432] - Update AppHub back end to extract markdown from apa\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-3277] - Show app master logs on UI for applications that fail at launch when we upgrade to Hadoop 2.4 or above\n\n\n[SPOI-9079] - Creation of example application for transform operator\n\n\n[SPOI-9235] - Allow users to create new configurations from Application Configurations view\n\n\n[SPOI-9240] - Create individual package view for AppHub artifacts\n\n\n[SPOI-9464] - Rename all references of AppHub on console UI to AppHub\n\n\n[SPOI-9574] - Rename title on AppHub list page\n\n\n[SPOI-9612] - Upgrade AppHub server deployment\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-9140] - dtManage shows \"Failed to parse\" error when 'Monitor' tab is refreshed\n\n\n[SPOI-9522] - DELETE call to /ws/v2/config/properties/{name} returns 500\n\n\n[SPOI-9727] - DTINSTALL_SOURCE incorrectly assumes file name\n\n\n\n\nVersion 3.5.0\n\n\nRelease date: Sep 26, 2016\n\n\nSummary\n\n\nDataTorrent RTS continues to deliver features that sets it apart in bringing operability in running an enterprise grade big data-in-motion platform. This particular release brought new features such as\n\n\n\n\nAllowing users to analyze \"stuck operator\" by obtaining stats from the running JVM (i.e. GC stats and thread dump)\n\n\nAbility to show/hide critical path in both logical and physical DAG\n\n\n\n\nApache Apex Malhar\n\n\nThe other important part of going to production is a library of operators that is more than just functional. They need to be fault tolerant, partitionable, support idempotency, and dynamically scalable. The recent release of Apache Apex Malhar 3.5.0 provides new and updated operators and APIs to bring those enterprise features. \n\n\n\n\nWindowed Operator that supports the windowing semantics outlined by Apache Beam and Google Cloud DataFlow, including the concepts of event time windows, session windows, watermarks, allowed lateness, and triggering.\n\n\nHigh level Java stream API now uses the aforementioned Windowed Operator to support stateful transformation with Apache Beam style windowing semantics.\n\n\nIntroduction of Spillable Data Structures that make use of Managed State.\n\n\nDeduper Operator to process  whether a given record is a duplicate or not\n\n\nEnricher Operator to join a stream with a lookup source and operate on any POJO object\n\n\nHBase input operator. Improve HBasePOJOInputOperator with support for threaded read\n\n\nFile Record reader module. It is useful for reading from files \"line by line\" in parallel and emit each line as seperate * tuple.\n\n\nJDBC Poll Input Operator\n\n\n\n\nFor the full release note, please go to\nhttps://blogs.apache.org/apex/entry/apache_apex_malhar_3_5\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-9232] Dedup with manage state operator marking all impression as duplicate\n\n\n[SPOI-9203]   \"check for updates\" option says 'no updated versions' and then displays updated packages\n\n\n[SPOI-9183]   Nested operator properties should follow order specified in the ORB on dtAssemble UI\n\n\n[SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-7343] - Ability to obtain a thread dump from a container\n\n\n[SPOI-8191] - Operator properties should follow order specified in the ORB on dtAssemble UI\n\n\n[SPOI-8352] - Warning message while restarting app should be changed\n\n\n[SPOI-8450] - Dedup ports connected to console should not write to log\n\n\n[SPOI-8722] - Logical DAG, Physical DAG - change \"Show/Hide String Locality\", \"Show/Hide Critical Path\" to checkbox.\n\n\n\n\nStory\n\n\n\n\n[SPOI-6794] - HBase Input Operator\n\n\n[SPOI-6795] - Creation of Concrete Cassandra Output\n\n\n[SPOI-6932] - Module to read from HDFS Input record by record and emit it downstream\n\n\n[SPOI-7948] - JDBC Input Operator\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-7836] - Trend widget fails after dashboard save/reload with PiDemo app\n\n\n[SPOI-7886] - Duplicate ports show up in the UI for POJO operators\n\n\n[SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same\n\n\n[SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500\n\n\n[SPOI-8721] - Column labeled buffer service size is showing bytes per second\n\n\n[SPOI-9084] - Refresh tokens failing in some scenarios with a login failure message\n\n\n[SPOI-9130] - dtConsole shows no applications on monitor tab\n\n\n[SPOI-9131] - gateway REST and WebSocket APIs for cluster metrics fail to report stats\n\n\n\n\nTask\n\n\n\n\n[SPOI-8411] - Deduper operator using Managed State\n\n\n[SPOI-9004] - Deduper Documentation\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-8389] - ORB defaults for FileSystem related operators\n\n\n[SPOI-8390] - Added operator default values in Application.json for user visibility\n\n\n[SPOI-8410] - Kafka Input Operator Unit test failed\n\n\n[SPOI-8461] - AbstractKafkaInputOperator problems\n\n\n\n\nVersion 3.4.0\n\n\nSummary\n\n\nAffinity rules provides a way to specify hints on how operators should be deployed in a Hadoop cluster. There are two types of rules: affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together. Anti-affinity rule, the new feature in Apache Apex 3.4.0, indicates that the group of operators should be allocated separately.\n\n\nThis release also includes a lot of bug fixes. Please see appendix for full list. \n\n\ndtManage\n\n\n\n\nUser can restart a killed application from dtManage\n\n\n\"Retrieve Ended Apps\" button changes to \"Hide Ended Apps\" after dtManage retrieves killed apps.\n\n\nUser can use mouse scroll to zoom in/out of physical DAG view\n\n\n\n\nApache Apex 3.4.0\n\n\n\n\nBlacklist problem nodes from future container requests\n\n\nSupport adding module to application using property file API.\n\n\nAbility to obtain a thread dump from a container\n\n\nRPC timeout is now configurable\n\n\nWhen an operator is blocked, it nows print out a warning instead of debug\n\n\n\n\nAppendix\n\n\nKnown Issues\n\n\n\n\n[SPOI-8518] - Support links from Dashboard to Application instance pages\n\n\n[SPOI-8516] - Datatorrent rpm version inconsistency\n\n\n[SPOI-8470] - Check for already existing app package should be done before uploading the whole package\n\n\n[SPOI-8436] - Documentation is needed on \"How to use Transform operators in ingestion solution app package\"\n\n\n[SPOI-8434] - Documentation is needed on \"How to use Generic JDBC/PostgreSQL operators in ingestion solution app package\"\n\n\n[SPOI-8433] - Documentation is needed on \"How to use Enrichment operator in ingestion solution app package\"\n\n\n[SPOI-8414] - Drop down is not shown for \"Fields to copy\" property of \"POJO Enricher\" unless output schema is not specified\n\n\n[SPOI-8352] - Warning message while restarting app should be changed\n\n\n[SPOI-8296] - \"File Path\" and \"Output File Name\" properties for HDFS File Output Operator should be clubbed together\n\n\n[SPOI-8295] - \"Include Fields\" parameter for \"POJO Enricher\" has misleading description\n\n\n[SPOI-8294] - Operator Class Name for \"Delimited Parser\" operator should not be \"CSV Parser\"\n\n\n[SPOI-8293] - File permission property is not working properly for HDFS File Output Operator\n\n\n[SPOI-8291] - Parameters on dtAssemble should be logically ordered\n\n\n[SPOI-8224] - Providing Field Infos value for JDBC POJO Input Operator is too complex\n\n\n[SPOI-8192] - Clicking on edit option for apps throws validation errors in activity panel\n\n\n[SPOI-8158] - Options to modify property values should be closer to the property name on dtAssemble canvas\n\n\n[SPOI-8144] - \"Tuple Schemas\" link at top right corner of dtAssemble canvas should open in new tab\n\n\n[SPOI-8143] - Tuple Schema page not available directly from Develop tab\n\n\n[SPOI-8133] - When stream is added in dtAssemble, user should be notified if schema is required\n\n\n[SPOI-8131] - Couple of parameters for Kafka Input Operator should have dropdown selection in dtAssemble\n\n\n[SPOI-8130] - Documentation for ingestion beta operators need to be improved\n\n\n[SPOI-8128] - Port names for operators are not intuitive when displayed on dtAssemble canvas\n\n\n[SPOI-8087] - JDBC input operator query should not require explicit ordering of column names\n\n\n[SPOI-8038] - Output file names for HDFS output should not contain timestamp and '.tmp' extension\n\n\n[SPOI-8522] - Unable to set role while creating user in a secure environment.  There is a workaround by create user with no role and then assign the role\n\n\n[SPOI-8523] - User can kill app even though privileges got revoked.  This applies to secure environment only.  \n\n\n[SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500\n\n\n[SPOI-8536] - DT RTS gateway log floods with WARN message\n\n\n[SPOI-8535] - Need to restart dtgateway for enabling password authentication in sandbox\n\n\n[SPOI-8534] - README.html for sandbox contains references to 'malhar-users'\n\n\n[SPOI-8533] - Importing 'Apache Apex Malhar Iteration Demo' throws error for 'property' tag in properties.xml\n\n\n[SPOI-8532] - Importing packages from dtHub sometimes gives ZipException\n\n\n[SPOI-8531] - Multiple MachineData demos are available at dtHub\n\n\n[SPOI-8524] - Clicking on \"generate new dashboard\" first navigates to 'Learn' tab on the dtConsole\n\n\n[SPOI-8523] - Users can kill the app even if privileges get revoked in secure environment\n\n\n[SPOI-8522] - Unable to set roles while creating user in secure environment\n\n\n[SPOI-8519] - Ingestion application on dtHub still shows 'requires Apex version' with \"-incubating\"\n\n\n[SPOI-8511] - Gateway Websocket API leaks information while unauthorized\n\n\n[SPOI-8509] - dtAssemble operator documentation shows '@link' markers\n\n\n[SPOI-8507] - Unable to launch an AppDataTracker application imported from dtHub\n\n\n[SPOI-8491] - UI mixes the order and ids of tuple recording ports\n\n\n[SPOI-8490] - gateway issues in SSL enabled cluster\n\n\n[SPOI-8479] - Uninstall does not work even though the install was successful previously\n\n\n[SPOI-8477] - Upgrade License Opens in dtManage window, it should be in opened up in new window\n\n\n[SPOI-8476] - Hadoop-common-tests library shouldn't be part of RTS build\n\n\n[SPOI-8474] - On addition of huge role name, non-specific errors are shown\n\n\n[SPOI-8473] - Gateway, console allows impractially longer user roles additions\n\n\n[SPOI-8472] - Visually ugly error presentation\n\n\n[SPOI-8469] - If Kerberos tickets are changed, you have to refresh whole UI\n\n\n[SPOI-8467] - installation script provides incorrect information\n\n\n[SPOI-8432] - JDBC input operator is failing with exception \"fetching metadata\"\n\n\n[SPOI-8424] - Dedup does not honor the expiryPeriod when error tuple is introduced in between two valid tuples\n\n\n[SPOI-8422] - Time properties for operators should have units mentioned for them\n\n\n[SPOI-8416] - dtAssemble Can't change application name\n\n\n[SPOI-8365] - HDFS sync app : Unable to sync 500 GB file\n\n\n[SPOI-8358] - Uptime and latency values are very high exactly after app is launched\n\n\n[SPOI-8317] - dtIngest 1.1.0 (Compiled against 3.2.0) can not be launched\n\n\n[SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same\n\n\n[SPOI-8213] - Application DAG is not displayed when clicked on app link\n\n\n[SPOI-8202] - Unable to add custom properties while launching apps\n\n\n[SPOI-8197] - Default values for \"Field Infos\" and \"Bucket Manager\" properties should be set appropriately\n\n\n[SPOI-7986] - Gateway proxy feature is not working\n\n\n[SPOI-7934] - Kafka-dedup-HDFS-solution: Ahead in time messages are lost from Dedup operator\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-7939] - Dynamic repartition causes application to hang\n\n\n[SPOI-8468] - Can't assign roles for users in secure environment\n\n\n[SPOI-8464] - \"Disable Reporting\" option on System Configuration page gives NullPointerException\n\n\n[SPOI-8024] - Gateway is leaving behind dtcheck temp files in HDFS\n\n\n[SPOI-7640] - Changing dashboard name in dashboard settings modal and then canceling does not revert dashboard name\n\n\n[SPOI-8077] - Gateway logs NPE if an app master is misbehaving\n\n\n[SPOI-8046] - Kerberos Cluster: Installation Wizard can not get past beyond Hadoop Configuration screen\n\n\n[SPOI-8055] - Console references to dt-text-tooltip no longer produce a tool tip\n\n\n[SPOI-7898] - Default JAAS support classes duplicated in dt-library\n\n\n[SPOI-7929] - Update log4j.properties in the DTX/dist/install to set debug level for org.apache.apex\n\n\n[SPOI-7943] - The demo applications fails due to numberOfBuckets is less than 1\n\n\n[SPOI-8092] - Problems in launching jobs with authentication enabled on secure cluster\n\n\n[SPOI-7794] - Monitor page not refreshing properly\n\n\n[SPOI-7889] - Cannot read property 'hideBreadcrumbs' of undefined\n\n\n[SPOI-7856] - Modify application packages dtHub import/update paths\n\n\n[SPOI-7907] - Downloads of AppPackages result in corrupted files during local testing\n\n\n[SPOI-7971] - After dynamic repartition application appears blocked\n\n\n\n\nApache Apex 3.4.0\n\n\nNew Feature\n\n\n\n\n[APEXCORE-10] - Enable non-affinity of operators per node (not containers)\n\n\n[APEXCORE-359] - Add clean-app-directories command to CLI to clean up data of terminated apps\n\n\n[APEXCORE-411] - Restart app without specifying app package\n\n\n\n\nImprovement\n\n\n\n\n[APEXCORE-92] - Blacklist problem nodes from future container requests\n\n\n[APEXCORE-107] - Support adding module to application using property file API.\n\n\n[APEXCORE-304] - Ability to add jars to classpath in populateDAG\n\n\n[APEXCORE-328] - CLI tests should not rely on default maven repository or mvn being on the PATH\n\n\n[APEXCORE-330] - Ability to obtain a thread dump from a container\n\n\n[APEXCORE-358] - Make RPC timeout configurable\n\n\n[APEXCORE-380] - Idle time sleep time should increase from 0 to a configurable max value\n\n\n[APEXCORE-383] - Time to sleep while reservoirs are full should increase from 0 to a configurable max value\n\n\n[APEXCORE-384] - For smaller InlineStream port queue size use ArrayBlockingQueueReservoir as default\n\n\n[APEXCORE-399] - Need better debug information in stram web service filter initializer\n\n\n[APEXCORE-400] - Create documentation for security\n\n\n[APEXCORE-401] - Create a separate artifact for checkstyle and other common configurations\n\n\n[APEXCORE-407] - Adaptive SPIN_MILLIS for input operators\n\n\n[APEXCORE-409] - Document json application format\n\n\n[APEXCORE-419] - When operator is blocked, print out a warning instead of debug\n\n\n[APEXCORE-447] - Document: update AutoMetrics with AppDataTracker link\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block\n\n\n[APEXCORE-201] - Reported latency is wrong when a downstream operator is behind more than 1000 windows\n\n\n[APEXCORE-326] - Iteration causes problems when there are multiple streams between two operators\n\n\n[APEXCORE-335] - StramLocalCluster should teardown StreaminContainerManager after run is complete\n\n\n[APEXCORE-349] - Application/operator/port attributes should be returned using string codec in REST service\n\n\n[APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers\n\n\n[APEXCORE-352] - Temp directories/files not created in temp directory specified by system property java.io.tmpdir\n\n\n[APEXCORE-353] - Buffer server may stop processing data\n\n\n[APEXCORE-355] - CLI list-*-attributes command name change\n\n\n[APEXCORE-362] - NPE in StreamingContainerManager\n\n\n[APEXCORE-363] - NPE in StreamingContainerManager\n\n\n[APEXCORE-374] - Block with positive reference count is found during buffer server purge\n\n\n[APEXCORE-375] - Container killed because of Out of Sequence tuple error.\n\n\n[APEXCORE-376] - CLI command 'dump-properties-file' does not work when connected to an app\n\n\n[APEXCORE-385] - Temp directories/files not always cleaned up when launching applications\n\n\n[APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily\n\n\n[APEXCORE-393] - Reset failure count when consecutive failed node is removed from blacklist\n\n\n[APEXCORE-397] - Allow configurability of stram web services authentication\n\n\n[APEXCORE-398] - Ack may not be delivered from buffer server to it's client\n\n\n[APEXCORE-403] - DelayOperator unit test fails intermittently\n\n\n[APEXCORE-413] - Collision between Sink.getCount() and SweepableReservoir.getCount()\n\n\n[APEXCORE-415] - Input Operator double checkpoint\n\n\n[APEXCORE-421] - Double Checkpointing May Happen In Input Node On Shutdown\n\n\n[APEXCORE-422] - Checkstyle rule related to allowSamelineParameterizedAnnotation suppression\n\n\n[APEXCORE-434] - ClassCastException when making webservice calls to STRAM in secure mode\n\n\n[APEXCORE-436] - Update log4j.properties in archetype test resources to set debug level for org.apache.apex\n\n\n[APEXCORE-439] - After dynamic repartition application appears blocked\n\n\n[APEXCORE-444] - 401 authentication errors when making webservice calls to STRAM in secure mode\n\n\n[APEXCORE-445] - Race condition in AsynFSStorageAgent.save()\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-293] - Add core and malhar documentation to project web site\n\n\n[APEXCORE-319] - Document backward compatibility guidelines\n\n\n[APEXCORE-340] - Rename dtcli script to apex\n\n\n[APEXCORE-345] - Upgrade to 0.7.0 japicmp\n\n\n[APEXCORE-381] - Upgrade async-http-client dependency version because of security issue\n\n\n[APEXCORE-410] - Upgrade to netlet 1.2.1\n\n\n[APEXCORE-423] - Fix style violations in Apex Core\n\n\n[APEXCORE-446] - Add source jar in the shaded-ning build\n\n\n\n\nSub-task\n\n\n\n\n[APEXCORE-254] - Introduce Abstract and Forwarding Reservoir classes\n\n\n[APEXCORE-269] - Provide concrete implementation of AbstractReservoir based on SpscArrayQueue\n\n\n[APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size\n\n\n[APEXCORE-369] - Fix timeout in AbstractReservoirTest.performanceTest\n\n\n[APEXCORE-402] - SpscArrayQueue to notify publishing thread on not full condition\n\n\n\n\nVersion 3.3.0\n\n\nSummary\n\n\ndtHub\n\n\nDataTorrent RTS allows companies to quickly build low latency real time Big Data application that can scale and fault tolerant. With the introduction of dtHub, DataTorrent now host and maintain an application distributor. You can access it through dtManage and easily install or update application without having to upgrade the whole RTS platform. Prior to dtHub, RTS installer packaged both platform and applications. In 3.3, they are decoupled with significantly reduced installer size. You can independently choose when to upgrade the platform and when to install/upgrade applications.\n\n\ndtManage\n\n\n\n\nTroubleshooting is made easier now that user can view the containers where physical operators have lived\n\n\nRTS Community Edition user can now view container logs and set log levels via dtManage. API for runtime DAG and property change are also available\n\n\nRTS Enterprise Edition is changed from 30 days to 60 days\n\n\nWhen an application is killed, RTS will delete the app directory according to policy  in dt-site.xml\n\n\n\n\ndtDasbhoard\n\n\n\n\nNew widgets for the dashboard: Geo coordinates with circles, Geo regions with gradient fill and single Value \n\n\n\n\ndtAssemble (beta)\n\n\n\n\nTop level \u201cDevelop\u201d takes users directly to Application Page\n\n\nNavigation changes on how user get to Tuple Schema. User goes to It is now \"Edit Application\" then \"Tuple Schema\"\n\n\nDevelopment (breadcrumb link)\n\n\n\n\nDocumentation\n\n\n\n\nFileSplitter: http://docs.datatorrent.com/operators/file_splitter/\n\n\nBlock Reader: http://docs.datatorrent.com/operators/block_reader/\n\n\n\n\nApache Apex 3.3\n\n\n\n\nSupport for iterative processing. It is a building block to support machine learning\n\n\nAbility to populate DAG at application launch time\n\n\nPre checkpoint operator callback so that it can execute a logic before the operator gets checkpointed (e.g. flush file to HDFS)\n\n\nProvide the option for operator to do checkpointing in a distribute in-memory store. It is faster than HDFS due to disk i/o latency\n\n\nAdd group ID information in an applicatin package.  It is visible for application grouping in dtHub.\n\n\n\n\nKnown Issues in 3.3\n\n\n\n\n[SPOI-7696] - Community edition (which gets activated after expired license) does not have newly added community features\n\n\n[SPOI-7471] - Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7697] - \"Set Logging Levels\" option on application details page does not show initial target/loglevel fields\n\n\n[SPOI-7668] - If AppDataTracker is disabled, changing the YARN queue does not restart it\n\n\n[SPOI-7652] - AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings\n\n\n[SPOI-7678] - On configuration complete, summary page should populate RTS version\n\n\n[SPOI-7479] - dtHub \"check for updates\" option says 'no updated versions' and then displays updated packages\n\n\n[SPOI-7626] - Stacked Area Chart widget shows NaN values on Firefox\n\n\n\n\nAppendix\n\n\ndtHub\n\n\n\n\n[SPOI-6787] - dtHub UI - explore, import, download\n\n\n[SPOI-7023] - dtHub UI - check for updates\n\n\n[SPOI-3643] - Remove import default packages from Application Packages page\n\n\n[SPOI-7451] - Add options summary to Application Packages screen\n\n\n[SPOI-6968] - Please make API return a new property that indicates whether gateway has internet access or not (for dtHub feature)\n\n\n[SPOI-7059] - add \"tags\" to import list\n\n\n\n\ndtManage\n\n\n\n\n[SPOI-3549] - dtManage - Ability to view container history where physical operator has lived\n\n\n[SPOI-7382] - UI evaluation license expiration message colour update\n\n\n[SPOI-7418] - Visualize AppDataTracker data\n\n\n[SPOI-7129] - Add countdown and links to enterprise evaluation in dtManage\n\n\n[SPOI-7226] - Remove choice of community edition and enterprise evaluation in install wizard\n\n\n\n\ndtDashboard\n\n\n\n\n[SPOI-6522] - Geo coordinates with weighted circles widget\n\n\n[SPOI-6523] - Geo regions with gradient fill widget\n\n\n[SPOI-7247] - Create dimensional single value widget\n\n\n[SPOI-7258] - Persist label changes in trend and single value widgets\n\n\n[SPOI-6023] - dtDashboard - Widgets that support dimension schema can support multi-value keys \n\n\n[SPOI-6021] - Support tag for  snapshot server and dimension store\n\n\n[SPOI-6331] - Notify user when widget is unable to automatically load data\n\n\n[SPOI-6658] - UI says \"no rows testing\" when no data available to display in tables\n\n\n[SPOI-6887] - Changing choropleth map class does not remove previously selected map\n\n\n[SPOI-7246] - Change default widget colors to be websafe\n\n\n[SPOI-7257] - add tags-based dimension query settings in trend widget \n\n\n\n\ndtAssemble (beta)\n\n\n\n\n[SPOI-7133] - Tuple Schemas change and Develop on top navigation bar change\n\n\n\n\nMegh\n\n\n\n\n[SPOI-7665] - Megh enhancements for TelecomDemo\n\n\n[SPOI-7230] - Add group id information to all megh app packages\n\n\n[SPOI-7251] - Add directory structure for modules in megh\n\n\n[SPOI-7693] - Add Telecom Demo To Megh\n\n\n\n\nDocs\n\n\n\n\n[SPOI-6471] - Documentation: FileSplitter\n\n\n[SPOI-6472] - Documentation: BlockReader\n\n\n\n\nRTS Community Edition\n\n\n\n\n[SPOI-7670] - Removing dtManage restrictions from community edition\n\n\n[SPOI-7682] - Lock down all auth/security features in community edition\n\n\n[SPOI-7130] - Community edition to have an upgrade button to Enterprise eval\n\n\n[SPOI-7243] - make all locked-feature places(e.g. Visualize link, logs button, new application button) to have the same effect of \"upgrade to enterprise                \" link in community edition, and to have a key icon instead of being crossing out\n\n\n\n\nRTS Enterprise Edition\n\n\n\n\n[SPOI-7127] - Change enterprise evaluation days from 30 days to 60 days\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-5622] - dtcli: Command 'show-physical-plan' fails with \"Failed web service request\" error\n\n\n[SPOI-6352] - Gateway's RM Proxy REST calls don't work with HA-enabled\n\n\n[SPOI-6424] - Gateway cannot determine its own local address to the cluster when RM HA is enabled\n\n\n[SPOI-6555] - Add Missing datatorrent.apppackage.classpath property to dt-demos\n\n\n[SPOI-6624] - ADT issues impacting dtingest Dashboard\n\n\n[SPOI-6674] - AbstractFileOuptutOperator refactoring and fixes\n\n\n[SPOI-6834] - fixing scope of jars in Megh\n\n\n[SPOI-6837] - Gateway has a lot of blocked threads under heavy load\n\n\n[SPOI-6886] - Selected map feature / object should be persisted\n\n\n[SPOI-6949] - Collation in BucketManager incurs a performance hit while writing to HDFS and is not an optimization\n\n\n[SPOI-6999] - Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7026] - only lists the ones that have different versions in update section\n\n\n[SPOI-7027] - fix a bug that loading is forever when there is no updates in check for updates\n\n\n[SPOI-7028] - compare version number and only list packages whose dtHub version is higher than installed version in update section\n\n\n[SPOI-7037] - support sorting, filter in string-type columns in import pkgs, update pkgs\n\n\n[SPOI-7040] - optimize visual layout of import pkgs page\n\n\n[SPOI-7042] - only shows packages that are compatible with the APEX version in check for update list\n\n\n[SPOI-7080] - remove bar chart widget that is not in use\n\n\n[SPOI-7106] - Update Megh japi version and fix the broken build because of semantic version\n\n\n[SPOI-7138] - Dimension unifier return empty results \n\n\n[SPOI-7236] - Console build fails due to jsHint issues after updating version\n\n\n[SPOI-7293] - Post installation links no longer available on datatorrent.com\n\n\n[SPOI-7296] - gateway spills out error trying to write to /var/log/datatorrent/ in local install\n\n\n[SPOI-7297] - Local install fails in secure mode\n\n\n[SPOI-7309] - HDHT Broken and Hangs After Wall And Purge Changes\n\n\n[SPOI-7338] - After changing configuration \"dt.appDataTracker.queue\" (e.g. to \"root.ashwin\") and kill system application AppDataTracker, AppDataTracker                 incorrectly starts with the default queue (\"root.dtadmin\").\n\n\n[SPOI-7350] - Installation wizard final step refers to invalid developers URL\n\n\n[SPOI-7361] - get-operator-attributes fails on CDH\n\n\n[SPOI-7383] - Update invalid links in UI console info section\n\n\n[SPOI-7447] - dtHub UI - check for updates - when there is no newer version for any installed package, loading image will be hanging forever\n\n\n[SPOI-7460] - On Visualize tab, link for documentation does not work\n\n\n[SPOI-7469] - Remove \"in HDHT\" from System Configuration page\n\n\n[SPOI-7471] - Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7474] - \"Disable App Data Tracker\" option does not work\n\n\n[SPOI-7481] - Launch macros for demo apps should be removed in dtcli\n\n\n[SPOI-7482] - Cloning/deleting application under Application package does not refresh the list\n\n\n[SPOI-7498] - Verify dtingest download link change on datatorrent website\n\n\n[SPOI-7502] - Lock mishandling in App Package local cache code\n\n\n[SPOI-7503] - Changing \"dt.appDataTracker.enable=true/false\" using REST API calls doesn't take effect unless gateway restarts\n\n\n[SPOI-7510] - Application launch notifications no longer show up\n\n\n[SPOI-7523] - Kill only AppDataTracker whose user is the current login user when App Data Tracker queue is changed.\n\n\n[SPOI-7542] - Unable to import app package from dtHub\n\n\n[SPOI-7574] - dtcli command 'dump-properties-file' does not work when connected to an app\n\n\n[SPOI-7577] - For 3.3.0 release, RTS version is shown as 3.3.1 in System Configuration\n\n\n[SPOI-7604] - HDHT bucket meta class is obfuscated incorrectly\n\n\n[SPOI-7615] - dtHub intermittently throws error as \"Failed to import\" while importing multiple pkgs at the same time \n\n\n[SPOI-7619] - specify bower link malhar-angular-dashboard to version 1.0.1\n\n\n[SPOI-7644] - Need to update installer script\n\n\n[SPOI-7679] - Unable to access newly added features for Community edition.\n\n\n\n\nApache Apex 3.3 Change Logs\n\n\nhttps://github.com/apache/incubator-apex-core/blob/v3.3.0-incubating/CHANGELOG.md\n\n\n\n\n[SPOI-7061] - Implement retention policy for terminated apps\n\n\n[SPOI-7492] - DT_GATEWAY_CLIENT_OPTS overrides all JVM options and there is no way to supply additional options to the default options\n\n\n[SPOI-5735] - Create local file cache for app package\n\n\n[SPOI-6981] - Move orderedOutput feature to AbstractDeduper. Rename AbstractDeduperOptimized to AbstractBloomFilterDeduper\n\n\n[SPOI-7448] - Work around the attribute bug detailed in APEXCORE-349 so that ADT still works\n\n\n[SPOI-7470] - Work around namenode NPE bug in hadoop 2.7.x to avoid throwing NPE to the user. https://issues.apache.org/jira/browse/APEXCORE-45 and https://issues.apache.org/jira/browse/HDFS-9851\n\n\n[SPOI-6381] - Support Dynamically Updating Enum Values For Keys In The Dimensions Store\n\n\n[SPOI-6545] - Allow Gateway to directly contact dtHub to install app packages\n\n\n\n\nNew Feature\n\n\n\n\n[APEXCORE-3] - Ability for an operator to populate DAG at launch time\n\n\n[APEXCORE-60] - Iterative processing support\n\n\n[APEXCORE-78] - Pre-Checkpoint Operator Callback\n\n\n[APEXCORE-276] - Make App Data Push transport pluggable and configurable\n\n\n[APEXCORE-283] - Operator checkpointing in distributed in-memory store\n\n\n[APEXCORE-288] - Add group id information to apex app package\n\n\n\n\nImprovement\n\n\n\n\n[APEXCORE-40] - Semver dependencies should be in Maven Central\n\n\n[APEXCORE-162] - Enhance StramTestSupport.TestMeta API\n\n\n[APEXCORE-181] - Expose methods in StramWSFilterInitializer to get the RM webapp address\n\n\n[APEXCORE-188] - Make type graph lazy load\n\n\n[APEXCORE-199] - CLI should check for version compatibility when launching app package\n\n\n[APEXCORE-228] - Add maven 3.0.5 as prerequisites to the Apex parent pom\n\n\n[APEXCORE-229] - Upgrade checkstyle maven plugin (2.17) and checkstyle dependency (6.11.2)\n\n\n[APEXCORE-291] - Provide a way for an operator to specify its metric aggregator instance\n\n\n[APEXCORE-305] - Enable checkstyle violations logging to console during maven build\n\n\n\n\nBug\n\n\n\n\n[APEXCORE-58] - endWindow is being called even when the operator is being undeployed\n\n\n[APEXCORE-83] - beginWindow not called on recovery\n\n\n[APEXCORE-193] - apex-app-archetype has extraneous entry that generates a warning when running it\n\n\n[APEXCORE-204] - Update checkstyle and codestyle to be the same\n\n\n[APEXCORE-211] - Brace placement after static blocks in checkstyle configuration\n\n\n[APEXCORE-263] - Checkpoint can be performed twice for same window\n\n\n[APEXCORE-274] - removeTerminatedPartition fails for Unifier operator\n\n\n[APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection\n\n\n[APEXCORE-278] - GenericNodeTest clutters test logs with unnecessary statement\n\n\n[APEXCORE-296] - Memory leak in operator stats processing\n\n\n[APEXCORE-300] - Fix checkstyle regular expression\n\n\n[APEXCORE-303] - Launch properties not evaluated\n\n\n\n\nTask\n\n\n\n\n[APEXCORE-24] - Takes out usage of Rhino as it is GPL 2.0\n\n\n[APEXCORE-186] - Enable license check in Travis CI\n\n\n[APEXCORE-253] - Apex archetype includes dependencies which do not belong to org.apache.apex\n\n\n[APEXCORE-298] - Reduce the severity of  line length check\n\n\n[APEXCORE-301] - Add \"io\" as a separate import to checkstyle rules\n\n\n[APEXCORE-302] - Update NOTICE copyright year\n\n\n[APEXCORE-308] - Implement findbugs plugin reporting\n\n\n[APEXCORE-317] - Run performance benchmark for the Apex Core 3.3.0 release\n\n\n\n\nSub-task\n\n\n\n\n[APEXCORE-104] - Expand Module DAG\n\n\n[APEXCORE-105] - Support injecting properties through xml file on modules.\n\n\n[APEXCORE-144] - Provide REST api for listing information about module.\n\n\n[APEXCORE-151] - Provide code style templates for major IDEs (Eclipse, IntelliJ and NetBeans)\n\n\n[APEXCORE-182] - Add Apache copyright to IntelliJ\n\n\n[APEXCORE-194] - Add support for ProxyPorts in Modules\n\n\n[APEXCORE-226] - Strictly enforce wrapping indentation in checkstyle\n\n\n[APEXCORE-227] - Enforce left brace placement for anonymous class on the next line\n\n\n[APEXCORE-230] - Limit line lengths to be 120\n\n\n[APEXCORE-239] - Upgrade checkstyle to 6.12 from 6.11.2\n\n\n[APEXCORE-248] - Increase wrapping indentation from 2 to 4.\n\n\n[APEXCORE-249] - Enforce class, method, constructor annotations on a separate line\n\n\n[APEXCORE-250] - Exclude DtCli from System.out checks\n\n\n[APEXCORE-267] - Fix existing checkstyle violations in api\n\n\n[APEXCORE-270] - Enforce checkstyle validations on test classes\n\n\n[APEXCORE-272] - Attributes added to operator inside Module is not preserved.\n\n\n[APEXCORE-273] - Fix existing checkstyle violations in bufferserver module\n\n\n[APEXCORE-306] - Recovery checkpoint handing in iteration loops\n\n\n\n\nVersion 3.2.1\n\n\nSummary\n\n\nThis release bundles a few fixes and features for customers who are on 3.2.0 and not ready to upgrade to 3.3.0  \n\n\nImprovement\n\n\n\n\n[SPOI-7900] add default role to first time login user\n\n\n[SPOI-7061] Implement retention policy for terminated apps\n\n\n\n\nBug Fixes\n\n\n\n\n[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7471] Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7971] After dynamic repartition application appears blocked\n\n\n\n\nKnown Issues\n\n\n\n\n[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS\n\n\n[SPOI-7061] Implement retention policy for terminated apps\n\n\n[SPOI-7471] Temp directories/files not cleaned up in Gateway\n\n\n[SPOI-7971]After dynamic repartition application appears blocked\n\n\n\n\nApache Apex\n\n\n\n\n[APEXCORE-327] - Implement proper semantic version checks in patch release branches\n\n\n[APEXCORE-358] - Make RPC timeout configurable\n\n\n[APEXCORE-410] - Upgrade to Netlet 1.2.1\n\n\n[APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size\n\n\n\n\nApache Apex Bug Fixes\n\n\n\n\n[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block\n\n\n[APEXCORE-274] - removeTerminatedPartition fails for unifier operator\n\n\n[APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection\n\n\n[APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers\n\n\n[APEXCORE-353] - Buffer server may stop processing data\n\n\n[APEXCORE-362] - NPE in StreamingContainerManager\n\n\n[APEXCORE-363] - NPE in StreamingContainerManager\n\n\n[APEXCORE-374] - Block with positive reference count is found during buffer server purge\n\n\n[APEXCORE-375] - Container killed because of Out of Sequence tuple error.\n\n\n[APEXCORE-385] - Temp directories/files not always cleaned up when launching applications\n\n\n[APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily\n\n\n[APEXCORE-398] - Ack may not be delivered from buffer server to it's client\n\n\n\n\nVersion 3.2.0\n\n\nNew Feature\n\n\n\n\n[SPOI-6351] - Add feature to REST API to get queue information from cluster\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n[SPOI-6641] - Implement \"forever\" bucket in DimensionComputation\n\n\n[DTIN-40] - Observed unused variables in SplunkBytesInputOperator\n\n\n[DTIN-69] - Move Query Operators implementation to newer one\n\n\n[DTIN-50] - [dtIngest] add parameter \"parallel readers\"\n\n\n\n\nBug\n\n\n\n\n[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory\n\n\n[SPOI-5216] - FileSplitter fails with ConcurrentModificationException\n\n\n[SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'\n\n\n[SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up\n\n\n[SPOI-5823] - Downstream container falling behind when buffer spooling is enabled\n\n\n[SPOI-5899] - Ability to retrieve schemas from an app package\n\n\n[SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation\n\n\n[SPOI-6098] - 'single run' doesnt work\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6236] - Add 1s aggregations to App Data Tracker\n\n\n[SPOI-6298] - Dimensions Store Can Become Blocked\n\n\n[SPOI-6383] - Sometimes expired query is still executed.\n\n\n[SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console\n\n\n[SPOI-6446] - Merge PojoEnrichment and TupleEnrichment\n\n\n[SPOI-6505] - Exceptions from asm when uploading apps\n\n\n[SPOI-6603] - Warning messages shown on console are not completely visible\n\n\n[SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks\n\n\n[SPOI-6846] - DT dashboard guide link is not working\n\n\n[SPOI-6855] - Broken links observed on summary page while DT RTS configuration \n\n\n[SPOI-6856] - Correct docs index.html links and title\n\n\n[DTIN-51] - bandwidth option was removed during merge\n\n\n[DTIN-101] - For message-based-input to message-based-output, compression \n encryption options should not be visible on config UI\n\n\n[DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum\n\n\n[DTIN-112] - Message based input to FTP output fails with IOException while appending the data\n\n\n[DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata\n\n\n[DTIN-123] - All files are not getting copied when bandwidth option is specified\n\n\n[DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning\n\n\n[DTIN-126] - 'Compact files' option should be disabled for message based Input type\n\n\n[DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file\n\n\n[DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only\n\n\n[DTIN-155] - Messages are dropped when bandwidth option is enabled\n\n\n[DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError\n\n\n[DTIN-161] - Message based input to S3N output fails with IOException while appending the data\n\n\n[DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka\n\n\n[DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled\n\n\n[DTIN-165] - Data source names for dtingest should not contain 'null'\n\n\n[DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream\n\n\n[DTIN-176] - Parallel read is not working in case of FTP and S3 as input\n\n\n[DTIN-186] - FileMerger failed with unable to merge file exception\n\n\n\n\nTask\n\n\n\n\n[SPOI-5173] - DAG validation: Attribute values Serializable\n\n\n[SPOI-5403] - Ingestion Splunk integration\n\n\n[SPOI-5801] - Make a MapR partner (datatorrent) sandbox\n\n\n[SPOI-5958] - dtView Integration for ingestion metric visualization\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n[SPOI-6439] - Run benchmark for 3.2.0 release\n\n\n[SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies\n\n\n[DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script\n\n\n[DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator\n\n\n[DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch\n\n\n[DTIN-54] - Disabling the splunk from dtIngest script\n\n\n[DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-5369] - Integrate schema support in Cassandra Input/Output\n\n\n[SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output\n\n\n[SPOI-5819] - Bandwidth control for file based sources\n\n\n[SPOI-5820] - Bandwidth control for message based sources\n\n\n[SPOI-5959] - Metrics for compression, encryption\n\n\n[SPOI-6337] - Expose bandwidth metrics\n\n\n[SPOI-6441] - Change console home page to be welcome screen instead of operations summary\n\n\n\n\nVersion 3.1.1\n\n\nImprovement\n\n\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n\n\nBug\n\n\n\n\n[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock\n\n\n[SPOI-6032] - App Builder should not show property from super class\n\n\n[SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6090] - Ingestion: Error decrypting files for message based sources\n\n\n[SPOI-6147] - Launch issue with \"Starter Application Pack\"\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6203] - -ve Memory reported for Application Master\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager\n\n\n[SPOI-6304] - Fix netlet dependency\n\n\n[SPOI-6313] - Work Around APEX-129\n\n\n[SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property\n\n\n\n\nTask\n\n\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-6148] - Update website with release 3.1\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n\n\nVersion 3.1.0\n\n\nNew Feature\n\n\n\n\n[SPOI-4670] - Enable message schema management in App Builder\n\n\n[SPOI-5844] - Retrieve older versions of schemas\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n\n\nBug\n\n\n\n\n[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.\n\n\n[SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5743] - App package import also needs to do the typegraph stuff as upload\n\n\n[SPOI-5831] - Add getter for properties for Twitter Demo\n\n\n[SPOI-5833] - Schema class not loaded during validation\n\n\n[SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts\n\n\n[SPOI-5857] - Gateway has trouble getting to STRAM until after restart\n\n\n[SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml\n\n\n[SPOI-5946] - Port compatibility when port require schemas\n\n\n[SPOI-6002] - AppBuilder fails to load operator due to NullPointerException\n\n\n\n\nTask\n\n\n\n\n[SPOI-5307] - Mocha based tests for Gateway API calls\n\n\n[SPOI-5749] - Certify MapR sandbox\n\n\n[SPOI-5750] - Create Splunk forwarder operators\n\n\n[SPOI-5751] - Create splunk forwarder input operator\n\n\n[SPOI-5752] - Create splunk forwarder output operator\n\n\n[SPOI-5753] - Create splunk forwarder configuration specification\n\n\n[SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-5756] - Test license expiry on sandbox as part of sandbox testing\n\n\n[SPOI-5764] - Gateway to show better error messages on all 500 errors\n\n\n[SPOI-5803] - Support of custom aggregation for ADT\n\n\n[SPOI-5893] - Changes for retrieving container and operator history information\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-4946] - Handle new @useSchema and @description property metadata in UI\n\n\n[SPOI-4980] - Schema Management in the UI\n\n\n[SPOI-5505] - handle Object-to-Object port compatibility with and without schema\n\n\n[SPOI-5506] - handle Object-to-Pojo port compat with schema\n\n\n[SPOI-5513] - handle port compatibility with generic type ports\n\n\n[SPOI-5747] - Automatically generate new eval license when sandbox starts up\n\n\n\n\nVersion 3.0.0\n\n\nSub-task\n\n\n\n\n[SPOI-1901] - Dynamic property changes lost on AM restart \n\n\n[SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package\n\n\n[SPOI-4891] - Example for schema meta data and property doclet tag\n\n\n[SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks\n\n\n[SPOI-4972] - Design Schema API for managing schemas\n\n\n[SPOI-4973] - Create a Schema resource which will contain schema calls\n\n\n[SPOI-4987] - Generate pojo class using schema provided in json\n\n\n[SPOI-4999] - Implement the rest call to save schema on the backend\n\n\n[SPOI-5211] - Support custom visualization link\n\n\n[SPOI-5237] - Type of attributes are wrong when it is an inner class or enum\n\n\n[SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp\n\n\n[SPOI-5361] - Button for dt.phoneHome.enable property on system config page\n\n\n[SPOI-5527] - Implement Queue Option\n\n\n\n\nBug\n\n\n\n\n[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs\n\n\n[SPOI-4633] - Resolve type variable across the type hierarchy \n\n\n[SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times \n\n\n[SPOI-4884] - Intermittent failure for CustomMetricTest\n\n\n[SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.\n\n\n[SPOI-4941] - For kafka operator in app builder certain properties need to be set twice\n\n\n[SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically\n\n\n[SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway\n\n\n[SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency\n\n\n[SPOI-5323] - Malhar operators are not packaged with distribution for 3.0\n\n\n[SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page\n\n\n[SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon\n\n\n[SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes\n\n\n[SPOI-5385] - No error message is returned when DTCli gets an NPE\n\n\n[SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments\n\n\n[SPOI-5433] - Asm code not working for jdk 1.8\n\n\n[SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout\n\n\n[SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle\n\n\n[SPOI-5484] - Ingestion FTP as output does not work with vsftpd\n\n\n[SPOI-5497] - Could not open pi demo in 3.0.0 RC2\n\n\n[SPOI-5498] - Typegraph exception with 3.0.0 RC2\n\n\n[SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404\n\n\n[SPOI-5504] - Make temporary file names unique to avoid lease expiry\n\n\n[SPOI-5530] - Failed to edit pidemo JSON based application\n\n\n[SPOI-5536] - App jar is missing from typegraph\n\n\n[SPOI-5552] - Non-concrete classes being returned with assignableClasses call\n\n\n[SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table\n\n\n[SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException \n\n\n[SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data\n\n\n[SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5583] - DFS root directory check is failing on MapR cluster\n\n\n[SPOI-5593] - App Builder search tooltip\n\n\n[SPOI-5597] - Bad log level WARNING in UI\n\n\n[SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal\n\n\n[SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo\n\n\n[SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators\n\n\n[SPOI-5627] - error message from install script from 3.0.0-RC4 \n\n\n[SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator\n\n\n[SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK \n\n\n[SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group\n\n\n[SPOI-5636] - dtcp is not getting packaged in installation (RC4)\n\n\n[SPOI-5637] - Allatori Configuration errors in ingestion pom.xml\n\n\n[SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out\n\n\n[SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.\n\n\n[SPOI-5644] - isChar validation should strip \\u000 character\n\n\n[SPOI-5646] - Megh demos is failing release build\n\n\n[SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined\n\n\n[SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode\n\n\n[SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user\n\n\n[SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids\n\n\n[SPOI-5661] - Set default store in HDHTReader\n\n\n[SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo\n\n\n[SPOI-5664] - Omit Aggregator Registry From UI\n\n\n[SPOI-5672] - App Data Tracker Compliation is failing\n\n\n[SPOI-5673] - Unable to launch ingestion app with JMS as input source \n\n\n[SPOI-5675] - Kafka keys population error\n\n\n[SPOI-5676] - Fix API doc generation\n\n\n[SPOI-5685] - Complete App Builder category and property name fixes\n\n\n[SPOI-5688] - Correct interactive demo tutorials available in the Learn section\n\n\n[SPOI-5689] - Installer produces error message about removing docs directory\n\n\n[SPOI-5690] - Images missing from lean section tutorials after installing release build\n\n\n[SPOI-5693] - Console does not upload default DT application packages\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5704] - App package references in docs\n\n\n[SPOI-5705] - Build version incorrect\n\n\n[SPOI-5706] - api and user docs not viewable through gateway\n\n\n[SPOI-5707] - Packaging utilities jar in Ingestion\n\n\n[SPOI-5708] - Update netlet dependency to release for 3.0\n\n\n[SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application\n\n\n[SPOI-4889] - Make BucketIdTagger fault tolerant\n\n\n[SPOI-5320] - License URL should open in new window \n\n\n[SPOI-5494] - Support \"queue\" query parameters from launch app modal\n\n\n[SPOI-5535] - Console should validate duplicate app name in the same app package\n\n\n[SPOI-5548] - Check compaction keys from UI\n\n\n[SPOI-5603] - Add syntax highlighting to Markdown code blocks\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n[SPOI-5623] - License upgrade link should open in separate tab\n\n\n[SPOI-5633] - Improve markdown content display styles\n\n\n[SPOI-5657] - Redirect to welcome screen of the Learn section after install\n\n\n[SPOI-5694] - Ingestion default app-config should populate meaningful defaults\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-4672] - Support for secure HA environments\n\n\n[SPOI-5288] - Add \"launch\" button to each item in the list of app packages\n\n\n[SPOI-5304] - Support ingestion install mode\n\n\n[SPOI-5313] - Obfuscate ADT in the distribution build\n\n\n[SPOI-5561] - Markdown documentation support in the Console\n\n\n\n\nTask\n\n\n\n\n[SPOI-4228] - The location of App Data Tracker from installation and from devel mode\n\n\n[SPOI-4658] - App Data Tracker Technical Doc\n\n\n[SPOI-4879] - Hide operators that have no input ports from App builder\n\n\n[SPOI-4983] - Installer for Ingestion app\n\n\n[SPOI-4984] - Obfuscate ingestion jar\n\n\n[SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core\n\n\n[SPOI-5032] - Review comparison for appDataFramework pull request to the master\n\n\n[SPOI-5050] - Licensing-related changes to the UI for 3.0\n\n\n[SPOI-5255] - Support enhancements to uiTypes\n\n\n[SPOI-5314] - Include obfuscated App Data Tracker in the install bundle\n\n\n[SPOI-5370] - Megh obfuscation\n\n\n[SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator\n\n\n[SPOI-5555] - [dtcp] set default app package for ingestion\n\n\n[SPOI-5574] - Update installer README\n\n\n[SPOI-5608] - Docs for backward compatibility in 3.0 \n\n\n[SPOI-5610] - Add testing to Markdown support models, pages, directives\n\n\n[SPOI-5621] - Include Ingestion utilities in DT installer\n\n\n[SPOI-5624] - [Ingestion] Hide message output destination when input is file source\n\n\n[SPOI-5635] - Changing product name to dtIngest\n\n\n[SPOI-5638] - Post ingestion packge to central DT maven repository\n\n\n[SPOI-5639] - App Data Tracker Release Job\n\n\n[SPOI-5645] - support \"short\" primitive type in app builder\n\n\n[SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements\n\n\n\n\nBug\n\n\n\n\n[MLHR-1726] - Bug in PojoUtils\n\n\n[MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives\n\n\n[MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing\n\n\n[MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application\n\n\n[MLHR-1789] - Complete Category and property name fixes\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1725] - Add Support for Setters to PojoUtils\n\n\n[MLHR-1757] - change scope of dt-engine in maven build", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-release-notes", 
            "text": "", 
            "title": "DataTorrent RTS Release Notes"
        }, 
        {
            "location": "/release_notes/#version-310", 
            "text": "Companies want to gain quick insights and take action on impacting events at the speed of their businesses. DataTorrent's Real-time Streaming (RTS) platform makes it easy to deliver fast data analytics applications that drive business outcomes using the latest innovation in data science and machine learning. It integrates best-of-breed open source technology innovation to provide all the features that a business needs to develop and deliver best-in-class, fast data applications.  DataTorrent RTS 3.10 version includes the following new features, resolved issues, and known issues.", 
            "title": "Version: 3.10"
        }, 
        {
            "location": "/release_notes/#features", 
            "text": "The key features of DataTorrent RTS 3.10 release are as follows:   New RTS Platform features  DataTorrent's RTS Apoxi\u2122 framework with added features  New and enhanced AppFactory applications  Miscellaneous features", 
            "title": "Features"
        }, 
        {
            "location": "/release_notes/#rts-platform", 
            "text": "The following RTS Platform features are included in this release:   Support for OLAP with Druid  Expanded Support for Machine Learning and AI", 
            "title": "RTS Platform"
        }, 
        {
            "location": "/release_notes/#support-for-real-time-olap-with-druid", 
            "text": "Provides customers with the ability to slice and dice data in real-time to get the information needed to compute and compare it to historical trends. This capability is delivered as a pre-built component that tightly integrates Apex, Druid, and Superset for real-time plus historical dashboards and visualizations.   Druid is an open-source project that is hardened and delivered as part of the RTS platform so that a separate, dedicated Druid cluster is not required.", 
            "title": "Support for Real-time OLAP with Druid"
        }, 
        {
            "location": "/release_notes/#expanded-support-for-machine-learning-and-ai", 
            "text": "DataTorrent RTS 3.10 helps customers capitalize on the value from the latest innovations in machine learning, AI, and data science.", 
            "title": "Expanded Support for Machine Learning and AI"
        }, 
        {
            "location": "/release_notes/#python-support", 
            "text": "RTS is extended to support the machine-scoring models that are written in Python.", 
            "title": "Python Support"
        }, 
        {
            "location": "/release_notes/#support-for-pmml-based-machine-scoring-models", 
            "text": "RTS also provides support for machine scoring models based on PMML which is an emerging standard.", 
            "title": "Support for PMML Based Machine Scoring Models"
        }, 
        {
            "location": "/release_notes/#datatorrents-rts-apoxitm-framework", 
            "text": "RTS Apoxi framework gives businesses operational readiness at any scale by providing the tooling required to assemble, manage, and operate fast data applications on any infrastructure. Apoxi is a framework that uniquely binds application building blocks or micro data services to create optimized, pre-built apps. It can also integrate independent applications to allow them to operate as one.  With Apoxi, DataTorrent can combine multiple micro data services, each with its own capability and operability, while also preserving the independence of different services, i.e. ingest and enrich, train and prepare, archive and persist, analyze and act.  For developers and data engineers, Apoxi saves time by providing facilities for streaming data visualization, lifecycle management, what-if data replay, and a built-in backplane for rapid integration.  Apoxi helps DevOps teams push applications to production with confidence by providing the management tooling, enterprise integrations, and operational metrics needed to meet production SLAs 24x7x365.  The following features of RTS Apoxi framework are included in 3.10 release:   Store and Replay  Drools Workbench  Application Backplane  Azure HDI Deployment  New iFrame Widget  Configuration Artifacts for Schemas and Rules  Enhanced RTS UI for Launch and Application Configuration", 
            "title": "DataTorrent's RTS Apoxi\u2122 Framework"
        }, 
        {
            "location": "/release_notes/#store-and-replay", 
            "text": "Store and Replay helps customers push to production with confidence. Customers can record and replay data from a specific point in time to evaluate the effectiveness of builds, models, and scenarios before they are put into production thereby removing the guesswork about what outcomes can occur.", 
            "title": "Store and Replay"
        }, 
        {
            "location": "/release_notes/#drools-workbench", 
            "text": "Drools Workbench integration makes it easier to modify complex event processing (CEP) rules and push new rules to production. The workbench enables customers to import data schema and visually edit and manage complex rules, easily using an intuitive graphical user interface.", 
            "title": "Drools Workbench"
        }, 
        {
            "location": "/release_notes/#application-backplane", 
            "text": "Application Backplane enables multiple applications to be simply and consistently integrated to share insights and actions. Combining numerous applications in real-time can result in significantly better outcomes, while still enabling separately managed applications to remain independent and benefit from a network-effect.\nFor example, you can integrate a fraud prevention application, such as Payment Card Fraud Prevention, with another fraud prevention application so that bothapplications see a net reduction in fraud by alerting each other whenever a fraud is detected in one.", 
            "title": "Application Backplane"
        }, 
        {
            "location": "/release_notes/#azure-hdi-deployment", 
            "text": "RTS now supports deployment to an Azure HDInsights Hadoop Cluster thereby enabling you to deploy RTS on Microsoft cloud. You can build streaming analytics applications and reduce the time to value with support for Azure Event Hubs as a source of incoming stream data.", 
            "title": "Azure HDI Deployment"
        }, 
        {
            "location": "/release_notes/#new-iframe-widget", 
            "text": "Now you can create rich dashboards that include visualizations from 3rd sources so that all your insights can be displayed in one, easy-to-use interface for exploring and visualizing data.   You can see this new capability in action as part of the new Online Analytics Service that provides real-time and historical visualizations, drill-down, query, etc.", 
            "title": "New iFrame Widget"
        }, 
        {
            "location": "/release_notes/#configuration-artifacts-for-schemas-and-rules", 
            "text": "DT RTS UI now allows you to include configuration artifacts which refers to schemas, libraries, rules and custom code that can be provided to an application through an application configuration. These artifacts can be uploaded manually and synchronized automatically from a Maven artifacts directory that is accessible by the Gateway. During the launch of an application, these artifacts are deployed as Application Configuration (*.apc) files.", 
            "title": "Configuration Artifacts for Schemas and Rules"
        }, 
        {
            "location": "/release_notes/#enhanced-rts-ui-for-launch-and-application-configuration", 
            "text": "The RTS UI is enhanced to include user-friendly screens for creating application configurations that are composed of multiple, separate services - enabling them to function as one end-to-end application.", 
            "title": "Enhanced RTS UI for Launch and Application Configuration"
        }, 
        {
            "location": "/release_notes/#appfactory", 
            "text": "The AppFactory includes an extensive library of popular open-source and proprietary operators for data input/output, enrichment, analytical functions, and more. These operators are further combined into pre-tested, streaming micro data services for common patterns such as data ingestion, real-time synchronization, anomaly detection, machine scoring, etc.  The following features are included in AppFactory for 3.10 release:   Omni-Channel Fraud Prevention Application V2  Account Takeover Prevention Application  New Use Case: Retail Recommender PoC  Micro Data Service (MDS): Online Analytics Service  Enhanced Drools Operator", 
            "title": "AppFactory "
        }, 
        {
            "location": "/release_notes/#omni-channel-fraud-prevention-application-v2", 
            "text": "The newest version of DataTorrent's Omni-channel Payment Fraud Prevention application integrates with the Druid OLAP component for real-time online analytical processing and enhanced historical trend analysis. This latest application also includes a reference architecture for integration with a variety of machine-trained analytical models for enhanced fraud prevention.  The improvements in this version of the Fraud Prevention application include the following:   Support for stateful rules via Drools operator which allows storing and using a state across multiple transactions.  Integrated with OAS which enables real-time analytics.  Support for rule editing with CEP Workbench.  Integrated with Apache Superset UI which lets you visualize and analyze trends on real-time and historical dashboards.", 
            "title": "Omni-Channel Fraud Prevention Application V2"
        }, 
        {
            "location": "/release_notes/#online-account-takeover-prevention", 
            "text": "RTS introduces  Account Takeover Prevention (ATO) application , a reference application that enables customers to prevent online account takeover and fraud by processing, enriching, analyzing, and acting on multiple streams of account event information in real-time.  Additional key features in this application include:   Integration with Online Analytics Service for OLAP trend analysis.  Support for rule editing with Drools Workbench.  Integration with Apache Superset UI for visualization.", 
            "title": "Online Account Takeover Prevention"
        }, 
        {
            "location": "/release_notes/#new-use-case-retail-recommender-reference-architecture", 
            "text": "Real-time, personalized product and service recommendations drive additional revenue for retail and ecommerce companies. DataTorrent's Retail Recommender provides a reference architecture that produces product recommendations in real-time by operationalizing the latest innovations in machine-learning.  Key features in this application include:   Integration with Online Analytics Service for OLAP trend analysis  Integration of Spark MLlib.  Integration with Apache Superset UI for visualization   For more details, refer to Retail Recommender Reference Architecture in our  AppFactory", 
            "title": "New Use Case: Retail Recommender - Reference Architecture"
        }, 
        {
            "location": "/release_notes/#online-analytics-service-oas", 
            "text": "Online Analytics Service (OAS) is delivered as a pre-built component that integrates Apex, Druid, and Superset for real-time BI dashboards and visualizations. OAS provides real-time as well as historical trend analysis, visualization, drill down, and query.  RTS enables OAS to receive event data from multiple data sources into a single Kafka topic while making the data available for analysis. RTS also provides a seamless way for target applications to push the data into Kafka topics for consumption by OAS.", 
            "title": "Online Analytics Service (OAS)"
        }, 
        {
            "location": "/release_notes/#enhanced-drools-operator", 
            "text": "The Drools operator is enhanced to provide support for stateful rules execution. The updated Drools operator is used in the latest version of Fraud Prevention application as well as in the Account Takeover Prevention application.", 
            "title": "Enhanced Drools Operator"
        }, 
        {
            "location": "/release_notes/#miscellaneous-features", 
            "text": "The following miscellaneous features are included in this release:   Encrypted StreamCodec  Dynamic Usage Statistics Reporting via Phone Home", 
            "title": "Miscellaneous Features"
        }, 
        {
            "location": "/release_notes/#encrypted-streamcodec", 
            "text": "Encrypted StreamCodec adds multiple implementations of StreamCodec and StatefulStreamCodec, which provides support for encryption of tuples before sending them downstream. All implementations are based on AES and secure RPC when launching operators in the cluster. The shared secret key is distributed to all the operators during deployment.", 
            "title": "Encrypted StreamCodec"
        }, 
        {
            "location": "/release_notes/#dynamic-usage-statistics-reporting-via-phone-home", 
            "text": "Dynamic usage statistics are reported to DataTorrent via the phone home feature. This aids DataTorrent to provide better support and to analyze when you are running out of data capacity. It also helps you to understand your RTS usage patterns over time.", 
            "title": "Dynamic Usage Statistics Reporting via Phone Home"
        }, 
        {
            "location": "/release_notes/#upgrade-information", 
            "text": "In case you want to use Services in DataTorrent RTS applications, you must install Docker on your system. However, this is not mandatory for upgrading to RTS 3.10 version.", 
            "title": "Upgrade Information"
        }, 
        {
            "location": "/release_notes/#resolved-issues", 
            "text": "The following resolved issues are included in this release:   Added a new recovery mode REUSE_INSTANCE to reuse operator instance across failures without reloading from checkpoint in certain cases. Now when the upstream operator is killed, the downstream operator does not reload the data from checkpoint from HDFS. Instead it uses the state in memory as-is. (SPOI-11219)  Support for option to use per user application directory in HDFS in secure mode. (SPOI-12134)  Out of order window ID was received in the begin window after recovery from a failure.  (SPOI-12633)  Support is provided for UI Active Directory. (SPOI-12663)  Container request is not sent, when the node report checks for the memory requested by the container. (SPOI-12708)  With anti-affinity STRAM, enough containers are not received from the resource manager to run all the operators. (SPOI-12791)  Files configured by the user were not available after RTS upgrade. (SPOI-12794)  Provide new permission that is VIEW_LOGS which determines if user can view the log files of the application instances. (SPOI-12936)  Authentication for Active Directory failed for domain users. (SPOI-12998)  Option provided to run snapshot server in stand-alone mode. (SPOI-13031)  SMTP configuration does not work without SSL. ( SPOI-13256)", 
            "title": "Resolved Issues"
        }, 
        {
            "location": "/release_notes/#known-issues", 
            "text": "The following known issues are included in RTS 3.10:   KafkaExactlyOnceOutputOperator  does not work with Kerberised cluster. (SPOI-13182)  Operator in fraud-prevention App dies while attempting to record a sample. (SPOI-13194)  The java script validation cannot terminate a java script, if it is running longer than the timeout period. (SPOI-12265)  Application does not shutdown even after consuming the end offset in Kafka. ( SPOI-12955)  OAS is unable to handle multiple intervals in select query. (SPOI-12865)   DT services with secure docker setup does not work. (SPOI-13104)   Workaround:  Docker in SSL mode requires mutual TLS (MTLS) which is not supported. Make sure your docker daemon uses either UNIX sockets or plain TCP sockets     Multiple redirections occur when logging into drools workbench through proxy URL. (SPOI-13254)   Workaround  : Click the  Back  button on the browser to go to the Drools WorkBench login page. You can also close the current tab and click the Proxy URL once again to go to the login page.     Transient services are not deleted when the corresponding app is terminated. (SPOI-12983)   Schema Aware operators should not perform validations in setter methods. (SPOI-13095)  Proxy URL for newly created Superset service (from UI) does not work. (SPOI-13454)  GATEWAY_CONNECT_ADDRESS parameter is misspelt as GATEWAY_CONNECT_ADDRESSS. (SPOI-13449)  Fraud App: Physical DAG is broken (SPOI-13388)  Security vulnerability is caused due to Key tab copy and incorrect permission issues. (SPOI-13366)  Application fails when both replay and archive properties are set to True. ( SPOI-13202)\nRelease date: Feb 21, 2018", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#version-392", 
            "text": "Release date: Dec 6, 2017  The following issues are fixed in DataTorrent RTS 3.9.2 release:", 
            "title": "Version: 3.9.2"
        }, 
        {
            "location": "/release_notes/#resolved-issues_1", 
            "text": "[SPOI-12791]  In the application when the anti-affinity is enabled, the application master gets containers from the Resource Manager for sometime only.  After this no more containers are received even after periodically requesting the STRAM for containers. Thus the functioning of the application gets disrupted.  [SPOI-12794]  While upgrading to RTS 3.9.1 with the latest rpm, the installation do not remain seamless. The LDAP configurations of the user are cleared from conf directory and the configurations had to be manually set up again.   [SPOI-12795]  Metrics platform was not supported with DT-Plus license and was only available with DT_Premium license as part of 3.9.1 release.   [SPOI-12832]  The application exits due to  java.lang.NullPointer  exception in the application master.   [SPOI-12855]  CassandraStore class only supports single node.  The CassandraStore must be enabled to support more than one cassandra nodes.  [SPOI-12820]  A new License API must be added to check the license restriction inside the Apex application container.  [SPOI-12898]  License is required for deploying RTS 3.9.2 on Sandbox.   [SPOI-12843]  The App Metrics platform must be updated with the new license checking code to support DT Premium or DT Plus license category.  [SPOI-12842]  The Drools CEP Rule Engine operator must be updated with the new license checking code to support DT Premium or DT Plus license category.  [SPOI-12841]  The Omni-channel Fraud Prevention v1 premium application must be updated with the new license checking code that supports DT Premium or DT Plus license category.  [SPOI-12932]  For Alert configurations, an e-mail address with hypen(-) character is not accepted.   [SPOI-12888]  The metrics data is generated even when the platform license was not set.  [SPOI-12872]  isValidLicense API  returns as  True  in cases where the license is not even configured.  [SPOI-12867]  The method  context.getAttributes().get() , that is used in License API, do not function properly in getting the application name property in the Apex Container code. This method must be replaced with  context.getValue()  method.  [SPOI-12811]  The filtering of auto complete list in tags do not input correctly.  [SPOI-12774]  If the key value contains a space in the key-combination, then that space is converted into hypen(-).\nFor example: If the key is  Las Vegas , then it gets converted to  Las-Vegas .  [SPOI-12749]  Nullpointer exception in found in FSWindowDataManager.", 
            "title": "Resolved Issues"
        }, 
        {
            "location": "/release_notes/#version-391", 
            "text": "Release date: Oct 10, 2017", 
            "title": "Version: 3.9.1"
        }, 
        {
            "location": "/release_notes/#features_1", 
            "text": "The following features and enhancements are included in the DataTorrent RTS 3.9.1 release:", 
            "title": "Features"
        }, 
        {
            "location": "/release_notes/#premium-applications-licensing", 
            "text": "DataTorrent applications and operators are now offered under the following license categories:   DataTorrent Premium Suite\u00ae  DataTorrent Service Plus\u00ae    These categories determine the type of access that you have for the premium applications and operators that are available in DataTorrent's AppFactory.  With  DT Premium , you can access  and download premium applications from the DataTorrent AppFactory such as the Omni-Channel Fraud Prevention application and operators such as Drools-based rule engine for complex event processing.   With  DT Plus , you can download only non-premium applications and operators that can be used for data ingestion.  Premium applications, although visible on the AppFactory, can be downloaded only if you subscribe to DataTorrent Premium Suite.  In this release, licensing support is extended to  Metrics module  and to  Omni-Channel Fraud Prevention  application.", 
            "title": "Premium Applications Licensing"
        }, 
        {
            "location": "/release_notes/#python-support_1", 
            "text": "DataTorrent now provides support to Python for building applications in Apex. You can create a DAG with Python as high level API support.", 
            "title": "Python Support"
        }, 
        {
            "location": "/release_notes/#single-value-widget", 
            "text": "The Single Value widget can be now added to a dashboard of Application Templates.", 
            "title": "Single Value Widget"
        }, 
        {
            "location": "/release_notes/#historical-time-range-results", 
            "text": "The Historical time range selection results now includes the   From  and  To  values.", 
            "title": "Historical Time Range Results"
        }, 
        {
            "location": "/release_notes/#enhancements-to-import-packaged-dashboards-section", 
            "text": "The following enhancements are done to the  Import Packaged Dashboards  section of the  Launch   Application   dialog box:   On the UI, the default selection for the replacement application is now the same as the currently launching application.  The  Name  field is now changed to  Dashboard Name  A detailed description is now provided for the  Select Replacement Applications  field.   The purpose of the application state can be viewed when you hover over the dashboard name.   The name of the user who packaged the dashboards is now no longer visible against the name of the dashboard.", 
            "title": "Enhancements to Import Packaged Dashboards Section"
        }, 
        {
            "location": "/release_notes/#artifact-query-api-filter", 
            "text": "The artifact query API now supports expansion to all versions, including older ones, as well as specifying exact RTS Version and limiting the results.", 
            "title": "Artifact Query API Filter"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-defect-fixes", 
            "text": "The following resolved issues are included in the DataTorrent RTS 3.9.1 release:", 
            "title": "DataTorrent RTS Defect Fixes"
        }, 
        {
            "location": "/release_notes/#defects", 
            "text": "[SPOI-11219] - Add a new recovery mode where the operator instance before a failure event can be reused in certain cases without reloading from checkpoint.  [SPOI-12665] - Autometric values of an operator is showing wrongly in App master.  [SPOI-12633] - Operator fails with catastrophic error message.", 
            "title": "Defects"
        }, 
        {
            "location": "/release_notes/#version-39", 
            "text": "Release date: Aug 23, 2017", 
            "title": "Version: 3.9"
        }, 
        {
            "location": "/release_notes/#summary", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#application-dashboards", 
            "text": "These dashboards help to deliver a complete application experience, showcasing the business-relevant metrics customers\u2019 desire.  DataTorrent\u2019s Applications Dashboards are 1) easy to create from library of widgets 2) show business relevant, real-time analytics and 3) can be easily associated with one or more streaming application(s). Now customers can construct the specific view that THEY want to see in one place.", 
            "title": "Application  Dashboards"
        }, 
        {
            "location": "/release_notes/#application-metrics-and-visualization", 
            "text": "Customers can create real-time metrics that are relevant to the overall streaming analytics application, so that business-relevant metrics can be easily computed and visualized.   DataTorrent\u2019s Application Metrics   Visualization capabilities enable customers to define not just operational metrics related to data processing, but now they can capture, store, and show the business relevant metrics too.  This is key to being able to show dashboards that are relevant to specific business problems (for ex: percentage of transactions where fraud occurs, fraud trends in real-time, or fraud breakdown by channels).", 
            "title": "Application Metrics and Visualization"
        }, 
        {
            "location": "/release_notes/#datatorrent-appfactory", 
            "text": "DataTorrent has evolved the AppHub, making it even easier for customers to solve problems and understand how real-time streaming analytics can make a difference for their business.    DataTorrent AppFactory is a centralized marketplace for streaming analytics applications that:\nSolve industry specific streaming analytics problems i.e. fraud prevention to prepare the data so that you can make quick/informed business decisions help customers deliver value quickly without requiring incremental DIY work  The AppFactory is arranged both by vertical application and Application Suites making it very intuitive for the customer to identify what might be most relevant/critical for their business needs.  Additionally, customers can see the Use Cases, Reference Architectures, and pre-built applications for download to get a full view of what\u2019s possible in the DataTorrent AppFactory.  At GA, DataTorrent AppFactory contains the following Application Suites:\nOmni-channel Payment Fraud Prevention Application Suite\nContinuous Big Data Cloud Sync Application Suite\nContinuous Big Data Archival Application Suite\nContinuous Big Data Preparation Application Suite\nContinuous Big Data Sync Application Suite  AppFactory is a marketplace of big data streaming analytics use cases, reference architectures, and downloadable applications that help you to make a positive impact on your business as quickly as possible. You can search by industry or technology to quickly find what is most relevant to your needs.", 
            "title": "DataTorrent AppFactory"
        }, 
        {
            "location": "/release_notes/#omni-channel-payment-fraud-prevention-application-suite", 
            "text": "Prevent payment fraud in real-time on all transactions across all your channels.\nDataTorrent\u2019s Omni-channel Payment Fraud Prevention Application delivers real-time, data-in-motion analytics built for 24/7/365 production with sub-second latency, self-healing, enterprise-grade reliability and a scale-out architecture built with a complete pipeline that includes real-time business and operational visualizations.", 
            "title": "Omni-channel Payment Fraud Prevention Application Suite"
        }, 
        {
            "location": "/release_notes/#continuous-big-data-cloud-sync-application-suite", 
            "text": "Continuous Big Data sync of on-premise and cloud infrastructures for availability, compliance, and archival.\nAllows customers to create various data storages within and across on-premise and cloud which can be continuously synced, with no data loss.", 
            "title": "Continuous Big Data Cloud Sync Application Suite"
        }, 
        {
            "location": "/release_notes/#continuous-big-data-archival-application-suite", 
            "text": "Continuous archival of Big Data for compliance and business continuity.\nEnables customers fast backup of large volumes of data with low latency, and dynamic scaling features.", 
            "title": "Continuous Big Data Archival Application Suite"
        }, 
        {
            "location": "/release_notes/#continuous-big-data-preparation-application-suite", 
            "text": "Streaming Big Data ingestion to prepare your data for insight and action.\nRenders your data \u201cdecision ready\u201d as close as possible to the time of creation, serving the business with continuous, clean, consistent, enriched data in the desired business template.", 
            "title": "Continuous Big Data Preparation Application Suite"
        }, 
        {
            "location": "/release_notes/#continuous-big-data-sync-application-suite", 
            "text": "Continuous delivery of Big Data to your Data Lake.\nAllows customers to create an Enterprise Data Lake which delivers continuous, clean, and consistent data while ensuring no data loss occurs during data movement.", 
            "title": "Continuous Big Data Sync Application Suite"
        }, 
        {
            "location": "/release_notes/#additional-product-features-that-further-increase-a-customers-time-to-value-include", 
            "text": "", 
            "title": "Additional product features that further increase a customer\u2019s time to value include:"
        }, 
        {
            "location": "/release_notes/#operator-library-improvements", 
            "text": "With every release, DataTorrent hardens the operators we ship with our applications and templates, continually adding to the Open Source community.  With RTS 3.9, we\u2019ve included the creation of an input operator for the latest version of Kafka so we now support Kafka 0.10.1.  Schema Support for Applications\nThis feature makes it easier to change the fields that are supported in your data.  With DataTorrent\u2019s Basic Schema Support for applications, it is easier to change the schema that are supported by the data.  Now users are able to associate fields to the data being processed by a DAG in order to associate this pipeline with a set of fields. When the fields are changed, the whole pipeline is updated accordingly.", 
            "title": "Operator Library Improvements"
        }, 
        {
            "location": "/release_notes/#licensing", 
            "text": "Every customer including eval and free edition users will need a new license file to use the RTS 3.9.0 product.  In 3.9, we have updated our software licensing mechanism. Customers, including those using our Free license, will be required to obtain a new software license file from DataTorrent in order to use or upgrade to version 3.9.  Existing customers can visit Upgrade License to obtain a new license when ready.  Customers who download our sandbox environment or request a free or evaluation license will automatically receive a 3.9-compatible license.", 
            "title": "Licensing"
        }, 
        {
            "location": "/release_notes/#security", 
            "text": "Security hardening enhancements in 3.9.0 enable users to configure LDAP Security directly in the product. While available previously, the additional functionality in our User Interface now makes LDAP configuration even easier, saving customers time.", 
            "title": "Security"
        }, 
        {
            "location": "/release_notes/#centralized-log-aggregation", 
            "text": "Troubleshooting just got easier with this feature that enables a centralized facility for log aggregation.  DataTorrent RTS 3.9 Support for 3rd Party Log Aggregation makes it even easier for customers to troubleshoot while in production with indexing, search and correlation, and a centralized facility for log aggregation. You can now integrate Elasticsearch, Logstash, Kibana (ELK) or Splunk with RTS out of the box.", 
            "title": "Centralized Log Aggregation"
        }, 
        {
            "location": "/release_notes/#additional-features-of-rts-390", 
            "text": "", 
            "title": "Additional Features of RTS 3.9.0"
        }, 
        {
            "location": "/release_notes/#apache-beam-support", 
            "text": "This feature is focused on making data processing easier, faster and less costly.\nApache Beam Support, aka Google DataFlow is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows and also data ingestion and integration flows, supporting EIPs and DSLs. This dataflow pipeline simplifies the mechanics of large-scale batch and streaming processing.", 
            "title": "Apache Beam Support"
        }, 
        {
            "location": "/release_notes/#basic-batch-processing-support", 
            "text": "This feature enables data to be read and processed in batch mode.\nDataTorrent\u2019s Batch Processing Support allows customers to use their batch-oriented architectures in order to integrate with support for external scheduling (i.e. through cron, Oozie, etc.).", 
            "title": "Basic Batch Processing Support"
        }, 
        {
            "location": "/release_notes/#deprecated-features-of-rts-390", 
            "text": "This section lists features and capabilities that have been either removed or planned for removal in DataTorrent RTS.   Application Data Tracker (ADT):  This feature will no longer be supported after the RTS 3.9.0 release.   ADT is determined to be not sufficiently scalable and requires execution of a separate application that is not directly built into the RTS platform.  ADT is replaced with the new Application Metrics feature.  The new Application Metrics feature available in RTS 3.9.0 provides the required scalability, is easier for developers to use, and does not require execution of a separate application.", 
            "title": "Deprecated Features of RTS 3.9.0"
        }, 
        {
            "location": "/release_notes/#apex-features", 
            "text": "[APEXCORE-594] Plugin support in Apex  [APEXCORE-579] Custom control tuple support  [APEXCORE-563] Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.  [APEXCORE-655] Support RELEASE as archetype version when creating a project  [APEXCORE-662] Raise StramEvent for heartbeat miss", 
            "title": "Apex Features"
        }, 
        {
            "location": "/release_notes/#apex-bugs", 
            "text": "[APEXCORE-674] DTConfiguration utility class ValueEntry access level was changed  [APEXCORE-663] Application restart not working.  [APEXCORE-648] Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()", 
            "title": "Apex Bugs"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-bug-fixes", 
            "text": "", 
            "title": "DataTorrent RTS Bug Fixes"
        }, 
        {
            "location": "/release_notes/#bugs", 
            "text": "[SPOI-8784] - Restarting application with originalAppId for long running app takes long time  [SPOI-9203] - AppHub \"check for updates\" option says 'no updated versions' and then displays updated packages  [SPOI-10409] - Updating app packages using \"check for updates\" option from AppHub gives wrong notification as undefined  [SPOI-10786] - Checkbox cannot be unchecked in the Widget Options screen  [SPOI-10989] - Gateway insists on using IP address for GATEWAY_CONNECT_ADDRESS  [SPOI-11143] - Launching PiDemoAppData with optional properties throws 500 Server Error  [SPOI-11235] - Stram Search Event Does Not Return Correct Result  [SPOI-11239] - Prevent Page Freeze in Monitor Application Page  [SPOI-11312] - Container log files contain \"\\t\" characters instead of a tab  [SPOI-11322] - AppMaster node is mentioned as \"N/A\" in the Failure Message  [SPOI-11381] - Search for \"heap reduction %\" column in GC Log Table does not work correctly  [SPOI-11425] - When operator fails recursively, few StartOperator/StartContainer events are grouped incorrectly  [SPOI-11446] - Search for status column in \"Logical Operators\" widget does not work  [SPOI-11482] - dtDashboard should ask for \"save changes\" while moving away from unsaved dashboard  [SPOI-11485] - Cannot switch back to \"Name-Value\" once clicked on \"JSON\" on Application Configuration details page  [SPOI-11498] - After machine reboot, DTGateway fails to get the uploaded license.  [SPOI-11523] - physical DAG view has become unresponsive  [SPOI-11580] - Shows the empty long description when uploading the existing app template with different version  [SPOI-11622] - Fix semantic versioning for ALL Application Templates  [SPOI-11784] - Getting Javascript error when viewing application summary - \"Unable to render dag due to TypeError: Cannot read property 'appInstance' of undefined\"  [SPOI-11791] - Application Package upload fails with \"Unexpected EOF reached\" error  [SPOI-11793] - Too many CLOSE_WAIT entries in netstat output when running applications with dashboards  [SPOI-11805] - AppHub timestamps do not follow App Package timestamp standards  [SPOI-11809] - Issues with Bar-Charts on Dashboard  [SPOI-11851] - 'Reset Position' option for DAG on Application details page does not work  [SPOI-11858] - Incorrect metrics are reported for the application  [SPOI-11859] - Metrics are not updated in real-time for long running applications  [SPOI-11860] - 'dt.metrics.baseDir' should default to \"DFS Root Directory/metrics\" instead of hard coded \"datatorrent/metrics\"  [SPOI-11869] - Gateway throws FileNotFoundException for applications without any metrics data stores  [SPOI-11870] - Issues faced while demoing app template(s)  [SPOI-11884] - Logical and Physical Dag widgets shrink when clicking on the left and right border  [SPOI-11889] - DTX-SNAPSHOT build packages apex core jar from different versions.  [SPOI-11897] - Launching Database to Database Sync App gives SQL syntax error  [SPOI-11915] - No options are shown under 'Predefined Conditions' dropdown while creating new alert  [SPOI-11929] - Table widget should not have \"Chart Axes\" settings  [SPOI-11930] - Historical App Metrics data in table widget does not get refreshed even when \"Load data continuously\" option is enabled  [SPOI-11931] - Table widget issue in selecting check boxes for aggregates  [SPOI-11932] - Misalignment in \"Time Range Selection\" option in table widget  [SPOI-11933] - Time Range Selection option for Table widget returns less number of entries than that of requested  [SPOI-11934] - Unable to list widgets for \"Snapshot :: App Stats\" data source  [SPOI-11936] - Stram events doesn't scroll to bottom on page load  [SPOI-11939] - \"Field to visualize\" option in Trend widget does not show any options in dropdown  [SPOI-11959] - latency:AVG values are not displayed in table widget for Historical :: App Stats  [SPOI-11961] - Improvement: \"failureCount\" for 'Historical :: Operator Stats' should be SUM and not LAST  [SPOI-11962] - Mouseover text in stacked area chart does not change for historical data points  [SPOI-11968] - Application search from app details page does not work  [SPOI-11969] - All selected aggregate values for a metric are not displayed on dashboard  [SPOI-11973] - Chart legends are not shown if number of fields to visualize is more than 12  [SPOI-11974] - Gateway showing invalid license when server time changed  [SPOI-11981] - Invalid license due to HDFS not ready in Sandbox.  [SPOI-12025] - Historical data is not fetched for all the requested minute intervals  [SPOI-12032] - Required properties are missing  [SPOI-12046] - Gateway should exclude registering metrics datasource which are not enabled for metrics  [SPOI-12047] - Gateway throws exceptions while getting restarted after security configuration  [SPOI-12049] - App Metric Aggregation throws NPE when user code is called  [SPOI-12076] - Launching application throws FileNotFoundException in gateway log for 'datatorrent/apps/${appId}/permissions.json'  [SPOI-12079] - Too many NullPointerExceptions in dtgateway log when app connected to dashboard is killed  [SPOI-12080] - Too many NullPointerExceptions in dtgateway log when time range for historical data is set to 'All'  [SPOI-12081] - Table widget for historical data should resize to number of rows available for display  [SPOI-12082] - Grouping by root cause feature is not available  [SPOI-12086] - 'logicalOperatorName' column is blank for historical OperatorStats table widget  [SPOI-12089] - Time Format used for displaying and querying table widget data should be same  [SPOI-12091] - Widget data source selection not working with filtering  [SPOI-12092] - Obfuscate public key value in RTS jar  [SPOI-12100] - Specifying which metrics to write throws UnsupportedOperationException  [SPOI-12102] - Snapshot schema is not refreshing  [SPOI-12113] - Fix bullet chart  [SPOI-12115] - Opening hdfs-line-copy app on AppHub gives FileNotFoundException in logs  [SPOI-12116] - Retention policy for terminated apps does not work on 3.9.0  [SPOI-12117] - Data sources get sorted only after adding first widget  [SPOI-12124] - Gateway should not read the list datasources from metrics platform every sec  [SPOI-12125] - Too many warning messages in gateway log with IOException for changed Blocklist  [SPOI-12126] - Too many warning messages in gateway logs with InterruptedException while submitting Auto publish executor  [SPOI-12127] - Issues in HDFSStoreReader and Record Data structure  [SPOI-12129] - X-axis labels for Stacked Area chart are misaligned  [SPOI-12140] - App level metric processor delivers empty metrics for an operator  [SPOI-12141] - Clicking on physical operator details page for hdfs-line-copy app hangs UI  [SPOI-12152] - Long running apps with metric data give \"Too many open files\" exception in apex log  [SPOI-12153] - User should not be allowed to exit the modal when dashboard name is blank  [SPOI-12159] - 'create new dashboard' option on app details page overwrites existing dashboard with same name  [SPOI-12161] - Dashboards for restarted apps get stuck at original app data  [SPOI-12164] - Configuration issues modal text overflow  [SPOI-12167] - AppMaster fails with OutOfMemoryError for apps with metric data  [SPOI-12170] - RTS installation fails with NoClassDefFoundError for jackson libraries  [SPOI-12178] - ConcurrentModificationException in GroupingManager  [SPOI-12181] - Launching from app builder provides invalid notification  [SPOI-12186] - Properties for some of the operators are not displayed  [SPOI-12192] - Unavailable data sources are shown while adding widgets  [SPOI-12193] - Metrics platform does not honour \"dt.write.metrics\" property  [SPOI-12199] - Metrics lib writer should include jackson libraries are runtime dependencies  [SPOI-12216] - Clicking on logical operators link in breadcrumb does not show any list for searching  [SPOI-12217] - Clicking on module name in logical DAG gives 404  [SPOI-12219] - Make sure all dashboard widgets have the debug settings  [SPOI-12223] - Filtering on data sources should be done on app name and data source name only  [SPOI-12224] - Historical time range selection gives wrong number of records  [SPOI-12257] - Date time selection in Control widget has issues  [SPOI-12258] - ProductDataEnricher Operator gives \u201cUnable to retrieve result \u201d  [SPOI-12260] - Application name is altered when control widget settings are applied to more than one applications  [SPOI-12261] - App Metrics should not expose \"appUser\" and \"appName\" keys as selectable dimensions for [HISTORICAL] data sources  [SPOI-12268] - When \"time.from\" and \"time.to\" are specified, \"time.latestNumBuckets\" should NOT be required.  [SPOI-12275] - Package Properties - value of a field (long list) is only partially visible.  [SPOI-12276] - Metrcis plugin throws IllegalStateException  [SPOI-12286] - Time range selection UI for Historical data is off  [SPOI-12287] - Querying with Historical time range does not return the data to widget  [SPOI-12290] - Geo circle widget does not honor custom fields for lon and lat  [SPOI-12293] - Historical time range selection settings are not preserved on refresh  [SPOI-12294] - Manually editing the value in Historical time range selection changes datetime format  [SPOI-12297] - Show all link in notification service causes page error  [SPOI-12302] - AppFactory - styling issues  [SPOI-12306] - Wrong sorting order for artifacts listing  [SPOI-12321] - Unable to view data on dashboards which after import on launch  [SPOI-12322] - X button to delete chosen application in Import Packaged Dashboard is hidden  [SPOI-12323] - Historical time range selection settings are not preserved  [SPOI-12332] - AppFactory - after importing appPackage, buttons should be refreshed automatically  [SPOI-12339] - Unable to retarget datasource in dashboard settings after adding widget", 
            "title": "Bugs"
        }, 
        {
            "location": "/release_notes/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_1", 
            "text": "[SPOI-11483] -  For FAILED applications, container state is shown as ACTIVE. Yarn returns containers of failed application as active.   Workaround  - Check for app status first.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#version-381", 
            "text": "Release date: Aug 14, 2017", 
            "title": "Version: 3.8.1"
        }, 
        {
            "location": "/release_notes/#summary_1", 
            "text": "This minor release primarily addresses issues related to installation of DataTorrent RTS on a Hadoop cluster configured for secure mode.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#rts-bug-fixes", 
            "text": "[SPOI-11694] : Fresh system-wide install fails with \"DFS Installation Location failed to load\" error\n[SPOI-11846] : Gateway fails when launching application in secure mode without authentication enabled\n[SPOI-11848] : Installation wizard fails on the hadoop configuration screen in secure mode\n[SPOI-11849] : The UI calls to retrieve properties in the wizard fail\n[SPOI-12047] : Gateway throws exceptions while getting restarted after security configuration", 
            "title": "RTS Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-bug-fixes", 
            "text": "[APEXCORE-737] : Buffer server may stop processing tuples when backpressure is enabled\n[APEXCORE-745] : AppMaster does not shut down because numRequestedContainers becomes negative", 
            "title": "Apache Apex Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-380", 
            "text": "Release date: Apr 18, 2017", 
            "title": "Version: 3.8.0"
        }, 
        {
            "location": "/release_notes/#summary_2", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#application-templates-apphub", 
            "text": "Pre-built data ingestion templates speed time-to-production  Deploy DataTorrent RTS application templates on a Hadoop distribution either on-premises or on the cloud. As part of this release, DataTorrent is providing AWS - EMR deployment script option for each available application template. As a result, development is simplified, enabling developers to more quickly and easily unlock value for customers. DataTorrent is focused on the goals of reducing complexity and removing dependency on Hadoop deployment, and this release represents progress in that direction.", 
            "title": "Application Templates (AppHub)"
        }, 
        {
            "location": "/release_notes/#application-configurations", 
            "text": "Simplifying customized application launches  Users can start with a single Application Package and create multiple Application Configurations to launch and run the applications on different environments (for instance, in test and development).  And, efficiencies can be realized across business units: one Application Package could have multiple configurations for multiple internal units.  Each Application Configuration introduces a safety feature, which ensures that only one instance of Application Configuration can run at a time.  The status whether Application Configuration is running or not, and controls to launch and stop the application instance are provided in the Application Configuration view.  This set of features improves management, adds safety, and increases transparency when managing and launching applications.", 
            "title": "Application Configurations"
        }, 
        {
            "location": "/release_notes/#debugging", 
            "text": "Improving log visualizations to speed up debugging", 
            "title": "Debugging"
        }, 
        {
            "location": "/release_notes/#stram-event-grouping", 
            "text": "The StrAM Events widget helps from development and operations perspectives to visualize notable events from the application launch and throughout its ongoing run.  With this release, StrAM events widgets now offers better readability by organizing these events into related groups.  For example, when multiple downstream operators are re-deployed due to a container failure. All events triggered by the system to restore normal function will be grouped under a single root event which caused the restarts.  With this improved readability, the user can quickly identify failure causes and then drill down into the logs for each event.", 
            "title": "StrAM Event Grouping"
        }, 
        {
            "location": "/release_notes/#garbage-collection-widgets", 
            "text": "With this release, developers can more easily visualize garbage collection data trends\u2014as opposed to sifting through logs. Three widgets are available for GC visualizations:   Garbage collection log chart by heap.  Visualizes when memory is allocated and deallocated   Garbage collection log table.  List memory allocation and deallocation details  Garbage collection log chart by duration.  Visualizes how long it takes to deallocate memory", 
            "title": "Garbage Collection Widgets"
        }, 
        {
            "location": "/release_notes/#log-tailing-search", 
            "text": "Users can follow the logs as they are generated (tailing) with the RTS UI Console.  In this release, users can now also perform searches even when tailing to focus on specific events and exclude the noise.", 
            "title": "Log Tailing &amp; Search"
        }, 
        {
            "location": "/release_notes/#alert-templates", 
            "text": "Simplifying and expanding alert functionality  In 3.7.0, RTS introduced monitoring with alerts that a DevOps engineer could set up based on specific conditions. The 3.8.0 release continues to simplify and expand this functionality by adding:   Nine predefined system alert templates, including cluster and application memory usage, application status, and active container count, killed container alerts, etc.  Option to disable alerts without deleting them.  The ability to configure custom SMTP settings for sending alert emails, instead of relying on Gateway\u2019s local node sendmail facility.", 
            "title": "Alert Templates"
        }, 
        {
            "location": "/release_notes/#security_1", 
            "text": "Security enhancement in 3.8.0 applies to RTS deployment on secure Hadoop with Kerberos enabled. User's own Kerberos credentials can now be used directly by RTS to launch applications.  It is better from a security perspective.  The previous model involved using a single system user with Kerberos credential (Hadoop impersonation)  to launch applications. That requires access to the system Kerberos credential in order to refresh tokens before they expire.  With user\u2019s own Kerberos credential, that is no longer the case.", 
            "title": "Security"
        }, 
        {
            "location": "/release_notes/#licensing_1", 
            "text": "With the release of 3.8.0, DataTorrent is updating and simplifying its licensing policy.  What has changed?    Starting with 3.8, the Community Edition is no longer available. We are replacing the Community Edition with Free Edition.  Community Edition limited the features available to you, such as security.   Now with Free Edition you have access to all the features and tools of RTS up to a 128GB processing limit.  Please refer to the DataTorrent website for additional details.", 
            "title": "Licensing"
        }, 
        {
            "location": "/release_notes/#licensing-faq", 
            "text": "I have a Community Edition license. Is that edition still available? \nStarting with 3.8, the Community Edition is no longer available.   You can continue to use the Community Edition with RTS version 3.7.  How is license memory consumption calculated? \nLicense memory consumption is the sum of all running applications as can be seen in Configuration - License Information and Monitor.  What will happen when my memory consumption exceed my license limit? \nYou\u2019ll receive a warning, which is shown for 30 minutes before most RTS features will be disabled. All existing applications will continue to run. Should you need to upgrade, you can easily contact DataTorrent for a new license.  How will I know when my license is going to expire and what happens if the license expires? \nWe provide warnings at 30 days and 7 days before expiration date.  When a license expires, most RTS features will be immediately disabled. All existing applications will continue to run. You can easily contact DataTorrent for a new license.    What can I do once RTS is locked (either because my license memory has been exceeded or its expiration date has passed)? \nUsers can view running applications and enter new license details to unlock RTS. The following capabilities are still accessible:  Configure - System Configuration is available   Configure - License Information is available  Configure - Installation Wizard is available  Monitor - Application kill, inspect and shutdown are available  Monitor - Application Overview (shutdown and kill only) per application is available  Monitor - Containers (kill only)  AppHub (download only, no import)  Learn  Can I reduce the number of applications and return to compliance? \nYes, you can do so if you have exceeded your memory capacity, assuming that your license has not expired  Can I reduce the memory usage of an application and bring it back to license compliance? \nYes, assuming that your license has not expired.  Please note that you will have to restart the application.  I have an enterprise license for my production cluster. Can I use the Free Edition in a non- production cluster? \nYes. It\u2019s worth noting that only community support is available for the Free Edition. Please visit the DataTorrent User group for RTS-related questions: https://groups.google.com/forum/#!aboutgroup/dt-users  For the Apache Apex mailing list and meetups information, please go to\nhttps://apex.apache.org/community.html#mailing-lists  Can I buy DataTorrent support for Free Edition? \nUnfortunately, no. DataTorrent support is sold as part of our Enterprise Edition. If you\u2019re seeking support, you may consider upgrading.  Is Application Master Container memory consumption included in the calculation for processing capacity? \nYes. Application Master Container consumes 1 GB by default and every application has its own Application Master. If application is not running, it does not run as well. It grows based on customer application build. Memory requirements increases along with the size of logical and physical DAG. Partitioners of an operator run in AppMaster.", 
            "title": "Licensing FAQ"
        }, 
        {
            "location": "/release_notes/#additional-features-of-380", 
            "text": "", 
            "title": "Additional Features of 3.8.0"
        }, 
        {
            "location": "/release_notes/#multiple-gateway-support", 
            "text": "This allows simultaneous multiple gateway operations and increase fault tolerance due to management console failure.   When there are multiple gateways (usually for High Availability), different developers may access them at the same time, with different or same user accounts. These activities will often result in simultaneous modification of the same resource stored in HDFS, and invalidate cache entries on each client. For example: When developer A tries to save a configuration package and developer B has edited and saved the same package, developer A will get an error. Developer A would then have to manually merge the differences. This release introduces a new file-based locking mechanism with HTTP ETag header to handle that scenario.\nKnown limitation: Alerts and visualization works correctly with single gateway only.", 
            "title": "Multiple gateway support"
        }, 
        {
            "location": "/release_notes/#retain-metric-selections-when-returning-to-monitor-physicallogical-view", 
            "text": "Better user experience since RTS will keep what users have selected to view (e.g. metrics) as they go from one screen to another.", 
            "title": "Retain metric selections when returning to Monitor - Physical/Logical view"
        }, 
        {
            "location": "/release_notes/#datatorrent-apex-core-fork", 
            "text": "RTS 3.8.0 is bundled with Apache Apex Core 3.5.0 plus forty feature and fixes that will be part of Apache Apex Core 3.6.0. Apex Core commits from Apr 3, 2017 will be included in RTS 3.9.0", 
            "title": "DataTorrent Apex Core fork"
        }, 
        {
            "location": "/release_notes/#new-feature", 
            "text": "[APEXCORE-579]        Custom control tuple support  [APEXCORE-563]        Have a pointer to container log filename and offset in StrAM events that deliver a container or operator failure event.", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvements", 
            "text": "[APEXCORE-676]        Show description for DefaultProperties only when user requests it  [APEXCORE-655]        Support RELEASE as archetype version when creating a project  [APEXCORE-611]        StrAM Event Log Levels  [APEXCORE-605]        Suppress bootstrap compiler warning  [APEXCORE-592]        Returning description field in defaultProperties during apex cli call get-app-package-info  [APEXCORE-572]        Remove dependency on hadoop-common test.jar  [APEXCORE-570]        Prevent upstream operators from getting too far ahead when downstream operators are slow  [APEXCORE-522]        Promote singleton usage pattern for String2String, Long2String and other StringCodecs  [APEXCORE-426]        Support work preserving AM recovery  [APEXCORE-294]        Graceful application shutdown  [APEXCORE-143]        Graceful shutdown of test applications", 
            "title": "Improvements"
        }, 
        {
            "location": "/release_notes/#task", 
            "text": "[APEXCORE-662]        Raise StramEvent for heartbeat miss", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#dependency-upgrade", 
            "text": "[APEXCORE-656]        Upgrade org.apache.httpcomponents.httpclient", 
            "title": "Dependency Upgrade"
        }, 
        {
            "location": "/release_notes/#bug", 
            "text": "[APEXCORE-674]        DTConfiguration utility class ValueEntry access level was changed  [APEXCORE-663]        Application restart not working.  [APEXCORE-648]        Unnecessary byte array copy in DefaultStatefulStreamCodec.toDataStatePair()  [APEXCORE-645]        StramLocalCluster does not wait for master thread termination  [APEXCORE-644]        get-app-package-operators with parent option does not work  [APEXCORE-636]        Ability to refresh tokens using user's own Kerberos credentials in a managed environment where the application is launched using an admin with impersonation  [APEXCORE-634]        Apex Platform unable to set unifier attributes for modules in DAG  [APEXCORE-627]        Unit test AtMostOnceTest intermittently fails  [APEXCORE-624]        Shutdown does not work because of incorrect logic in the AppMaster  [APEXCORE-617]        InputNodeTest intermittently fails with ConcurrentModificationException  [APEXCORE-616]        Application fails to start Kerberised cluster  [APEXCORE-610]        Avoid multiple getBytes() calls in Tuple.writeString  [APEXCORE-608]        Streaming Containers use stale RPC proxy after connection is closed  [APEXCORE-598]        Embedded mode execution does not use APPLICATION_PATH for checkpointing  [APEXCORE-597]        BufferServer needs to shut down all created execution services  [APEXCORE-596]        Committed method on operators not called when stream locality is THREAD_LOCAL  [APEXCORE-595]        Master incorrectly updates committedWindowId when all partitions are terminated.  [APEXCORE-593]        apex cli get-app-package-info could not retrieve properties defined in properties.xml  [APEXCORE-591]        SubscribeRequestTuple has wrong buffer size when mask is zero  [APEXCORE-585]        Latency should be calculated only after the first window has been complete  [APEXCORE-583]        Buffer Server LogicalNode should not be reused by Subscribers  [APEXCORE-558]        Do not use yellow color to display command strings in help output  [APEXCORE-504]        Possible race condition in StreamingContainerAgent.getStreamCodec()  [APEXCORE-471]        Requests for container allocation are not re-submitted", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-bug-fixes_1", 
            "text": "[SPOI-10021]  DTX Logical Operator page - BufferServerReadBytesPSMA and BufferServerWriteBytesPSMA to be removed  [SPOI-10107]  Application service returns DAG which is null  [SPOI-10118]  Upon launch of application, application details do not show up.  [SPOI-10153]  Add System Properties \"change\" button should be warm in color  [SPOI-10208]  Container state for failed attempt of app is shown as RUNNING  [SPOI-10266]  \"host\" information is not available in appattempts API call  [SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens  [SPOI-10331]  Delete user modal gives error when clicked outside the frame  [SPOI-10332]  Cancelling delete user action throws TypeError in developer console  [SPOI-10335]  Jersey throwing exceptions   excessive logging when WADL is enabled  [SPOI-10355]  Update Buttons and Text  [SPOI-10357]  Redirect User to Login Page  [SPOI-10363]  CheckPermission should not throw exception when auth is not enabled.  [SPOI-10416]  dtAssemble does not show connection between operators correctly  [SPOI-10421]  Fix oauth login  [SPOI-10438]  Hide Top Nav Menu Dropdown When Item is Clicked  [SPOI-10463]  Enhance Date/Time Picker for StrAM Events Date Range  [SPOI-10587]  Implement Date/Time Picker for Dashboard Widget  [SPOI-10588]  Change Button Label to Close During Launching  [SPOI-10862]  Multiple containers are labelled as AppMaster after dynamic partitioning  [SPOI-10866]  Time Range Selection Saved Settings Not Loaded Correctly  [SPOI-10894]  dtAssemble - Inspector contents not showing up consistently  [SPOI-10911]  determine AppMaster container by id that contains _000001 instead of by including 0 operators  [SPOI-10963]  appInstance page - fail to show \"packagedDashboard\" which is included in appPackage that appInstance is launched from  [SPOI-10970]  AppHub on gateway does not load in HTTPS  [SPOI-10996]  Subscribers/DataListeners may not be scheduled to execute even when they have data to process  [SPOI-10997]  BufferServer needs to shut down all created execution services  [SPOI-11000]  Upgrade org.apache.httpcomponents.httpclient  [SPOI-11024]  Alerts Icon Issue  [SPOI-11057]  Restart of the apps are failing  [SPOI-11108]  DAG View Javascript Error  [SPOI-11127]  Enhance \"lastNbytes\" to behave like \"tail\" command  [SPOI-11142]  Unable to launch app if another app with default APPLICATION_NAME is already running  [SPOI-11152]  Avoid usage of Apache Apex engine core class com.datatorrent.stram.client.DTConfiguration.ValueEntry  [SPOI-11163]  Cannot launch application from application details page  [SPOI-11164]  \"Add default properties\" option under \"Specify launch properties\" is missing  [SPOI-11168]  License API Does Not Return Latest State Information  [SPOI-11179]  Update Root Cause Failure to Use Newer Object Structure  [SPOI-11185]  Invalid license expiration message sent by Gateway  [SPOI-11186]  AppHub should be visible if license is invalid  [SPOI-11188]  App Packages search does not work for \"format\" column  [SPOI-11190]  Toggling \"system apps\" option does not show ended system apps  [SPOI-11192]  Search for \"lifetime\" column on Monitor screen does not work  [SPOI-11193]  Search for \"memory\" column on Monitor screen does not work  [SPOI-11194]  Search on \"state\" column on Monitor page is not alphabetical  [SPOI-11197]  Search on locality/source/sinks columns under Streams widget on logical tab does not work  [SPOI-11198]  Search for \"allocated mem\" and \"free memory\" under Containers widget does not work  [SPOI-11199]  \"download file\" option for empty container log files should be disabled  [SPOI-11200]  Search for \"allocated mem\" and \"started time\" under Containers widget  for app attempt does not work  [SPOI-11208]  DTgateway install screen messed up  [SPOI-11210]  On entering corrupt license file error message should be a proper one  [SPOI-11212]  Trailing and non trailing search should have same string  [SPOI-11213]  Unable to save SMTP configuration using gmail  [SPOI-11214]  Launching application with \u00ef\u00bf\u00bc\"Enable Garbage Collection\" throws 404  [SPOI-11215]  ADMIN_NOT_CONFIGURED warning is only shown to Dev user instead of admin  [SPOI-11220]  Fresh RTS installation fails because of blank response from \"/ws/v2/config\" api call  [SPOI-11222]  StackTrace feature not available from physical tab containers widget  [SPOI-11223]  Show \"Password change\" warning for dtadmin only  [SPOI-11231]  AppHub fails to load previous package versions  [SPOI-11234]  Show all AppHub package versions option is missing in list view  [SPOI-11236]  Extend application-level gc.log API to take in new parameter \"descendingOrder\" (false/true)  [SPOI-11237]  Angular not resolving certain dtText-wrapped expressions in Console modals  [SPOI-11238]  AppHub Check for Updates Does Not Show In All Cases  [SPOI-11242]  \"Upload package\" option on Application Packages should not be available with invalid/no license  [SPOI-11265]  Invalid message displayed when no license is uploaded  [SPOI-11266]  Alert notification is delayed  [SPOI-11270]  System Alert history is empty after relogin  [SPOI-11271]  Developer user cannot create system alert  [SPOI-11281]  .class file generated by tuple schema manager is invalid  [SPOI-11291]  Creating clone of JSON application gives 500 Server Error  [SPOI-11303]  Creating clone of JSON application gives 500 Server Error (UI)  [SPOI-11304]  App package load errors after migrating from 3.7 to 3.8  [SPOI-11307]  Search for \"lifetime\" column on Monitor screen does not work  [SPOI-11318]  Copy To Clipboard option from Failure Message modal does not work  [SPOI-11323]  Upgrade from 3.7.0 evaluation edition to 3.8.0 retains old license  [SPOI-11325]  Selecting KILLED applications and then disabling ended apps, shows shutdown/kill options  [SPOI-11326]  Clean install of 3.8.0 comes with no license  [SPOI-11327]  StramEvents are grouped incorrectly  [SPOI-11366]  Copy to clipboard not working when viewing stram event stack trace  [SPOI-11367]  Wrong 'uptime' value in Application Overview  [SPOI-11368]  Log and info icons should be right aligned in stram events  [SPOI-11369]  Socket is unsubscribed by Console when critical path is not checked  [SPOI-11371]  \"source package\" column in Application Configurations table is not sortable  [SPOI-11372]  Unable to view gc.log on UI  [SPOI-11375]  Wrong stream locality is shown in Physical DAG widget  [SPOI-11377]  Incorrect GC stats for logical operators if they are connected by CONTAINER_LOCAL stream  [SPOI-11379]  Heap reduction percentage is negative for some GC events  [SPOI-11382]  UI hangs when trying to change settings for GC Log Chart widgets  [SPOI-11383]  Selecting a container in GC Log Chart widgets throws error in Developer console  [SPOI-11417]  Memory usage of App Data Tracker is not counted against license  [SPOI-11418]  Event grouping: UI should ignore groupId 0   null  [SPOI-11421]  Unable to use Security Configuration feature with free license  [SPOI-11422]  Admin user should not be able to delete its own user account  [SPOI-11424]  Show tooltip for version string in application overview widget  [SPOI-7887]   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}] should return error instead of success where there is error \"Failed to load\"  [SPOI-8248]   Packaged dashboards do not reconnect with new app instances  [SPOI-8477]   Upgrade License Opens in dtManage window, it should be in opened up in new window  [SPOI-8610]   Disable editing operator properties which are of Object type  [SPOI-9375]   Uptime values shown on UI are out of whack immediately after app is launched  [SPOI-9474]   Failed to restart DT application in MapR secure cluster  [SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors  [SPOI-9945]   Top navigation menu is wrapping into two lines", 
            "title": "DataTorrent RTS Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-371", 
            "text": "Release date: Feb 28, 2017", 
            "title": "Version: 3.7.1"
        }, 
        {
            "location": "/release_notes/#summary_3", 
            "text": "This is primarily for users who install RTS in a Kerberized cluster as application fails to launch in 3.7.0. This release fixes the issue.   Other issues that are also fixed:   In 3.6.0, users have the option to override certain properties from the config file. This ability was missing in 3.7.0.  Gateway fails to recognize AppDataTracker application and it continuously relaunched on a Kerberized cluster.  On a Kerberized cluster, when the configuration parameters are specified in the  dt-site.xml  file in the user's home directory, the installation wizard does not allow the user to continue with the installation.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#appendix_1", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#bug-fixes", 
            "text": "[SPOI-10698] - Allow Custom Properties with Config XML file while launching an application  [SPOI-10737] - AppDataTracker application relaunches continuously and fails in a Kerberized cluster.  [SPOI-10932] - Installation Wizard does not allow to complete gateway configuration  [SPOI-10773] - Application fails to run in fully enabled Kerberized mode (APEXCORE-616 - https://issues.apache.org/jira/browse/APEXCORE-616)", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-370", 
            "text": "Release date: Dec 30, 2016", 
            "title": "Version: 3.7.0"
        }, 
        {
            "location": "/release_notes/#summary_4", 
            "text": "The new features on this release are functionalities that will ease debugging an application and administering application alerts in production.   Operation related features for a Dev Ops role:     Manage and see a history of previous alerts so that users can be aware of potential issues before they become critical.   View operator ID(s) and name(s) in the dtManage-Physical-Container list table to quickly identify what operators are in each container.   Filter matching tuple recordings by searching across tuple recording data.   Debugging features: When trying to troubleshoot or debug a distributed system, these capabilities allow users to quickly identify problem areas and easily drill down into relevant details (i.e. logs).    Notify users when log files have been removed  New Application Attempts section under Monitor. It is located along other views such as logical and physical  New Application Master logs in Application Overview section  New log button shortcut in Stram Events and Physical Operator section  Show dead container logs in Running applications. That is to show history of physical operator containers and logs in the physical operator view. For each previous incarnation, there are start time, end time, link to corresponding logs, root cause with error code, and recovery window id.  Show the same details for killed or finished application like running application view.  Show container history as default in Physical Operator view  New historic count field in Physical Operator view  Get thread dump from a container. It is useful to analyze issues such as \"stuck operator\", and obtain statistics from the running JVM. In production environments users often don't have direct access to the machines, thus making it available through the REST API will help.   Option to auto tail container logs. When viewing a container log via the UI, there is an option to periodically poll for more data (i.e. \"tail -f\" effect).   dtAssemble   New validate button so that user can validate DAG without having to save.   Remove auto save function. Save will be initiated by user only.  Support custom JSON input for tuple schema creation. This is particularly useful when user needs to add a large number of fields.    dtDashboard   New gauge widget   AppHub   Continued to refine application templates in AppHub (introduced in RTS 3.6.0).     RTS 3.7.0 is based on Apache Apex Core 3.5.0 (released Dec 19, 2016) and Apache Apex Malhar 3.6.0 (released Dec 8, 2016).", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#apache-apex-core-350", 
            "text": "This release upgrades the Apache Hadoop YARN dependency from 2.2 to 2.6. The community determined that current users run on versions equal or higher than 2.6 and Apex can now take advantage of more recent capabilities of YARN. The release contains a number of important bug fixes and operability improvements. \nChange log: https://github.com/apache/apex-core/blob/v3.5.0/CHANGELOG.md", 
            "title": "Apache Apex Core 3.5.0"
        }, 
        {
            "location": "/release_notes/#apache-apex-malhar-360", 
            "text": "The release adds first iteration of SQL support via Apache Calcite. Features include SELECT, INSERT, INNER JOIN with non-empty equi join condition, WHERE clause, SCALAR functions that are implemented in Calcite, custom scalar functions. Endpoint can be file, Kafka or internal streaming port for both input and output. CSV format is implemented for both input and output. See examples for usage of the new API.  The windowed state management has been improved (WindowedOperator). There is now an option to use spillable data structures for the state storage. This enables the operator to store large states and perform efficient checkpointing.  There was also benchmarking on WindowedOperator with the spillable data structures. From the result, the community significantly improved how objects are serialized and reduced garbage collection considerably in the Managed State layer. Work is still in progress for purging state that is not needed any more and further improving the performance of Managed State that the spillable data structures depend on. More information about the windowing support can be found at http://apex.apache.org/docs/malhar/operators/windowedOperator/.  This release also adds a new, alternative Cassandra output operator (non-transactional, upsert based) and support for fixed length file format to the enrichment operator. \nChange log: https://github.com/apache/apex-malhar/blob/v3.6.0/CHANGELOG.md", 
            "title": "Apache Apex Malhar 3.6.0"
        }, 
        {
            "location": "/release_notes/#appendix_2", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_2", 
            "text": "[SPOI-9921]   Delete widgets on default pane of physical operators needs to have warm colors  [SPOI-9923]   Custom panes' on operator monitoring page should have character limits for titles  [SPOI-9924]   Can not escape the \"custom\" pane name  [SPOI-9925]   Limit number of custom panes  [SPOI-9947]   JDBC Poll Input operator processes extra records when its container is killed  [SPOI-9948]   JDBC Poll Input operator does not process new records when they are inserted while the app is processing the existing records  [SPOI-9965]   Restarting the KILLED application with JDBC Poll Input operator plays the duplicate data  [SPOI-10046]  Deleting a property directly creates tuple schema with remaining properties  [SPOI-10049]  Limit the number of characters in role name   [SPOI-10107]  Application service returns dag which is null  [SPOI-10151]  Add System Properties modal should validate the properties  [SPOI-10153]  Add System Properties \"change\" button should be warm in color  [SPOI-10154]  Rerun Install wizard allows extension of trial  [SPOI-10158]  Logical/Physical plan view does not retain metric selections in drop-down  [SPOI-10165]  Container logs, dt.log files produced by chklogs.py have HTML escapes  [SPOI-10202]  About API call gives out information without authentication.  [SPOI-10203]  User can set any non existent package name   [SPOI-10208]  Container state for failed attempt of app is shown as RUNNING  [SPOI-10267]  If number of alerts are huge, gateway starts slowing down  [SPOI-10274]  Moving the mouse over the operator shows it as clickable but nothing happens  [SPOI-10275]  Alert contains an exception trace  [SPOI-10292]  Delay in cluster metrics when the authentication is enabled  [SPOI-10315]  \"requires Apex version\" information is missing for latest app packages on AppHub  [SPOI-10332]  Canceling delete user action throws TypeError in developer console  [SPOI-10333]  Configuration issue modal content goes out of modal and is not scrollable  [SPOI-10334]  App restart doesn't take to new page  [SPOI-10373]  FinishedTime/EndTime information is available only after application if killed/shutdown for CDH  [SPOI-10379]  Enable Reporting button should have cool colors  [SPOI-10385]  The app name field should not be editable at launch time  [SPOI-10389]  UI Console should not allow creation of config pkg with special characters  [SPOI-10396]  Application configuration should require save before launch  [SPOI-10398]  UI says \"An error occurred while fetching data\" immediately after launching apps (intermittent)  [SPOI-10399]  New permission do not reflect unless user logs out  [SPOI-10401]  Deleted user can do any operations  [SPOI-10406]  Application Configurations upload modal title should say \"Application Configuration Upload\"  [SPOI-10409]  Updating app packages using \"check for updates\" option from AppHub gives wrong notification  [SPOI-10410]  Deleting a property while creating tuple schema gives error in developer console", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#new-feature_1", 
            "text": "[SPOI-7720]   DT Hub shows just one version of an application  [SPOI-10182]  dtAssemble - if there is unsaved change, keep launch button enabled which will pop up a dialog box asking save-and-launch when being clicked.  [SPOI-8879]   Support custom JSON input for tuple schema creation  [SPOI-9877]   Alerts History  [SPOI-9876]   Alerts Notification  [SPOI-9875]   Alerts Management  [SPOI-8570]   Ability to filter tuple recording  [SPOI-9525]   UI for dtDebug - Logs  [SPOI-8499]   Create diagnostic tool for analyzing RM and container logs  [SPOI-10364]  Update launch and configuration package views for simplified properties  [SPOI-9772]   App Launch properties  [SPOI-8764]   UI Console Configuration Packages support  [SPOI-8611]   Support gateway configuration changes in UI  [SPOI-10323]  Always show User profile even when license type is not enterprise", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement", 
            "text": "[SPOI-9785]   send GA events instead of page views for apphub page events  [SPOI-10290]  Certification tool - RTS Installer Support for tool  [SPOI-10179]  dtAssemble should warn about the unsaved changes when navigating away  [SPOI-8989]   Show operator names under Physical -  Containers  [SPOI-10163]  dtDebug utilities README should have list of requirements    [SPOI-9881]   appAttempt page - Containers table - \"containerLogsUrl\" column - change it from showing a hyperlink to \"logs\" button.  [SPOI-7343]   Ability to obtain a thread dump from a container  [SPOI-3553]   Option to auto-tail container logs  [SPOI-9133]   Gateway restart modal and button has soothing colors  [SPOI-9132]   Gateway restart UI button has soothing colors  [SPOI-9105]   Refactor security validation in console (consolidate resolve:{} from many places into one place)  [SPOI-9047]   Make Package Upload Message clickable  [SPOI-8766]   Make Physical DAG has the same metric selection (Top dropdown, Bottom dropdown) like Logical DAG does.  [SPOI-8645]   Logical DAG, Physical DAG - show spinner in panel instead of blank panel before graph has been rendered and displayed.  [SPOI-8644]   Do not show graph options(Show/Hide Stream Locality, Reset Position, Top dropdown, Bottom dropdown) until graph has been rendered and displayed.  [SPOI-7810]   dtManage: Number of failures for an operator should have 'number search' option instead of 'string search'  [SPOI-7277]   Ability to upload configuration file during app launch ( like -conf in dtcli )  [SPOI-9418]   ConfigPackages backend support  [SPOI-9400]   Change licensing for RTS to be managed/displayed in GB instead of MB  [SPOI-9086]   Add support for DIGEST enabled Hadoop web services environment  [SPOI-7963]   Show container stack trace in dtManage  [SPOI-10230]  containerLogsUrl shown in appattempt table can be pretty-printed", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#task_1", 
            "text": "[SPOI-9291]   Calcite  [SPOI-8027]   TBD: Apex Java high level API - Aggregation Part 1  [SPOI-9769]   Tiles for apps on AppHub  [SPOI-7965]   Productize certification tool to size RTS  [SPOI-6350]   Gauge widget  [SPOI-7433]   Update all relevant docs with AppHub  [SPOI-9693]   Add Validate button in dtAssemble  [SPOI-9688]   Remove autosave from dtAssemble  [SPOI-9971]   Verify that alerts can only be sent by mail  [SPOI-9364]   dtDebug Logs backend feature  [SPOI-9695]   Launch application not using config package name  [SPOI-9413]   Permission changes for tenancy   [SPOI-7966]   Provide user ability to configure security through dtManage UI (only password option)  [SPOI-8736]   dtManage should alert user when there's a potential Hadoop config issue  [SPOI-9575]   Create demo app  [SPOI-9021]   State management benchmark  [SPOI-8933]   Change Megh repository to ASL    [SPOI-8865]   Operator Maturity Framework - Cassandra Output  [SPOI-8788]   Operator Maturity Framework - Enhancement of FS Output Operator  [SPOI-9966]   App-templates misc   [SPOI-9774]   Update Database to HDFS app template   [SPOI-9234]   AppHub - App Pipeline creation with continuous iteration  [SPOI-10063]  Create new apex core build based on master  [SPOI-9859]   Log retrieval tool  [SPOI-9043]   Requirements discussion on Batch Support - Definition of Batch, Scheduling of Batch DAG, State of Batch, Replay of Batch and Monitoring of Batch    [SPOI-8990]   DAG Editor - unable to drag a connection stream from a port if having restriction DISABLE_APP_EDIT_STRUCTURE  [SPOI-9972]   Document DT Gateway System Alerts  [SPOI-9970]   Modify access to allow Admin (and ONLY admin) to set alerts  [SPOI-9653]   Plan and implement (1 item in v1)for Gateway Alerts  [SPOI-10261]  Show friendly message when user logs files have been removed.  [SPOI-9163]   Refactor configPackages to configurations  [SPOI-8223]   Troubleshooting improvements in dtManage  [SPOI-9382]   QA - Operator Maturity Framework - AbstractFileInputOperator  [SPOI-8885]   Operator Testing (Operator Maturity Framework)  [SPOI-9483]   Superuser role and oAuth cleanups  [SPOI-8996]   Add authentication configuration web services spc to gateway REST api doc", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug-fixes_1", 
            "text": "[SPOI-5852]   App Package page has a single word \"ago\" for modification time after importing pi demo  [SPOI-6651]   Launching App from UI ignores APPLICATION_NAME attribute defined in properties.xml file  [SPOI-7062]   dtHub UI - tags column - filter - searching from the beginning of a tag.  [SPOI-7652]   AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings  [SPOI-8039]   UI says \"An error occurred fetching data.\" after launching the application  [SPOI-8349]   User should not be able to delete the default apps in ingestion-solution package  [SPOI-8379]   Property editor for array of enum is not rendered correctly  [SPOI-8489]   Application_Name attribute from the config file is not honored.  [SPOI-8507]   Unable to launch an AppDataTracker application imported from dtHub  [SPOI-8516]   DataTorrent rpm version inconsistency  [SPOI-8522]   Unable to set roles while creating user in secure environment  [SPOI-8523]   Users can kill the app even if privileges get revoked in secure environment   [SPOI-8531]   Multiple MachineData demos are available at dtHub  [SPOI-8534]   README.html for sandbox contains references to 'malhar-users'  [SPOI-8536]   DT RTS gateway log floods with WARN message  [SPOI-8542]   Installation: User home directory is not created by default  [SPOI-8566]   Tuple Recording Modal Fixes  [SPOI-8622]   Exception in retrieving app state in certification when application has not yet reached running state  [SPOI-8630]   \"merge\" configurations option for appPackages also creates new applications  [SPOI-8717]   Updating sandbox generates errors  [SPOI-8781]   Buffer server metrics not available for physical operator in Metrics Chart  [SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available  [SPOI-8888]   Unable to see imported/uploaded/running applications on DT UI in SSL enable envornment   [SPOI-8995]   Running through the unit tests in dtx creates a residual file  [SPOI-9007]   Kryo Exception while re-deploying the DimensionsComputationFlexibleSingleSchemaPOJO operator  [SPOI-9127]   Wrong notification provided by dtConsole when package upload is failed  [SPOI-9134]   Gateway restart modal has incorrect focus  [SPOI-9135]   Gateway restart modal should be horizontally and vertically aligned  [SPOI-9140]   dtConsole shows \"Failed to parse\" error when 'Monitor' tab is refreshed  [SPOI-9152]   Application package link is not working  [SPOI-9153]   Hyperlink not required on AppPackage tab  [SPOI-9161]   Recordings rest API gives wrong number of totalTuples  [SPOI-9199]   Error running application due to YARN API exception  [SPOI-9200]   dt-site.xml has a misguiding warning  [SPOI-9245]    Set logging level  cannot delete the set logs  [SPOI-9431]   Disable tenant option in TenancyFilter  [SPOI-9496]   Use the AppPackageOwner field instead of logged in user, while working with configPackages.  [SPOI-9503]   AbstractFileInputOperator does not honor filePatternRegexp parameter  [SPOI-9580]   APEXMALHAR-2314 Improper functioning in partitioning of sequentialFileRead property of FSRecordReader  [SPOI-9658]   Apps are not filtered correctly using tags column on AppHub   [SPOI-9660]   AppHub navigation tab is still seen as dtHub  [SPOI-9696]   Prevent Navigation in DAG Diagram When Dragging Image Around  [SPOI-9738]   DAG Diagram Doesn't Display Sometimes  [SPOI-9783]   Operators stay in PENDING_DEPLOY  [SPOI-9787]   Configuration package spelling error  [SPOI-9790]   Recording Tuple Dialog Display Bug  [SPOI-9791]   Fix log line format  [SPOI-9793]   Can not start gateway after installation  [SPOI-9794]   Can not start dtcli after installation   [SPOI-9908]   Container StackTrace is not functioning  [SPOI-9914]   Container buttons should be contextual based on state  [SPOI-9916]   Kafka Input Operator (0.9) validation app is missing from QA/test-apps repository  [SPOI-9933]   Unable to run JDBC Poll app on latest SNAPSHOT build (3.7.0)  [SPOI-9934]   Notification History links, when clicked modal doesn't get closed  [SPOI-9939]   Links to log files should not be restricted to enterprise edition  [SPOI-9941]   Gateway password security errors  [SPOI-9974]   Unable to upload packages to the gateway  [SPOI-9977]   Unable to launch applications using apex CLI, gives ClassNotFoundException  [SPOI-10002]  Config Package Page Not Showing Saved Properties  [SPOI-10016]  Config package upload fails  [SPOI-10018]  Unnecessary check boxes present for applications under Develop tab  [SPOI-10020]  Tuple recording feature is not working  [SPOI-10036]  Last modified time for imported packages is always shown with additional 2 minutes.  [SPOI-10043]  User should not be allowed to save tupleSchema with blank values  [SPOI-10044]  Editing existing tuple schema gives TypeError  [SPOI-10046]  Deleting a property while creating tuple schema directly creates final schema with remaining properties  [SPOI-10047]  Tuple schema created using JSON input does not take latest JSON as input  [SPOI-10050]  Can not record samples  [SPOI-10051]  Delete roles modal should have warm colors  [SPOI-10052]  Restore roles modal should have warm colors  [SPOI-10054]  Launch application for configuration package not using local settings for application naming  [SPOI-10056]  Malhar-angular-table temporary fix for application packages and app properties lists  [SPOI-10065]  User is allowed to \"Add System Property\" with blank value  [SPOI-10068]  dtGateway script doesn't return correct status when gateway is down  [SPOI-10080]  Issues with containers table from UI console  [SPOI-10110]  AppPackage get info should not be used to show the configPackage apps  [SPOI-10143]  StramEvents API gives 500 error  [SPOI-10147]  \"cluster/metrics\" API gives 500 error  [SPOI-10148]  Add system properties has weird titles  [SPOI-10150]  Change system properties modal button should not be in cool colors  [SPOI-10152]  Disable appdatatracker modal buttons should be in warm color  [SPOI-10155]  AppDataTracker can not be enabled  [SPOI-10156]  Clicking on \"ended apps\" and \"system apps\" multiple times shows multiple shadows of \"system apps\"  [SPOI-10157]  Inspect port UI hangs  [SPOI-10159]  Can not upload application packages  [SPOI-10181]  killed applications - (1) \"AM Logs\" dropdown is empty. (2) AppMaster container does not have purple label in id column.   [SPOI-10195]  Selected schema doesn't show the fields in the schema  [SPOI-10200]  Button for \"Delete logging level\" is misaligned  [SPOI-10209]  Link is missing for \"originalTrackingUrl\" field on currently running app attempt  [SPOI-10228]  Sorting by  host  in the Physical Plan -  Containers tab is not working  [SPOI-10229]  \"attempts\" tab for dtDebug is available in community edition  [SPOI-10233]  Application attempts API does not return startedTime and finishedTime for FAILED attempts  [SPOI-10235]  Kill Application Master container modal should have warm colors  [SPOI-10236]  Kill selected container title has unwanted text  [SPOI-10242]  Configuration Packages missing  [SPOI-10257]  An error is shown for a while while launching Application Configurations  [SPOI-10258]  Application package launch modal shows wrong \"Use configuration file\" option instead of \"Use configuration package\"   [SPOI-10259]  Security configuration page on console has illegible content  [SPOI-10260]  Links in Alert configuration modal are broken  [SPOI-10277]  Stacktrace is not fully shown in alerts detail  [SPOI-10279]  Update app hub description.  [SPOI-10300]  ValidateApplication   PutApplication should have CheckViewPermission instead of checkModifyPermission  [SPOI-10343]  UI errors while displaying containers in Physical tab  [SPOI-10350]  Jackson jars are missing from the RTS after Hadoop upgrade to 2.6 causing API failures  [SPOI-10356]  Update Buttons Labels  [SPOI-10358]  Schemas in ConfigPackages  [SPOI-10359]  Schema in ConfigPackage Permission on 2 APIs should be reduced to view  [SPOI-10361]  Upload of configPackage failed  [SPOI-10381]  Unable to create configPackage from java application  [SPOI-10387]  Unable to launch application configurations from \"Application Configuration details\" page  [SPOI-10390]  Use Configuration Package Should be Enabled", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-360", 
            "text": "Release date: Nov 9, 2016", 
            "title": "Version: 3.6.0"
        }, 
        {
            "location": "/release_notes/#summary_5", 
            "text": "DataTorrent RTS releases AppHub, a repository of application templates for various Big Data use cases. The key of this release is that RTS now have an infrastructure to distribute application templates easily. Developers can reduce the time to develop Big Data applications using templates. There are five templates in this release with many more to come.    HDFS Sync   Amazon S3 to HDFS Sync  Kafka to HDFS Sync  HDFS to HDFS Line Copy  HDFS to Kafka Sync", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#appendix_3", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#improvement_1", 
            "text": "[SPOI-9136] - Enforce DefaultOutputPort.emit() or Sink.put() thread affinity", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#task_2", 
            "text": "[SPOI-9118] - Publish App Templates for Ingestion on AppHub  [SPOI-9419] - Update AppHub API to include the markdown content  [SPOI-9432] - Update AppHub back end to extract markdown from apa", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task", 
            "text": "[SPOI-3277] - Show app master logs on UI for applications that fail at launch when we upgrade to Hadoop 2.4 or above  [SPOI-9079] - Creation of example application for transform operator  [SPOI-9235] - Allow users to create new configurations from Application Configurations view  [SPOI-9240] - Create individual package view for AppHub artifacts  [SPOI-9464] - Rename all references of AppHub on console UI to AppHub  [SPOI-9574] - Rename title on AppHub list page  [SPOI-9612] - Upgrade AppHub server deployment", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug-fixes_2", 
            "text": "[SPOI-9140] - dtManage shows \"Failed to parse\" error when 'Monitor' tab is refreshed  [SPOI-9522] - DELETE call to /ws/v2/config/properties/{name} returns 500  [SPOI-9727] - DTINSTALL_SOURCE incorrectly assumes file name", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-350", 
            "text": "Release date: Sep 26, 2016", 
            "title": "Version 3.5.0"
        }, 
        {
            "location": "/release_notes/#summary_6", 
            "text": "DataTorrent RTS continues to deliver features that sets it apart in bringing operability in running an enterprise grade big data-in-motion platform. This particular release brought new features such as   Allowing users to analyze \"stuck operator\" by obtaining stats from the running JVM (i.e. GC stats and thread dump)  Ability to show/hide critical path in both logical and physical DAG", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#apache-apex-malhar", 
            "text": "The other important part of going to production is a library of operators that is more than just functional. They need to be fault tolerant, partitionable, support idempotency, and dynamically scalable. The recent release of Apache Apex Malhar 3.5.0 provides new and updated operators and APIs to bring those enterprise features.    Windowed Operator that supports the windowing semantics outlined by Apache Beam and Google Cloud DataFlow, including the concepts of event time windows, session windows, watermarks, allowed lateness, and triggering.  High level Java stream API now uses the aforementioned Windowed Operator to support stateful transformation with Apache Beam style windowing semantics.  Introduction of Spillable Data Structures that make use of Managed State.  Deduper Operator to process  whether a given record is a duplicate or not  Enricher Operator to join a stream with a lookup source and operate on any POJO object  HBase input operator. Improve HBasePOJOInputOperator with support for threaded read  File Record reader module. It is useful for reading from files \"line by line\" in parallel and emit each line as seperate * tuple.  JDBC Poll Input Operator   For the full release note, please go to\nhttps://blogs.apache.org/apex/entry/apache_apex_malhar_3_5", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/release_notes/#appendix_4", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_3", 
            "text": "[SPOI-9232] Dedup with manage state operator marking all impression as duplicate  [SPOI-9203]   \"check for updates\" option says 'no updated versions' and then displays updated packages  [SPOI-9183]   Nested operator properties should follow order specified in the ORB on dtAssemble UI  [SPOI-8827]   \"Check for updates\" option keeps loading the page when no updates are available", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#improvement_2", 
            "text": "[SPOI-7343] - Ability to obtain a thread dump from a container  [SPOI-8191] - Operator properties should follow order specified in the ORB on dtAssemble UI  [SPOI-8352] - Warning message while restarting app should be changed  [SPOI-8450] - Dedup ports connected to console should not write to log  [SPOI-8722] - Logical DAG, Physical DAG - change \"Show/Hide String Locality\", \"Show/Hide Critical Path\" to checkbox.", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#story", 
            "text": "[SPOI-6794] - HBase Input Operator  [SPOI-6795] - Creation of Concrete Cassandra Output  [SPOI-6932] - Module to read from HDFS Input record by record and emit it downstream  [SPOI-7948] - JDBC Input Operator", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#bug-fixes_3", 
            "text": "[SPOI-7836] - Trend widget fails after dashboard save/reload with PiDemo app  [SPOI-7886] - Duplicate ports show up in the UI for POJO operators  [SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same  [SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500  [SPOI-8721] - Column labeled buffer service size is showing bytes per second  [SPOI-9084] - Refresh tokens failing in some scenarios with a login failure message  [SPOI-9130] - dtConsole shows no applications on monitor tab  [SPOI-9131] - gateway REST and WebSocket APIs for cluster metrics fail to report stats", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#task_3", 
            "text": "[SPOI-8411] - Deduper operator using Managed State  [SPOI-9004] - Deduper Documentation", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_1", 
            "text": "[SPOI-8389] - ORB defaults for FileSystem related operators  [SPOI-8390] - Added operator default values in Application.json for user visibility  [SPOI-8410] - Kafka Input Operator Unit test failed  [SPOI-8461] - AbstractKafkaInputOperator problems", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-340", 
            "text": "", 
            "title": "Version 3.4.0"
        }, 
        {
            "location": "/release_notes/#summary_7", 
            "text": "Affinity rules provides a way to specify hints on how operators should be deployed in a Hadoop cluster. There are two types of rules: affinity and anti-affinity rules. Affinity rule indicates that the group of operators in the rule should be allocated together. Anti-affinity rule, the new feature in Apache Apex 3.4.0, indicates that the group of operators should be allocated separately.  This release also includes a lot of bug fixes. Please see appendix for full list.", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#dtmanage", 
            "text": "User can restart a killed application from dtManage  \"Retrieve Ended Apps\" button changes to \"Hide Ended Apps\" after dtManage retrieves killed apps.  User can use mouse scroll to zoom in/out of physical DAG view", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#apache-apex-340", 
            "text": "Blacklist problem nodes from future container requests  Support adding module to application using property file API.  Ability to obtain a thread dump from a container  RPC timeout is now configurable  When an operator is blocked, it nows print out a warning instead of debug", 
            "title": "Apache Apex 3.4.0"
        }, 
        {
            "location": "/release_notes/#appendix_5", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#known-issues_4", 
            "text": "[SPOI-8518] - Support links from Dashboard to Application instance pages  [SPOI-8516] - Datatorrent rpm version inconsistency  [SPOI-8470] - Check for already existing app package should be done before uploading the whole package  [SPOI-8436] - Documentation is needed on \"How to use Transform operators in ingestion solution app package\"  [SPOI-8434] - Documentation is needed on \"How to use Generic JDBC/PostgreSQL operators in ingestion solution app package\"  [SPOI-8433] - Documentation is needed on \"How to use Enrichment operator in ingestion solution app package\"  [SPOI-8414] - Drop down is not shown for \"Fields to copy\" property of \"POJO Enricher\" unless output schema is not specified  [SPOI-8352] - Warning message while restarting app should be changed  [SPOI-8296] - \"File Path\" and \"Output File Name\" properties for HDFS File Output Operator should be clubbed together  [SPOI-8295] - \"Include Fields\" parameter for \"POJO Enricher\" has misleading description  [SPOI-8294] - Operator Class Name for \"Delimited Parser\" operator should not be \"CSV Parser\"  [SPOI-8293] - File permission property is not working properly for HDFS File Output Operator  [SPOI-8291] - Parameters on dtAssemble should be logically ordered  [SPOI-8224] - Providing Field Infos value for JDBC POJO Input Operator is too complex  [SPOI-8192] - Clicking on edit option for apps throws validation errors in activity panel  [SPOI-8158] - Options to modify property values should be closer to the property name on dtAssemble canvas  [SPOI-8144] - \"Tuple Schemas\" link at top right corner of dtAssemble canvas should open in new tab  [SPOI-8143] - Tuple Schema page not available directly from Develop tab  [SPOI-8133] - When stream is added in dtAssemble, user should be notified if schema is required  [SPOI-8131] - Couple of parameters for Kafka Input Operator should have dropdown selection in dtAssemble  [SPOI-8130] - Documentation for ingestion beta operators need to be improved  [SPOI-8128] - Port names for operators are not intuitive when displayed on dtAssemble canvas  [SPOI-8087] - JDBC input operator query should not require explicit ordering of column names  [SPOI-8038] - Output file names for HDFS output should not contain timestamp and '.tmp' extension  [SPOI-8522] - Unable to set role while creating user in a secure environment.  There is a workaround by create user with no role and then assign the role  [SPOI-8523] - User can kill app even though privileges got revoked.  This applies to secure environment only.    [SPOI-8552] - App Package cache throws uncaught exception when package is not found, resulting in http status 500  [SPOI-8536] - DT RTS gateway log floods with WARN message  [SPOI-8535] - Need to restart dtgateway for enabling password authentication in sandbox  [SPOI-8534] - README.html for sandbox contains references to 'malhar-users'  [SPOI-8533] - Importing 'Apache Apex Malhar Iteration Demo' throws error for 'property' tag in properties.xml  [SPOI-8532] - Importing packages from dtHub sometimes gives ZipException  [SPOI-8531] - Multiple MachineData demos are available at dtHub  [SPOI-8524] - Clicking on \"generate new dashboard\" first navigates to 'Learn' tab on the dtConsole  [SPOI-8523] - Users can kill the app even if privileges get revoked in secure environment  [SPOI-8522] - Unable to set roles while creating user in secure environment  [SPOI-8519] - Ingestion application on dtHub still shows 'requires Apex version' with \"-incubating\"  [SPOI-8511] - Gateway Websocket API leaks information while unauthorized  [SPOI-8509] - dtAssemble operator documentation shows '@link' markers  [SPOI-8507] - Unable to launch an AppDataTracker application imported from dtHub  [SPOI-8491] - UI mixes the order and ids of tuple recording ports  [SPOI-8490] - gateway issues in SSL enabled cluster  [SPOI-8479] - Uninstall does not work even though the install was successful previously  [SPOI-8477] - Upgrade License Opens in dtManage window, it should be in opened up in new window  [SPOI-8476] - Hadoop-common-tests library shouldn't be part of RTS build  [SPOI-8474] - On addition of huge role name, non-specific errors are shown  [SPOI-8473] - Gateway, console allows impractially longer user roles additions  [SPOI-8472] - Visually ugly error presentation  [SPOI-8469] - If Kerberos tickets are changed, you have to refresh whole UI  [SPOI-8467] - installation script provides incorrect information  [SPOI-8432] - JDBC input operator is failing with exception \"fetching metadata\"  [SPOI-8424] - Dedup does not honor the expiryPeriod when error tuple is introduced in between two valid tuples  [SPOI-8422] - Time properties for operators should have units mentioned for them  [SPOI-8416] - dtAssemble Can't change application name  [SPOI-8365] - HDFS sync app : Unable to sync 500 GB file  [SPOI-8358] - Uptime and latency values are very high exactly after app is launched  [SPOI-8317] - dtIngest 1.1.0 (Compiled against 3.2.0) can not be launched  [SPOI-8222] - Operator/module names under Operator Library, dtAssemble canvas and right side panel should be same  [SPOI-8213] - Application DAG is not displayed when clicked on app link  [SPOI-8202] - Unable to add custom properties while launching apps  [SPOI-8197] - Default values for \"Field Infos\" and \"Bucket Manager\" properties should be set appropriately  [SPOI-7986] - Gateway proxy feature is not working  [SPOI-7934] - Kafka-dedup-HDFS-solution: Ahead in time messages are lost from Dedup operator", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#bug-fixes_4", 
            "text": "[SPOI-7939] - Dynamic repartition causes application to hang  [SPOI-8468] - Can't assign roles for users in secure environment  [SPOI-8464] - \"Disable Reporting\" option on System Configuration page gives NullPointerException  [SPOI-8024] - Gateway is leaving behind dtcheck temp files in HDFS  [SPOI-7640] - Changing dashboard name in dashboard settings modal and then canceling does not revert dashboard name  [SPOI-8077] - Gateway logs NPE if an app master is misbehaving  [SPOI-8046] - Kerberos Cluster: Installation Wizard can not get past beyond Hadoop Configuration screen  [SPOI-8055] - Console references to dt-text-tooltip no longer produce a tool tip  [SPOI-7898] - Default JAAS support classes duplicated in dt-library  [SPOI-7929] - Update log4j.properties in the DTX/dist/install to set debug level for org.apache.apex  [SPOI-7943] - The demo applications fails due to numberOfBuckets is less than 1  [SPOI-8092] - Problems in launching jobs with authentication enabled on secure cluster  [SPOI-7794] - Monitor page not refreshing properly  [SPOI-7889] - Cannot read property 'hideBreadcrumbs' of undefined  [SPOI-7856] - Modify application packages dtHub import/update paths  [SPOI-7907] - Downloads of AppPackages result in corrupted files during local testing  [SPOI-7971] - After dynamic repartition application appears blocked", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-340_1", 
            "text": "", 
            "title": "Apache Apex 3.4.0"
        }, 
        {
            "location": "/release_notes/#new-feature_2", 
            "text": "[APEXCORE-10] - Enable non-affinity of operators per node (not containers)  [APEXCORE-359] - Add clean-app-directories command to CLI to clean up data of terminated apps  [APEXCORE-411] - Restart app without specifying app package", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_3", 
            "text": "[APEXCORE-92] - Blacklist problem nodes from future container requests  [APEXCORE-107] - Support adding module to application using property file API.  [APEXCORE-304] - Ability to add jars to classpath in populateDAG  [APEXCORE-328] - CLI tests should not rely on default maven repository or mvn being on the PATH  [APEXCORE-330] - Ability to obtain a thread dump from a container  [APEXCORE-358] - Make RPC timeout configurable  [APEXCORE-380] - Idle time sleep time should increase from 0 to a configurable max value  [APEXCORE-383] - Time to sleep while reservoirs are full should increase from 0 to a configurable max value  [APEXCORE-384] - For smaller InlineStream port queue size use ArrayBlockingQueueReservoir as default  [APEXCORE-399] - Need better debug information in stram web service filter initializer  [APEXCORE-400] - Create documentation for security  [APEXCORE-401] - Create a separate artifact for checkstyle and other common configurations  [APEXCORE-407] - Adaptive SPIN_MILLIS for input operators  [APEXCORE-409] - Document json application format  [APEXCORE-419] - When operator is blocked, print out a warning instead of debug  [APEXCORE-447] - Document: update AutoMetrics with AppDataTracker link", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_1", 
            "text": "[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block  [APEXCORE-201] - Reported latency is wrong when a downstream operator is behind more than 1000 windows  [APEXCORE-326] - Iteration causes problems when there are multiple streams between two operators  [APEXCORE-335] - StramLocalCluster should teardown StreaminContainerManager after run is complete  [APEXCORE-349] - Application/operator/port attributes should be returned using string codec in REST service  [APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers  [APEXCORE-352] - Temp directories/files not created in temp directory specified by system property java.io.tmpdir  [APEXCORE-353] - Buffer server may stop processing data  [APEXCORE-355] - CLI list-*-attributes command name change  [APEXCORE-362] - NPE in StreamingContainerManager  [APEXCORE-363] - NPE in StreamingContainerManager  [APEXCORE-374] - Block with positive reference count is found during buffer server purge  [APEXCORE-375] - Container killed because of Out of Sequence tuple error.  [APEXCORE-376] - CLI command 'dump-properties-file' does not work when connected to an app  [APEXCORE-385] - Temp directories/files not always cleaned up when launching applications  [APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily  [APEXCORE-393] - Reset failure count when consecutive failed node is removed from blacklist  [APEXCORE-397] - Allow configurability of stram web services authentication  [APEXCORE-398] - Ack may not be delivered from buffer server to it's client  [APEXCORE-403] - DelayOperator unit test fails intermittently  [APEXCORE-413] - Collision between Sink.getCount() and SweepableReservoir.getCount()  [APEXCORE-415] - Input Operator double checkpoint  [APEXCORE-421] - Double Checkpointing May Happen In Input Node On Shutdown  [APEXCORE-422] - Checkstyle rule related to allowSamelineParameterizedAnnotation suppression  [APEXCORE-434] - ClassCastException when making webservice calls to STRAM in secure mode  [APEXCORE-436] - Update log4j.properties in archetype test resources to set debug level for org.apache.apex  [APEXCORE-439] - After dynamic repartition application appears blocked  [APEXCORE-444] - 401 authentication errors when making webservice calls to STRAM in secure mode  [APEXCORE-445] - Race condition in AsynFSStorageAgent.save()", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_4", 
            "text": "[APEXCORE-293] - Add core and malhar documentation to project web site  [APEXCORE-319] - Document backward compatibility guidelines  [APEXCORE-340] - Rename dtcli script to apex  [APEXCORE-345] - Upgrade to 0.7.0 japicmp  [APEXCORE-381] - Upgrade async-http-client dependency version because of security issue  [APEXCORE-410] - Upgrade to netlet 1.2.1  [APEXCORE-423] - Fix style violations in Apex Core  [APEXCORE-446] - Add source jar in the shaded-ning build", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_2", 
            "text": "[APEXCORE-254] - Introduce Abstract and Forwarding Reservoir classes  [APEXCORE-269] - Provide concrete implementation of AbstractReservoir based on SpscArrayQueue  [APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size  [APEXCORE-369] - Fix timeout in AbstractReservoirTest.performanceTest  [APEXCORE-402] - SpscArrayQueue to notify publishing thread on not full condition", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-330", 
            "text": "", 
            "title": "Version 3.3.0"
        }, 
        {
            "location": "/release_notes/#summary_8", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#dthub", 
            "text": "DataTorrent RTS allows companies to quickly build low latency real time Big Data application that can scale and fault tolerant. With the introduction of dtHub, DataTorrent now host and maintain an application distributor. You can access it through dtManage and easily install or update application without having to upgrade the whole RTS platform. Prior to dtHub, RTS installer packaged both platform and applications. In 3.3, they are decoupled with significantly reduced installer size. You can independently choose when to upgrade the platform and when to install/upgrade applications.", 
            "title": "dtHub"
        }, 
        {
            "location": "/release_notes/#dtmanage_1", 
            "text": "Troubleshooting is made easier now that user can view the containers where physical operators have lived  RTS Community Edition user can now view container logs and set log levels via dtManage. API for runtime DAG and property change are also available  RTS Enterprise Edition is changed from 30 days to 60 days  When an application is killed, RTS will delete the app directory according to policy  in dt-site.xml", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#dtdasbhoard", 
            "text": "New widgets for the dashboard: Geo coordinates with circles, Geo regions with gradient fill and single Value", 
            "title": "dtDasbhoard"
        }, 
        {
            "location": "/release_notes/#dtassemble-beta", 
            "text": "Top level \u201cDevelop\u201d takes users directly to Application Page  Navigation changes on how user get to Tuple Schema. User goes to It is now \"Edit Application\" then \"Tuple Schema\"  Development (breadcrumb link)", 
            "title": "dtAssemble (beta)"
        }, 
        {
            "location": "/release_notes/#documentation", 
            "text": "FileSplitter: http://docs.datatorrent.com/operators/file_splitter/  Block Reader: http://docs.datatorrent.com/operators/block_reader/", 
            "title": "Documentation"
        }, 
        {
            "location": "/release_notes/#apache-apex-33", 
            "text": "Support for iterative processing. It is a building block to support machine learning  Ability to populate DAG at application launch time  Pre checkpoint operator callback so that it can execute a logic before the operator gets checkpointed (e.g. flush file to HDFS)  Provide the option for operator to do checkpointing in a distribute in-memory store. It is faster than HDFS due to disk i/o latency  Add group ID information in an applicatin package.  It is visible for application grouping in dtHub.", 
            "title": "Apache Apex 3.3"
        }, 
        {
            "location": "/release_notes/#known-issues-in-33", 
            "text": "[SPOI-7696] - Community edition (which gets activated after expired license) does not have newly added community features  [SPOI-7471] - Temp directories/files not cleaned up in Gateway  [SPOI-7697] - \"Set Logging Levels\" option on application details page does not show initial target/loglevel fields  [SPOI-7668] - If AppDataTracker is disabled, changing the YARN queue does not restart it  [SPOI-7652] - AppDataTracker does not show up under \"Choose apps to visualize\" in dashboard settings  [SPOI-7678] - On configuration complete, summary page should populate RTS version  [SPOI-7479] - dtHub \"check for updates\" option says 'no updated versions' and then displays updated packages  [SPOI-7626] - Stacked Area Chart widget shows NaN values on Firefox", 
            "title": "Known Issues in 3.3"
        }, 
        {
            "location": "/release_notes/#appendix_6", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/release_notes/#dthub_1", 
            "text": "[SPOI-6787] - dtHub UI - explore, import, download  [SPOI-7023] - dtHub UI - check for updates  [SPOI-3643] - Remove import default packages from Application Packages page  [SPOI-7451] - Add options summary to Application Packages screen  [SPOI-6968] - Please make API return a new property that indicates whether gateway has internet access or not (for dtHub feature)  [SPOI-7059] - add \"tags\" to import list", 
            "title": "dtHub"
        }, 
        {
            "location": "/release_notes/#dtmanage_2", 
            "text": "[SPOI-3549] - dtManage - Ability to view container history where physical operator has lived  [SPOI-7382] - UI evaluation license expiration message colour update  [SPOI-7418] - Visualize AppDataTracker data  [SPOI-7129] - Add countdown and links to enterprise evaluation in dtManage  [SPOI-7226] - Remove choice of community edition and enterprise evaluation in install wizard", 
            "title": "dtManage"
        }, 
        {
            "location": "/release_notes/#dtdashboard", 
            "text": "[SPOI-6522] - Geo coordinates with weighted circles widget  [SPOI-6523] - Geo regions with gradient fill widget  [SPOI-7247] - Create dimensional single value widget  [SPOI-7258] - Persist label changes in trend and single value widgets  [SPOI-6023] - dtDashboard - Widgets that support dimension schema can support multi-value keys   [SPOI-6021] - Support tag for  snapshot server and dimension store  [SPOI-6331] - Notify user when widget is unable to automatically load data  [SPOI-6658] - UI says \"no rows testing\" when no data available to display in tables  [SPOI-6887] - Changing choropleth map class does not remove previously selected map  [SPOI-7246] - Change default widget colors to be websafe  [SPOI-7257] - add tags-based dimension query settings in trend widget", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/release_notes/#dtassemble-beta_1", 
            "text": "[SPOI-7133] - Tuple Schemas change and Develop on top navigation bar change", 
            "title": "dtAssemble (beta)"
        }, 
        {
            "location": "/release_notes/#megh", 
            "text": "[SPOI-7665] - Megh enhancements for TelecomDemo  [SPOI-7230] - Add group id information to all megh app packages  [SPOI-7251] - Add directory structure for modules in megh  [SPOI-7693] - Add Telecom Demo To Megh", 
            "title": "Megh"
        }, 
        {
            "location": "/release_notes/#docs", 
            "text": "[SPOI-6471] - Documentation: FileSplitter  [SPOI-6472] - Documentation: BlockReader", 
            "title": "Docs"
        }, 
        {
            "location": "/release_notes/#rts-community-edition", 
            "text": "[SPOI-7670] - Removing dtManage restrictions from community edition  [SPOI-7682] - Lock down all auth/security features in community edition  [SPOI-7130] - Community edition to have an upgrade button to Enterprise eval  [SPOI-7243] - make all locked-feature places(e.g. Visualize link, logs button, new application button) to have the same effect of \"upgrade to enterprise                \" link in community edition, and to have a key icon instead of being crossing out", 
            "title": "RTS Community Edition"
        }, 
        {
            "location": "/release_notes/#rts-enterprise-edition", 
            "text": "[SPOI-7127] - Change enterprise evaluation days from 30 days to 60 days", 
            "title": "RTS Enterprise Edition"
        }, 
        {
            "location": "/release_notes/#bug-fixes_5", 
            "text": "[SPOI-5622] - dtcli: Command 'show-physical-plan' fails with \"Failed web service request\" error  [SPOI-6352] - Gateway's RM Proxy REST calls don't work with HA-enabled  [SPOI-6424] - Gateway cannot determine its own local address to the cluster when RM HA is enabled  [SPOI-6555] - Add Missing datatorrent.apppackage.classpath property to dt-demos  [SPOI-6624] - ADT issues impacting dtingest Dashboard  [SPOI-6674] - AbstractFileOuptutOperator refactoring and fixes  [SPOI-6834] - fixing scope of jars in Megh  [SPOI-6837] - Gateway has a lot of blocked threads under heavy load  [SPOI-6886] - Selected map feature / object should be persisted  [SPOI-6949] - Collation in BucketManager incurs a performance hit while writing to HDFS and is not an optimization  [SPOI-6999] - Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7026] - only lists the ones that have different versions in update section  [SPOI-7027] - fix a bug that loading is forever when there is no updates in check for updates  [SPOI-7028] - compare version number and only list packages whose dtHub version is higher than installed version in update section  [SPOI-7037] - support sorting, filter in string-type columns in import pkgs, update pkgs  [SPOI-7040] - optimize visual layout of import pkgs page  [SPOI-7042] - only shows packages that are compatible with the APEX version in check for update list  [SPOI-7080] - remove bar chart widget that is not in use  [SPOI-7106] - Update Megh japi version and fix the broken build because of semantic version  [SPOI-7138] - Dimension unifier return empty results   [SPOI-7236] - Console build fails due to jsHint issues after updating version  [SPOI-7293] - Post installation links no longer available on datatorrent.com  [SPOI-7296] - gateway spills out error trying to write to /var/log/datatorrent/ in local install  [SPOI-7297] - Local install fails in secure mode  [SPOI-7309] - HDHT Broken and Hangs After Wall And Purge Changes  [SPOI-7338] - After changing configuration \"dt.appDataTracker.queue\" (e.g. to \"root.ashwin\") and kill system application AppDataTracker, AppDataTracker                 incorrectly starts with the default queue (\"root.dtadmin\").  [SPOI-7350] - Installation wizard final step refers to invalid developers URL  [SPOI-7361] - get-operator-attributes fails on CDH  [SPOI-7383] - Update invalid links in UI console info section  [SPOI-7447] - dtHub UI - check for updates - when there is no newer version for any installed package, loading image will be hanging forever  [SPOI-7460] - On Visualize tab, link for documentation does not work  [SPOI-7469] - Remove \"in HDHT\" from System Configuration page  [SPOI-7471] - Temp directories/files not cleaned up in Gateway  [SPOI-7474] - \"Disable App Data Tracker\" option does not work  [SPOI-7481] - Launch macros for demo apps should be removed in dtcli  [SPOI-7482] - Cloning/deleting application under Application package does not refresh the list  [SPOI-7498] - Verify dtingest download link change on datatorrent website  [SPOI-7502] - Lock mishandling in App Package local cache code  [SPOI-7503] - Changing \"dt.appDataTracker.enable=true/false\" using REST API calls doesn't take effect unless gateway restarts  [SPOI-7510] - Application launch notifications no longer show up  [SPOI-7523] - Kill only AppDataTracker whose user is the current login user when App Data Tracker queue is changed.  [SPOI-7542] - Unable to import app package from dtHub  [SPOI-7574] - dtcli command 'dump-properties-file' does not work when connected to an app  [SPOI-7577] - For 3.3.0 release, RTS version is shown as 3.3.1 in System Configuration  [SPOI-7604] - HDHT bucket meta class is obfuscated incorrectly  [SPOI-7615] - dtHub intermittently throws error as \"Failed to import\" while importing multiple pkgs at the same time   [SPOI-7619] - specify bower link malhar-angular-dashboard to version 1.0.1  [SPOI-7644] - Need to update installer script  [SPOI-7679] - Unable to access newly added features for Community edition.", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#apache-apex-33-change-logs", 
            "text": "https://github.com/apache/incubator-apex-core/blob/v3.3.0-incubating/CHANGELOG.md   [SPOI-7061] - Implement retention policy for terminated apps  [SPOI-7492] - DT_GATEWAY_CLIENT_OPTS overrides all JVM options and there is no way to supply additional options to the default options  [SPOI-5735] - Create local file cache for app package  [SPOI-6981] - Move orderedOutput feature to AbstractDeduper. Rename AbstractDeduperOptimized to AbstractBloomFilterDeduper  [SPOI-7448] - Work around the attribute bug detailed in APEXCORE-349 so that ADT still works  [SPOI-7470] - Work around namenode NPE bug in hadoop 2.7.x to avoid throwing NPE to the user. https://issues.apache.org/jira/browse/APEXCORE-45 and https://issues.apache.org/jira/browse/HDFS-9851  [SPOI-6381] - Support Dynamically Updating Enum Values For Keys In The Dimensions Store  [SPOI-6545] - Allow Gateway to directly contact dtHub to install app packages", 
            "title": "Apache Apex 3.3 Change Logs"
        }, 
        {
            "location": "/release_notes/#new-feature_3", 
            "text": "[APEXCORE-3] - Ability for an operator to populate DAG at launch time  [APEXCORE-60] - Iterative processing support  [APEXCORE-78] - Pre-Checkpoint Operator Callback  [APEXCORE-276] - Make App Data Push transport pluggable and configurable  [APEXCORE-283] - Operator checkpointing in distributed in-memory store  [APEXCORE-288] - Add group id information to apex app package", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_4", 
            "text": "[APEXCORE-40] - Semver dependencies should be in Maven Central  [APEXCORE-162] - Enhance StramTestSupport.TestMeta API  [APEXCORE-181] - Expose methods in StramWSFilterInitializer to get the RM webapp address  [APEXCORE-188] - Make type graph lazy load  [APEXCORE-199] - CLI should check for version compatibility when launching app package  [APEXCORE-228] - Add maven 3.0.5 as prerequisites to the Apex parent pom  [APEXCORE-229] - Upgrade checkstyle maven plugin (2.17) and checkstyle dependency (6.11.2)  [APEXCORE-291] - Provide a way for an operator to specify its metric aggregator instance  [APEXCORE-305] - Enable checkstyle violations logging to console during maven build", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_2", 
            "text": "[APEXCORE-58] - endWindow is being called even when the operator is being undeployed  [APEXCORE-83] - beginWindow not called on recovery  [APEXCORE-193] - apex-app-archetype has extraneous entry that generates a warning when running it  [APEXCORE-204] - Update checkstyle and codestyle to be the same  [APEXCORE-211] - Brace placement after static blocks in checkstyle configuration  [APEXCORE-263] - Checkpoint can be performed twice for same window  [APEXCORE-274] - removeTerminatedPartition fails for Unifier operator  [APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection  [APEXCORE-278] - GenericNodeTest clutters test logs with unnecessary statement  [APEXCORE-296] - Memory leak in operator stats processing  [APEXCORE-300] - Fix checkstyle regular expression  [APEXCORE-303] - Launch properties not evaluated", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_5", 
            "text": "[APEXCORE-24] - Takes out usage of Rhino as it is GPL 2.0  [APEXCORE-186] - Enable license check in Travis CI  [APEXCORE-253] - Apex archetype includes dependencies which do not belong to org.apache.apex  [APEXCORE-298] - Reduce the severity of  line length check  [APEXCORE-301] - Add \"io\" as a separate import to checkstyle rules  [APEXCORE-302] - Update NOTICE copyright year  [APEXCORE-308] - Implement findbugs plugin reporting  [APEXCORE-317] - Run performance benchmark for the Apex Core 3.3.0 release", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_3", 
            "text": "[APEXCORE-104] - Expand Module DAG  [APEXCORE-105] - Support injecting properties through xml file on modules.  [APEXCORE-144] - Provide REST api for listing information about module.  [APEXCORE-151] - Provide code style templates for major IDEs (Eclipse, IntelliJ and NetBeans)  [APEXCORE-182] - Add Apache copyright to IntelliJ  [APEXCORE-194] - Add support for ProxyPorts in Modules  [APEXCORE-226] - Strictly enforce wrapping indentation in checkstyle  [APEXCORE-227] - Enforce left brace placement for anonymous class on the next line  [APEXCORE-230] - Limit line lengths to be 120  [APEXCORE-239] - Upgrade checkstyle to 6.12 from 6.11.2  [APEXCORE-248] - Increase wrapping indentation from 2 to 4.  [APEXCORE-249] - Enforce class, method, constructor annotations on a separate line  [APEXCORE-250] - Exclude DtCli from System.out checks  [APEXCORE-267] - Fix existing checkstyle violations in api  [APEXCORE-270] - Enforce checkstyle validations on test classes  [APEXCORE-272] - Attributes added to operator inside Module is not preserved.  [APEXCORE-273] - Fix existing checkstyle violations in bufferserver module  [APEXCORE-306] - Recovery checkpoint handing in iteration loops", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-321", 
            "text": "", 
            "title": "Version 3.2.1"
        }, 
        {
            "location": "/release_notes/#summary_9", 
            "text": "This release bundles a few fixes and features for customers who are on 3.2.0 and not ready to upgrade to 3.3.0", 
            "title": "Summary"
        }, 
        {
            "location": "/release_notes/#improvement_5", 
            "text": "[SPOI-7900] add default role to first time login user  [SPOI-7061] Implement retention policy for terminated apps", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug-fixes_6", 
            "text": "[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7471] Temp directories/files not cleaned up in Gateway  [SPOI-7971] After dynamic repartition application appears blocked", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/release_notes/#known-issues_5", 
            "text": "[SPOI-6999] Gateway stops updating application information after problems with YARN or HDFS  [SPOI-7061] Implement retention policy for terminated apps  [SPOI-7471] Temp directories/files not cleaned up in Gateway  [SPOI-7971]After dynamic repartition application appears blocked", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/#apache-apex", 
            "text": "[APEXCORE-327] - Implement proper semantic version checks in patch release branches  [APEXCORE-358] - Make RPC timeout configurable  [APEXCORE-410] - Upgrade to Netlet 1.2.1  [APEXCORE-365] - Buffer server handling for tuple length that exceeds data list block size", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/release_notes/#apache-apex-bug-fixes_1", 
            "text": "[APEXCORE-130] - Throwing A Runtime Exception In Setup Causes The Operator To Block  [APEXCORE-274] - removeTerminatedPartition fails for unifier operator  [APEXCORE-275] - Two threads can try to reconnect to websocket server upon disconnection  [APEXCORE-350] - STRAM's REST service sometimes returns duplicate and conflicting Content-Type headers  [APEXCORE-353] - Buffer server may stop processing data  [APEXCORE-362] - NPE in StreamingContainerManager  [APEXCORE-363] - NPE in StreamingContainerManager  [APEXCORE-374] - Block with positive reference count is found during buffer server purge  [APEXCORE-375] - Container killed because of Out of Sequence tuple error.  [APEXCORE-385] - Temp directories/files not always cleaned up when launching applications  [APEXCORE-391] - AsyncFSStorageAgent creates tmp directory unnecessarily  [APEXCORE-398] - Ack may not be delivered from buffer server to it's client", 
            "title": "Apache Apex Bug Fixes"
        }, 
        {
            "location": "/release_notes/#version-320", 
            "text": "", 
            "title": "Version 3.2.0"
        }, 
        {
            "location": "/release_notes/#new-feature_4", 
            "text": "[SPOI-6351] - Add feature to REST API to get queue information from cluster", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_6", 
            "text": "[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest  [SPOI-5828] - Stackstraces should not be shown on errors  [SPOI-6641] - Implement \"forever\" bucket in DimensionComputation  [DTIN-40] - Observed unused variables in SplunkBytesInputOperator  [DTIN-69] - Move Query Operators implementation to newer one  [DTIN-50] - [dtIngest] add parameter \"parallel readers\"", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_3", 
            "text": "[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory  [SPOI-5216] - FileSplitter fails with ConcurrentModificationException  [SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'  [SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up  [SPOI-5823] - Downstream container falling behind when buffer spooling is enabled  [SPOI-5899] - Ability to retrieve schemas from an app package  [SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation  [SPOI-6098] - 'single run' doesnt work  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6236] - Add 1s aggregations to App Data Tracker  [SPOI-6298] - Dimensions Store Can Become Blocked  [SPOI-6383] - Sometimes expired query is still executed.  [SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console  [SPOI-6446] - Merge PojoEnrichment and TupleEnrichment  [SPOI-6505] - Exceptions from asm when uploading apps  [SPOI-6603] - Warning messages shown on console are not completely visible  [SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks  [SPOI-6846] - DT dashboard guide link is not working  [SPOI-6855] - Broken links observed on summary page while DT RTS configuration   [SPOI-6856] - Correct docs index.html links and title  [DTIN-51] - bandwidth option was removed during merge  [DTIN-101] - For message-based-input to message-based-output, compression   encryption options should not be visible on config UI  [DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum  [DTIN-112] - Message based input to FTP output fails with IOException while appending the data  [DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata  [DTIN-123] - All files are not getting copied when bandwidth option is specified  [DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning  [DTIN-126] - 'Compact files' option should be disabled for message based Input type  [DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file  [DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only  [DTIN-155] - Messages are dropped when bandwidth option is enabled  [DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError  [DTIN-161] - Message based input to S3N output fails with IOException while appending the data  [DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka  [DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled  [DTIN-165] - Data source names for dtingest should not contain 'null'  [DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream  [DTIN-176] - Parallel read is not working in case of FTP and S3 as input  [DTIN-186] - FileMerger failed with unable to merge file exception", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_6", 
            "text": "[SPOI-5173] - DAG validation: Attribute values Serializable  [SPOI-5403] - Ingestion Splunk integration  [SPOI-5801] - Make a MapR partner (datatorrent) sandbox  [SPOI-5958] - dtView Integration for ingestion metric visualization  [SPOI-6241] - Change in enterprise license  [SPOI-6439] - Run benchmark for 3.2.0 release  [SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies  [DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script  [DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator  [DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch  [DTIN-54] - Disabling the splunk from dtIngest script  [DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_4", 
            "text": "[SPOI-5369] - Integrate schema support in Cassandra Input/Output  [SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output  [SPOI-5819] - Bandwidth control for file based sources  [SPOI-5820] - Bandwidth control for message based sources  [SPOI-5959] - Metrics for compression, encryption  [SPOI-6337] - Expose bandwidth metrics  [SPOI-6441] - Change console home page to be welcome screen instead of operations summary", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-311", 
            "text": "", 
            "title": "Version 3.1.1"
        }, 
        {
            "location": "/release_notes/#improvement_7", 
            "text": "[SPOI-5828] - Stackstraces should not be shown on errors", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_4", 
            "text": "[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock  [SPOI-6032] - App Builder should not show property from super class  [SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6090] - Ingestion: Error decrypting files for message based sources  [SPOI-6147] - Launch issue with \"Starter Application Pack\"  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6203] - -ve Memory reported for Application Master  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager  [SPOI-6304] - Fix netlet dependency  [SPOI-6313] - Work Around APEX-129  [SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_7", 
            "text": "[SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-6148] - Update website with release 3.1  [SPOI-6241] - Change in enterprise license", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-310_1", 
            "text": "", 
            "title": "Version 3.1.0"
        }, 
        {
            "location": "/release_notes/#new-feature_5", 
            "text": "[SPOI-4670] - Enable message schema management in App Builder  [SPOI-5844] - Retrieve older versions of schemas", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_8", 
            "text": "[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_5", 
            "text": "[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.  [SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5743] - App package import also needs to do the typegraph stuff as upload  [SPOI-5831] - Add getter for properties for Twitter Demo  [SPOI-5833] - Schema class not loaded during validation  [SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts  [SPOI-5857] - Gateway has trouble getting to STRAM until after restart  [SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml  [SPOI-5946] - Port compatibility when port require schemas  [SPOI-6002] - AppBuilder fails to load operator due to NullPointerException", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_8", 
            "text": "[SPOI-5307] - Mocha based tests for Gateway API calls  [SPOI-5749] - Certify MapR sandbox  [SPOI-5750] - Create Splunk forwarder operators  [SPOI-5751] - Create splunk forwarder input operator  [SPOI-5752] - Create splunk forwarder output operator  [SPOI-5753] - Create splunk forwarder configuration specification  [SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file  [SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-5756] - Test license expiry on sandbox as part of sandbox testing  [SPOI-5764] - Gateway to show better error messages on all 500 errors  [SPOI-5803] - Support of custom aggregation for ADT  [SPOI-5893] - Changes for retrieving container and operator history information", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_5", 
            "text": "[SPOI-4946] - Handle new @useSchema and @description property metadata in UI  [SPOI-4980] - Schema Management in the UI  [SPOI-5505] - handle Object-to-Object port compatibility with and without schema  [SPOI-5506] - handle Object-to-Pojo port compat with schema  [SPOI-5513] - handle port compatibility with generic type ports  [SPOI-5747] - Automatically generate new eval license when sandbox starts up", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-300", 
            "text": "", 
            "title": "Version 3.0.0"
        }, 
        {
            "location": "/release_notes/#sub-task_6", 
            "text": "[SPOI-1901] - Dynamic property changes lost on AM restart   [SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package  [SPOI-4891] - Example for schema meta data and property doclet tag  [SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks  [SPOI-4972] - Design Schema API for managing schemas  [SPOI-4973] - Create a Schema resource which will contain schema calls  [SPOI-4987] - Generate pojo class using schema provided in json  [SPOI-4999] - Implement the rest call to save schema on the backend  [SPOI-5211] - Support custom visualization link  [SPOI-5237] - Type of attributes are wrong when it is an inner class or enum  [SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp  [SPOI-5361] - Button for dt.phoneHome.enable property on system config page  [SPOI-5527] - Implement Queue Option", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_6", 
            "text": "[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs  [SPOI-4633] - Resolve type variable across the type hierarchy   [SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times   [SPOI-4884] - Intermittent failure for CustomMetricTest  [SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.  [SPOI-4941] - For kafka operator in app builder certain properties need to be set twice  [SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically  [SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway  [SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency  [SPOI-5323] - Malhar operators are not packaged with distribution for 3.0  [SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page  [SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon  [SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes  [SPOI-5385] - No error message is returned when DTCli gets an NPE  [SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments  [SPOI-5433] - Asm code not working for jdk 1.8  [SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout  [SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle  [SPOI-5484] - Ingestion FTP as output does not work with vsftpd  [SPOI-5497] - Could not open pi demo in 3.0.0 RC2  [SPOI-5498] - Typegraph exception with 3.0.0 RC2  [SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404  [SPOI-5504] - Make temporary file names unique to avoid lease expiry  [SPOI-5530] - Failed to edit pidemo JSON based application  [SPOI-5536] - App jar is missing from typegraph  [SPOI-5552] - Non-concrete classes being returned with assignableClasses call  [SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table  [SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException   [SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data  [SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5583] - DFS root directory check is failing on MapR cluster  [SPOI-5593] - App Builder search tooltip  [SPOI-5597] - Bad log level WARNING in UI  [SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal  [SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo  [SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators  [SPOI-5627] - error message from install script from 3.0.0-RC4   [SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator  [SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK   [SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group  [SPOI-5636] - dtcp is not getting packaged in installation (RC4)  [SPOI-5637] - Allatori Configuration errors in ingestion pom.xml  [SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out  [SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.  [SPOI-5644] - isChar validation should strip \\u000 character  [SPOI-5646] - Megh demos is failing release build  [SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined  [SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode  [SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user  [SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids  [SPOI-5661] - Set default store in HDHTReader  [SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo  [SPOI-5664] - Omit Aggregator Registry From UI  [SPOI-5672] - App Data Tracker Compliation is failing  [SPOI-5673] - Unable to launch ingestion app with JMS as input source   [SPOI-5675] - Kafka keys population error  [SPOI-5676] - Fix API doc generation  [SPOI-5685] - Complete App Builder category and property name fixes  [SPOI-5688] - Correct interactive demo tutorials available in the Learn section  [SPOI-5689] - Installer produces error message about removing docs directory  [SPOI-5690] - Images missing from lean section tutorials after installing release build  [SPOI-5693] - Console does not upload default DT application packages  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5704] - App package references in docs  [SPOI-5705] - Build version incorrect  [SPOI-5706] - api and user docs not viewable through gateway  [SPOI-5707] - Packaging utilities jar in Ingestion  [SPOI-5708] - Update netlet dependency to release for 3.0  [SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_9", 
            "text": "[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application  [SPOI-4889] - Make BucketIdTagger fault tolerant  [SPOI-5320] - License URL should open in new window   [SPOI-5494] - Support \"queue\" query parameters from launch app modal  [SPOI-5535] - Console should validate duplicate app name in the same app package  [SPOI-5548] - Check compaction keys from UI  [SPOI-5603] - Add syntax highlighting to Markdown code blocks  [SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default  [SPOI-5623] - License upgrade link should open in separate tab  [SPOI-5633] - Improve markdown content display styles  [SPOI-5657] - Redirect to welcome screen of the Learn section after install  [SPOI-5694] - Ingestion default app-config should populate meaningful defaults", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_6", 
            "text": "[SPOI-4672] - Support for secure HA environments  [SPOI-5288] - Add \"launch\" button to each item in the list of app packages  [SPOI-5304] - Support ingestion install mode  [SPOI-5313] - Obfuscate ADT in the distribution build  [SPOI-5561] - Markdown documentation support in the Console", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_9", 
            "text": "[SPOI-4228] - The location of App Data Tracker from installation and from devel mode  [SPOI-4658] - App Data Tracker Technical Doc  [SPOI-4879] - Hide operators that have no input ports from App builder  [SPOI-4983] - Installer for Ingestion app  [SPOI-4984] - Obfuscate ingestion jar  [SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core  [SPOI-5032] - Review comparison for appDataFramework pull request to the master  [SPOI-5050] - Licensing-related changes to the UI for 3.0  [SPOI-5255] - Support enhancements to uiTypes  [SPOI-5314] - Include obfuscated App Data Tracker in the install bundle  [SPOI-5370] - Megh obfuscation  [SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator  [SPOI-5555] - [dtcp] set default app package for ingestion  [SPOI-5574] - Update installer README  [SPOI-5608] - Docs for backward compatibility in 3.0   [SPOI-5610] - Add testing to Markdown support models, pages, directives  [SPOI-5621] - Include Ingestion utilities in DT installer  [SPOI-5624] - [Ingestion] Hide message output destination when input is file source  [SPOI-5635] - Changing product name to dtIngest  [SPOI-5638] - Post ingestion packge to central DT maven repository  [SPOI-5639] - App Data Tracker Release Job  [SPOI-5645] - support \"short\" primitive type in app builder  [SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_7", 
            "text": "[MLHR-1726] - Bug in PojoUtils  [MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives  [MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing  [MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application  [MLHR-1789] - Complete Category and property name fixes", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_10", 
            "text": "[MLHR-1725] - Add Support for Setters to PojoUtils  [MLHR-1757] - change scope of dt-engine in maven build", 
            "title": "Improvement"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary\n\n\n\n\n\n\n\n\nTerms\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAffinity\n\n\nThe relationship that occur due to the communication between components of Apex applications (Containers) even if they co-exist or not within a node in a cluster.\n\n\n\n\n\n\nApache\n\n\nPrefix to Apache projects. Refers to Apache Software Foundation (ASF).\n\n\n\n\n\n\nApache Apex\n\n\nApache Apex\nis a fast, big data engine that can run both streaming as well as batch jobs. Apex processes big data in-motion such that is highly scalable, highly performant, fault tolerant, stateful, secure, distributed, and easily operable.\n\n\n\n\n\n\nApache Apex Core\n\n\nApache Apex Core is a unified platform for big data streaming and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. It is usually referred as \nApex\n, \nApex-Core\n, or \nApex Platform\n.\n\n\n\n\n\n\nApache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apex platform to build fast big data applications. The operators in Malhar implement Apex API.\n\n\n\n\n\n\nApache Hadoop\n\n\nApache Hadoop \n is a big data framework that supports processing of large data sets in a distributed environment. It consists of two logical grids, namely compute grid (managed by YARN), and storage grid (HDFS).\n\n\n\n\n\n\nApache Kafka\n\n\nCommonly used message bus in big data ecosystem.\n\n\n\n\n\n\nApoxi\n\n\nApoxi\u2122 framework binds together application building blocks (data services) to create optimized, pre-built apps and integrates independent apps to allow them to operate as one.\n\n\n\n\n\n\nApp Master / Application Master\n\n\nApp Master/ \nApplication Master\n  is the monitoring process for applications responsible for negotiating resources with the Resource Manager and interacting with Node Managers to get the allocated containers started. Application Master runs within a container and manages the resources of the application.\n\n\n\n\n\n\nAppFactory\n\n\nAppFactory\n is a collection of pre-built micro data services and applications. In the AppFactory there are many applications for various use cases to help jumpstart the development effort. The aim is to provide 80% to 90% of common parts and thereby reduce TTV and TCO.\n\n\n\n\n\n\nApplication\n\n\nIn DataTorrent, Application is a collection of micro data services that are interconnected using Apoxi to achieve a business outcome.\n\n\n\n\n\n\nApplication Configuration\n\n\nApplication Configurations are created for applications using properties that override and supplement the ones that are already set in the original Application.\n\n\n\n\n\n\nApplication Metrics\n\n\nData for operational insights into the application. This data conveys how the application is performing and provides valuable insights for the business analyst and data engineers.\n\n\n\n\n\n\nApplication Package\n\n\nAn application package is a collection or bundle of related applications. It contains all the necessary files to launch and operate the bundled applications.\n\n\n\n\n\n\nApplication Templates\n\n\nThese are pre-built templates that are available at AppFactory, which can speed up the time to production.\n\n\n\n\n\n\nApplication Window Count\n\n\nNumber of streaming windows taken by an operator to complete one unit of work.\n\n\n\n\n\n\nCEP\n\n\nComplex Event Processing. An industry term to represent processing events from multiple sources to identify patterns and get to a business outcome.\n\n\n\n\n\n\nCloud Agnosticism\n\n\nAbility to run the same application or micro-data service on different cloud with minimal to no changes. This also refers to DataTorrent\ns value prop to guarantee cloud agnosticism and help enterprises avoid cloud locking.\n\n\n\n\n\n\nCloud computing\n\n\nCloud computing is an information technology (IT) paradigm that enables ubiquitous access to shared pools of configurable system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet. Cloud computing relies on sharing of resources to achieve coherence and economy of scale, similar to a utility.A public cloud collectively refers to Amazon AWS, Microsoft Azure, and Google Cloud. A private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.\n\n\n\n\n\n\nContainer\n\n\nRepresents a resource (memory) on a single node in a cluster. It is scheduled by the Resource Manager and supervised by the Node Manager\n\n\n\n\n\n\nControl Tuple\n\n\nSmallest unit of control data that moves in the Apex application\n\n\n\n\n\n\nCritical Path\n\n\nThe path in a DAG that has maximum latency.\n\n\n\n\n\n\nDAG (Directed Acyclic Graph)\n\n\nA representation of unidirectional data flow. Within an Apex application a DAG represents unidirectional functional code (operators) connected through message bus (streams).\n\n\n\n\n\n\nData Tuple\n\n\nSmallest unit of application data that moves in the application processes between operators. A data tuple is atomic in nature.\n\n\n\n\n\n\nDataTorrent Platform\n\n\nSee DataTorrent RTS\n\n\n\n\n\n\nDataTorrent RTS\n\n\nDataTorrent\ns Real-time Streaming (RTS) platform provides all the functionality a business needs to develop and deliver best-in-class, fast data applications by integrating best-of-breed open source technology innovation.  RTS tightly binds scalable, fault tolerant stream processing with Apache Apex, online analytical processing with Druid, a rules engine for complex event processing based on Drools, and support for running real-time machine scoring based on Python, SparkML, and PMML.   The RTS platform supports popular choices for long-term data persistence including HDFS, S3, and others, as well as supporting the latest Hadoop distributions from Cloudera, Hortonworks, and MAPR running on-premises or in the cloud.RTS is used for both batch and streaming applications.\n\n\n\n\n\n\nDev Grid (Cluster)\n\n\nA grid is used to develop big data application and test it. This grid is characterized by looser SLA \n frequent launches.\n\n\n\n\n\n\nDistributed Architecture/Infrastructure\n\n\nAn architecture that leverages multiple physical servers. This is a basic concept behind big data.\n\n\n\n\n\n\nDocker\n\n\nSoftware that gives containers, supported by Docker Inc. It provides a layer of virtualization over Unix \n Windows. \nContainer\n.\n\n\n\n\n\n\nDrools\n\n\nDrools is a rules engine which is used to enable customers to set rules on data to achieve business outcomes.\n\n\n\n\n\n\nDT Gateway\n\n\nThis is the main component of DataTorrent RTS which is accessible through \ndtManage\n. It is a Java-based multi-threaded web server that allows you to easily access information and perform various operations on DataTorrent RTS. dtGateway constantly communicates with all the running RTS App Masters (StrAM), as well as the Node Managers and the Resource Manager in the Hadoop cluster, to gather all the information and to perform all the operations. It can run on any node in a  Hadoop cluster or any other node that can access the Hadoop nodes, and is installed as a system service automatically by the RTS installer.\n\n\n\n\n\n\nDT Malhar\n\n\nA private copy of Apache Apex Malhar code on which DataTorrent develop features.\n\n\n\n\n\n\ndtConsole\n\n\nCollection of UI pages (dtManage, dtDashboard, dtAssemble) that is served by DT Gateway.\n\n\n\n\n\n\ndtDashboard\n\n\ndtDashboard\n is the visualization component of DataTorrent RTS and is seamlessly integrated with the metrics framework.\n\n\n\n\n\n\ndtManage\n\n\nThe web based interface to install/upload/import, configure, manage, and monitor Apex applications running in a Hadoop cluster.\n\n\n\n\n\n\nEngines\n\n\nEngines are the frameworks that are used to deliver micro data services such as Apex, Druid, and Drools.\n\n\n\n\n\n\nExecution Plan\n\n\nAnnotation or elaboration of physical plan by adding data where the operators are exactly running. This data includes the nodes, YARN containers etc.\n\n\n\n\n\n\nFast Data\n\n\nFast data is the application of big data analytics to smaller data sets in near-real or real-time to solve a problem or create business value. The goal of fast data is to quickly gather and mine structured and unstructured data for quick actions This is a design pattern where data movement is rapid. Usually refers to a setup where data is processed in memory and is immediately sent to the next processor. This could be a streaming or a batch job.\n\n\n\n\n\n\nGarbage Collection\n\n\nAs part of memory management, Java periodically collects unused memory objects for reuse. This process is called garbage collection.\n\n\n\n\n\n\nGarbage Collection Logging\n\n\nJava writes to logs when garbage collection triggers. Analysis of these logs is important from a performance perspective.\n\n\n\n\n\n\nHadoop Ecosystem\n\n\nHadoop ecosystem consists of many technologies that run on Hadoop. Most of the current technologies are based on MapReduce. Notable names are MapReduce, Pig, Hive, Oozie.\n\n\n\n\n\n\nHDFS\n\n\nThe Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop applications.It provides high-performance access to data across Hadoop clusters. Its a key tool for managing pools of big data and supporting big data analytics applications.\n\n\n\n\n\n\nHeartbeat\n\n\nContainers send periodic signals to application master. Heartbeats are used by application master to know that containers are alive and running properly. Containers can also send some other statistics to application master using the heartbeats.\n\n\n\n\n\n\nHybrid Grid\n\n\nAn application or a business process that runs on cloud as well as on-premise. Most common situation is when dev \n QA are run on cloud and production is run on-prem.\n\n\n\n\n\n\nInput Operator\n\n\nUsually an Input operator is the starting point of the DAG or the pipeline. Tuples flow from input to output. It is responsible for getting data from an external system.\n\n\n\n\n\n\nInput Port\n\n\nPort through which operator receives data from the output port of the upstream operator.\n\n\n\n\n\n\nJAR Artifact\n\n\nArtifacts can be used to provide libraries, rules, schemas, and custom code to applications via Application Configurations. Artifact JARs follow Apache Maven standards, which require groupId, artifactId, and version to be specified and can be uploaded manually and synchronized automatically from a Maven artifacts directory accessible by the Gateway.\n\n\n\n\n\n\nKerberos\n\n\nA network authentication protocol pioneered by MIT. This is the most common authentication protocol in big data and is supported by DT RTS.\n\n\n\n\n\n\nKubernetes\n\n\nAn open source container orchestration software backed by Google. In 2017 Amazon and Azure picked up kubernetes enabling it to be some day de-facto orchestration software for cloud.\n\n\n\n\n\n\nLog Aggregation\n\n\nThe process of aggregating distributed logs in one place. In a distributed application, the processes write logs to local hard disk. Thus logs of an application are distributed and hence it is very time consuming to debug a distributed application. Aggregating logs in one place saves time to debug.\n\n\n\n\n\n\nLogical Plan\n\n\nLogical representation of an Apex application, where the computational nodes are called \nOperators\n and the data-flow edges are called \nStreams\n.\n\n\n\n\n\n\nLoose Coupling\n\n\nAbility to loosely couple parts (operators or micro data services) to get them to serve a particular function as a whole. In this architecture the interaction is via a message bus. Loose coupling helps reduce time to value and maintenance cost by enabling parts of an application to be upgraded/modified without touching the rest.\n\n\n\n\n\n\nMessage Bus\n\n\nA logical infrastructure component that enables a publisher to publish data. Consumers, also called subscribers, subscribe to topics or queues. Message bus is a core concept behind loosely creating application(s) from loosely coupled micro data services. Bufferserver is an example of a message bus within an Apex application.\n\n\n\n\n\n\nMetrics\n\n\nOperational insights need metadata of the application or cluster to be available. These collection of numbers are expressed as \nmetrics\n. Metrics are of two types, namely application metrics that give operational insight into application data and system metrics that gives operational insights into the system data. For example,  application data is the number of phone calls successfully processed in a one hour period. System data is the system memory, tuples per second etc.\n\n\n\n\n\n\nMicro Data Service\n\n\nA collection of operators that provide a specific function. Usually this function by itself is not sufficient to get a complete business outcome.\n\n\n\n\n\n\nMulti Grid Application\n\n\nAn application that is created by connecting micro data services running on different grids.\n\n\n\n\n\n\nNode Manager\n\n\nNode manager handles individual Hadoop nodes in the compute grid. It oversees life-cycle management of the container resources in a node and monitors resource usage of individual containers on a node. It updates status check with the Resource Manager.\n\n\n\n\n\n\nOmni-Channel Payment Fraud  Prevention Application\n\n\nOmni-Channel Payment Fraud Prevention\n Application is a pre-built application which can be used to identify fraud in financial transactions. This application is designed to ingest, transform, analyze, act, and visualize data as soon as it is generated; thereby, preventing fraud transactions in real-time. This fraud prevention \nsolution\n processes each record through a series of business rules and triggers one or more actions.\n\n\n\n\n\n\nOnline Analytics Service (OAS)\n\n\nEngine for OLAP. Druid APIs are used in DT application which helps to deliver fast data analytics to our customers to enable them to achieve business outcomes.\n\n\n\n\n\n\nOn-Prem\n\n\nShort form for \nOn Premise\n which is within a customer\ns premise as opposed to in the cloud.\n\n\n\n\n\n\nOperators\n\n\nOperators are basic building blocks of an Apex application. An application may consist of one or more operators each of which define some logical operation to be done on the tuples arriving at the operator. These operators are connected together using streams.\n\n\n\n\n\n\nOutput Operator\n\n\nOutput operators are endpoints in an Apex application. It writes the data out to external system.\n\n\n\n\n\n\nOutput Port\n\n\nThe port through which an operator emits tuples to a downstream operator\ns input port via a stream.\n\n\n\n\n\n\nPartitioning\n\n\nPartitioning is a mechanism to eliminate bottlenecks in your application and increase throughput. If an operator is performing a resource intensive operation, it risks becoming a bottleneck as the rate of incoming tuples increases. One way to cope is to replicate the operator as many times as necessary so that the load is evenly distributed across the replicas, thus eliminating the bottleneck. This technique assumes that your cluster has adequate resources (CPU, memory and network bandwidth) to support all the replicas.\n\n\n\n\n\n\nPhysical Operator\n\n\nPhysical instance of an operator, which contains information such as the name of the container and the Hadoop node where operator instance is running.\n\n\n\n\n\n\nPhysical Plan\n\n\nLogical plan is transformed into a physical plan by applying partitioning attributes to the logical plan. This is a precursor to the creation of an execution plan.\n\n\n\n\n\n\nPMML\n\n\nPMML\n stands for \nPredictive Model Markup Language\n. PMML allows for different statistical and data mining tools to speak the same language. The structure of the models is described by an XML Schema. One or more mining models can be contained in a PMML document.\n\n\n\n\n\n\nPMML Scoring Operator\n\n\nDataTorrent RTS provides a scoring operator which takes XML file as input. This file contains the PMML model trained via any third-party tool and exported to PMML. The scoring operator currently supports Naive Bayes classification, SVM classification and K-Means clustering algorithms.\n\n\n\n\n\n\nPort\n\n\nEach operator can have port/s on which it can receive or emit data to and from other operators. Input operator does not have input port and Output operator does not have output port.\n\n\n\n\n\n\nProduction Grid (cluster)\n\n\nA grid used to run production applications or micro data services. This application(s) constitute the product from which business outcomes are achieved. Production grids have very tight SLA. Direct launches to production grid are very infrequent.\n\n\n\n\n\n\nQA Grid (cluster)\n\n\nA grid used to certify applications or micro data services for production. This grid is characterized by a looser SLA. Launches are less frequent as they mimic launch to production. QA launch is usually done as a precursor to production launch.\n\n\n\n\n\n\nRecovery Window ID\n\n\nIdentifier of the last computational window at which the operator state was checkpointed into HDFS.\n\n\n\n\n\n\nResource Manager\n\n\nMain component of YARN that allocates and arbitrates the resources such as CPU, Memory and Network. Manages all the distributed resources in the cluster. Resource manager works with per-node Node Managers (NMs) and per-application Application Masters (AMs)\n\n\n\n\n\n\nSchema\n\n\nA method used to give structure to data. Big data applications almost always collect unstructured data. Assigning schemas are among the first tasks needed in the process of converting unstructured data to structured data.\n\n\n\n\n\n\nSchema Repository\n\n\nA repository to manage and maintain all schemas being used by the Enterprise. A schema repository may be shared by different applications.\n\n\n\n\n\n\nSecurity\n\n\nAbility to secure data. There are two main parts of security, namely (a) authenticating users, (b) specifying and enforcing authorization of access.\n\n\n\n\n\n\nStrAM\n\n\nStreaming Application Manager is the first process that is activated upon application launch. It orchestrates the deployment, management, and monitoring of the Apex applications throughout their lifecycle.\n\n\n\n\n\n\nStream\n\n\nA stream consists of data and control tuples that flow from an output port of an operator to an input port of another. It is a connector (edge) abstraction and a fundamental building block of the platform.\n\n\n\n\n\n\nStream Modes/Stream Locality\n\n\nStreams have four modes, namely in-line, in-node, in-rack, and other. They are defined as follows: \nTHREAD_LOCAL\n: In the same thread, uses thread stack (intra-thread). This mode can only be used for a downstream operator which has only one input port connected. This is also called in-line.  \nCONTAINER_LOCAL\n: In the same container (intra-process). This is also called in-container. \nNODE_LOCAL\n: In the same Hadoop node (inter processes, skips NIC). This is also called in-node.  \nRACK_LOCAL\n: On nodes in the same rack. This is also called in-rack. \nUnspecified\n: This could be anywhere within the cluster\n\n\n\n\n\n\nStreaming Application\n\n\nAn application that is run on a streaming engine.\n\n\n\n\n\n\nStreaming Container / Container\n\n\nA streaming container is a process that runs a part of the application business logic. It is a container deployed on a node in the grid. The part of business logic is implemented via an operator. Multiple operators connected together make up the complete application and hence there are multiple streaming containers in an application.\n\n\n\n\n\n\nSystem Metrics\n\n\nData for operational insights into the system performance of the application. This data conveys how the resources used by the application are performing. They provide valuable insights to DevOps and data engineers.\n\n\n\n\n\n\nTCO\n\n\nTotal cost of ownership of any project incurred by the customer. In DataTorrent\ns case it relates to total cost of ownership of a fast big data analytics application. This cost includes cost of development, testing, launching, and equally (or more importantly) cost of ongoing operations. Cost includes salary, hardware, etc. aka all costs are to be accounted for.\n\n\n\n\n\n\nTTV\n\n\nTime to value of any project. This is the time it takes for a customer to successfully go from inception to getting value out of a project.\n\n\n\n\n\n\nYARN\n\n\nApache Hadoop YARN \n(Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.\n\n\n\n\n\n\nWindow ID\n\n\nThe sequentially increasing identifier of a specific computation period within an Apex engine.", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary", 
            "text": "Terms  Description          Affinity  The relationship that occur due to the communication between components of Apex applications (Containers) even if they co-exist or not within a node in a cluster.    Apache  Prefix to Apache projects. Refers to Apache Software Foundation (ASF).    Apache Apex  Apache Apex is a fast, big data engine that can run both streaming as well as batch jobs. Apex processes big data in-motion such that is highly scalable, highly performant, fault tolerant, stateful, secure, distributed, and easily operable.    Apache Apex Core  Apache Apex Core is a unified platform for big data streaming and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. It is usually referred as  Apex ,  Apex-Core , or  Apex Platform .    Apache Apex Malhar  Apache Apex Malhar is an open source operator and codec library that can be used with the Apex platform to build fast big data applications. The operators in Malhar implement Apex API.    Apache Hadoop  Apache Hadoop   is a big data framework that supports processing of large data sets in a distributed environment. It consists of two logical grids, namely compute grid (managed by YARN), and storage grid (HDFS).    Apache Kafka  Commonly used message bus in big data ecosystem.    Apoxi  Apoxi\u2122 framework binds together application building blocks (data services) to create optimized, pre-built apps and integrates independent apps to allow them to operate as one.    App Master / Application Master  App Master/  Application Master   is the monitoring process for applications responsible for negotiating resources with the Resource Manager and interacting with Node Managers to get the allocated containers started. Application Master runs within a container and manages the resources of the application.    AppFactory  AppFactory  is a collection of pre-built micro data services and applications. In the AppFactory there are many applications for various use cases to help jumpstart the development effort. The aim is to provide 80% to 90% of common parts and thereby reduce TTV and TCO.    Application  In DataTorrent, Application is a collection of micro data services that are interconnected using Apoxi to achieve a business outcome.    Application Configuration  Application Configurations are created for applications using properties that override and supplement the ones that are already set in the original Application.    Application Metrics  Data for operational insights into the application. This data conveys how the application is performing and provides valuable insights for the business analyst and data engineers.    Application Package  An application package is a collection or bundle of related applications. It contains all the necessary files to launch and operate the bundled applications.    Application Templates  These are pre-built templates that are available at AppFactory, which can speed up the time to production.    Application Window Count  Number of streaming windows taken by an operator to complete one unit of work.    CEP  Complex Event Processing. An industry term to represent processing events from multiple sources to identify patterns and get to a business outcome.    Cloud Agnosticism  Ability to run the same application or micro-data service on different cloud with minimal to no changes. This also refers to DataTorrent s value prop to guarantee cloud agnosticism and help enterprises avoid cloud locking.    Cloud computing  Cloud computing is an information technology (IT) paradigm that enables ubiquitous access to shared pools of configurable system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet. Cloud computing relies on sharing of resources to achieve coherence and economy of scale, similar to a utility.A public cloud collectively refers to Amazon AWS, Microsoft Azure, and Google Cloud. A private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.    Container  Represents a resource (memory) on a single node in a cluster. It is scheduled by the Resource Manager and supervised by the Node Manager    Control Tuple  Smallest unit of control data that moves in the Apex application    Critical Path  The path in a DAG that has maximum latency.    DAG (Directed Acyclic Graph)  A representation of unidirectional data flow. Within an Apex application a DAG represents unidirectional functional code (operators) connected through message bus (streams).    Data Tuple  Smallest unit of application data that moves in the application processes between operators. A data tuple is atomic in nature.    DataTorrent Platform  See DataTorrent RTS    DataTorrent RTS  DataTorrent s Real-time Streaming (RTS) platform provides all the functionality a business needs to develop and deliver best-in-class, fast data applications by integrating best-of-breed open source technology innovation.  RTS tightly binds scalable, fault tolerant stream processing with Apache Apex, online analytical processing with Druid, a rules engine for complex event processing based on Drools, and support for running real-time machine scoring based on Python, SparkML, and PMML.   The RTS platform supports popular choices for long-term data persistence including HDFS, S3, and others, as well as supporting the latest Hadoop distributions from Cloudera, Hortonworks, and MAPR running on-premises or in the cloud.RTS is used for both batch and streaming applications.    Dev Grid (Cluster)  A grid is used to develop big data application and test it. This grid is characterized by looser SLA   frequent launches.    Distributed Architecture/Infrastructure  An architecture that leverages multiple physical servers. This is a basic concept behind big data.    Docker  Software that gives containers, supported by Docker Inc. It provides a layer of virtualization over Unix   Windows.  Container .    Drools  Drools is a rules engine which is used to enable customers to set rules on data to achieve business outcomes.    DT Gateway  This is the main component of DataTorrent RTS which is accessible through  dtManage . It is a Java-based multi-threaded web server that allows you to easily access information and perform various operations on DataTorrent RTS. dtGateway constantly communicates with all the running RTS App Masters (StrAM), as well as the Node Managers and the Resource Manager in the Hadoop cluster, to gather all the information and to perform all the operations. It can run on any node in a  Hadoop cluster or any other node that can access the Hadoop nodes, and is installed as a system service automatically by the RTS installer.    DT Malhar  A private copy of Apache Apex Malhar code on which DataTorrent develop features.    dtConsole  Collection of UI pages (dtManage, dtDashboard, dtAssemble) that is served by DT Gateway.    dtDashboard  dtDashboard  is the visualization component of DataTorrent RTS and is seamlessly integrated with the metrics framework.    dtManage  The web based interface to install/upload/import, configure, manage, and monitor Apex applications running in a Hadoop cluster.    Engines  Engines are the frameworks that are used to deliver micro data services such as Apex, Druid, and Drools.    Execution Plan  Annotation or elaboration of physical plan by adding data where the operators are exactly running. This data includes the nodes, YARN containers etc.    Fast Data  Fast data is the application of big data analytics to smaller data sets in near-real or real-time to solve a problem or create business value. The goal of fast data is to quickly gather and mine structured and unstructured data for quick actions This is a design pattern where data movement is rapid. Usually refers to a setup where data is processed in memory and is immediately sent to the next processor. This could be a streaming or a batch job.    Garbage Collection  As part of memory management, Java periodically collects unused memory objects for reuse. This process is called garbage collection.    Garbage Collection Logging  Java writes to logs when garbage collection triggers. Analysis of these logs is important from a performance perspective.    Hadoop Ecosystem  Hadoop ecosystem consists of many technologies that run on Hadoop. Most of the current technologies are based on MapReduce. Notable names are MapReduce, Pig, Hive, Oozie.    HDFS  The Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop applications.It provides high-performance access to data across Hadoop clusters. Its a key tool for managing pools of big data and supporting big data analytics applications.    Heartbeat  Containers send periodic signals to application master. Heartbeats are used by application master to know that containers are alive and running properly. Containers can also send some other statistics to application master using the heartbeats.    Hybrid Grid  An application or a business process that runs on cloud as well as on-premise. Most common situation is when dev   QA are run on cloud and production is run on-prem.    Input Operator  Usually an Input operator is the starting point of the DAG or the pipeline. Tuples flow from input to output. It is responsible for getting data from an external system.    Input Port  Port through which operator receives data from the output port of the upstream operator.    JAR Artifact  Artifacts can be used to provide libraries, rules, schemas, and custom code to applications via Application Configurations. Artifact JARs follow Apache Maven standards, which require groupId, artifactId, and version to be specified and can be uploaded manually and synchronized automatically from a Maven artifacts directory accessible by the Gateway.    Kerberos  A network authentication protocol pioneered by MIT. This is the most common authentication protocol in big data and is supported by DT RTS.    Kubernetes  An open source container orchestration software backed by Google. In 2017 Amazon and Azure picked up kubernetes enabling it to be some day de-facto orchestration software for cloud.    Log Aggregation  The process of aggregating distributed logs in one place. In a distributed application, the processes write logs to local hard disk. Thus logs of an application are distributed and hence it is very time consuming to debug a distributed application. Aggregating logs in one place saves time to debug.    Logical Plan  Logical representation of an Apex application, where the computational nodes are called  Operators  and the data-flow edges are called  Streams .    Loose Coupling  Ability to loosely couple parts (operators or micro data services) to get them to serve a particular function as a whole. In this architecture the interaction is via a message bus. Loose coupling helps reduce time to value and maintenance cost by enabling parts of an application to be upgraded/modified without touching the rest.    Message Bus  A logical infrastructure component that enables a publisher to publish data. Consumers, also called subscribers, subscribe to topics or queues. Message bus is a core concept behind loosely creating application(s) from loosely coupled micro data services. Bufferserver is an example of a message bus within an Apex application.    Metrics  Operational insights need metadata of the application or cluster to be available. These collection of numbers are expressed as  metrics . Metrics are of two types, namely application metrics that give operational insight into application data and system metrics that gives operational insights into the system data. For example,  application data is the number of phone calls successfully processed in a one hour period. System data is the system memory, tuples per second etc.    Micro Data Service  A collection of operators that provide a specific function. Usually this function by itself is not sufficient to get a complete business outcome.    Multi Grid Application  An application that is created by connecting micro data services running on different grids.    Node Manager  Node manager handles individual Hadoop nodes in the compute grid. It oversees life-cycle management of the container resources in a node and monitors resource usage of individual containers on a node. It updates status check with the Resource Manager.    Omni-Channel Payment Fraud  Prevention Application  Omni-Channel Payment Fraud Prevention  Application is a pre-built application which can be used to identify fraud in financial transactions. This application is designed to ingest, transform, analyze, act, and visualize data as soon as it is generated; thereby, preventing fraud transactions in real-time. This fraud prevention  solution  processes each record through a series of business rules and triggers one or more actions.    Online Analytics Service (OAS)  Engine for OLAP. Druid APIs are used in DT application which helps to deliver fast data analytics to our customers to enable them to achieve business outcomes.    On-Prem  Short form for  On Premise  which is within a customer s premise as opposed to in the cloud.    Operators  Operators are basic building blocks of an Apex application. An application may consist of one or more operators each of which define some logical operation to be done on the tuples arriving at the operator. These operators are connected together using streams.    Output Operator  Output operators are endpoints in an Apex application. It writes the data out to external system.    Output Port  The port through which an operator emits tuples to a downstream operator s input port via a stream.    Partitioning  Partitioning is a mechanism to eliminate bottlenecks in your application and increase throughput. If an operator is performing a resource intensive operation, it risks becoming a bottleneck as the rate of incoming tuples increases. One way to cope is to replicate the operator as many times as necessary so that the load is evenly distributed across the replicas, thus eliminating the bottleneck. This technique assumes that your cluster has adequate resources (CPU, memory and network bandwidth) to support all the replicas.    Physical Operator  Physical instance of an operator, which contains information such as the name of the container and the Hadoop node where operator instance is running.    Physical Plan  Logical plan is transformed into a physical plan by applying partitioning attributes to the logical plan. This is a precursor to the creation of an execution plan.    PMML  PMML  stands for  Predictive Model Markup Language . PMML allows for different statistical and data mining tools to speak the same language. The structure of the models is described by an XML Schema. One or more mining models can be contained in a PMML document.    PMML Scoring Operator  DataTorrent RTS provides a scoring operator which takes XML file as input. This file contains the PMML model trained via any third-party tool and exported to PMML. The scoring operator currently supports Naive Bayes classification, SVM classification and K-Means clustering algorithms.    Port  Each operator can have port/s on which it can receive or emit data to and from other operators. Input operator does not have input port and Output operator does not have output port.    Production Grid (cluster)  A grid used to run production applications or micro data services. This application(s) constitute the product from which business outcomes are achieved. Production grids have very tight SLA. Direct launches to production grid are very infrequent.    QA Grid (cluster)  A grid used to certify applications or micro data services for production. This grid is characterized by a looser SLA. Launches are less frequent as they mimic launch to production. QA launch is usually done as a precursor to production launch.    Recovery Window ID  Identifier of the last computational window at which the operator state was checkpointed into HDFS.    Resource Manager  Main component of YARN that allocates and arbitrates the resources such as CPU, Memory and Network. Manages all the distributed resources in the cluster. Resource manager works with per-node Node Managers (NMs) and per-application Application Masters (AMs)    Schema  A method used to give structure to data. Big data applications almost always collect unstructured data. Assigning schemas are among the first tasks needed in the process of converting unstructured data to structured data.    Schema Repository  A repository to manage and maintain all schemas being used by the Enterprise. A schema repository may be shared by different applications.    Security  Ability to secure data. There are two main parts of security, namely (a) authenticating users, (b) specifying and enforcing authorization of access.    StrAM  Streaming Application Manager is the first process that is activated upon application launch. It orchestrates the deployment, management, and monitoring of the Apex applications throughout their lifecycle.    Stream  A stream consists of data and control tuples that flow from an output port of an operator to an input port of another. It is a connector (edge) abstraction and a fundamental building block of the platform.    Stream Modes/Stream Locality  Streams have four modes, namely in-line, in-node, in-rack, and other. They are defined as follows:  THREAD_LOCAL : In the same thread, uses thread stack (intra-thread). This mode can only be used for a downstream operator which has only one input port connected. This is also called in-line.   CONTAINER_LOCAL : In the same container (intra-process). This is also called in-container.  NODE_LOCAL : In the same Hadoop node (inter processes, skips NIC). This is also called in-node.   RACK_LOCAL : On nodes in the same rack. This is also called in-rack.  Unspecified : This could be anywhere within the cluster    Streaming Application  An application that is run on a streaming engine.    Streaming Container / Container  A streaming container is a process that runs a part of the application business logic. It is a container deployed on a node in the grid. The part of business logic is implemented via an operator. Multiple operators connected together make up the complete application and hence there are multiple streaming containers in an application.    System Metrics  Data for operational insights into the system performance of the application. This data conveys how the resources used by the application are performing. They provide valuable insights to DevOps and data engineers.    TCO  Total cost of ownership of any project incurred by the customer. In DataTorrent s case it relates to total cost of ownership of a fast big data analytics application. This cost includes cost of development, testing, launching, and equally (or more importantly) cost of ongoing operations. Cost includes salary, hardware, etc. aka all costs are to be accounted for.    TTV  Time to value of any project. This is the time it takes for a customer to successfully go from inception to getting value out of a project.    YARN  Apache Hadoop YARN  (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.    Window ID  The sequentially increasing identifier of a specific computation period within an Apex engine.", 
            "title": "Glossary"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Apache Apex\n\n\nFor more information, please go to \nApache Apex\n.  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.\n\n\nDataTorrent RTS\n\n\n\n\nApex Comparison\n\n\nArchitecture\n\n\nFeatures Overview\n\n\nBlogs\n\n\nFeatured Resources\n\n\nWebinars\n\n\nRelease Notes\n\n\n\n\nDocumentation Archive\n\n\n\n\nDataTorrent RTS 3.1.0\n\n\nDataTorrent RTS 3.0.0\n\n\nDataTorrent RTS 2.0.1\n\n\nDataTorrent RTS 2.0.0\n\n\nDataTorrent RTS 1.0.4\n\n\nWebsite Documentation Links", 
            "title": "Resources"
        }, 
        {
            "location": "/additional_docs/#apache-apex", 
            "text": "For more information, please go to  Apache Apex .  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/additional_docs/#datatorrent-rts", 
            "text": "Apex Comparison  Architecture  Features Overview  Blogs  Featured Resources  Webinars  Release Notes", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/#documentation-archive", 
            "text": "DataTorrent RTS 3.1.0  DataTorrent RTS 3.0.0  DataTorrent RTS 2.0.1  DataTorrent RTS 2.0.0  DataTorrent RTS 1.0.4  Website Documentation Links", 
            "title": "Documentation Archive"
        }
    ]
}